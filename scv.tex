\documentclass[12pt,openany]{book}

%FIXME: for PRINT run for lulu or kdp, search for %PRINT

%\usepackage{pdf14}

\usepackage[shortlabels,inline]{enumitem}
\usepackage{ifpdf}
\usepackage[shortalphabetic,msc-links]{amsrefs}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{titlesec}
\usepackage{import}
\usepackage{vogtwidebar}
\usepackage{nicefrac}
\usepackage{mathdots}
\usepackage{microtype}
\usepackage{cancel}
\usepackage{framed}
\usepackage{import}
\usepackage{varioref}
\usepackage{faktor}
\usepackage{tabto}

\usepackage{perpage}

\usepackage{tikz}
\usetikzlibrary{cd}
\usepackage{rotating}

\usepackage{cellspace}
\usepackage[toc,nopostdot,sort=use,nomain,automake]{glossaries}

%Palatino
\usepackage[theoremfont]{newpxtext}
\usepackage[vvarbb]{newpxmath}
\linespread{1.05}
\usepackage[scr=boondoxo]{mathalfa} % but we want the nice fancy script fonts

\usepackage[T1]{fontenc}

%symmetric for web
\usepackage[inner=1.2in,outer=1.2in,top=1in,bottom=1in]{geometry}
%PRINT asymetric for book
%\usepackage[inner=1.4in,outer=1.0in,top=1in,bottom=1in]{geometry}

\usepackage[margin=10pt,font=small,labelfont=bf,labelsep=colon,singlelinecheck=false]{caption}

%PRINT
% (not for the coil version or any version other than crown quatro using the
%  ghostscript conversion)
%Now cut page size a bit.  I'll run it through ghostcript anyway
%to convert to the right size, but this is good for crown quatro
%conversion, don't use for the full letter size versions
%\addtolength{\paperwidth}{-0.25in}
%\addtolength{\paperheight}{-0.5in}
%\addtolength{\topmargin}{-0.13in}
%\addtolength{\oddsidemargin}{-0.125in}
%\addtolength{\evensidemargin}{-0.125in}


\usepackage{url}
\usepackage{imakeidx}
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref} % do NOT set [ocgcolorlinks] here!

%If you have an older tex installation you might need
%to comment out the next line:
%PRINT (COMMENT OUT FOR PRINT)
\usepackage[ocgcolorlinks]{ocgx2} %perhaps run without for lulu/kdp

\usepackage[all]{hypcap}

\usepackage{draftwatermark}
\SetWatermarkText{Draft of v4.0 as of \today. May change substantially!}
\SetWatermarkAngle{90}
\SetWatermarkHorCenter{0.5in}
\SetWatermarkColor[gray]{0.7}
\SetWatermarkScale{0.18}

\definecolor{gray75}{gray}{0.75}

\titleformat{\chapter}[hang]{\Huge\bfseries\filleft}%
{\thechapter\hspace*{10pt}{\textcolor{gray75}{$\backslash\!\!\backslash$}}}%
{10pt}%
{}
[\vspace{-1ex}\rule{0.5\textwidth}{0.5pt}\rule{0.5\textwidth}{1pt}]

\titleformat{\section}[hang]{\Large\bfseries}%
{\thesection\hspace*{10pt}{\textcolor{gray75}{$\backslash$}}}%
{10pt}%
{}%

\titleformat{\subsection}[hang]{\large\bfseries}%
{\thesubsection\hspace*{10pt}{\textcolor{gray75}{$\cdot$}}}%
{10pt}%
{}%

\assignpagestyle{\chapter}{empty}


% Footnotes should use symbols, not numbers.  Numbered footnotes are
% evil, not using footmisc as it conflicts with hyperref
%\usepackage[perpage,symbol*]{footmisc}

\usepackage{footnote}

% The [2] would not use star
%\MakePerPage[2]{footnote}
\MakePerPage{footnote}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}

% to avoid counter error on footnotes on first pass
\makeatletter
\gdef\@ctrerr{%
  \@latex@warning{Counter too large}}
\makeatother

% Enumitem extra penalties
\setlist[enumerate]{beginpenalty=100,midpenalty=-5}

% discourage pagebreak at end of display, put before \end{equation}
\newcommand{\avoidbreak}{\postdisplaypenalty=100}

\clubpenalty=500
\widowpenalty=500

%\overfullrule=10mm

% useful
\newcommand{\ignore}[1]{}

% analysis/geometry stuff
\newcommand{\ann}{\operatorname{ann}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Orb}{\operatorname{Orb}}
\newcommand{\hol}{\operatorname{hol}}
\newcommand{\aut}{\operatorname{aut}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\sing}{\operatorname{sing}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\Arg}{\operatorname{Arg}}
\newcommand{\Log}{\operatorname{Log}}

% reals
\newcommand{\esssup}{\operatorname{ess~sup}}
\newcommand{\essran}{\operatorname{essran}}
\newcommand{\innprod}[2]{\langle #1 | #2 \rangle}
\newcommand{\linnprod}[2]{\langle #1 , #2 \rangle}
\newcommand{\blinnprod}[2]{\bigl\langle #1 , #2 \bigr\rangle}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\Nul}{\operatorname{Nul}}
\newcommand{\Ran}{\operatorname{Ran}}
\newcommand{\sabs}[1]{\lvert {#1} \rvert}
\newcommand{\snorm}[1]{\lVert {#1} \rVert}
\newcommand{\babs}[1]{\bigl\lvert {#1} \bigr\rvert}
\newcommand{\bnorm}[1]{\bigl\lVert {#1} \bigr\rVert}
\newcommand{\Babs}[1]{\Bigl\lvert {#1} \Bigr\rvert}
\newcommand{\Bnorm}[1]{\Bigl\lVert {#1} \Bigr\rVert}
\newcommand{\bbabs}[1]{\biggl\lvert {#1} \biggr\rvert}
\newcommand{\bbnorm}[1]{\biggl\lVert {#1} \biggr\rVert}
\newcommand{\BBabs}[1]{\Biggl\lvert {#1} \Biggr\rvert}
\newcommand{\BBnorm}[1]{\Biggl\lVert {#1} \Biggr\rVert}
\newcommand{\abs}[1]{\left\lvert {#1} \right\rvert}
\newcommand{\norm}[1]{\left\lVert {#1} \right\rVert}

% sets (some)
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\D}{{\mathbb{D}}}
\newcommand{\F}{{\mathbb{F}}}

% consistent
\newcommand{\bB}{{\mathbb{B}}}
\newcommand{\bC}{{\mathbb{C}}}
\newcommand{\bR}{{\mathbb{R}}}
\newcommand{\bZ}{{\mathbb{Z}}}
\newcommand{\bN}{{\mathbb{N}}}
\newcommand{\bQ}{{\mathbb{Q}}}
\newcommand{\bD}{{\mathbb{D}}}
\newcommand{\bF}{{\mathbb{F}}}
\newcommand{\bH}{{\mathbb{H}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bP}{{\mathbb{P}}}
\newcommand{\bK}{{\mathbb{K}}}
\newcommand{\bV}{{\mathbb{V}}}
\newcommand{\CP}{{\mathbb{CP}}}
\newcommand{\RP}{{\mathbb{RP}}}
\newcommand{\HP}{{\mathbb{HP}}}
\newcommand{\OP}{{\mathbb{OP}}}
\newcommand{\sA}{{\mathscr{A}}}
\newcommand{\sB}{{\mathscr{B}}}
\newcommand{\sC}{{\mathscr{C}}}
\newcommand{\sF}{{\mathscr{F}}}
\newcommand{\sG}{{\mathscr{G}}}
\newcommand{\sH}{{\mathscr{H}}}
\newcommand{\sM}{{\mathscr{M}}}
\newcommand{\sO}{{\mathscr{O}}}
\newcommand{\sP}{{\mathscr{P}}}
\newcommand{\sQ}{{\mathscr{Q}}}
\newcommand{\sR}{{\mathscr{R}}}
\newcommand{\sS}{{\mathscr{S}}}
\newcommand{\sI}{{\mathscr{I}}}
\newcommand{\sL}{{\mathscr{L}}}
\newcommand{\sK}{{\mathscr{K}}}
\newcommand{\sU}{{\mathscr{U}}}
\newcommand{\sV}{{\mathscr{V}}}
\newcommand{\sX}{{\mathscr{X}}}
\newcommand{\sY}{{\mathscr{Y}}}
\newcommand{\sZ}{{\mathscr{Z}}}
\newcommand{\fS}{{\mathfrak{S}}}

\newcommand{\interior}{\operatorname{int}}

% Topo stuff
\newcommand{\id}{\textit{id}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Tor}{\operatorname{Tor}}
\newcommand{\Torsion}{\operatorname{Torsion}}
\newcommand{\Ext}{\operatorname{Ext}}
\newcommand{\Hom}{\operatorname{Hom}}

%extra thingies
%\newcommand{\mapsfrom}{\ensuremath{\text{\reflectbox{$\mapsto$}}}}
\newcommand{\from}{\ensuremath{\leftarrow}}
\newcommand{\dhat}[1]{\hat{\hat{#1}}}

\definecolor{mypersianblue}{rgb}{0.11, 0.22, 0.73}

\hypersetup{
    pdfborderstyle={/S/U/W 0.5}, %this just in case ocg isn't there
    %PRINT (for print use the below and comment out the above):
    %pdfborder={0 0 0},
    citecolor=mypersianblue,
    filecolor=mypersianblue,
    linkcolor=mypersianblue,
    urlcolor=mypersianblue,
    pdftitle={Tasty Bits of Several Complex Variables},
    pdfsubject={Several Complex Variables},
    pdfkeywords={several complex variables, complex analysis},
    pdfauthor={Jiri Lebl}
}

% Set up our index
\makeindex

% Very simple indexing
\newcommand{\myindex}[1]{#1\index{#1}}

\author{Ji\v{r}\'i Lebl}

\title{Tasty Bits of Several Complex Variables}

% Don't include subsections
\setcounter{tocdepth}{1}

% Better "outline"
%\setcounter{tocdepth}{2}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{claim}[thm]{Claim}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}

\newtheoremstyle{exercise}% name
  {}% Space above
  {}% Space below
  {\itshape}% Body font
  {}% Indent amount 1
  {\bfseries \itshape}% Theorem head font
  {:}% Punctuation after theorem head
  {.5em}% Space after theorem head 2
  {}% Theorem head spec (can be left empty, meaning "normal")

\newenvironment{exbox}{%
    \def\FrameCommand{\vrule width 1pt \relax\hspace{10pt}}%
    \MakeFramed{\advance\hsize-\width\FrameRestore}%
}{%
    \endMakeFramed
}

\newenvironment{exparts}{%
    \leavevmode\begin{enumerate}[a),noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
}{%
    \end{enumerate}
}
\newenvironment{exnumparts}{%
    \leavevmode\begin{enumerate}[1),noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
}{%
    \end{enumerate}
}
\newenvironment{expartshor}[1]{%
    \begingroup%
    \NumTabs{#1}%
    \leavevmode%
    \par%
    \begin{enumerate*}[a),itemjoin={\tab}]
}{%
    \end{enumerate*}\endgroup\par
}

\newenvironment{myfig}{%
\begin{figure}[h!t]
\noindent\rule{\textwidth}{0.5pt}\vspace{12pt}\par\centering}%
{\par\noindent\rule{\textwidth}{0.5pt}
\end{figure}}

\theoremstyle{exercise}
\newtheorem{exercise}{Exercise}[section]

\newtheoremstyle{example}% name
  {}% Space above
  {}% Space below
  {}% Body font
  {}% Indent amount 1
  {\bfseries}% Theorem head font
  {:}% Punctuation after theorem head
  {.5em}% Space after theorem head 2
  {}% Theorem head spec (can be left empty, meaning "normal")

\theoremstyle{example}
\newtheorem{example}[thm]{Example}

% referencing
\newcommand{\figureref}[1]{\hyperref[#1]{Figure~\ref*{#1}}}
\newcommand{\tableref}[1]{\hyperref[#1]{Table~\ref*{#1}}}
\newcommand{\chapterref}[1]{\hyperref[#1]{chapter~\ref*{#1}}}
\newcommand{\Chapterref}[1]{\hyperref[#1]{Chapter~\ref*{#1}}}
\newcommand{\Chdotref}[1]{\hyperref[#1]{Ch.~\ref*{#1}}}
\newcommand{\appendixref}[1]{\hyperref[#1]{appendix~\ref*{#1}}}
\newcommand{\Appendixref}[1]{\hyperref[#1]{Appendix~\ref*{#1}}}
\newcommand{\sectionref}[1]{\hyperref[#1]{section~\ref*{#1}}}
\newcommand{\subsectionref}[1]{\hyperref[#1]{\S~\ref*{#1}}}
\newcommand{\exerciseref}[1]{\hyperref[#1]{Exercise~\ref*{#1}}}
\newcommand{\exampleref}[1]{\hyperref[#1]{Example~\ref*{#1}}}
\newcommand{\thmref}[1]{\hyperref[#1]{Theorem~\ref*{#1}}}
\newcommand{\propref}[1]{\hyperref[#1]{Proposition~\ref*{#1}}}
\newcommand{\lemmaref}[1]{\hyperref[#1]{Lemma~\ref*{#1}}}
\newcommand{\corref}[1]{\hyperref[#1]{Corollary~\ref*{#1}}}
\newcommand{\defnref}[1]{\hyperref[#1]{Definition~\ref*{#1}}}
\newcommand{\remarkref}[1]{\hyperref[#1]{Remark~\ref*{#1}}}

% List of Symbols/Notation
\newglossary[nlg]{notation}{not}{ntn}{List of Notation}

\loadglsentries{notations}
\makeglossaries

\begin{document}

\ifpdf
  \pdfbookmark{Title Page}{title}
\fi
\newlength{\centeroffset}
\setlength{\centeroffset}{-0.5\oddsidemargin}
\addtolength{\centeroffset}{0.5\evensidemargin}
%\addtolength{\textwidth}{-\centeroffset}
\thispagestyle{empty}
\vspace*{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\Huge\bfseries \sffamily Tasty Bits of Several Complex Variables }
\noindent\rule[-1ex]{\textwidth}{5pt}\\[2.5ex]
\hfill\emph{\Large \sffamily A whirlwind tour of the subject }
\end{minipage}}

\vspace{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\bfseries 
%by
Ji{\v r}\'i Lebl\\[3ex]} 
\today
\\
(version 3.3)
\end{minipage}}

%\addtolength{\textwidth}{\centeroffset}
\vspace{\stretch{2}}


\pagebreak

\vspace*{\fill}

%\begin{small} 
\noindent
Typeset in \LaTeX.

\bigskip

\noindent
Copyright \copyright 2014--2020 Ji{\v r}\'i Lebl

%PRINT
% not for the coil version
%\noindent
%ISBN 978-0-359-64225-0

\bigskip

%\begin{floatingfigure}{1.4in}
%\vspace{-0.05in}
\noindent
\includegraphics[width=1.38in]{figures/license}
\quad
\includegraphics[width=1.38in]{figures/license2}
%\end{floatingfigure}

\bigskip

\noindent
\textbf{License:}
\\
This work is dual licensed under
the Creative Commons
Attribution-Non\-commercial-Share Alike 4.0 International License and
the Creative Commons
Attribution-Share Alike 4.0 International License.
To view a
copy of these licenses, visit
\url{https://creativecommons.org/licenses/by-nc-sa/4.0/}
or
\url{https://creativecommons.org/licenses/by-sa/4.0/}
or send a letter to
Creative Commons
PO Box 1866, Mountain View, CA 94042, USA\@.
%Creative Commons, 171 Second Street, Suite 300, San Francisco, California,
%94105, USA.

\bigskip

\noindent
You can use, print, duplicate, share this book as much as you want.  You can
base your own notes on it and reuse parts if you keep the license the
same.  You can assume the license is either the CC-BY-NC-SA or CC-BY-SA\@,
whichever is compatible with what you wish to do, your derivative works must
use at least one of the licenses.

\bigskip

\noindent
\textbf{Acknowledgments:}
\\
I would like to thank Debraj Chakrabarti, Anirban Dawn, Alekzander Malcom,
John Treuer, Jianou Zhang, Liz Vivas, Trevor Fancher,
Nicholas Lawson McLean, Alan Sola, Achinta Nandi,
Sivaguru Ravisankar, Richard L\"ark\"ang, Elizabeth Wulcan,
Tomas Rodriguez,
and students in my classes for pointing out typos/errors
and helpful suggestions. 

\bigskip

\noindent
During the writing of this book, 
the author was in part supported by NSF grant DMS-1362337.

\bigskip

\noindent
\textbf{More information:}
\\
See \url{https://www.jirka.org/scv/} for more information
(including contact information).

\medskip

\noindent
The \LaTeX\ source for the book is available
for possible modification and customization
at github: \url{https://github.com/jirilebl/scv}


% For large print do this
%\large

\microtypesetup{protrusion=false}
\tableofcontents
\microtypesetup{protrusion=true}

%\addtocontents{toc}{\protect\vspace{-2\baselineskip}}
%\addtocontents{toc}{\protect\vspace{-\baselineskip}}
%\addtocontents{toc}{\protect\enlargethispage{\baselineskip}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction} \label{ch:intro}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{INTRODUCTION}{INTRODUCTION}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This book is a polished version of my course notes for Math 6283, Several
Complex Variables, given in
Spring 2014, Spring 2016, and Spring 2019 semesters
at Oklahoma State University.
It is meant for a semester-long course.
Quite a few
exercises of various difficulty are
sprinkled throughout the text, and I hope a reader is
at least attempting all of them.  Many are required later in the
text.  The reader should attempt exercises in sequence as earlier exercises
can help or even be required to solve later ones.

The prerequisites are a decent knowledge of vector calculus, basic
real analysis, and a working knowledge of complex analysis in one variable.
Measure theory (Lebesgue integral and its convergence theorems) is useful,
but it is not essential except in a couple of places later in the book.
The first two chapters and most of the third
is accessible to beginning graduate students after one semester
of a standard single-variable complex
analysis graduate course.
From time to time (e.g.\ proof of Baouendi--Tr\`eves in
\chapterref{ch:crfunctions},
and most of
\chapterref{ch:dbar}, and \chapterref{ch:integralkernels}),
basic knowledge of differential forms is useful, and
in \chapterref{ch:analyticvarieties}
we use some basic ring theory from algebra.
By design, it can replace the second semester of complex analysis.

This book is not meant as an exhaustive reference.  It is simply a whirlwind
tour of several complex variables with slightly more material than can
be covered within a semester.  See the end of the book
for a \hyperref[ch:furtherreading]{list of books} for
reference and further reading.  There are also appendices for
a list of one-variable results, an overview of differential forms,
and some basic algebra.
See \appendixref{ap:onevarresults},
\appendixref{ap:diffforms}, and
\appendixref{ap:algebra}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation, single variable, and Cauchy's formula} \label{sec:motivation}


Let us start with some standard notation.
We use \glsadd{not:C}$\C$ for the complex numbers, \glsadd{not:R}$\R$
for real numbers,
\glsadd{not:Z}$\Z$ for integers,
\glsadd{not:N}$\N = \{ 1,2,3,\ldots \}$ for natural
numbers,
\glsadd{not:i}$i = \sqrt{-1}$.  Throughout this book, we use
the standard terminology of \emph{\myindex{domain}} to mean connected open
set.  We will try to avoid using it if connectedness is not needed, but
sometimes we use it just for simplicity.

As complex analysis deals with the complex numbers, perhaps we should begin
with $\sqrt{-1}$.  We start with the real numbers, $\R$, and we add
$\sqrt{-1}$ into our field.  We call this square root $i$, and we write the
complex numbers, $\C$, by
identifying 
$\C$ with $\R^2$ using
\begin{equation*}
z = x+iy,
\end{equation*}
where $z \in \C$, and $(x,y) \in \R^2$.  
A subtle philosophical issue is that there are two square roots of $-1$.
Two chickens\index{chicken!imaginary} are running around in our yard, and because we like to
know which is which, we catch one and write ``$i$'' on it.  If we happened
to have caught the other chicken, we would have gotten an exactly equivalent
theory, which we could not tell apart from the original.

Given a complex number $z$, its ``opposite'' is 
the \emph{\myindex{complex conjugate}} of $z$ and is defined as
\glsadd{not:conj}%
\begin{equation*}
\bar{z} \overset{\text{def}}{=} x-iy.
\end{equation*}
The size of $z$ is measured by the so-called \emph{\myindex{modulus}},
which is just the \emph{\myindex{Euclidean distance}}:
\glsadd{not:mod}%
\begin{equation*}
\abs{z} \overset{\text{def}}{=} \sqrt{z \bar{z}} = \sqrt{x^2+y^2} .
\end{equation*}

If $z = x+iy \in \C$ for $x,y \in \R$, then $x$ is called the
\emph{\myindex{real part}} and $y$ is called the
\emph{\myindex{imaginary part}}.  We write
\glsadd{not:real}%
\glsadd{not:imag}%
\begin{equation*}
\Re z = 
\Re (x+iy) =
\frac{z+\bar{z}}{2}
= x, \qquad 
\Im z = 
\Im (x+iy) =
\frac{z-\bar{z}}{2i}
=
y .
\end{equation*}


A function $f \colon U \subset \R^n \to \C$ for an open set $U$
is said to be continuously differentiable, or $C^1$ if the first (real)
partial derivatives exist and are continuous.
\glsadd{not:Ck}%
Similarly, it is $C^k$ or \emph{$C^k$-smooth}
\index{Ck-smooth function@$C^k$-smooth function}
if the first $k$ partial derivatives all exist and are differentiable.
\glsadd{not:Cinfty}%
Finally, a function is said to be $C^\infty$ or simply
\emph{smooth}\index{smooth function}\footnote{%
While $C^\infty$ is the common definition of \emph{smooth}, not everyone
always means
the same thing by the word \emph{smooth}.  I have seen it mean
differentiable, $C^1$, piecewise-$C^1$, $C^\infty$, holomorphic, \ldots}
if it is \emph{\myindex{infinitely differentiable}},
or in other words, if it is $C^k$ for all $k \in \N$.

\medskip

Complex analysis is the study of holomorphic (or complex-analytic)
functions.
Holomorphic functions are a generalization of polynomials,
and to get there one leaves the land of algebra to arrive in the realm of
analysis.
One can do an awful lot with polynomials, but sometimes they are
just not enough.  For example, there is no nonzero polynomial function that solves
the simplest of differential equations, $f' = f$.  We need the exponential
function, which is holomorphic.  

Let us start with polynomials.  In one variable, a polynomial in $z$ is
an expression of the form
\begin{equation*}
P(z) = \sum_{j=0}^d c_j \, z^j ,
\end{equation*}
where $c_j \in \C$ and $c_d \not= 0$.  The number $d$ is called the
\emph{degree}\index{degree of a polynomial}
of the
polynomial $P$.  We can plug in some number $z$ and simply compute
$P(z)$, so we have a function $P \colon \C \to \C$.

We try to write
\begin{equation*}
f(z) = \sum_{j=0}^\infty c_j \, z^j
\end{equation*}
and all is very fine until we wish to know what $f(z)$ is for some number
$z \in \C$.
We usually mean 
\begin{equation*}
\sum_{j=0}^\infty c_j \, z^j
=
\lim_{d\to\infty}
\sum_{j=0}^d c_j \, z^j .
\end{equation*}
As long as the limit exists, we have a function.  You know all
this; it is your one-variable complex analysis.  We usually
start with the functions and prove that we can expand into series.

Let $U \subset \C$ be open.  A function $f \colon U \to \C$
is \emph{\myindex{holomorphic}}
(or \emph{\myindex{complex-analytic}}) if it is
\emph{\myindex{complex-differentiable}} at every point,
that is, if
\begin{equation*}
f'(z)
%=
%\lim_{w \in \C \to z} \frac{f(w)-f(z_0)}{w-z} 
=
\lim_{\xi \in \C \to 0} \frac{f(z+\xi)-f(z)}{\xi} 
\qquad \text{exists for all $z \in U$.}
\end{equation*}
Importantly, the limit is taken with respect to
complex %$w$ or
$\xi$.
Another vantage point is to start with a continuously 
differentiable\footnote{Holomorphic functions end up being infinitely
differentiable anyway, so this hypothesis is not overly restrictive.} $f$,
and say $f = u + i\, v$ is holomorphic if it satisfies
the \emph{\myindex{Cauchy--Riemann equations}}:
\begin{equation*}
\frac{\partial u}{\partial x} = 
\frac{\partial v}{\partial y} ,
\qquad
\frac{\partial u}{\partial y} = 
-
\frac{\partial v}{\partial x} .
\end{equation*}
The so-called \emph{\myindex{Wirtinger operators}},
\begin{equation*}
\frac{\partial}{\partial z}
\overset{\text{def}}{=}
\frac{1}{2}
\left(
\frac{\partial}{\partial x} - i
\frac{\partial}{\partial y}
\right),
~ ~ ~ ~ ~
\frac{\partial}{\partial \bar{z}}
\overset{\text{def}}{=}
\frac{1}{2}
\left(
\frac{\partial}{\partial x} + i
\frac{\partial}{\partial y}
\right)
,
\end{equation*}
provide an easier way to understand the
Cauchy--Riemann equations.
These operators are determined by insisting
\glsadd{not:wirt}%
\begin{equation*}
\frac{\partial}{\partial z} z = 1, \quad
\frac{\partial}{\partial z} \bar{z} = 0, \quad
\frac{\partial}{\partial \bar{z}} z = 0, \quad
\frac{\partial}{\partial \bar{z}} \bar{z} = 1.
\end{equation*}

The function $f$ is holomorphic if and only if
\begin{equation*}
\frac{\partial f}{\partial \bar{z}} = 0 .
\end{equation*}
That seems a far nicer statement of the Cauchy--Riemann equations, and it is
just one complex equation.  It says
a function is holomorphic if and only if it depends on $z$ but not on
$\bar{z}$ (perhaps that might not make a whole lot of sense at first
glance).
Let us check:
\begin{equation*}
\frac{\partial f}{\partial \bar{z}} 
=
\frac{1}{2}
\left(
\frac{\partial f}{\partial x} + i
\frac{\partial f}{\partial y}
\right)
=
%\frac{1}{2}
%\left(
%\frac{\partial }{\partial x} (u + iv) + i
%\frac{\partial }{\partial y} (u + iv)
%\right)
%=
\frac{1}{2}
\left(
\frac{\partial u}{\partial x} 
+ i \frac{\partial v}{\partial x} 
+ i \frac{\partial u}{\partial y}
- \frac{\partial v}{\partial y}
\right) 
=
\frac{1}{2}
\left(
\frac{\partial u}{\partial x} 
- \frac{\partial v}{\partial y}
\right)
+
\frac{i}{2}
\left(
\frac{\partial v}{\partial x} 
+ \frac{\partial u}{\partial y}
\right) .
\end{equation*}
This expression is zero if and only if the real parts and the imaginary
parts are zero.  In other words, %if and only if
\begin{equation*}
\frac{\partial u}{\partial x} 
- \frac{\partial v}{\partial y}
= 0,
\qquad
\text{and}
\qquad
\frac{\partial v}{\partial x} 
+ \frac{\partial u}{\partial y} = 0
.
\end{equation*}
That is, the Cauchy--Riemann equations are satisfied.

%Another common way to define a holomorphic function is to say that
%the complex derivative at each point exists.  If $f'$ exists
%at every point, it equals the derivative in $z$.
If $f$ is holomorphic, the derivative in $z$ is the standard complex derivative you know and love:
\begin{equation*}
\frac{\partial f}{\partial z} (z_0)
=
f'(z_0)
=
\lim_{\xi \to 0} \frac{f(z_0+\xi)-f(z_0)}{\xi} .
\end{equation*}
That is because 
\begin{equation*}
\begin{split}
\frac{\partial f}{\partial z} 
=
\frac{1}{2}
\left(
\frac{\partial u}{\partial x} 
+ \frac{\partial v}{\partial y}
\right)
+
\frac{i}{2}
\left( \frac{\partial v}{\partial x} - \frac{\partial u}{\partial y}
\right) 
& =
\frac{\partial u}{\partial x} 
+ i \frac{\partial v}{\partial x}
 =
\frac{\partial f}{\partial x}
\\
& =
\frac{1}{i} \left(
\frac{\partial u}{\partial y}
+ i
\frac{\partial v}{\partial y} 
\right)
 =
\frac{\partial f}{\partial (iy)}
.
\end{split}
\end{equation*}

A function on $\C$ is really a function defined on
$\R^2$ as identified above and hence it is a function of $x$ and $y$.
Writing
$x = \frac{z+\bar{z}}{2}$ and
$y = \frac{z-\bar{z}}{2i}$, we think of it as a function of two
complex variables $z$ and $\bar{z}$.  Pretend for a moment as if $\bar{z}$ did not
depend on $z$.
The Wirtinger operators
work as if $z$ and $\bar{z}$ really were independent variables.  For
instance:
\begin{equation*}
\frac{\partial}{\partial z}
\left[ z^2 \bar{z}^3 + z^{10} \right]
=
2z \bar{z}^3 + 10 z^{9}
\qquad
\text{and}
\qquad
\frac{\partial}{\partial \bar{z}}
\left[ z^2 \bar{z}^3 + z^{10} \right]
=
z^2 ( 3 \bar{z}^2 ) + 0 .
\end{equation*}
So a holomorphic function is a function not depending on $\bar{z}$.

The most important theorem in one variable is
the \emph{\myindex{Cauchy integral formula}}\index{Cauchy formula}.

\begin{thm}[Cauchy integral formula]
Let $U \subset \C$ be a bounded domain where the boundary $\partial U$
is a piecewise smooth
simple closed path (a Jordan curve).  Let $f \colon \widebar{U} \to \C$ be a continuous function,
holomorphic in $U$.
Orient $\partial U$ positively (going around counter-clockwise).
Then
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta 
\qquad \text{for all $z \in U$.}
\end{equation*}
\end{thm}

%The path integral for a smooth path $\gamma \colon [a,b] \to \C$ is defined as
%\begin{equation*}
%\int_\gamma f(z) \, dz
%=
%\int_a^b f\bigl(\gamma(t)\bigr) \gamma'(t) \, dt .
%\end{equation*}

The Cauchy formula is the essential ingredient we need from 
one complex variable.  It follows
from Green's theorem\footnote{If you wish to feel inadequate,
note that this theorem, on which all of complex analysis (and all of physics)
rests, was proved by
George Green, who was the son of a miller and had one year of formal
schooling.}  (Stokes' theorem in two
dimensions).  You can look forward to
\thmref{thm:generalizedcauchy} for a proof of a more general formula,
the Cauchy--Pompeiu integral formula.

As a differential form, \glsadd{not:dz}$dz = dx + i \, dy$.  If you are uneasy
about differential forms, you probably defined the path integral above
directly using
the Riemann--Stieltjes integral in your one-complex-variable class.
Let us write down the formula in terms of the standard Riemann integral
in a special case.  Take the \emph{\myindex{unit disc}}
\glsadd{not:D}%
\begin{equation*}
\D
\overset{\text{def}}{=}
\bigl\{ z \in \C : \sabs{z} < 1 \bigr\} .
\end{equation*}
The boundary is the unit circle
$\partial \D = \bigl\{ z \in \C : \sabs{z} = 1 \bigr\}$ oriented positively,
that is, counter-clockwise.   Parametrize $\partial \D$
by $e^{it}$, where $t$ goes from 0 to $2\pi$.  If $\zeta = e^{it}$,
then $d\zeta = ie^{it}dt$, and
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial \D}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta 
=
\frac{1}{2\pi}
\int_0^{2\pi}
\frac{f(e^{it}) e^{it} }{e^{it}-z}
\,
dt .
\end{equation*}
If you are not completely comfortable
with
path %or surface
integrals, try to think about how you would parametrize the path, and
write the integral as an integral any calculus student would recognize.

I venture a guess that 90\% of what you learned in a one-variable complex analysis
course (depending on who taught it)
is more or less a straightforward consequence of the Cauchy
integral formula.
An important theorem from one variable that follows from
the Cauchy formula is the
\emph{\myindex{maximum modulus principle}} (or just
the \emph{\myindex{maximum principle})}.
Let us give its simplest version.

\begin{thm}[Maximum modulus principle]
Suppose $U \subset \C$ is a domain and $f \colon U \to \C$
is holomorphic.
If for some $z_0 \in U$
\begin{equation*}
\sup_{z \in U} \, \sabs{f(z)} = \sabs{f(z_0)} ,
\end{equation*}
\glsadd{not:identeq}%
then $f$ is constant, that is, $f \equiv f(z_0)$.
\end{thm}

That is, if the supremum is attained in the interior of the domain,
then the function must be constant.  Another way to state the maximum
principle is to say: If $f$ extends continuously to the boundary of a
domain, then the supremum of $\sabs{f(z)}$ is attained on the boundary.
In
one variable you learned that the maximum principle is really a
property of harmonic functions.

\begin{thm}[Maximum principle]
Let $U \subset \C$ be a domain and $h \colon U \to \R$
harmonic, that is,
\glsadd{not:laplacian}%
\begin{equation*}
\nabla^2 h = \frac{\partial^2 h}{\partial x^2} + \frac{\partial^2 h}{\partial
y^2} = 0 .
\end{equation*}
If for some $z_0 \in U$
\begin{equation*}
\sup_{z \in U} \, h(z) = h(z_0)
\qquad \text{or} \qquad
\inf_{z \in U} \, h(z) = h(z_0) ,
\end{equation*}
then $h$ is constant, that is, $h \equiv h(z_0)$.
\end{thm}

In one variable, if $f = u+iv$ is holomorphic for real valued $u$ and $v$,
then $u$ and $v$ are harmonic.  Locally, any harmonic function is
the real (or imaginary) part of a holomorphic function, so in
one complex variable, studying 
harmonic functions is almost equivalent to studying holomorphic
functions.  Things are decidedly different
in two or more variables.

\medskip

Holomorphic functions admit a power series representation in $z$
at each point $a$:
\begin{equation*}
f(z) = \sum_{j=0}^\infty c_j {(z-a)}^j .
\end{equation*}
No $\bar{z}$ is necessary,
since $\frac{\partial f}{\partial \bar{z}} = 0$.

Let us see the proof using the Cauchy integral formula as we will
require this computation in several variables as well.
Given $a \in \C$ and $\rho > 0$, define the disc of radius $\rho$ around $a$
\glsadd{not:disc}%
\begin{equation*}
\Delta_\rho(a)
\overset{\text{def}}{=}
\bigl\{ z \in \C : \sabs{z-a} < \rho \bigr\} .
\end{equation*}
Suppose $U \subset \C$ is open, $f \colon U \to \C$ is holomorphic,
$a \in U$, and $\overline{\Delta_\rho(a)} \subset U$ (that is, the closure
of the disc is in $U$, and so its boundary $\partial \Delta_\rho(a)$ is also in $U$).

For $z \in \Delta_\rho(a)$ and $\zeta \in \partial \Delta_\rho(a)$, 
\begin{equation*}
\abs{\frac{z-a}{\zeta-a}} =
\frac{\sabs{z-a}}{\rho} < 1 .
\end{equation*}
In fact, if $\sabs{z-a} \leq \rho' < \rho$, then
$\abs{\frac{z-a}{\zeta-a}} \leq \frac{\rho'}{\rho} < 1$.  Therefore,
the geometric series
\begin{equation*}
\sum_{j=0}^\infty
{\left(\frac{z-a}{\zeta-a}\right)}^j
=
\frac{1}{1-
\frac{z-a}{\zeta-a}}
=
\frac{\zeta-a}{\zeta-z}
\end{equation*}
converges uniformly absolutely for $(z,\zeta) \in \overline{\Delta_{\rho'}(a)}
\times \partial \Delta_\rho(a)$ (that is, $\sum_j {\bigl\lvert
\frac{z-a}{\zeta-a} \bigr\rvert}^j$
converges uniformly).
%In particular, the
%series converges uniformly absolutely in $z$ on compact subsets of $\Delta_{\rho}(a)$. 

Let $\gamma$
be the path going around 
$\partial \Delta_\rho(a)$ once in the positive direction.  Compute
\begin{equation*}
\begin{split}
f(z)
& =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta 
\\
& =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-a}
\frac{\zeta-a}{\zeta-z}
\,
d \zeta 
\\
& =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-a}
\sum_{j=0}^\infty
{\left(\frac{z-a}{\zeta-a}\right)}^j
\,
d \zeta 
\\
& =
\sum_{j=0}^\infty
\left(
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{j+1}}
\,
d \zeta 
\right)
{(z-a)}^j .
\end{split}
\end{equation*}
In the last equality, we may 
interchange the limit on the sum with the integral either
via Fubini's theorem or via uniform convergence:
$z$ is fixed and if $M$ is the supremum of $\abs{\frac{f(\zeta)}{\zeta-a}} =
\frac{\sabs{f(\zeta)}}{\rho}$ on $\partial \Delta_\rho(a)$,
then
\begin{equation*}
\abs{
\frac{f(\zeta)}{\zeta-a}
{\left(\frac{z-a}{\zeta-a}\right)}^j
}
\leq
M 
{\left(\frac{\abs{z-a}}{\rho}\right)}^j,
\qquad \text{and} \qquad
\frac{\abs{z-a}}{\rho} < 1 .
\end{equation*}

The key point is writing the \emph{\myindex{Cauchy kernel}}
$\frac{1}{\zeta-z}$ as
\begin{equation*}
\frac{1}{\zeta-z}
=
\frac{1}{\zeta-a}
\frac{\zeta-a}{\zeta-z} ,
\end{equation*}
and then using the geometric series.

Not only have we proved that $f$ has a power series, but we computed
that the radius of convergence is at least $R$, where $R$ is the maximum $R$
such that $\Delta_R(a) \subset U$.  We also obtained a formula for the
coefficients
\begin{equation*}
c_j = 
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{j+1}}
\,
d \zeta  .
\end{equation*}

For a set $K$, denote the \emph{\myindex{supremum norm}}:
\glsadd{not:supnorm}%
\begin{equation*}
\snorm{f}_K
\overset{\text{def}}{=}
\sup_{z \in K} \sabs{f(z)} .
\end{equation*}
By a brute force estimation, we obtain the very useful
\emph{\myindex{Cauchy estimates}}:
\begin{equation*}
\sabs{c_j} = 
\abs{
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{j+1}}
\,
d \zeta 
}
\leq
\frac{1}{2\pi}
\int_{\gamma}
\frac{\snorm{f}_{\gamma}}{\rho^{j+1}}
\,
\sabs{d \zeta} 
=
\frac{\snorm{f}_{\gamma}}{\rho^{j}} .
\end{equation*}

We differentiate Cauchy's formula $j$ times (using the Wirtinger
$\frac{\partial}{\partial z}$ operator),
\begin{equation*}
f^{(j)}(z)
=
\frac{\partial^j f}{\partial z^j} (z)
=
\frac{1}{2\pi i}
\int_{\gamma}
\frac{j! f(\zeta)}{{(\zeta-z)}^{j+1}}
\,
d \zeta  ,
\end{equation*}
and therefore
\begin{equation*}
j! \, c_j =
f^{(j)}(a)
=
\frac{\partial^j f}{\partial z^j}(a) .
\end{equation*}
Hence, we can control derivatives of $f$
by the size of the function:
\begin{equation*}
\babs{f^{(j)}(a)}
=
\abs{\frac{\partial^j f}{\partial z^j}(a)}
\leq
\frac{j! \snorm{f}_{\gamma}}{\rho^{j}} .
\end{equation*}
This estimate is one of the key properties of
holomorphic functions, and the reason why the correct topology for
the set of holomorphic functions is the same as the topology for continuous functions.
Consequently,
obstructions to solving problems in complex analysis are often topological
in character.

For further review of one-variable results,
see \appendixref{ap:onevarresults}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Holomorphic functions in several variables} \label{ch:holfunc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Onto several variables} \label{sec:ontosevvar}

Let $\C^n = \overbrace{\C \times \C \times \cdots \times \C}^{\text{$n$
times}}$ denote the $n$-dimensional
\emph{\myindex{complex Euclidean space}}\index{Euclidean space}.  Denote
by $z = (z_1,z_2,\ldots,z_n)$ the coordinates of $\C^n$.
Let $x =
(x_1,x_2,\ldots,x_n)$ and $y = (y_1,y_2,\ldots,y_n)$ denote coordinates in
$\R^n$.
We 
identify $\C^n$ with
$\R^{2n}$ by letting
$z = x+iy$, that is, $z_j = x_j + i y_j$ for every $j$.
As in one complex variable, we write $\bar{z} = x-iy$.
We call $z$ the \emph{\myindex{holomorphic coordinates}}
and $\bar{z}$ the \emph{\myindex{antiholomorphic coordinates}}.

\begin{defn}
For $\rho = (\rho_1,\rho_2,\ldots,\rho_n)$ where $\rho_j > 0$ and $a \in \C^n$,
define
a \emph{\myindex{polydisc}}
\glsadd{not:disc}%
\begin{equation*}
\Delta_\rho(a)  \overset{\text{def}}{=}
\bigl\{ z \in \C^n : \sabs{z_j - a_j} < \rho_j ~\text{for $j=1,2,\ldots,n$} \bigr\} .
\end{equation*}
We call $a$ the
\emph{center}\index{center of a polydisc}
and $\rho$ the
\emph{polyradius}\index{polyradius of a polydisc}
or simply the
\emph{radius}\index{radius of a polydisc}
of the polydisc $\Delta_\rho(a)$.
If $\rho > 0$ is a number, then
\begin{equation*}
\Delta_\rho(a)  \overset{\text{def}}{=}
\bigl\{ z \in \C^n : \sabs{z_j - a_j} < \rho ~\text{for $j=1,2,\ldots,n$} \bigr\} .
\end{equation*}
In two variables, a polydisc is sometimes called a \emph{\myindex{bidisc}}.
As there is the unit disc $\D$ in one variable, so is there
the \emph{\myindex{unit polydisc}} in several variables:
\begin{equation*}
\D^n = \D \times \D \times \cdots \times \D = \Delta_1(0) = 
\bigl\{ z \in \C^n : \sabs{z_j} < 1 ~\text{for $j=1,2,\ldots,n$} \bigr\} .
\end{equation*}
\end{defn}

In more than one complex dimension, it is difficult to draw exact pictures
for lack of real dimensions on our paper.
We visualize the unit polydisc in two variables (bidisc)
by drawing the in \figureref{fig:polydisc}
picture by plotting just against the modulus of the variables.
\begin{myfig}
\subimport*{figures/}{polydisc.pdf_t}
\caption{The bidisc.\label{fig:polydisc}}
\end{myfig}

Recall the
\emph{\myindex{Euclidean inner product}} on $\C^n$:
\glsadd{not:hermprod}%
\begin{equation*}
\linnprod{z}{w} \overset{\text{def}}{=} %z \cdot \bar{w}
z_1 \bar{w}_1 +
z_2 \bar{w}_2 + \cdots +
z_n \bar{w}_n .
\end{equation*}
The inner product gives us the standard
\emph{\myindex{Euclidean norm}} on $\C^n$:
\glsadd{not:eucnorm}%
\begin{equation*}
\snorm{z}  \overset{\text{def}}{=} \sqrt{\linnprod{z}{z}}
=
\sqrt{\sabs{z_1}^2 +
\sabs{z_2}^2 + \cdots +
\sabs{z_n}^2} .
\end{equation*}
This norm agrees with the standard Euclidean norm on $\R^{2n}$.
Define \emph{balls}\index{ball} as in $\R^{2n}$:
\glsadd{not:ball}%
\begin{equation*}
B_\rho(a)  \overset{\text{def}}{=}
\bigl\{ z \in \C^n : \snorm{z - a} < \rho \bigr\} ,
\end{equation*}
And define the \emph{\myindex{unit ball}} as
\glsadd{not:unitball}%
\begin{equation*}
\bB_n \overset{\text{def}}{=} B_1(0) =
\bigl\{ z \in \C^n : \snorm{z} < 1 \bigr\} .
\end{equation*}

A ball centered at the origin
can also be pictured by plotting against the modulus of the
variables since the inequality defining the ball only
depends on the moduli of the variables.
Not every domain can be drawn like this, but if it can,
it is called a \emph{Reinhardt domain}, more on this later.
A picture of $\bB_2$ is in \figureref{fig:ball}.
\begin{myfig}
\subimport*{figures/}{ball.pdf_t}
\caption{The ball $\bB_2$ as a Reinhardt domain.\label{fig:ball}}
\end{myfig}

\begin{defn}
Let $U \subset
\C^n$ be an open set.  A
function
$f \colon U \to \C$ is \emph{\myindex{holomorphic}} if
it is \emph{locally bounded}%
\index{locally bounded function}\footnote{For
every $p \in U$, there is a neighborhood $N$ of $p$
such that $f|_N$ is bounded.  It is a deep result of Hartogs that we might
in fact just drop ``locally bounded'' from the definition and obtain the same
set of functions.}
%A deep result of Hartogs, which we skip, says that
%we do not need to assume $f$ to be locally bounded.}
and holomorphic in each variable separately.
In other words, $f$ is holomorphic if it is locally bounded
and complex-differentiable in each variable separately:
\begin{equation*}
\lim_{\xi \in \C \to 0} \frac{f(z_1,\ldots,z_k+\xi,\ldots,z_n) - f(z)}{\xi}
\qquad \text{exists for all $z \in U$ and all $k=1,2,\ldots,n$}.
\end{equation*}
In this book, however, the words ``differentiable'' and ``derivative''
(without the ``complex-'') refer
to plain vanilla real differentiability.
\end{defn}

As in one variable, we define the \emph{\myindex{Wirtinger operators}}
\glsadd{not:wirt}%
\begin{equation*}
\frac{\partial}{\partial z_j}  \overset{\text{def}}{=}
\frac{1}{2} \left(
\frac{\partial}{\partial x_j} - i \frac{\partial}{\partial y_j}
\right) ,
\qquad
\frac{\partial}{\partial \bar{z}_j}  \overset{\text{def}}{=}
\frac{1}{2} \left(
\frac{\partial}{\partial x_j} + i \frac{\partial}{\partial y_j}
\right) .
\end{equation*}
An alternative definition is to say that a continuously
differentiable function $f \colon U \to \C$ is
\emph{holomorphic} if
it satisfies the
\emph{\myindex{Cauchy--Riemann equations}}
\begin{equation*}
\frac{\partial f}{\partial \bar{z}_j}  = 0 \qquad \text{for $j=1,2,\ldots,n$}.
\end{equation*}
As in one variable, if you defined partial derivatives
of holomorphic functions
as the limits of the definition above,
then you would get the Wirtinger $\frac{\partial}{\partial z_k}$
for holomorphic functions.  That is, if $f$ is holomorphic, then
\begin{equation*}
\frac{\partial f}{\partial z_k}(z)
=
\lim_{\xi \in \C \to 0} \frac{f(z_1,\ldots,z_k+\xi,\ldots,z_n) - f(z)}{\xi}
.
\end{equation*}

Due to the following proposition, the alternative definition using the
Cauchy--Riemann equations is just as good as the definition we gave.

\begin{prop}
Let $U \subset \C^n$ be an open set and
suppose $f \colon U \to \C$ is holomorphic.  Then $f$ is infinitely
differentiable.
\end{prop}

\begin{proof}
Suppose $\Delta = \Delta_{\rho}(a) = \Delta_1 \times \cdots \times \Delta_n$
is a polydisc centered at $a$, where each $\Delta_j$ is a disc
and suppose $\overline{\Delta} \subset U$, that is, $f$ is holomorphic
on a neighborhood of the closure of $\Delta$.
Let $z$ be in $\Delta$.
Orient $\partial \Delta_1$ positively and
apply the Cauchy formula (after all $f$ is holomorphic in $z_1$):
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial \Delta_1}
\frac{f(\zeta_1,z_2,\ldots,z_n)}{\zeta_1-z_1}
\,
d \zeta_1 .
\end{equation*}
Apply it again on the second variable, again orienting
$\partial \Delta_2$ positively:
\begin{equation*}
f(z) =
\frac{1}{{(2\pi i)}^2}
\int_{\partial \Delta_1}
\int_{\partial \Delta_2}
\frac{f(\zeta_1,\zeta_2,z_3,\ldots,z_n)}{(\zeta_1-z_1)(\zeta_2-z_2)}
\,
d \zeta_2
\,
d \zeta_1 .
\end{equation*}

Applying the formula $n$ times we obtain
\begin{equation} \label{iteratedcauchy:eq}
f(z) =
\frac{1}{{(2\pi i)}^n}
\int_{\partial \Delta_1}
\int_{\partial \Delta_2}
\cdots
\int_{\partial \Delta_n}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{(\zeta_1-z_1)(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_n
\cdots
d \zeta_2
\,
d \zeta_1 .
\end{equation}
As $f$ is bounded on the compact set
$\partial \Delta_1 \times \cdots \times \partial \Delta_n$,
we find that $f$ is continuous in $\Delta$.
We may differentiate underneath the
integral, because $f$ is bounded on 
$\partial \Delta_1 \times \cdots \times \partial \Delta_n$
and so are the derivatives of the integrand
with respect to $x_j$ and $y_j$, where $z_j=x_j+i y_j$, as long as
$z$ is a positive distance away from
$\partial \Delta_1 \times \cdots \times \partial \Delta_n$.
We may differentiate as many times as we wish.
%We are really differentiating only in the real and imaginary
%parts of the $z_j$ variables, and the integrand is
%infinitely differentiable in those variables.
\end{proof}

%In the definition of holomorphicity,
%we may have assumed $f$ is smooth and satisfies
%the Cauchy--Riemann equations.  However, the way we stated the
%definition makes it easier to apply.

In \eqref{iteratedcauchy:eq} above,
we derived the Cauchy integral formula in several variables.  To
write the formula more concisely we apply the Fubini's theorem to write it as
a single integral.  We will write it down using differential forms.  If you
are unfamiliar with differential forms, think of the integral
as the iterated integral above, and you can read the next few paragraphs
a little lightly.
It is enough to understand real differential forms; we simply allow
complex coefficients here.
See \appendixref{ap:diffforms} for an overview of differential forms,
or
Rudin~\cite{Rudin:principles} for an introduction with all the details.

Given real coordinates $x = (x_1,\ldots,x_n)$, a one-form $d x_j$ is a linear functional on tangent vectors
such that $d x_j \bigl( \frac{\partial}{\partial x_j} \bigr) = 1$ and
$d x_j \bigl( \frac{\partial}{\partial x_k} \bigr) = 0$ if $j \not= k$.
Because
$z_j = x_j + i y_j$ and
$\bar{z}_j = x_j - i y_j$,
\glsadd{not:dz}\glsadd{not:dzbar}%
\begin{equation*}
d z_j = d x_j + i \, d y_j , 
\qquad
d \bar{z}_j = d x_j - i \, d y_j . 
\end{equation*}
Let
\glsadd{not:kronecker}%
$\delta_{j}^k$ be the Kronecker delta, that is, $\delta_j^j = 1$,
and $\delta_j^k = 0$ if $j \not= k$.  Then as expected,
\begin{equation*}
d z_j \left( \frac{\partial}{\partial z_k} \right) = \delta_j^k ,
\qquad
d z_j \left( \frac{\partial}{\partial \bar{z}_k} \right) = 0 ,
\qquad
d \bar{z}_j \left( \frac{\partial}{\partial z_k} \right) = 0 ,
\qquad
d \bar{z}_j \left( \frac{\partial}{\partial \bar{z}_k} \right) = \delta_j^k
.
\end{equation*}
%\begin{equation*}
%\begin{array}{lll}
%d z_j \left( \frac{\partial}{\partial z_k} \right) = \delta_j^k ,
%& \quad &
%d z_j \left( \frac{\partial}{\partial \bar{z}_k} \right) = 0 , \\
%d \bar{z}_j \left( \frac{\partial}{\partial z_k} \right) = 0 ,
%& \quad &
%d \bar{z}_j \left( \frac{\partial}{\partial \bar{z}_k} \right) = \delta_j^k
%,
%\end{array}
%\end{equation*}
%\glsadd{not:kronecker}%
%where $\delta_{j}^k$ is the Kronecker delta, that is, $\delta_j^j = 1$,
%and $\delta_j^k = 0$ if $j \not= k$.
One-forms are objects of the form
\begin{equation*}
\sum_{j=1}^n \alpha_j \, d z_j + 
\beta_j \, d \bar{z}_j ,
\end{equation*}
where $\alpha_j$ and $\beta_j$ are functions (of $z$).
Two-forms are combinations of wedge products,
$\omega \wedge \eta$, of one-forms.  A wedge of a two-form and
a one-form is a three-form, etc.
A $k$-form is an object that 
can be integrated on a so-called $k$-chain, for example, a
$k$-dimensional surface.  The wedge product takes care of the orientation
as it is anticommutative on one-forms:
For one-forms $\omega$ and $\eta$, we have
$\omega \wedge \eta = - \eta \wedge \omega$.

At this point, we need to talk about orientation in $\C^n$, that is,
ordering of the real coordinates.  There are two
natural real-linear isomorphisms of $\C^n$ and $\R^{2n}$.  We
identify $z = x+iy$ as either
\begin{equation*}
(x,y) = (x_1,\ldots,x_n,y_1,\ldots,y_n) \qquad
\text{or} \qquad
(x_1,y_1,x_2,y_2,\ldots,x_n,y_n) .
\end{equation*}
If we take the natural orientation of $\R^{2n}$,
it is possible (if $n$ is even) that we obtain
two opposite orientations on $\C^n$ (if $n$ is even, the real linear map
that takes one ordering to the other has determinant $-1$).
The orientation we take as the natural orientation of $\C^n$ (in this book)
corresponds to
the second ordering above, that
is, $(x_1,y_1,\ldots,x_n,y_n)$.  Either isomorphism may be used
in computation as long as it is used consistently, and the underlying
orientation is kept in mind.

\begin{thm}[Cauchy integral formula]
\index{Cauchy integral formula in several variables}\index{Cauchy formula}
Let $\Delta \subset \C^n$ be a polydisc. 
% centered at $a \in \C^n$.
Suppose
$f \colon \overline{\Delta} \to \C$ is a continuous function
holomorphic in $\Delta$.
Write $\Gamma = \partial \Delta_1 \times \cdots \times \partial \Delta_n$
oriented appropriately (each $\partial \Delta_j$ has positive orientation).
Then for $z \in \Delta$
\begin{equation*}
f(z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{(\zeta_1-z_1)(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1 
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n .
\end{equation*}
\end{thm}

We stated a more general result where $f$ is only continuous 
on $\overline{\Delta}$ and holomorphic in $\Delta$.  The proof of this
slight generalization is contained within the next two exercises.

\begin{exbox}
\begin{exercise}
Suppose $f \colon \overline{\D^2} \to \C$ is continuous and holomorphic
on $\D^2$.  For every $\theta \in \R$, prove
\begin{equation*}
g_1(\xi) = f(\xi,e^{i\theta}) \qquad \text{and} \qquad
g_2(\xi) = f(e^{i\theta},\xi)
\avoidbreak
\end{equation*}
are holomorphic in $\D$.
\end{exercise}

\begin{exercise}
Prove the theorem above, that is, the slightly more general Cauchy integral
formula where $f$ is only continuous on $\overline{\Delta}$ and
holomorphic in $\Delta$.
\end{exercise}
\end{exbox}


The Cauchy integral formula shows
an important and subtle point about holomorphic functions in several
variables:
The value of
the function $f$ on $\Delta$ is completely determined by the values of $f$ on
the set $\Gamma$, which is much smaller than the boundary of the polydisc
$\partial \Delta$.  In fact, the $\Gamma$ is of real dimension $n$, while
the boundary of the polydisc has real dimension $2n-1$.
The set $\Gamma = \partial \Delta_1 \times \cdots \times \partial \Delta_n$
is called
the \emph{\myindex{distinguished boundary}}.
See \figureref{fig:polydisc-dist} for the distinguished boundary of the
bidisc.
\begin{myfig}
\subimport*{figures/}{polydisc-dist.pdf_t}
\caption{The distinguished boundary of of $\D^2$.\label{fig:polydisc-dist}}
\end{myfig}

The set $\Gamma$ is a 2-dimensional torus, like the surface of a
donut.  Whereas the set $\partial \D^2 =
(\partial \D \times \overline{\D}) \cup
(\overline{\D} \times \partial \D)$ is the union of two filled donuts, or more
precisely it is both the inside and the outside of the donut put together,
and these two things meet on the surface of the donut.  So the
set $\Gamma$ is quite small in comparison to the entire boundary
$\partial \D^2$.

\begin{exbox}
\begin{exercise}
%Prove the following stronger version of the maximum principle.
Suppose $\Delta$ is a polydisc, $\Gamma$ its distinguished boundary,
and $f \colon \overline{\Delta} \to \C$ is continuous on $\overline{\Delta}$
and holomorphic on $\Delta$.
Prove
$\sabs{f(z)}$ achieves its maximum on $\Gamma$.
\end{exercise}

\begin{exercise}
A ball is different from a polydisc.  Prove that for every $p \in \partial \bB_n$
there exists a continuous $f \colon \overline{\bB_n} \to \C$, holomorphic
on $\bB_n$ such that $\sabs{f(z)}$ achieves a strict maximum at $p$.
\end{exercise}

\begin{exercise}
Show that in the real setting, differentiable
in each variable separately does not imply differentiable even if
the function is locally bounded.
Let $f(x,y) = \frac{xy}{x^2+y^2}$ outside the origin
and $f(0,0) = 0$.  Prove that $f$ is a
locally bounded function in $\R^2$, which is differentiable
in each variable separately (all partial derivatives exist at every point), but 
$f$ is not even continuous.  There is something very
special about the holomorphic category.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is open.
Prove that $f \colon U \to \C$ is holomorphic if and only if
$f$ is locally bounded and
for every $a,b \in \C^n$, the
function
$\zeta \mapsto f(\zeta a + b)$ is holomorphic on
the open set $\{ \zeta \in \C : \zeta a + b \in U \}$.
\end{exercise}

\begin{exercise}
Prove a several complex variables version of Morera's theorem (see
\thmref{thm:onevarmorera}).
A triangle $T \subset \C^n$ is the closed convex hull of three points, so
including the inside.  Orient $T$ in some way %(will not matter which way)
and orient $\partial T$ accordingly.
A triangle $T$ \emph{lies in a complex line} if
its vertices $a,b,c$ satisfy
$\zeta (b-a) = c-a$ for some $\zeta \in \C$.
Suppose $U \subset \C^n$ is open and $f \colon U \to \C$ is continuous.
Prove that $f$ is holomorphic if and only if
\begin{equation*}
\int_{\partial T} f(z) \, dz_k = 0
\end{equation*}
for every triangle $T \subset U$ that lies in a complex line,
and every $k=1,2,\ldots,n$.
Hint: The previous exercise may be useful.
\end{exercise}

\begin{exercise}
For $0 < \epsilon < 1$,
suppose a continuous
$f \colon \overline{\D^2} \setminus \overline{\Delta_\epsilon(0)} \to \C$ is holomorphic
on $\D^2 \setminus \overline{\Delta_\epsilon(0)}$.
Prove that $f$ is bounded.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Power series representation}

As you noticed, writing out all the components can be a pain.
%It would become even more painful later on.
Just as we write 
vectors as $z$ instead of $(z_1,z_2,\ldots,z_n)$, we similarly
define the so-called
\emph{\myindex{multi-index notation}}
to deal with more complicated formulas such as the ones above.

Let $\alpha \in \N_0^n$
be a vector of nonnegative integers
\glsadd{not:N0}%
(where $\N_0 = \N \cup \{ 0\}$).
We write
\glsadd{not:zalpha}%
\glsadd{not:dzmulti}%
\glsadd{not:absalpha}%
\glsadd{not:alphabang}%
\glsadd{not:alphader}%
\begin{align*}
z^\alpha & \overset{\text{def}}{=} z_1^{\alpha_1}z_2^{\alpha_2} \cdots
z_n^{\alpha_n} ,
&
\sabs{z}^\alpha & \overset{\text{def}}{=} \sabs{z_1}^{\alpha_1}\sabs{z_2}^{\alpha_2} \cdots
\sabs{z_n}^{\alpha_n} ,
\displaybreak[0]\\
\frac{1}{z} & \overset{\text{def}}{=} \frac{1}{z_1z_2 \cdots z_n} ,
&
\frac{z}{w} & \overset{\text{def}}{=}
\left(\frac{z_1}{w_1}, \frac{z_2}{w_2}, \ldots, \frac{z_n}{w_n} \right) ,
\displaybreak[0]\\
\frac{\partial^{\sabs{\alpha}}}{\partial z^\alpha} & \overset{\text{def}}{=}
\frac{\partial^{\alpha_1}}{\partial z_1^{\alpha_1}}
\frac{\partial^{\alpha_2}}{\partial z_2^{\alpha_2}}
\cdots
\frac{\partial^{\alpha_n}}{\partial z_n^{\alpha_n}} ,
&
dz & \overset{\text{def}}{=} dz_1 \wedge dz_2 \wedge \cdots \wedge dz_n ,
\displaybreak[0]\\
\sabs{\alpha} & \overset{\text{def}}{=} \alpha_1 + \alpha_2 + \cdots + \alpha_n ,
&
\alpha! & \overset{\text{def}}{=} \alpha_1!\alpha_2! \cdots \alpha_n! .
\end{align*}
We can also make sense of this notation, especially the notation $z^\alpha$,
if $\alpha \in \Z^n$, that is, if it includes negative integers.
Although usually, $\alpha$ is assumed to be in $\N_0^n$.
Furthermore, when we use $1$ as a vector it means $(1,1,\ldots,1)$.
If $z \in \C^n$, then
\begin{equation*}
1-z = (1-z_1,1-z_2,\ldots,1-z_n) ,
\qquad \text{or} \qquad
z^{\alpha+1} = z_1^{\alpha_1+1}z_2^{\alpha_2+1} \cdots z_n^{\alpha_n+1} .
\end{equation*}
It goes without saying that when using this notation it is
important to be careful to always realize which symbol lives where,
and most of all, to not get carried away.  For
instance, one could interpret $\frac{1}{z}$ in two different ways depending
on if we interpret $1$ as a vector or not, and if we are
expecting a vector or a number.  Best to just keep to the limited set of
cases as given above.

In this notation, the Cauchy formula becomes the perhaps deceptively simple
\begin{equation*}
f(z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta .
\end{equation*}

Let us move to power series.  For simplicity, we start with
power series at the origin.  Using the multinomial notation, we write
such a series as
\begin{equation*}
\sum_{\alpha \in \N_0^n} c_\alpha {z}^\alpha .
\end{equation*}
You must admit that the
above is far nicer to write than, for example, for 3 variables writing
\begin{equation} \label{eq:iteratedsum}
\sum_{j=0}^\infty
\sum_{k=0}^\infty
\sum_{\ell=0}^\infty
c_{jk\ell} z_1^jz_2^kz_3^\ell ,
\end{equation}
which is not even exactly the definition of the series sum (see
below).
When it is clear
from context that we are talking about a power series
and all the powers are nonnegative,
we often write just
\glsadd{not:zalphasum}%
\begin{equation*}
\sum_{\alpha} c_\alpha {z}^\alpha .
\end{equation*}

It is important to note what this means.  The sum does not
have a natural ordering.  We are summing over $\alpha \in \N_0^n$, and there
is no natural ordering of $\N_0^n$.  It makes no sense to
talk about conditional convergence.  When we say the series
\emph{converges}%
\index{power series convergence}%
\index{convergence of power series}, we mean absolutely.
Fortunately, power series converge
absolutely, and so the ordering does not matter.  If you want to write
the limit in terms of partial sums, you pick some ordering of the
multi-indices, $\alpha(1), \alpha(2), \ldots$, and then
\begin{equation*}
\sum_{\alpha}
c_\alpha {z}^\alpha 
=
\lim_{m \to \infty}
\sum_{k=1}^m 
c_{\alpha(k)} {z}^{\alpha(k)} .
\end{equation*}
By the Fubini theorem (for sums) this limit is also equal to the iterated
sum such as \eqref{eq:iteratedsum}.
A power series $\sum_\alpha c_\alpha z^\alpha$
\emph{\myindex{converges uniformly absolutely}}
\index{uniform absolute convergence}
for $z \in X$ when $\sum_\alpha \sabs{c_\alpha z^\alpha}$
converges uniformly for $z \in X$.

The \emph{\myindex{geometric series in several variables}} is the
series $\sum_{\alpha} z^\alpha$.
For
$z \in \D^n$ (unit polydisc),
\begin{equation*}
\begin{split}
\frac{1}{1-z} & =
\frac{1}{(1-z_1)(1-z_2)\cdots(1-z_n)} =
\left(
\sum_{k=0}^\infty {z_1}^k
\right)
\left(
\sum_{k=0}^\infty {z_2}^k
\right)
\cdots
\left(
\sum_{k=0}^\infty {z_n}^k
\right)
\\
& = 
\sum_{k_1=0}^\infty
\sum_{k_2=0}^\infty
\cdots
\sum_{k_n=0}^\infty
\left(
{z_1}^{k_1}
{z_n}^{k_2}
\cdots
{z_n}^{k_n}
\right)
=
\sum_{\alpha} z^\alpha .
\end{split}
\end{equation*}
The series converges uniformly absolutely
on all compact subsets of the unit
polydisc:  Any compact set in the unit
polydisc is contained in a closed polydisc $\overline{\Delta}$
centered at 0 of radius $1-\epsilon$
for some $\epsilon > 0$.  The convergence is uniformly absolute on 
$\overline{\Delta}$.  This claim follows by simply noting the
same fact for each factor is true in one dimension.

Holomorphic functions are precisely those having
a power series expansion:

\begin{thm}
Let $\Delta = \Delta_\rho(a) \subset \C^n$ be a polydisc.
Suppose
$f \colon \overline{\Delta} \to \C$ is a continuous function
holomorphic in $\Delta$.
Then on $\Delta$, $f$ is equal to a power series 
converging uniformly absolutely on compact subsets of $\Delta$:
\begin{equation} \label{holfunc:ps}
f(z) = \sum_{\alpha} c_\alpha {(z-a)}^\alpha .
\end{equation}

Conversely, if $f \colon \Delta \to \C$ is defined by \eqref{holfunc:ps} converging
uniformly absolutely on compact subsets of $\Delta$, then $f$ is holomorphic on
$\Delta$.
\end{thm}

\begin{proof}
Suppose a continuous $f \colon \overline{\Delta} \to \C$ is holomorphic on $\Delta$.
Let
$\Gamma = \partial \Delta_1 \times \cdots \times \partial \Delta_n$
be oriented positively.
Take $z \in \Delta$ and $\zeta \in \Gamma$.
As in one variable, write the Cauchy kernel as
\begin{equation*}
\frac{1}{\zeta-z} =
\frac{1}{\zeta-a}\frac{1}{\left(1-\frac{z-a}{\zeta-a}\right)} =
\frac{1}{\zeta-a}
\sum_{\alpha}
{\left(\frac{z-a}{\zeta-a}\right)}^\alpha .
\end{equation*}
Interpret the formulas as
$\frac{1}{\zeta-z} = \frac{1}{(\zeta_1-z_1) \cdots (\zeta_n-z_n)}$,
$\frac{1}{\zeta-a} = \frac{1}{(\zeta_1-a_1) \cdots (\zeta_n-a_n)}$
and
$\frac{z-a}{\zeta-a} =
\left(
\frac{z_1-a_1}{\zeta_1-a_1}, \ldots,
\frac{z_n-a_n}{\zeta_n-a_n}
\right)$.
The multivariable geometric series is a product of geometric series
in one variable, and geometric series in one variable
are uniformly absolutely convergent on compact subsets of the unit disc.
So the series
above converges uniformly absolutely for $(z,\zeta) \in K \times \Gamma$ 
for every compact subset $K$ of $\Delta$.

Compute,
\begin{equation*}
\begin{split}
f(z)
& =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{\zeta-z}
d \zeta 
%\\
% (this step doesn't work without extra notation in several variables.)
%& =
%\frac{1}{{(2\pi i)}^n}
%\int_{\Gamma}
%\frac{f(\zeta)}{\zeta-a}
%\frac{\zeta-a}{\zeta-z}
%d \zeta 
\\
& =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{\zeta-a}
\sum_{\alpha}
{\left(\frac{z-a}{\zeta-a}\right)}^{\alpha}
d \zeta 
\\
& =
\sum_{\alpha}
\left(
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{\alpha+1}}
\,
d \zeta 
\right)
{(z-a)}^{\alpha} .
\end{split}
\end{equation*}
The last equality follows by Fubini or uniform convergence just as it does in one variable.
%because the convergence of the sum is uniform in
%$\zeta \in \Gamma$ for a fixed $z$.
Uniform absolute convergence (as $z$ moves) on compact subsets of the final
series follows from the
uniform absolute convergence of the geometric series.  It is also a direct
consequence of the Cauchy estimates below.

We have shown that
\begin{equation*}
f(z) =
\sum_{\alpha}
c_{\alpha}
{(z-a)}^{\alpha} ,
\end{equation*}
where
\begin{equation*}
c_\alpha
=
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{\alpha+1}}
\,
d \zeta .
\end{equation*}
Notice how strikingly similar the computation is to one variable.

Let us prove the converse statement.
The limit of the series
is continuous as it is a uniform-on-compact-sets limit of continuous
functions, and hence it is locally bounded in $\Delta$.  Next,
we restrict to each variable in turn, 
\begin{equation*}
z_j \mapsto \sum_{\alpha} c_\alpha {(z-a)}^\alpha .
\end{equation*}
This function is holomorphic via the corresponding one-variable argument.
\end{proof}

The converse statement also follows by applying the Cauchy--Riemann
equations to the series termwise.
We leave that as an exercise.
First, one must show that the term-by-term derivative
series also converges uniformly absolutely on compact subsets.
Then one applies the theorem from real analysis about derivatives
of limits: If a sequence of functions and its derivatives converges
uniformly, then the derivatives converge to the derivative of the limit.

\begin{exbox}
\begin{exercise}
Prove the claim above that if a power series converges uniformly absolutely
on compact subsets of a polydisc $\Delta$, then the term-by-term derivative
converges.
Do the proof without using the analogous result for single variable series.
\end{exercise}
\end{exbox}

A third way to prove the converse statement of the theorem
is to note that partial sums are
holomorphic and write them using the Cauchy formula.  Uniform
convergence shows that the limit also satisfies the Cauchy formula, and
differentiating under the integral obtains the result.

\begin{exbox}
\begin{exercise}
Follow the logic above to prove the converse of the
theorem.
Do the proof without using the analogous result for single variable series.
Hint:
Let
$\Delta'' \subset \Delta' \subset \Delta$
be two polydiscs with the same center $a$
such that $\overline{\Delta''} \subset \Delta'$
and $\overline{\Delta'} \subset \Delta$.
Apply Cauchy formula on $\Delta'$
for $z \in \overline{\Delta''}$.
\end{exercise}
\end{exbox}

We organize some consequences of the theorem
and the calculation in the proof.

\begin{prop}
\pagebreak[2]
Let $\Delta = \Delta_\rho(a) \subset \C^n$ be a polydisc,
and $\Gamma$ its distinguished boundary.
Suppose
$f \colon \overline{\Delta} \to \C$ is a continuous function
holomorphic in $\Delta$.
Then, for $z \in \Delta$,
\begin{equation*}
\frac{\partial^{\sabs{\alpha}}f}{\partial z^\alpha} (z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{\alpha! f(\zeta)}{{(\zeta-z)}^{\alpha+1}}
\,
d \zeta .
\end{equation*}
In particular, if $f$ is given by \eqref{holfunc:ps}, then
\begin{equation*}
c_\alpha = \frac{1}{\alpha!} \frac{\partial^{\sabs{\alpha}}f}{\partial
z^\alpha} (a),
\avoidbreak
\end{equation*}
and we have the \emph{\myindex{Cauchy estimates}}:
\begin{equation*}
\abs{c_\alpha} \leq \frac{\snorm{f}_\Gamma}{\rho^\alpha} .
\end{equation*}
\end{prop}

Namely, the coefficients of the power series depend only on
the derivatives of $f$ at $a$
(and so on the values of $f$ in an arbitrarily small
neighborhood of $a$) and not the specific polydisc used in the theorem.

\begin{proof}
By the Leibniz rule (taking derivatives under the integral),
if $z \in \Delta$ (not on the boundary),
we can differentiate under the integral.  We are talking regular real
partial differentiation, and we use it to apply the Wirtinger operator.
The point is that
\begin{equation*}
\frac{\partial}{\partial z_\ell} \left[
\frac{1}{{(\zeta_\ell-z_\ell)}^k} \right]
=
\frac{k}{{(\zeta_\ell-z_\ell)}^{k+1}} .
\end{equation*}
Let us do a single derivative to
get the idea:
\begin{equation*}
\begin{split}
\frac{\partial f}{\partial z_1}(z) &=
\frac{\partial}{\partial z_1} \left[
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{(\zeta_1-z_1)(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1 
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n 
\right]
\\
& =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{{(\zeta_1-z_1)}^2(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1 
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n .
\end{split}
\end{equation*}
How about we do it a second time:
\begin{equation*}
\frac{\partial^2 f}{\partial z_1^2}(z) 
=
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{2 f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{{(\zeta_1-z_1)}^3(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1 
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n .
\end{equation*}
Notice the 2 before the $f$.  Next derivative, a 3 is coming out.  After $j$
derivatives in $z_1$ you get the constant $j!$.
It is exactly the same thing that happens in one variable.  A moment's
thought will convince you that the following formula is correct for
$\alpha \in \N_0^n$:
\begin{equation*}
\frac{\partial^{\sabs{\alpha}}f}{\partial z^\alpha} (z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{\alpha! f(\zeta)}{{(\zeta-z)}^{\alpha+1}}
\,
d \zeta .
\end{equation*}

Therefore,
\begin{equation*}
\alpha! \, c_\alpha = 
\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha} (a) .
\end{equation*}
We obtain the Cauchy estimates as before:
\begin{equation*}
\abs{\frac{\partial^{\sabs{\alpha}}f}{\partial z^\alpha}(a)}
=
\abs{
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{\alpha! f(\zeta)}{{(\zeta-a)}^{\alpha+1}}
\,
d \zeta }
\leq
\frac{1}{{(2\pi)}^n}
\int_{\Gamma}
\frac{\alpha! \sabs{f(\zeta)}}{\rho^{\alpha+1}}
\,
\sabs{d \zeta}
\leq
\frac{\alpha!}{\rho^\alpha}
\snorm{f}_\Gamma . \qedhere
\end{equation*}
%Or
%\begin{equation*}
%\sabs{c_\alpha} \leq 
%\frac{\snorm{f}_\Gamma}{\rho^\alpha} .
%\end{equation*}
\end{proof}

As in one-variable theory, the Cauchy estimates prove the following
proposition.

\begin{prop}
Let $U \subset \C^n$ be an open set.
Suppose the sequence $f_j \colon U \to \C$ converges uniformly on compact subsets
to $f \colon U \to \C$.  If every $f_j$ is holomorphic, then $f$ is
holomorphic and 
the sequence
$\left\{ \frac{\partial^{\sabs{\alpha}} f_j}{\partial z^\alpha}\right\}$
converges to
$\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha}$ uniformly on compact
subsets.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove the proposition above.
\end{exercise}
\end{exbox}

Let $W \subset \C^n$ be the set where a power series converges
such that it diverges on the complement.  The interior of $W$
is called the
\emph{\myindex{domain of convergence}}.
In one variable, every domain of convergence is a disc, and hence it is
described with a single number (the radius).
In several variables, the domain of convergence
is not as easy to describe.
For the multivariable geometric series 
the domain of convergence is the unit polydisc, but more
complicated examples are easy to find.

\begin{example}
In $\C^2$, the power series
\begin{equation*}
\sum_{k=0}^\infty z_1 z_2^k
\end{equation*}
converges absolutely on the set
\begin{equation*}
\bigl\{ z \in \C^2 : \sabs{z_2} < 1 \bigr\}
\cup
\bigl\{ z \in \C^2 : z_1 = 0 \bigr\} ,
\end{equation*}
and nowhere else.
This set is not quite a polydisc.  It is neither an open set nor a closed set,
and its closure is not the closure of the domain of convergence,
which is the set $\bigl\{ z \in \C^2 : \sabs{z_2} < 1 \bigr\}$.
\end{example}

\begin{example}
The power series
\begin{equation*}
\sum_{k=0}^\infty z_1^k z_2^k
\end{equation*}
converges absolutely exactly on the set
\begin{equation*}
\bigl\{ z \in \C^2 : \sabs{z_1 z_2} < 1 \bigr\} .
\end{equation*}
The picture is definitely more complicated than a polydisc.
See \figureref{fig:convergence-example-2}.
\begin{myfig}
\subimport*{figures/}{convergence-example-2.pdf_t}
\caption{Domain of convergence of $\sum_k z_1^kz_2^k$.\label{fig:convergence-example-2}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Find the domain of convergence of $\sum_{j,k} \frac{1}{k!} z_1^jz_2^k$
and draw the corresponding picture.
\end{exercise}

\begin{exercise}
Find the domain of convergence of $\sum_{j,k} c_{j,k} z_1^jz_2^k$
and draw the corresponding picture if $c_{k,k} = 2^k$, $c_{0,k} = c_{j,0} =
1$ and $c_{j,k} = 0$ otherwise.
\end{exercise}

\begin{exercise}
Suppose a power series in two variables can be written
as a sum of a power series in $z_1$ and a power series in $z_2$.
Show that the domain of convergence is a polydisc.
\end{exercise}
\end{exbox}

Suppose $U \subset \C^n$ is a domain such that if $z \in U$
and
$\sabs{z_k} = \sabs{w_k}$ for all $k$, then $w \in U$.
Such a $U$ is called a
\emph{\myindex{Reinhardt domain}}.  The domains we were drawing so far are
Reinhardt domains.
They are exactly the domains that you can
draw by plotting what happens for the moduli of the variables.
A domain is called a
\emph{\myindex{complete Reinhardt domain}}\index{Reinhardt domain!complete} if whenever
$z \in U$, then for $r = (r_1,\ldots,r_n)$ where $r_k = \sabs{z_k}$
for all $k$, we have that the whole polydisc $\overline{\Delta_r(0)} \subset U$.
So a complete Reinhardt domain is a union (possibly infinite) of polydiscs
centered at the origin.

\begin{exbox}
\begin{exercise}
Let $W \subset \C^n$ be the set where a certain
power series at the origin converges.
Show that the interior of $W$ is a complete Reinhardt domain.
\end{exercise}
\end{exbox}

\begin{thm}[Identity theorem\index{identity theorem}]
Let $U \subset \C^n$ be a domain (connected open set) and let
$f \colon U \to \C$ be holomorphic.  If 
$f|_N \equiv 0$ for a nonempty open
subset $N \subset U$,
then $f \equiv 0$.
\end{thm}

\begin{proof}
Let $Z$ be the set where all derivatives of $f$ are zero; then
$N \subset Z$, so $Z$ is nonempty.  The set $Z$ is closed in $U$
as all derivatives are continuous.
Take an arbitrary $a \in Z$.
Expand $f$
in a power series around $a$ converging to $f$ in a polydisc
$\Delta_\rho(a) \subset U$. 
As the coefficients are given by derivatives 
of $f$, the power series is the zero series.  Hence, $f$ is
identically zero in $\Delta_\rho(a)$.  Therefore, $Z$ is open. 
As $Z$ is also closed and nonempty, and $U$ is connected, we have
$Z = U$.
\end{proof}

The theorem is often used to show that 
if two holomorphic functions $f$ and $g$ 
are equal on a small open set,
then $f \equiv g$.

\begin{thm}[Maximum principle\index{maximum principle}]
Let $U \subset \C^n$ be a domain.
Let $f \colon U \to \C$ be holomorphic and suppose $\sabs{f(z)}$
attains a local maximum at some $a \in U$.  Then $f \equiv f(a)$.
\end{thm}

\begin{proof}
Suppose $\sabs{f(z)}$ attains a local maximum at $a \in U$.  Consider a polydisc
$\Delta = \Delta_1 \times \cdots \times \Delta_n \subset U$
centered at $a$.  The function
\begin{equation*}
z_1 \mapsto f(z_1,a_2,\ldots,a_n) 
\end{equation*}
is holomorphic
on the disc $\Delta_1$ and its modulus attains the maximum
at the center.  Therefore, it is constant by maximum principle in one variable,
that is, $f(z_1,a_2,\ldots,a_n)  = f(a)$ for all $z_1 \in \Delta_1$.  For
any fixed $z_1
\in \Delta_1$, consider the function
\begin{equation*}
z_2 \mapsto f(z_1,z_2,a_3,\ldots,a_n)  .
\end{equation*}
This function, holomorphic on the disc $\Delta_2$, again attains its maximum modulus at the center of $\Delta_2$
and hence is constant on $\Delta_2$.  Iterating this procedure we obtain
that $f(z) = f(a)$ for all $z \in \Delta$.  The identity theorem says
that $f(z) = f(a)$ for all $z \in U$.
\end{proof}

\begin{exbox}
\begin{exercise} \label{exercise:averageDelta}
Let $V$ be the volume measure on $\R^{2n}$ and hence on $\C^n$.
Suppose $\Delta$ centered at $a \in \C^n$, and $f$ is a function holomorphic on
a neighborhood of $\overline{\Delta}$.  Prove
\begin{equation*}
f(a) =
\frac{1}{V(\Delta)}
\int_{\Delta} f(\zeta) \, dV(\zeta) ,
\end{equation*}
where $V(\Delta)$ is the volume of $\Delta$ and $dV$ is the volume measure.
That is, $f(a)$ is an average of the values on a polydisc centered at $a$.
\end{exercise}

\begin{exercise}
Prove the maximum principle by using the Cauchy formula instead.  (Hint:
Use the previous exercise)
\end{exercise}

\begin{exercise}
Prove a several variables analogue of the \emph{\myindex{Schwarz's lemma}}:
Suppose $f$ is holomorphic in a neighborhood of $\overline{\D^n}$,
$f(0) = 0$, and for some $k \in \N$ we have
$\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha} (0) =
0$ whenever $\sabs{\alpha} < k$.  Further suppose 
for all $z \in \D^n$,
$\sabs{f(z)} \leq M$ for some $M$.  Show that
\begin{equation*}
\sabs{f(z)} \leq M \snorm{z}^k
\qquad
\text{for all $z \in \overline{\D^n}$}.
\end{equation*}
\end{exercise}

\begin{exercise}
Apply the one-variable Liouville's theorem to prove it for several variables.
That is, suppose $f \colon \C^n \to \C$ is holomorphic and bounded.
Prove $f$ is constant.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Improve Liouville's theorem slightly in $\C^2$.
A complex line though the origin is
the image of a linear map $L \colon \C \to \C^n$.
\begin{exparts}
\item
Prove that 
for any collection of finitely many complex lines through the origin,
there exists an entire nonconstant holomorphic function ($n \geq 2$)
bounded (hence constant) on these complex lines.
\item
Prove that if an entire holomorphic function in $\C^2$ is bounded on
countably many distinct
complex lines through the origin, then it is constant.
\item
Find a nonconstant entire holomorphic function in $\C^3$ that is
bounded on
countably many distinct
complex lines through the origin.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]
Prove the several variables version of \emph{\myindex{Montel's theorem}}:
Suppose $\{ f_k \}$ is a uniformly bounded sequence of holomorphic functions on an open set $U
\subset \C^n$.
Show that there exists a subsequence $\{ f_{k_j} \}$ that converges
uniformly on compact subsets to some holomorphic function $f$.
Hint: Mimic the one-variable proof.
\end{exercise}

\begin{exercise}
Prove a several variables version of \emph{\myindex{Hurwitz's theorem}}:
Suppose $\{ f_k \}$ is a sequence of nowhere zero
holomorphic functions on a domain $U
\subset \C^n$ converging uniformly on compact subsets to a function $f$.
Show that either $f$ is identically zero, or that $f$ is nowhere zero.
Hint: Feel free to use the \hyperref[thm:onevarhurwitz]{one-variable result}.
\end{exercise}

\begin{exercise}
Suppose $p \in \C^n$ is a point and $D \subset \C^n$ is a ball centered at
$p \in D$.
A holomorphic function $f \colon D \to \C$ can be
\emph{analytically continued}\index{analytic continuation}
along a path
$\gamma \colon [0,1] \to \C^n$, $\gamma(0) = p$, if for every $t \in [0,1]$ there exists
a ball $D_t$ centered at $\gamma(t)$, where $D_0 = D$, and a holomorphic function
$f_t \colon D_t \to \C$, where $f_0 = f$, and for each $t_0 \in [0,1]$ there is an
$\epsilon > 0$ such that if $\sabs{t-t_0} < \epsilon$, then $f_t = f_{t_0}$
in $D_t \cap D_{t_0}$.
Prove a several variables version of the \emph{\myindex{Monodromy theorem}}:
If $U \subset \C^n$ is a simply connected domain, $D \subset U$ a ball and
$f \colon D \to \C$ a holomorphic function that can be analytically
continued from $p \in D$ to every $q \in U$, then there exists
a unique holomorphic function $F \colon U \to \C$ such that $F|_D = f$.
\end{exercise}
\end{exbox}

Let us define notation for the set of holomorphic functions.
At the same time, we notice that the set of holomorphic functions is
a commutative ring under pointwise addition and multiplication.

\begin{defn}
Let $U \subset \C^n$ be an open set.
\glsadd{not:O}%
Define $\sO(U)$ to be the \emph{\myindex{ring of holomorphic functions}}.
The letter $\sO$ is used to recognize the fundamental contribution to
several complex variables by
Kiyoshi Oka\footnote{%
See \url{https://en.wikipedia.org/wiki/Kiyoshi_Oka}}.
\end{defn}


\begin{exbox}
\begin{exercise}
Prove that $\sO(U)$ is actually a commutative ring with the operations
\begin{equation*}
(f+g)(z) = f(z)+g(z), \qquad (fg)(z) = f(z)g(z) .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $\sO(U)$ is an \myindex{integral domain} (has no zero divisors) if and only
if $U$ is connected.  
That is, show that $U$ being connected is equivalent to showing that
if $h(z) = f(z)g(z)$ is identically zero for $f,g \in \sO(U)$,
then either $f$ or $g$ is
identically zero.
\end{exercise}
\end{exbox}

A function $F$ defined on a dense open subset of $U$ is
\emph{\myindex{meromorphic}} if locally near every $p \in U$,
$F = \nicefrac{f}{g}$ for $f$ and $g$ holomorphic in some neighborhood of
$p$.
We remark that it follows from a deep result
of Oka that
for domains $U \subset \C^n$, every meromorphic function can be represented
as $\nicefrac{f}{g}$ globally.  That is, the ring of meromorphic functions
is the field of fractions of $\sO(U)$.
This problem is the so-called
\emph{\myindex{Poincar\'e problem}}, and its solution is no longer positive
once we generalize $U$ to complex manifolds.
The points of $U$ through which $F$ does not extend holomorphically are called the
\emph{\myindex{poles}} of $F$.
Namely, poles are the points where
$g=0$ for every possible representation $\nicefrac{f}{g}$.
Unlike in one variable, in several variables poles are never isolated
points.
There is also a new type of singular
point for meromorphic functions in more than one
variable:

\begin{exbox}
\begin{exercise}
In two variables one can no longer think of a meromorphic function $F$ simply
taking on the value of $\infty$, when the denominator vanishes.  Show that $F(z,w) = \nicefrac{z}{w}$ achieves all values of $\C$
in every neighborhood of the origin.  The origin is called
a \emph{\myindex{point of indeterminacy}}.
\end{exercise}

\begin{exercise} \label{exercise:noisolatedzeros}
Prove that zeros (and so poles as we will see later, though this is a bit harder to see)
are never isolated in $\C^n$ for $n \geq 2$.
Hint: Consider $z_1 \mapsto f(z_1,z_2,\ldots,z_n)$ as you move
$z_2,\ldots,z_n$ around, and use perhaps
\hyperref[thm:onevarhurwitz]{Hurwitz}.
\end{exercise}
\end{exbox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Derivatives}

Given a function $f = u+iv$, the complex conjugate is $\bar{f} = u-iv$,
defined simply by $z \mapsto \overline{f(z)}$.
When $f$ is holomorphic, then $\bar{f}$ is
called an \emph{\myindex{antiholomorphic function}}.  An antiholomorphic function is a
function that depends on $\bar{z}$ but not on $z$.
So if we write the variable, we write $\bar{f}$ as
$\bar{f}(\bar{z})$.  Let us see why this makes sense.  Using the definitions
of the Wirtinger operators,
\begin{equation*}
\frac{\partial \bar{f}}{\partial z_j} = 
\overline{\frac{\partial f}{\partial \bar{z}_j}} = 0,
\qquad
\frac{\partial \bar{f}}{\partial \bar{z}_j} = \overline{
\left(\frac{\partial f}{\partial z_j} \right) } ,
\qquad
\text{for all $j=1,\ldots,n$.}
\end{equation*}

For functions that are neither holomorphic or antiholomorphic, we
pretend they depend on both $z$ and $\bar{z}$.
Since we want to write functions in terms of $z$ and $\bar{z}$,
let us figure out how the chain rule works for Wirtinger derivatives,
rather than writing derivatives in terms of $x$ and $y$.

\begin{prop}[Complex chain rule]\index{complex chain rule}
Suppose 
$U \subset \C^n$ and $V \subset \C^m$ are open sets and suppose
$f \colon U \to V$, and $g \colon V \to \C$ are (real) differentiable
functions (mappings).  Write the variables as
$z = (z_1,\ldots,z_n) \in U \subset \C^n$ and $w = (w_1,\ldots,w_m) \in V
\subset \C^m$.  Then for $j=1,\ldots,n$,
\begin{equation} \label{eq:chainrule}
\begin{aligned}
&
\frac{\partial}{\partial z_j} \left[ g \circ f \right]
=
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial w_\ell}
\frac{\partial f_\ell}{\partial z_j}
+
\frac{\partial g}{\partial \bar{w}_\ell}
\frac{\partial \bar{f}_\ell}{\partial z_j}
\right),
\\
&
\frac{\partial}{\partial \bar{z}_j} \left[ g \circ f \right]
=
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial w_\ell}
\frac{\partial f_\ell}{\partial \bar{z}_j}
+
\frac{\partial g}{\partial \bar{w}_\ell}
\frac{\partial \bar{f}_\ell}{\partial \bar{z}_j}
\right) .
\end{aligned}
\end{equation}
\end{prop}

\begin{proof}
Write $f = u+iv$, $z = x+iy$, $w=s+it$,
$f$ is a function of $z$, and $g$ is a function of $w$.
The composition plugs in $f$ for $w$, and so it plugs in $u$ for $s$, and
$v$ for $t$.
Using the standard chain rule,
\begin{equation*}
\begin{split}
\frac{\partial}{\partial z_j} \left[ g \circ f \right]
& =
\frac{1}{2}
\left(
\frac{\partial}{\partial x_j} - i
\frac{\partial}{\partial y_j}
\right)
 \left[ g \circ f \right]
\\
& = 
\frac{1}{2}
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial s_\ell} \frac{\partial u_\ell}{\partial x_j}
+
\frac{\partial g}{\partial t_\ell} \frac{\partial v_\ell}{\partial x_j}
-
i
\left(
\frac{\partial g}{\partial s_\ell} \frac{\partial u_\ell}{\partial y_j}
+
\frac{\partial g}{\partial t_\ell} \frac{\partial v_\ell}{\partial y_j}
\right)
\right)
\\
& = 
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial s_\ell}
\,
\frac{1}{2}
\left(
\frac{\partial u_\ell}{\partial x_j}
-
i
\frac{\partial u_\ell}{\partial y_j}
\right)
+
\frac{\partial g}{\partial t_\ell}
\,
\frac{1}{2}
\left(
\frac{\partial v_\ell}{\partial x_j}
-
i
\frac{\partial v_\ell}{\partial y_j}
\right)
\right)
\\
& = 
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial s_\ell}
\frac{\partial u_\ell}{\partial z_j}
+
\frac{\partial g}{\partial t_\ell}
\frac{\partial v_\ell}{\partial z_j}
\right) .
\end{split}
\end{equation*}
For $\ell = 1, \ldots, m$,
\begin{equation*}
\frac{\partial}{\partial s_\ell}
=
\frac{\partial}{\partial w_\ell}
+
\frac{\partial}{\partial \bar{w}_\ell} ,
\qquad
\frac{\partial}{\partial t_\ell}
=
i \left(
\frac{\partial}{\partial w_\ell}
-
\frac{\partial}{\partial \bar{w}_\ell}
\right) .
\end{equation*}
Continuing:
\begin{equation*}
\begin{split}
\frac{\partial}{\partial z_j} \left[ g \circ f \right]
& = 
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial s_\ell}
\frac{\partial u_\ell}{\partial z_j}
+
\frac{\partial g}{\partial t_\ell}
\frac{\partial v_\ell}{\partial z_j}
\right)
\\
& = 
\sum_{\ell=1}^m \left(
\left(
\frac{\partial g}{\partial w_\ell}
\frac{\partial u_\ell}{\partial z_j}
+
\frac{\partial g}{\partial \bar{w}_\ell}
\frac{\partial u_\ell}{\partial z_j}
\right)
+
i
\left(
\frac{\partial g}{\partial w_\ell}
\frac{\partial v_\ell}{\partial z_j}
-
\frac{\partial g}{\partial \bar{w}_\ell}
\frac{\partial v_\ell}{\partial z_j}
\right)
\right)
\\
& = 
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial w_\ell}
\left(
\frac{\partial u_\ell}{\partial z_j}
+
i
\frac{\partial v_\ell}{\partial z_j}
\right)
+
\frac{\partial g}{\partial \bar{w}_\ell}
\left(
\frac{\partial u_\ell}{\partial z_j}
-i
\frac{\partial v_\ell}{\partial z_j}
\right)
\right)
\\
& = 
\sum_{\ell=1}^m \left(
\frac{\partial g}{\partial w_\ell}
\frac{\partial f_\ell}{\partial z_j}
+
\frac{\partial g}{\partial \bar{w}_\ell}
\frac{\partial \bar{f}_\ell}{\partial z_j}
\right) .
\end{split}
\end{equation*}

The $\bar{z}$ derivative works similarly.
\end{proof}

Because of the proposition,
when we deal with a possibly
nonholomorphic function $f$, we often write $f(z,\bar{z})$ and treat $f$ as
a function of $z$ and $\bar{z}$.

\begin{remark}
It is good to notice the subtlety of what we just said.  Formally it seems
as if we are treating $z$ and $\bar{z}$ as independent variables when taking
derivatives, but in reality they are not independent if we actually wish to
evaluate the function.  Under the hood, a smooth function that is not
necessarily holomorphic is really a function of the real variables
$x$ and $y$, where $z = x+iy$.
\end{remark}

\begin{remark}
Another remark is that we could have swapped $z$ and $\bar{z}$, by
flipping the bars everywhere.  There is no difference between the two,
they are twins in effect.  We just need to know which one is which.
After all, it all starts with taking the two square roots of $-1$ and
deciding which one is $i$ (remember the chickens?).
There is no ``natural choice'' for that, but once
we make that choice we must be consistent.  And once we picked which
root
is $i$, we also picked what is holomorphic and what is
antiholomorphic.  This is a subtle philosophical as much as a mathematical point.
\end{remark}

\begin{defn}
Let $U \subset \C^n$ be open.  A mapping $f \colon U \to \C^m$
is said to be \emph{holomorphic}\index{holomorphic mapping}
if each component is holomorphic.  That
is, if $f = (f_1,\ldots,f_m)$, then each $f_j$ is a holomorphic function.
\end{defn}

As in one variable, the composition of holomorphic functions (mappings) is
holomorphic.

\begin{thm}
Let $U \subset \C^n$ and $V \subset \C^m$ be open sets and suppose
$f \colon U \to V$ and $g \colon V \to \C^k$ are both holomorphic.
\glsadd{not:composition}%
Then the composition $g \circ f$ is holomorphic.
\end{thm}

\begin{proof}
The proof is almost trivial by chain rule.
Again let $g$ be a function of $w \in V$ and $f$ be a function of $z \in U$.
For any $j = 1,\ldots,n$ and any $p=1,\ldots,k$, we compute
\begin{equation*}
\frac{\partial}{\partial \bar{z}_j} \left[ g_p \circ f \right]
=
\sum_{\ell=1}^m 
\left(
\frac{\partial g_p}{\partial w_\ell} 
\cancelto{0}{\frac{\partial f_\ell}{\partial \bar{z}_j}}
+
\cancelto{0}{\frac{\partial g_p}{\partial \bar{w}_\ell}}
\frac{\partial \bar{f}_\ell}{\partial \bar{z}_j} 
\right)
=
0 . \qedhere
\end{equation*}
\end{proof}

For holomorphic functions the chain rule simplifies, and it formally looks
like the familiar vector calculus rule.
Suppose again
$U \subset \C^n$ and $V \subset \C^m$ are open, and 
$f \colon U \to V$ and $g \colon V \to \C$ are holomorphic.
Name the variables
$z = (z_1,\ldots,z_n) \in U \subset \C^n$ and $w = (w_1,\ldots,w_m) \in V
\subset \C^m$.  In formula \eqref{eq:chainrule} for the $z_j$ derivative,
the $\bar{w}_j$ derivative of $g$ is zero and the $z_j$ derivative of
$\bar{f}_\ell$ is also zero because $f$ and $g$ are holomorphic.
Therefore, for any $j=1,\ldots,n$,
\begin{equation*}
\frac{\partial}{\partial z_j} \left[ g \circ f \right]
=
\sum_{\ell=1}^m 
\frac{\partial g}{\partial w_\ell}
\frac{\partial f_\ell}{\partial z_j} .
\end{equation*}

\begin{exbox}
\begin{exercise}
Prove using only the Wirtinger derivatives that a holomorphic function
that is real-valued must be constant.
\end{exercise}

\begin{exercise}
Let $f$ be a holomorphic function on $\C^n$.
When we write $\bar{f}$ we mean the function $z \mapsto \overline{f(z)}$,
and we usually write $\bar{f}(\bar{z})$ as the function is antiholomorphic.
However,
if we write $\bar{f}(z)$ we really mean $z \mapsto \overline{f(\bar{z})}$,
that is, composing both the function and the argument with conjugation.
Prove $z \mapsto \bar{f}(z)$ is holomorphic, and prove $f$ is
real-valued on $\R^n$ (when $y=0$) if and only if $f(z) =
\bar{f}(z)$ for all $z \in \C$.
\end{exercise}
\end{exbox}

For a $U \subset \C^n$, a holomorphic mapping $f \colon U \to \C^m$,
and a point $p \in U$,
define the holomorphic derivative, sometimes called the
\emph{\myindex{Jacobian matrix}},
\glsadd{not:Df}%
\begin{equation*}
Df(p)
\overset{\text{def}}{=}
\left[
\frac{\partial f_j}{\partial z_k} (p)
\right]_{jk} .
\end{equation*}
The notation $f'(p) = Df(p)$ is also used.

\begin{exbox}
\begin{exercise}
Suppose $U \subset \C^n$ is open, $\R^n$ is naturally embedded in $\C^n$.
Consider a holomorphic mapping $f \colon U \to \C^m$ and suppose that
$f|_{U \cap \R^n}$ maps into $\R^m \subset \C^m$.  Prove that given
$p \in U \cap \R^n$, the real
Jacobian matrix at $p$ of the map
$f|_{U \cap \R^n} \colon U \cap \R^n \to \R^m$ is equal to the holomorphic
Jacobian matrix of the map $f$ at $p$.  In particular, $Df(p)$ is a matrix
with real entries.
\end{exercise}
\end{exbox}

By the holomorphic chain rule above, as in the theory of real functions,
the derivative of the composition is the composition of derivatives
(multiplied as matrices).

\begin{prop}[Chain rule for holomorphic mappings\index{chain rule for holomorphic mappings}]
Let $U \subset \C^n$ and $V \subset \C^m$ be open sets.  Suppose
$f \colon U \to V$ and $g \colon V \to \C^k$ are both holomorphic,
and $p \in U$.  Then
\begin{equation*}
D(g \circ f)(p) = Dg\bigl(f(p)\bigr) \, Df(p) .
\end{equation*}
\end{prop}

In shorthand, we often simply write $D(g \circ f) = Dg Df$.

\begin{exbox}
\begin{exercise}
Prove the proposition.
\end{exercise}
\end{exbox}

Suppose $U \subset \C^n$, $p \in U$, and $f \colon U \to \C^m$
is a differentiable function at $p$.
Since $\C^n$ is identified with $\R^{2n}$, the mapping $f$
takes $U \subset \R^{2n}$ to $\R^{2m}$.  The normal vector calculus Jacobian at $p$
of this mapping (a $2m \times 2n$ real matrix) is called the
\emph{\myindex{real Jacobian}}, and we write it as
\glsadd{not:DRf}%
$D_\R f (p)$.

\begin{prop}
Let $U \subset \C^n$ be an open set, $p \in U$, and 
$f \colon U \to \C^n$ be holomorphic.  Then
\begin{equation*}
\abs{\det D f(p) }^2 = 
\det D_\R f(p) .
\end{equation*}
\end{prop}

The expression $\det D f(p)$ is called the \emph{\myindex{Jacobian
determinant}} and clearly it is important to know if we are talking about
the holomorphic Jacobian determinant or the standard real Jacobian
determinant $\det D_\R f(p)$.  Recall from vector calculus that
if the real Jacobian determinant $\det D_\R
f(p)$ of a smooth mapping is positive, then the mapping preserves
orientation.  In particular, the proposition
says that holomorphic mappings preserve orientation.

\begin{proof}
The real mapping using our identification is
$(\Re f_1,\Im f_1, \ldots, \Re f_n, \Im f_n)$
as a function of $(x_1,y_1,\ldots,x_n,y_n)$.
The statement is about the two Jacobians at $p$, that is, the derivatives
at $p$.  Hence, we can assume that
$p=0$ and $f$ is complex linear, $f(z) = Az$ for some $n \times n$
matrix $A$.  It is just a statement about matrices.
The matrix $A$ is the (holomorphic) Jacobian matrix of $f$.
Let $B$ be the real Jacobian matrix of $f$.

Let us change the basis of $B$ to be $(z,\bar{z})$
using $z = x+iy$ and $\bar{z}=x-iy$
on both the target and the source.
The change of basis is some invertible
complex matrix $M$ such that
$M^{-1} B M$ (the real Jacobian matrix $B$ in these new coordinates)
is a matrix 
of the derivatives of
$(f_1,\ldots,f_n,\bar{f}_1,\ldots,\bar{f}_n)$
in terms of
$(z_1,\ldots,z_n,\bar{z}_1,\ldots,\bar{z}_n)$.
In other words,
\begin{equation*}
M^{-1} B M =
\begin{bmatrix}
A & 0 \\
0 & \widebar{A}
\end{bmatrix} .
\end{equation*}
Thus
\begin{multline*}
\det (B) =
\det(M^{-1}M B)
=
\det(M^{-1} B M)
\\
=
\det(A) \det(\widebar{A})
=
\det(A) \, \overline{\det(A)}
=
\abs{\det(A)}^2 .  \qedhere
\end{multline*}
\end{proof}

The regular implicit function theorem and the chain rule
give that the implicit function theorem holds in the holomorphic setting.
The main thing to check is to check that the solution given by the
standard implicit function theorem is holomorphic, which follows by the
chain rule.

\begin{thm}[Implicit function theorem\index{implicit function
theorem}\index{holomorphic implicit function theorem}] \label{thm:ift}
Let $U \subset \C^{n} \times \C^{m}$ be an open set, let  $(z,w) \in \C^n \times
\C^m$ be our coordinates, and let $f \colon U \to \C^m$
be a holomorphic mapping.  Let $(z^0,w^0) \in U$ be a point such that
$f(z^0,w^0) = 0$ and such that the $m \times m$ matrix
\begin{equation*}
\left[
\frac{\partial f_j}{\partial w_k} (z^0,w^0)
\right]_{jk}
\end{equation*}
is invertible.
Then there exists an
open set $V \subset \C^n$ with $z^0 \in V$,
open set $W \subset \C^m$ with $w^0 \in W$,
$V \times W \subset U$,
and
a holomorphic
mapping $g \colon V \to W$, with $g(z^0) = w^0$
such that
for every $z \in V$, the point $g(z)$ is the unique point in $W$
such that
\begin{equation*}
f\bigl(z,g(z)\bigr) = 0 .
\end{equation*}
\end{thm}

\begin{exbox}
\begin{exercise}
Prove the holomorphic implicit function theorem above.
Hint: Check that the normal implicit function theorem for $C^1$
functions applies, and then show that the $g$ you obtain is holomorphic.
\end{exercise}

\begin{exercise}
State and prove a holomorphic version of the inverse function theorem.
\end{exercise}
\end{exbox}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inequivalence of ball and polydisc}

\begin{defn}
Two domains $U \subset \C^n$ and $V \subset \C^n$ are said to be
\emph{\myindex{biholomorphic}} or
\emph{\myindex{biholomorphically equivalent}}
if there exists a one-to-one and onto holomorphic map $f
\colon U \to V$ such that the inverse
\glsadd{not:inverse}%
$f^{-1} \colon V \to U$ is holomorphic.
The mapping $f$ is said to be a
\emph{biholomorphic map}\index{map!biholomorphic} or a 
\emph{\myindex{biholomorphism}}.
\end{defn}

As function theory on two biholomorphic domains is the same,
one of the main questions in complex analysis is to classify domains up
to biholomorphic transformations.  In one variable, there is the rather
striking theorem due to Riemann:

\begin{thm}[Riemann mapping theorem\index{Riemann mapping theorem}]
If $U \subset \C$ is a nonempty simply connected domain such that $U \neq \C$,
then $U$ is biholomorphic to $\D$.
\end{thm}

In one variable, a topological property on $U$ is enough to classify a whole
class of domains.  It is one of the reasons why studying the disc is so
important in one variable, and why many theorems are stated for
the disc only.
There is simply no such theorem in several variables.
We will show momentarily that the unit ball and the polydisc,
\begin{equation*}
\bB_n = \bigl\{ z \in \C^n : \snorm{z} < 1 \bigr\}
\qquad \text{and} \qquad
\D^n = \bigl\{ z \in \C^n : \sabs{z_j} < 1 ~\text{for $j=1,\ldots,n$} \bigr\} ,
\end{equation*}
are \emph{not} biholomorphically equivalent.  Both are simply
connected (have no holes), and they are the two most obvious generalizations
of the disc to several variables.  They are homeomorphic, that is, topology
does not see any difference.

\begin{exbox}
\begin{exercise}
Prove that there exists a \emph{\myindex{homeomorphism}} $f \colon \bB_n \to
\D^n$,
that is, $f$ is a bijection, and both $f$ and $f^{-1}$ are continuous.
\end{exercise}
\end{exbox}


Let us stick with $n=2$.
Instead of proving that $\bB_2$ and
$\D^2$ are biholomorphically 
inequivalent we will prove a stronger theorem.  First a
definition.

\begin{defn}
Suppose $f \colon X \to Y$ is a continuous map between two topological
spaces.  Then $f$ is a \emph{\myindex{proper map}}\index{map!proper} if for every compact
\glsadd{not:compact}%
\glsadd{not:pullback}%
$K \subset \subset Y$, the set $f^{-1}(K)$ is compact.
\end{defn}

The notation ``$\subset \subset$'' is a common notation for relatively
compact subset, that is, the closure is compact in the relative (subspace)
topology.  Often the distinction between compact and relatively
compact is not important.  For instance, in the definition above we can replace
compact with relatively compact.
So the notation is sometimes used if ``compact'' is meant.

Vaguely, ``proper'' means that ``boundary goes to the boundary.''
As a continuous map, $f$ pushes compacts to compacts; a proper map is
one where the inverse does so too.  If the inverse is a continuous
function, then clearly $f$ is proper,
but not every proper map is invertible.  For
example, the map $f \colon \D \to \D$ given by $f(z) = z^2$ is proper, but
not invertible.  The codomain of $f$ is important.  If we 
replace $f$ by $g \colon \D \to \C$, still given by $g(z)=z^2$,
then the map is
no longer proper.  Let us state the main result of this section.

\begin{thm}[Rothstein 1935] \label{thm:Rothstein}
\index{Rothstein's theorem}
There exists no proper holomorphic mapping of the unit bidisc $\D^2 = \D \times \D
\subset \C^2$ to the unit ball $\bB_2 \subset \C^2$.
\end{thm}

As a biholomorphic mapping is proper,
the unit bidisc is not biholomorphically
equivalent to the unit ball in $\C^2$.  This fact was first proved by
Poincar\'e by computing the automorphism groups of $\D^2$ and $\bB_2$,
although his proof
assumed the maps extended past the boundary.  The first
complete proof was by Henri Cartan in 1931, though popularly the theorem is
attributed to Poincar\'e.  It seems standard practice that any general audience talk
about several complex variables contains a mention of Poincar\'e,
and often the reference is to this exact theorem.

We need some lemmas before we get to the proof of the result.  First,
a certain one-dimensional object plays an important role in the geometry
of several complex variables.  It allows us to apply one-variable
results in several variables.  It is especially important in
understanding the boundary behavior of holomorphic functions.  It also
prominently appears in complex geometry.

\begin{defn}
A nonconstant holomorphic mapping
$\varphi \colon \D \to \C^n$ is called an \emph{\myindex{analytic disc}}.
If the mapping $\varphi$ extends continuously to the closed unit disc
$\overline{\D}$, then the mapping
$\varphi \colon \overline{\D} \to \C^n$ is called
a \emph{\myindex{closed analytic disc}}.

Often we call the image $\Delta = \varphi(\D)$ the analytic disc
rather than the mapping.  For a closed analytic disc we write
$\partial \Delta = \varphi( \partial \D)$ and call it the boundary
of the analytic disc.
\end{defn}

In some sense, analytic discs play the role of line segments in $\C^n$.  It
is important to always have in mind that there is a mapping defining the
disc, even if we are more interested in the set.  Obviously for a given
image, the mapping $\varphi$ is not unique.

Let us consider the boundaries of 
the unit bidisc $\D \times \D \subset \C^2$
and the unit ball $\bB_2 \subset \C^2$.  We notice the boundary
of the unit bidisc contains analytic discs $\{p\} \times \D$
and $\D \times \{p\}$ for $p \in \partial \D$.  That is, through
every point in the boundary, except for the distinguished
boundary $\partial \D \times \partial \D$, there exists an analytic disc
lying entirely inside the boundary.  On the other hand, the ball
contains no analytic discs in its boundary.

\begin{prop}
\glsadd{not:unitsphere}%
The unit sphere $S^{2n-1} = \partial \bB_n \subset \C^n$ 
contains no analytic discs.
\end{prop}

\begin{proof}
Suppose there is a holomorphic function $g \colon \D \to \C^n$
such that the image $g(\D)$ is inside the unit sphere.  In other words,
\begin{equation*}
\snorm{g(z)}^2 = \sabs{g_1(z)}^2 + \sabs{g_2(z)}^2 + \cdots + \sabs{g_n(z)}^2 = 1
\end{equation*}
for all $z \in \D$.  Without loss of generality (after composing with a
unitary matrix) assume that
$g(0) = (1,0,0,\ldots,0)$.  Consider the first component
and notice that $g_1(0) = 1$.  If a sum of
positive numbers is less than or equal to 1,
then they all are, and hence $\sabs{g_1(z)} \leq 1$.  Maximum principle
says
that $g_1(z) = 1$ for all $z \in \D$.  But then $g_j(z) = 0$
for all $j=2,\ldots,n$ and all $z \in \D$.  Therefore, $g$ is constant and
thus not an analytic disc.
\end{proof}

The fact that the sphere contains no analytic discs
is the most important geometric distinction between the boundary of
the polydisc and the sphere.

\begin{exbox}
\begin{exercise}
Modify the proof to show some stronger results.
\begin{exparts}
\item
Let $\Delta$ be an analytic disc
and $\Delta \cap \partial \bB_n \not= \emptyset$.
Prove $\Delta$ contains points not in
$\overline{\bB_n}$.
\item
Let $\Delta$ be an analytic disc.
Prove that $\Delta \cap \partial \bB_n$ is nowhere dense in $\Delta$.
\item
Find an analytic disc in $\C^2$, such that $(1,0) \in \Delta$, $\Delta \cap \bB_2 =
\emptyset$, and 
locally near
$(1,0) \in \partial \bB_2$, the set
$\Delta \cap \partial \bB_2$ is the
curve defined by $\Im z_1=0$, $\Im z_2=0$,
${(\Re z_1)}^2+ {(\Re z_2)}^2 = 1$.
\end{exparts}
\end{exercise}
\end{exbox}

Before we prove the theorem, let us prove a lemma making the statement about
proper maps taking boundary to boundary precise.

\begin{lemma} \label{lemma:bndrytobndry}
Let $U \subset \R^n$ and $V \subset \R^m$ be bounded domains and
let $f \colon U \to V$ be continuous.
Then $f$ is proper if and only if
for every sequence $\{ p_k \}$ in $U$ such that $p_k \to p \in \partial U$,
the set of limit points of $\bigl\{ f(p_k) \bigr\}$ lies in $\partial V$.
\end{lemma}

\begin{proof}
Suppose $f$ is proper.  Take a 
sequence $\{ p_k \}$ in $U$ such that $p_k \to p \in \partial U$.
Then take any convergent subsequence $\bigl\{ f(p_{k_j}) \bigr\}$ of
$\bigl\{ f(p_k) \bigr\}$
converging to some $q \in \widebar{V}$.  Consider
$E = \bigl\{ f(p_{k_j}) \bigr\}$ as a set.  Let $\widebar{E}$ be the closure of $E$
in $V$ (subspace topology).  If $q \in V$, then $\widebar{E} = E \cup \{ q
\}$ and $\widebar{E}$ is compact.
Otherwise, if $q \notin V$, then
$\widebar{E} = E$.
The inverse image $f^{-1}(\widebar{E})$
is not compact (it contains a sequence going to $p \in \partial U$)
and hence $\widebar{E}$ is not
compact either as $f$ is proper.  Thus $q \notin V$, and hence $q \in
\partial V$.  As we took an arbitrary subsequence of $\bigl\{ f(p_k) \bigr\}$, $q$ was
an arbitrary limit point.  Therefore, all limit points are in $\partial V$.

Let us prove the converse.
Suppose that for every sequence
$\{ p_k \}$ in $U$ such that $p_k \to p \in \partial U$,
the set of limit points of $\bigl\{ f(p_k) \bigr\}$ lies in $\partial V$.
Take a closed set $E \subset V$ (subspace topology) and look at $f^{-1}(E)$.  If $f^{-1}(E)$
is not compact, then there exists a sequence $\{ p_k \}$ in $f^{-1}(E)$
such that $p_k \to p \in \partial U$.  That is because $f^{-1}(E)$ is closed
(in $U$) but not compact.  The hypothesis then says that the limit points of
$\bigl\{ f(p_k) \bigr\}$ are in $\partial V$.  Hence $E$ has limit points in
$\partial V$ and is not compact.
\end{proof}

\begin{exbox}
\begin{exercise}
Let $U \subset \R^n$ and $V \subset \R^m$ be bounded domains and
let $f \colon \widebar{U} \to \widebar{V}$ be continuous.
Suppose $f(U) \subset V$, and $g \colon U \to V$ is defined by
$g(x) = f(x)$ for all $x \in U$.
Prove that $g$ is proper if and only if $f(\partial U) \subset \partial V$.
\end{exercise}

\begin{exercise}
Let $f \colon X \to Y$ be a continuous function of locally compact Hausdorff topological spaces.
Let $X_\infty$ and $Y_\infty$ be the 
one-point compactifications of $X$ and $Y$.
Then $f$ is a proper map if and only if it extends as a continuous map
$f_\infty \colon X_\infty \to Y_\infty$ by letting
$f_\infty |_X = f$ and
 $f_\infty(\infty) = \infty$.
\end{exercise}
\end{exbox}

We now have all the lemmas needed to prove the theorem of Rothstein.

\begin{proof}[Proof of \thmref{thm:Rothstein}]
Suppose there is a proper holomorphic map $f \colon \D^2
\to \bB_2$.
Fix some $e^{i\theta}$ in the boundary of the disc $\D$.  Take a sequence
$w_k \in \D$ such that $w_k \to e^{i\theta}$.   The functions
$g_k(\zeta) =  f(\zeta,w_k)$ map the unit disc into $\bB_2$.  By the standard
\hyperref[thm:onevarmontel]{Montel's theorem}, by passing to a subsequence we assume that
the sequence of functions converges (uniformly on compact subsets) to
a limit $g \colon \D \to \overline{\bB}_2$.  As $(\zeta,w_k) \to
(\zeta,e^{i\theta}) \in \partial \D^2$, then by
\lemmaref{lemma:bndrytobndry} we have that $g(\D) \subset \partial \bB_2$
and hence $g$ must be constant.

Let $g_k'$ denote the derivative (we differentiate each component).
The functions $g_k'$ converge to $g' = 0$.
So for an arbitrary fixed $\zeta \in \D$,
$\frac{\partial f}{\partial z_1} (\zeta, w_k) \to 0$.
This limit holds for all $e^{i\theta}$ and some subsequence of
an arbitrary sequence $\{ w_k \}$ where $w_k \to e^{i\theta}$.  The
holomorphic mapping $w \mapsto \frac{\partial f}{\partial z_1} (\zeta, w)$,
therefore, extends continuously
to the closure $\overline{\D}$ and is zero on $\partial \D$.
We apply the maximum
principle or the Cauchy formula and the fact that $\zeta$ was arbitrary to find 
$\frac{\partial f}{\partial z_1} \equiv 0$.  By symmetry
$\frac{\partial f}{\partial z_2} \equiv 0$.  Therefore, $f$ is constant,
which is a contradiction as $f$ was proper.

The proof is illustrated in \figureref{fig:rothstein}.
In the picture, on the left-hand side is the bidisc, and we
restrict $f$ to the horizontal gray lines (where the second component is
fixed to be $w_k$) and take a limit to produce an analytic disc
in the boundary of $\bB_2$.  We then show that $\frac{\partial f}{\partial
z_1} = 0$ on the vertical gray line (where the first component is fixed to
be $\zeta$).  The right-hand side shows the disc where $z_1 = \zeta$ is
fixed, which corresponds to the vertical gray line on the left.
\begin{myfig}
\subimport*{figures/}{rothstein.pdf_t}
\caption{The proof of Rothstein's theorem.\label{fig:rothstein}}
\end{myfig}
\end{proof}

The proof says that the reason why there is not even a proper mapping is the fact
that the boundary of the polydisc contains analytic discs, while
the sphere does not.
The proof extends easily to higher dimensions as well, and the proof
of the generalization is left as an exercise.

\begin{thm} \label{thm:nopropmapprodandnodisc}
Let $U = U' \times U'' \subset \C^n \times \C^k$, $n,k \geq 1$, and $V
\subset \C^m$, $m \geq 1$, be bounded
domains such that $\partial V$ contains no analytic discs.
Then there exist no proper
holomorphic mapping $f \colon U \to V$.
\end{thm}

\begin{exbox}
\begin{exercise}
Prove \thmref{thm:nopropmapprodandnodisc}.
\end{exercise}
\end{exbox}

The key takeaway from this section is that
in several variables, to see if two domains are equivalent,
the geometry 
of the boundaries makes a difference, not just the topology
of the domains.

\medskip

The following is a fun exercise in one dimension about proper maps of discs:

\begin{exbox}
\begin{exercise}
Let $f \colon \D \to \D$ be a proper holomorphic  map.  Then
\begin{equation*}
f(z) = 
e^{i\theta} \prod_{k=1}^m \frac{z-a_k}{1-\bar{a}_k z} ,
\end{equation*}
for some real $\theta$ and some $a_k \in \D$ (that is, $f$ is a finite
Blaschke product).  Hint: Consider $f^{-1}(0)$.
\end{exercise}
\end{exbox}

In several variables, when $\D$ is replaced by a ball,
this question (what are the proper maps)
becomes far more involved, and if the dimensions of the balls are
different, it is not solved in general.

\begin{exbox}
\begin{exercise}
Suppose $f \colon U \to \D$ be a proper holomorphic map where $U \subset
\C^n$ is a nonempty domain.  Prove that $n=1$.  Hint: Consider the same idea as in
\exerciseref{exercise:noisolatedzeros}.
\end{exercise}

\begin{exercise}
Suppose $f \colon \overline{\bB_n} \to \C^m$ is a nonconstant continuous
map such that $f|_{\bB_n}$ is holomorphic and $\snorm{f(z)} = 1$ whenever
$\snorm{z}=1$.  Prove that 
$f|_{\bB_n}$ maps into $\bB_m$ and furthermore that this map is proper.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Cartan's uniqueness theorem}

The following theorem is another analogue of Schwarz's lemma to
several variables.  It says that for a bounded domain, it is enough to know
that a self mapping is the identity at a single point to show that it is the
identity everywhere.  As there are quite a few theorems named for Cartan,
this one is often referred to as the
\emph{\myindex{Cartan's uniqueness theorem}}.  It is useful in
computing the automorphism groups of certain domains.
An \emph{\myindex{automorphism}} of $U$ is a biholomorphic map from $U$ onto $U$.
Automorphisms form a group under composition, called the
\emph{\myindex{automorphism group}}.  As 
exercises,
you will use the theorem to compute the automorphism groups of $\bB_n$ and $\D^n$.

\begin{thm}[Cartan]
Suppose $U \subset \C^n$ is a bounded domain, $a \in U$, $f \colon U \to U$ is a
holomorphic mapping, $f(a) = a$, and $Df(a)$ is the identity.  Then
$f(z) = z\,$
for all $z \in U$.
\end{thm}

\begin{exbox}
\begin{exercise}
Find a counterexample to the theorem if $U$ is unbounded.  Hint: For simplicity take
$a=0$ and $U=\C^n$.
\end{exercise}
\end{exbox}


Before we get into the proof, let us write down the Taylor
series of a function in a nicer way, splitting it up into parts of different
degree.

A polynomial $P \colon \C^n \to \C$ is \emph{\myindex{homogeneous}} 
of degree $d$ if
\begin{equation*}
P(s z) = s^d P(z)
\end{equation*}
for all $s \in \C$ and $z \in \C^n$.
A homogeneous polynomial of degree $d$ is a polynomial whose
every monomial
is of total degree $d$.  For instance, $z^2w-iz^3+9zw^2$ is homogeneous of
degree 3 in the variables $(z,w) \in \C^2$.
A polynomial vector-valued mapping is homogeneous of degree $d$,
if each component is.
If $f$ is holomorphic near $a \in \C^n$, then
write the power series of $f$ at $a$ as
\begin{equation*}
\sum_{j=0}^{\infty} f_j(z-a) ,
\end{equation*}
where $f_j$ is a homogeneous polynomial of degree $j$.  The $f_j$ is 
called the
\emph{\myindex{degree $j$ homogeneous part}}\index{homogeneous part} of $f$
at $a$.  The $f_j$ would be vector-valued if $f$ is vector-valued, such as in the statement of the theorem.
In the proof, we will require the vector-valued
Cauchy estimates (exercise below)\footnote{The normal Cauchy estimates 
could also be used in the proof of Cartan by applying them
componentwise.}.

\begin{exbox}
\begin{exercise}
Prove a vector-valued version of the Cauchy estimates.  Suppose $f
\colon \overline{\Delta_r(a)} \to \C^m$ is continuous function holomorphic
on a polydisc $\Delta_r(a) \subset \C^n$.  Let $\Gamma$ denote the distinguished
boundary of $\Delta$.  Show that for any multi-index $\alpha$ we get
\begin{equation*}
\norm{\frac{\partial^{\abs{\alpha}}f}{\partial z^\alpha} (a)}
\leq
\frac{\alpha!}{r^\alpha} \sup_{z\in \Gamma} \norm{f(z)} .
\end{equation*}
\end{exercise}
\end{exbox}



\begin{proof}[Proof of Cartan's uniqueness theorem]
Without loss of generality, assume $a=0$.  Write $f$ 
as a power series at the origin, written in homogeneous parts:
\begin{equation*}
f(z) = z + f_k(z) + \sum_{j=k+1}^\infty f_j(z) ,
\end{equation*}
where $k \geq 2$ is an integer such that $f_2(z),f_3(z),\ldots,f_{k-1}(z)$ is zero.
The degree-one homogeneous part is simply the vector $z$,
because
the derivative of $f$ at the origin is the identity.
Compose $f$ with itself $\ell$ times:
\begin{equation*}
f^\ell(z) = \underbrace{f \circ f \circ \cdots \circ f}_{\ell\text{ times}}
(z) .
\end{equation*}
As $f(U) \subset U$, then $f^\ell$ is a holomorphic map
of $U$ to $U$.  As $U$ is bounded, there is an $M$ such that $\snorm{z} \leq
M$ for all $z \in U$.  Therefore, $\snorm{f(z)} \leq M$ for all $z \in U$, and
$\snorm{f^\ell(z)} \leq M$ for all $z \in U$.

If we plug in $z + f_k(z) + {}$higher order terms
into $f_k$, we get $f_k(z) + {}$some other higher order terms.
Therefore, $f^2(z) = z + 2 f_k(z) + {}$higher order terms.
Continuing this procedure,
\begin{equation*}
f^\ell(z) = z + \ell f_k(z) + \sum_{j=k+1}^\infty \tilde{f}_j(z) ,
\end{equation*}
for some other degree $j$ homogeneous polynomials $\tilde{f}_j$.  Suppose $\Delta_r(0)$ is a polydisc whose
closure is in $U$.
Via Cauchy estimates,
for any multinomial $\alpha$ with $\sabs{\alpha}=k$,
\begin{equation*}
\frac{\alpha!}{r^\alpha} M
\geq
\norm{\frac{\partial^{\sabs{\alpha}} f^\ell}{\partial z^\alpha}(0)}
=
\ell
\norm{\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha}(0)} .
\end{equation*}
The inequality holds for all $\ell \in \N$, and so
$\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha}(0) = 0$.  Therefore,
$f_k \equiv 0$.  On the domain of convergence of the expansion,
we get $f(z) = z$, as there is no other
nonzero homogeneous part in the expansion of $f$.  As $U$ is connected,
then the identity theorem says $f(z) = z$ for all $z \in U$.
\end{proof}

As an application, let us classify all biholomorphisms of all bounded
circular domains that fix a point.
A \emph{\myindex{circular domain}} is a domain
$U \subset \C^n$ such that if $z \in U$, then $e^{i\theta} z \in U$ for
all $\theta \in \R$.

\begin{cor}
Suppose $U, V \subset \C^n$ are bounded circular domains with $0 \in U$, $0 \in
V$, and $f \colon U \to V$ is
a biholomorphic map such that $f(0) = 0$.  Then $f$ is linear.
\end{cor}

For example, $\bB_n$ is circular and bounded.  So a biholomorphism of $\bB_n$
(an automorphism)
that fixes the origin is linear.  Similarly, a polydisc centered at zero is
also circular and bounded.

\begin{proof}
The map $g(z) = f^{-1}\bigl(e^{-i\theta}f(e^{i\theta} z)\bigr)$ is an
automorphism of $U$ and via the
chain rule, $g'(0) = I$.  Therefore,
$f^{-1}\bigl(e^{-i\theta}f(e^{i\theta} z)\bigr) = z$, or in other words
\begin{equation*}
f(e^{i\theta} z) = e^{i\theta}f(z) .
\end{equation*}
Write $f$ near zero as $f(z) = \sum_{j=1}^\infty f_j(z)$ where $f_j$ are
homogeneous polynomials of degree $j$ (notice $f_0 = 0$).  Then
\begin{equation*}
\sum_{j=1}^\infty e^{i\theta} f_j(z) 
=
e^{i\theta} \sum_{j=1}^\infty f_j(z) 
=
\sum_{j=1}^\infty f_j(e^{i\theta} z)
=
\sum_{j=1}^\infty e^{ij\theta}f_j(z) .
\end{equation*}
By the uniqueness of the Taylor expansion, 
$e^{i\theta} f_j(z)  = e^{ij\theta} f_j(z)$, or
$f_j(z)  = e^{i(j-1)\theta} f_j(z)$,
for all $j$, all $z$, and all $\theta$.
If $j\not=1$, we obtain that $f_j \equiv
0$, which proves the claim.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that every automorphism $f$ of $\D^n$ (that is, a biholomorphism $f \colon \D^n \to \D^n$)
is given as
\begin{equation*}
f(z) = P \left(
e^{i\theta_1} \frac{z_1-a_1}{1-\bar{a}_1z_1} ,
e^{i\theta_2} \frac{z_2-a_2}{1-\bar{a}_2z_2} , \ldots,
e^{i\theta_n} \frac{z_n-a_n}{1-\bar{a}_nz_n} \right)
\end{equation*}
for $\theta \in \R^n$, $a \in \D^n$, and
a permutation matrix $P$.
\end{exercise}

\begin{exercise}
Given $a \in \bB_n$, define the linear map $P_a z =
\frac{\linnprod{z}{a}}{\linnprod{a}{a}}a$ if $a \not= 0$ and $P_0z = 0$.
Let $s_a = \sqrt{1-\snorm{a}^2}$.  Show that every automorphism $f$ of
$\bB_n$ (that is, a biholomorphism $f \colon \bB_n \to \bB_n$)
can be written as
\begin{equation*}
f(z) = U \frac{a-P_az-s_a(I-P_a)z}{1-\linnprod{z}{a}}
\end{equation*}
for a unitary matrix $U$ and some $a \in \bB_n$.
\end{exercise}

\begin{exercise}
Using the previous two exercises, show that $\D^n$ and $\bB_n$, $n \geq 2$,
are not biholomorphic via a method more in the spirit of what Poincar\'e
used: Show that the groups of automorphisms of the two domains are
different groups when $n \geq 2$.
\end{exercise}

\begin{exercise} \label{exercise:boundedeigen}
Suppose $U \subset \C^n$ is a bounded open set, $a \in U$, and $f \colon U \to U$ is a
holomorphic mapping such that $f(a) = a$.  Show that every eigenvalue
$\lambda$ of the matrix $Df(a)$ satisfies $\sabs{\lambda} \leq 1$.
\end{exercise}

\begin{exercise}[Tricky]
Find a domain $U \subset \C^n$ such that the only biholomorphism $f \colon U
\to U$ is the identity $f(z) = z$.  Hint: Take the polydisc (or the ball)
and remove some number of points (be careful in how you choose them).  Then
show that $f$ extends to a biholomorphism of the polydisc.  Then see what
happens to those points you took out.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Show that Cartan's uniqueness theorem is not true in the real case,
even for rational
functions.  That is, find a rational function $R(t)$ of
a real variable $t$, such that $R$ that takes $(-1,1)$ to
$(-1,1)$, $R'(0) = 1$, and $R(t)$ is not the identity.  You can even make
$R$ bijective.
\item
Show that also \exerciseref{exercise:boundedeigen} is not true in the real
case.
For any $\alpha \in \R$ find a rational function $R(t)$ of
a real variable $t$, such that $R$ takes $(-1,1)$ to $(-1,1)$ and
$R'(0) = \alpha$.
\end{exparts}
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set, $a \in U$,
$f \colon U \to U$ is a holomorphic mapping,
$f(a) = a$, and suppose that $\sabs{\lambda} < 1$
for every eigenvalue $\lambda$ of
$D f(a)$.  Prove that there exists a neighborhood $W$ of $a$, such that
$\lim_{\ell \to \infty} f^{\ell}(z) = a$ for all $z \in W$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Riemann extension, zero sets, and injective maps}
\label{sec:riemannextzerosetsinjmaps}

In one dimension if a function is holomorphic in $U
\setminus \{ p \}$ and
locally bounded\index{locally bounded in $U$}\footnote{%
$f \colon U \setminus X \to \C$ is locally bounded in $U$
if for every $p \in U$, there is a neighborhood $W$ of
$p$ such that $f$ is bounded on $W \cap (U \setminus X)$.}
in $U$, in particular bounded near
$p$, then the function extends holomorphically to $U$ (see
\propref{prop:onevarclassifysing} \ref{prop:onevarclassifysing:i}).  In several
variables the same theorem holds, and the analogue of a single point
is the zero set of a holomorphic function.

\begin{thm}[\myindex{Riemann extension theorem}]
Let $U \subset \C^n$ be a domain,  $g \in \sO(U)$, and $g$ is not
identically zero.  Let
$N = g^{-1}(0)$ be the zero set of $g$.
If 
$f \in \sO(U \setminus N)$
is locally bounded in $U$,
then there exists a unique $F \in \sO(U)$ such that $F|_{U \setminus N} = f$.
\end{thm}

The proof is an application of the Riemann extension theorem from one dimension.

\begin{proof}
Take any $p \in N$, and let $L$ be a complex line through $p$.
That is,
$L$ is an image of an affine mapping
$\varphi \colon \C \to \C^n$ defined by
$\varphi(\xi) = a\xi + p$, for a vector $a \in \C^n$.
The composition $g \circ \varphi$
is a holomorphic function of one variable, and it
is either identically zero, or
the zero at $\xi=0$ is isolated.
The function $g$ is not identically zero in any neighborhood of $p$.
So there is some line $L$ such that $g \circ \varphi$
is not identically zero, or in other words, $p$
is an isolated point of $L \cap N$.

Write $z' =
(z_1,\ldots,z_{n-1})$ and $z=(z',z_n)$.
Without loss of generality $p = 0$, and $L$ is the line
obtained by $z' = 0$.
So $g \circ \varphi$ is $\xi \mapsto g(0,\xi)$.
There is some small
$r > 0$ such that $g$ is nonzero on the set
given by $\sabs{z_n} = r$ and $z' = 0$.
By continuity,
$g$ is nonzero on the set given by
$\sabs{z_n} = r$ and $\snorm{z'} <\epsilon$ for some $\epsilon >0$.
In particular, for any fixed small $s \in \C^{n-1}$, with $\snorm{s} < \epsilon$,
setting $z' = s$,
the zeros of $\xi \mapsto g(s,\xi)$ are isolated.  See
\figureref{fig:riemann-ext-zeros}.

\begin{myfig}
\subimport*{figures/}{riemann-ext-zeros.pdf_t}
\caption{Good neighborhood of the origin with respect to
the zero set of $g$.\label{fig:riemann-ext-zeros}}
\end{myfig}

For $\snorm{z'} <
\epsilon$ and $\abs{z_n} < r$, write
\begin{equation*}
F(z',z_n) =
\frac{1}{2\pi i}
\int_{\sabs{\xi}=r} \frac{f(z',\xi)}{\xi-z_n} \,d\xi .
\end{equation*}
The function $\xi \to f(z',\xi)$ extends holomorphically to the entire
disc of radius $r$ by the Riemann extension from one dimension.  By Cauchy
integral formula,
$F$ is equal to $f$ at the points where they are both defined.
By differentiating under the integral, the function $F$ is holomorphic
in all variables.

In a neighborhood of each point of 
$N$, $f$ extends to a continuous (holomorphic in fact) function.
A continuous extension of $f$ must be unique
on the closure of
$U \setminus N$ in the subspace topology,
$\overline{(U \setminus N)} \cap U$.  The set $N$ has empty interior,
so $\overline{(U \setminus N)} \cap U = U$.  Hence, $F$ is the unique
continuous extension of $f$ to $U$.
\end{proof}

\begin{exbox}
\begin{exercise}
Let $F$ be a meromorphic function on an open set $U \subset \C^n$.  Show
that if $p \in U$ is a pole (near $p$, $F=\nicefrac{f}{g}$, and $F$ does not
extend through $p$), then there exists a sequence $\{ p_k \}$
converging to $p$ such that $F(p_k) \to \infty$.  Namely, $F$ is
unbounded near $p$.
\end{exercise}
\end{exbox}

The set of zeros of a holomorphic function has a nice structure at most
points.

\begin{thm} \label{thm:regptsdense}
Let $U \subset \C^n$ be a domain and
$f \in \sO(U)$ and $f$ is not identically zero.
Let $N = f^{-1}(0)$.  Then there exists a open and dense (subspace topology) subset
$N_{\mathit{reg}} \subset N$
such that near each $p \in N_{\mathit{reg}}$, after possibly reordering variables,
$N$ can be locally written as
\begin{equation*}
z_n = g(z_1,\ldots,z_{n-1})
\end{equation*}
for a holomorphic function $g$.
\end{thm}

\begin{proof}
If $N$ is locally a graph at $p$, then it is a graph
for every point of $N$ near $p$.  So $N_{\mathit{reg}}$
is open.
If for every point $p_0 \in N$
and every neighborhood $W$ of $p_0$, we show that
$N \cap W$ has a regular point, then $N_{\mathit{reg}}$ is dense.
Replacing $N$ with $N \cap W$, it thus suffices to show
$N_{\mathit{reg}}$ is nonempty.

Since $f$ is not identically zero, then not all derivatives (of arbitrary
order) of $f$
vanish identically on $N$.
If some first order derivative of $f$ does not vanish identically on $N$,
let $h=f$.
Otherwise, suppose $k$ is such that a derivative of $f$ of order $k$
does not vanish identically on $N$, and
all derivatives of $f$ order less than $k$ vanish identically on
$N$.  Let $h$ be one of the derivatives of order $k-1$.
We obtain a function $h \colon U \to \C$, holomorphic, vanishing on $N$,
and such that 
without loss of generality the $z_n$ derivative does not vanish identically
on $N$.  Then there is some point $p \in N$ such that $\frac{\partial
h}{\partial z_n}(p) \not= 0$.
We apply the implicit function theorem at $p$ to find $g$ such that
\begin{equation*}
h\bigr(z_1,\ldots,z_{n-1},g(z_1,\ldots,z_{n-1})\bigr) = 0 ,
\end{equation*}
and $z_n = g(z_1,\ldots,z_{n-1})$ is the unique solution to
$h=0$ near $p$.

The zero set of $h$ contains $N$, the zero set of $f$.
We must show equality near $p$.  That is, we need to show that
near $p$, every zero of $h$ is also a zero of $f$.
Write $p = (p',p_n)$.  Then the function
\begin{equation*}
\xi \mapsto f(p',\xi)
\end{equation*}
has an isolated zero in a small disc $\Delta$ around $p_n$ and is
nonzero on the circle $\partial \Delta$.  By
\hyperref[thm:onevarrouche]{Rouch\'e's theorem},
$\xi \mapsto f(z',\xi)$ must have a zero for all $z'$ sufficiently close to $p'$
(close enough to make $\sabs{f(q',\xi)-f(z',\xi)} < \sabs{f(q',\xi)}$ for all $\xi \in
\partial \Delta$).
Since $g(z')$ is the unique solution $z_n$ to $h(z',z_n) = 0$
near $p$ and the
zero set of $f$ is contained in the zero set of $h$, we are done.
\end{proof}

The zero set $N$ of a holomorphic function is a so-called
\emph{\myindex{subvariety}}
or an \emph{\myindex{analytic set}}
although the general definition of a
subvariety is more complicated, and includes more sets.
See \chapterref{ch:analyticvarieties}.
Points where $N$ is a graph of a holomorphic mapping are called
\emph{regular points}\index{regular point}, and we write them as
$N_{\mathit{reg}}$ as above.  In particular,
since $N$ is a graph of a single holomorphic function, they are called
regular points of (complex) dimension $n-1$, or (complex) codimension 1.
The set of regular points is what is called an
$(n-1)$-dimensional \emph{\myindex{complex submanifold}}.  It is also a real
submanifold of real dimension $2n-2$.
The points on a subvariety that are not regular are called
\emph{singular points}\index{singular point}.

\begin{example}
For $U = \C^2$,
let $f(z) = z_1^2-z_2^2$ and consider $X = f^{-1}(0)$. As $\nabla f =
(2z_1,2z_2)$,
outside of the origin, we can solve for $z_1$ or $z_2$ and so
all points of $X \setminus 0$ are regular.  In fact,
$z_1 = z_2$ and $z_1 = -z_2$ are the two possibilities.
In no neighborhood of the origin, however, is there a way to solve for either
$z_1$ or $z_2$, since you always get two possible solutions:  If you could
solve $z_1 = g(z_2)$, then both $z_2 = g(z_2)$ and $-z_2 = g(z_2)$ must be
true, a contradiction for any nonzero $z_2$.  Similarly,
we cannot solve for $z_1$.
So the origin is a singular point.

To see that you may have need to use derivatives of the function, notice
that the function $\varphi(z) = {(z_1^2-z_2^2)}^2$ has the same zero set $X$,
but both
$\frac{\partial \varphi}{\partial z_1}$ and
$\frac{\partial \varphi}{\partial z_2}$ vanish on $X$.  Using
$h= \frac{\partial \varphi}{\partial z_1}$ or
$h= \frac{\partial \varphi}{\partial z_2}$ in the proof will work.

It may be also good to consider a function such as $f(z) = z_1^2z_2$ to see
that we do not always get the entire set of regular functions with one
neighborhood $W$.  The
zero set is where $z_1=0$ or where $z_2=0$, and all points except the origin
are regular.  We use $h=f$ to show that points outside the origin
where $z_2=0$ are regular.
To show that points outside the origin
where $z_1=0$ are regular, we must restrict to some neighborhood $W$
of those points and use $h = \frac{\partial f}{\partial z_1}$.
\end{example}

\begin{example}
The theorem is not true in the nonholomorphic setting.  The set
where $x_1^2 +  x_2^2 = 0$ in $\R^2$ is only the origin, clearly not a graph
of any function of one variable.  The first part of the theorem works, but
the $h$ you find is either $2x_1$ or $2x_2$, and its zero set is too big.
\end{example}

\begin{exbox}
\begin{exercise}
Find all the regular points of the subvariety
$X = \bigl\{ z \in \C^2 : z_1^2 = z_2^3 \bigr\}$.
Hint: The trick is showing that you've found all of them.
\end{exercise}

\begin{exercise} \label{exercise:connectedcomplement}
Suppose $U \subset \C^n$ is a domain and $f \in \sO(U)$.
Show that the complement of the zero set, $U \setminus f^{-1}(0)$, is
connected.
\end{exercise}
\end{exbox}

Let us now prove that a one-to-one holomorphic
mapping is biholomorphic, a result definitely not true in the
smooth setting: $x \mapsto x^3$ is smooth, one-to-one, onto map
of $\R$ to $\R$, but the inverse is not differentiable.

\begin{thm} \label{thm:injective}
Suppose $U \subset \C^n$ is an open set and $f \colon U \to \C^n$ is
holomorphic and one-to-one.  Then the Jacobian determinant is never equal to zero 
on $U$.

In particular, if a holomorphic map $f \colon U \to V$ is
one-to-one and onto for two open sets $U,V \subset \C^n$, then $f$ is
biholomorphic.
\end{thm}

The function $f$ is locally biholomorphic, in particular
$f^{-1}$ is holomorphic,
on the set where the Jacobian determinant $J_f$, that is the determinant
\begin{equation*}
J_f(z) = \det Df(z) = \det \left[ \frac{\partial f_j}{\partial z_k}(z)
\right]_{jk} ,
\end{equation*}
is not zero.  This follows from the inverse function theorem, which is just
a special case of the implicit function theorem.
The trick is to show that $J_f$ happens to be nonzero
everywhere.

In one complex dimension, every holomorphic function $f$ can, in
the proper local holomorphic coordinates (and up to adding a constant),
be written as $z^d$ for $d=0,1,2,\ldots$:
Near a $z_0 \in \C$,
there exists a constant $c$ and a local biholomorphic $g$
with $g(z_0) = 0$ such that
$f(z) = c + {\bigl( g(z) \bigr)}^d$.
Such a simple result
does not hold in several variables in general, but if the mapping is
locally one-to-one, then the present theorem says that such a mapping can be
locally written as the identity.

\begin{proof}[Proof of the theorem]
We proceed by induction.  We know the theorem for $n=1$.
Suppose $n > 1$ and suppose we know the theorem is true for dimension $n-1$.

Suppose for contradiction that $J_f = 0$ somewhere.
First suppose that $J_f$ is not identically zero.
Find a regular point $q$ on the zero set of $J_f$.
Write the zero set of $J_f$ near $q$ as
\begin{equation*}
z_n = g(z_1,\ldots,z_{n-1})
\end{equation*}
for some holomorphic $g$.
If we prove the theorem near $q$, we are done.  Without loss of generality
assume $q=0$.  The biholomorphic (near the origin) map
\begin{equation*}
\Psi(z_1,\ldots,z_n) = \bigl(z_1,z_2,\ldots,z_{n-1},z_n-g(z_1,\ldots,z_{n-1}) \bigr)
\end{equation*}
takes the zero set of $J_f$ to the set given by $z_n=0$.  By considering
$f \circ \Psi^{-1}$ instead of $f$, we may assume
that $J_f = 0$ on the set given by $z_n=0$.  We may also
assume that $f(0) = 0$.

If $J_f$ vanishes identically, then there is no need to do anything other
than a translation.  In either case,
we may assume that $0 \in U$, $f(0)=0$, and
$J_f = 0$ when $z_n=0$.

We wish to show that all the derivatives of $f$ in the $z_1,\ldots,z_{n-1}$
variables vanish whenever $z_n = 0$.  This
would clearly contradict $f$ being one-to-one,
as $f(z_1,\ldots,z_{n-1},0)$ would be constant.
So for any point on $z_n=0$ we consider one of the components
of $f$ and one of the derivatives of that component.
Without loss of generality, suppose the point is 0, and
for contradiction suppose
$\frac{\partial f_1}{\partial z_1}(0) \not= 0$.
The map
\begin{equation*}
G(z_1,\ldots,z_n) = \bigl(f_1(z),z_2,\ldots,z_n\bigr)
\end{equation*}
is biholomorphic on a small neighborhood of the origin.
The function $f \circ G^{-1}$ is holomorphic and one-to-one on a small
neighborhood.  By the definition of $G$,
\begin{equation*}
f \circ G^{-1} (w_1,\ldots,w_n) = \bigl(w_1,h(w)\bigr) ,
\end{equation*}
where $h$ is a holomorphic mapping taking a neighborhood of the
origin in $\C^n$ to $\C^{n-1}$.
The mapping
\begin{equation*}
\varphi(w_2,\ldots,w_n) = h(0,w_2,\ldots,w_n)
\end{equation*}
is a one-to-one holomorphic mapping of a neighborhood of the origin in
$\C^{n-1}$ to $\C^{n-1}$.  By the induction hypothesis, the Jacobian determinant of
$\varphi$ is nowhere zero.

If we differentiate $f \circ G^{-1}$, we notice 
$D(f \circ G^{-1}) = Df \circ D(G^{-1})$.
So at the origin
\begin{equation*}
\det D(f \circ G^{-1}) = \bigl(\det Df\bigr) \bigl(\det D(G^{-1})\bigr) = 0.
\end{equation*}
We obtain a contradiction, as at the origin
\begin{equation*}
\det 
D(f \circ G^{-1})
= \det D\varphi \not= 0 . \qedhere
\end{equation*}
\end{proof}

The theorem is no longer true if the domain and range dimensions of the
mapping are not equal.

\begin{exbox}
\begin{exercise}
Take the subvariety
$X = \bigl\{ z \in \C^2 : z_1^2 = z_2^3 \bigr\}$.
Find a one-to-one holomorphic mapping $f \colon \C \to X$.
Then note that the derivative of $f$ vanishes at a certain point.
So \thmref{thm:injective} has no analogue when the domain and range have
different dimension.
\end{exercise}

\begin{exercise}
Find a continuous function $f \colon \R \to \R^2$ that is one-to-one but
such that the inverse $f^{-1} \colon f(\R) \to \R$ is not continuous.
\end{exercise}
\end{exbox}

\pagebreak[1]
This is an appropriate place to state a well-known and as yet unsolved conjecture (and most
likely ridiculously hard to solve):
the \emph{\myindex{Jacobian conjecture}}.
This conjecture is a converse to the 
theorem above in a special case:
\emph{Suppose $F \colon \C^n \to \C^n$ is a polynomial map (each component is a
polynomial) and the Jacobian derivative $J_F$ is never zero, then $F$ is
invertible with a polynomial inverse.}
Clearly $F$ would be locally one-to-one, but proving (or
disproving)
the existence of a global polynomial inverse is the content of the conjecture.

\begin{exbox}
\begin{exercise}
Prove the Jacobian conjecture for $n=1$.  That is, prove that if
$F \colon \C \to \C$ is a polynomial such that $F'$ is never zero,
then $F$ has an inverse, which is a polynomial.
\end{exercise}

\begin{exercise}
Let $F \colon \C^n \to \C^n$ be an injective polynomial map.
Prove $J_F$ is a nonzero constant.
\end{exercise}

\begin{exercise}
Prove that the Jacobian conjecture is false if
``polynomial'' is replaced with ``entire holomorphic,'' even for $n=1$.
\end{exercise}

\begin{exercise}
Prove that if a holomorphic $f \colon \C \to \C$ is injective, then it is
onto, and therefore $f(z) = az + b$ for $a \not= 0$.
\end{exercise}
\end{exbox}

Let us also remark that while every injective holomorphic
map of $f \colon \C \to \C$ is onto, the same is not true in higher
dimensions.
In $\C^n$, $n \geq 2$, there exist so-called
\emph{Fatou--Bieberbach domains}\index{Fatou--Bieberbach domain},
that is, proper subsets of $\C^n$ that are biholomorphic to $\C^n$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Convexity and pseudoconvexity} \label{ch:convexity}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Domains of holomorphy and holomorphic extensions}

It turns out that not every domain in $\C^n$ is a natural domain for
holomorphic functions.

\begin{defn} \label{defn:domainofhol}
Let $U \subset \C^n$ be a domain\footnote{\emph{Domain of
holomorphy} can make sense for disconnected sets (not domains), and some authors
do define it so.}
(connected open set).  The set $U$ is
a \emph{\myindex{domain of holomorphy}} if there do not exist
nonempty open sets
$V$ and $W$, with $V \subset U \cap W$, $W \not\subset U$, and $W$
connected, such that for every $f \in \sO(U)$ there exists an $F \in
\sO(W)$ with $f(z) = F(z)$ for all $z \in V$.  See
\figureref{fig:domain-of-hol-def}.
\end{defn}

\begin{myfig}
\subimport*{figures/}{domain-of-hol-def.pdf_t}
\caption{Definition of domain of holomorphy.\label{fig:domain-of-hol-def}}
\end{myfig}

The idea is that if a domain $U$
is not a domain of holomorphy and $V$, $W$ exist as in the
definition, then $f$ ``extends across the boundary'' somewhere.

\begin{example}
The unit ball $\bB_n \subset \C^n$ is a domain of holomorphy.  Proof: 
Consider $U=\bB_n$, and suppose $V$, $W$ as in the definition exist.
As $W$ is
connected and open, it is path connected.  There exist
points in $W$ that are not in $\bB_n$, so there
is a path $\gamma$ in $W$ that goes
from a point $q \in V$ to some $p \in \partial \bB_n \cap W$.
Without loss of generality (after composing with
rotations, that is, unitary matrices), assume $p =
(1,0,0,\ldots,0)$.  Take the function $f(z) = \frac{1}{1-z_1}$.  
The function $F$ must agree with $f$ on the component of
$\bB_n \cap W$ that contains $q$.  But that component also contains $p$ and
so $F$ must blow up (in particular it cannot be holomorphic) at
$p$.  The contradiction shows that no $V$ and $W$ exist.
\end{example}

In one dimension this notion has no real content: Every domain is a domain
of holomorphy.

\begin{exbox}
\begin{exercise}[Easy]
In $\C$, every domain is a domain of holomorphy.
\end{exercise}

\begin{exercise}
If $U_j \subset \C^n$ are domains of holomorphy (possibly an infinite set of
domains), then the interior of
$\bigcap_{j} U_j$
is either empty or every connected component is a domain of holomorphy.
\end{exercise}

\begin{exercise}[Easy]
Show that a polydisc in $\C^n$ is a domain of holomorphy.
\end{exercise}

\begin{exercise}
Suppose $U_k \subset \C^{n_k}$, $k=1,\ldots,\ell$ are domains of holomorphy,
show that
$U_1 \times \cdots \times U_\ell$ is a domain of holomorphy.
In particular every cartesian product of domains in $\C$ is a domain of
holomorphy.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Given $p \in \partial \bB_n$, find a function $f$ holomorphic on
$\bB_n$,
$C^\infty$-smooth on $\overline{\bB_n}$ (all real partial derivatives of
all orders extend
continuously to $\overline{\bB_n}$), that does not extend past $p$
as a holomorphic function.
Hint: For the principal branch of $\sqrt{\cdot}$ the function $\xi \mapsto
e^{-1/\sqrt{\xi}}$ is holomorphic for $\Re \xi > 0$ and extends to
be continuous (even smooth) on all of $\Re \xi \geq 0$.
\item
Find a function $f$ holomorphic on $\bB_n$
that does not extend past any point of
$\partial \bB_n$.
\end{exparts}
\end{exercise}
\end{exbox}

Various notions of convexity will play a big role later on.
A set $S$ is \emph{\myindex{geometrically convex}}\index{convex}
if $t x + (1-t)y \in S$ 
for all $x,y \in S$ and $t \in [0,1]$.
The exercise below
says that every geometrically convex domain is a domain of holomorphy.
Domains of holomorphy are often not geometrically convex
(e.g.\ any domain in $\C$ is a domain of holomorphy),
so classical convexity is not the correct notion, but it is
in the right direction.

\begin{exbox}
\begin{exercise}
Show that a geometrically convex domain in $\C^n$ is a domain of holomorphy.
\end{exercise}
\end{exbox}

In the following when we say $f \in \sO(U)$ extends holomorphically to $V$ where
$U \subset V$, we mean that there
exists a function $F \in \sO(V)$ such that $f = F$ on
$U$.

\begin{remark}
The subtlety of the definition of a domain of holomorphy is that it does not
necessarily talk about functions extending to a larger set, since we must
take into account single-valuedness.  For instance, let $f$ be the principal branch
of the logarithm defined on the slit plane
$U = \C \setminus \{ z \in \C : \Im z = 0, \Re z \leq 0 \}$.
We can locally define an extension from one side through the boundary
of the domain, but we cannot define an extension on a open set that
contains $U$.  This example should be motivation for why we let $V$ 
be a proper subset of $U \cap W$, and why $W$ need not include all of $U$.
\end{remark}

In dimension two or more, not every domain is a domain of holomorphy.  We have
the following theorem.  The domain $H$ in the theorem is called the
\emph{\myindex{Hartogs figure}}.

\begin{thm} \label{thm:extensionhartogsfigure}
Let $(z,w) = (z_1,\ldots,z_m,w_{1},\ldots,w_{k}) \in \C^m \times \C^k$ be the coordinates.  For two numbers
$0 < a,b < 1$, define the set $H \subset \D^{m+k}$
by
\begin{multline*}
H = \bigl\{ (z,w) \in \D^{m+k} : \sabs{z_j} > a ~\text{for $j=1,\ldots,m$}
\bigr\}
\\
\cup
\bigl\{ (z,w) \in \D^{m+k} : \sabs{w_j} < b ~\text{for $j=1,\ldots,k$}
\bigr\} .
\end{multline*}
If $f \in \sO(H)$, then $f$ extends holomorphically to $\D^{m+k}$.
\end{thm}

In $\C^2$ if $m=1$ and $k=1$, see 
\figureref{fig:hartogs-figure} (the $c$ will come up in
the proof).

\begin{myfig}
\newcommand{\hartogstext}{\parbox[t]{2.3in}{In diagrams, the Hartogs figure is
often drawn as:}}
\subimport*{figures/}{hartogs-figure.pdf_t}
\caption{Hartogs figure.\label{fig:hartogs-figure}}
\end{myfig}

\begin{proof}
Pick a $c \in (a,1)$.  Let
\begin{equation*}
\Gamma =
\bigl\{ z \in \D^{m} : \sabs{z_j} = c ~\text{for $j=1,\ldots,m$ } \bigr\}.
\end{equation*}
That is, $\Gamma$ is the distinguished boundary of $c \D^m$,
a polydisc centered at 0 of radius $c$ in $\C^m$.
Define the function
\begin{equation*}
F(z,w)
=
\frac{1}{{(2\pi i)}^m}
\int_\Gamma \frac{f(\xi,w)}{\xi-z} \, d\xi .
\end{equation*}
Clearly, $F$ is well-defined on
\begin{equation*}
c\D^m \times \D^k
\end{equation*}
as $\xi$ only
ranges through $\Gamma$ and so as long as $w \in \D^k$ then $(\xi,w) \in H$.

The function $F$ is holomorphic in $w$ as we can differentiate
underneath the integral and $f$ is holomorphic in $w$ on $H$.  Furthermore,
$F$ is holomorphic in $z$ as the kernel $\frac{1}{\xi-z}$ is holomorphic in
$z$ as long as $z \in c\D^m$.

For any fixed $w$ with $\sabs{w_j} < b$ for all $j$,
the Cauchy integral formula says
$F(z,w) = f(z,w)$ for all $z \in c \D^m$.
Hence, $F=f$ on the open set
$c\D^m \times b\D^k$,
and so they are equal on 
$(c\D^m \times \D^k) \cap H$.
Combining $F$ and $f$ we obtain a holomorphic
function on $\D^{m+k}$ that extends $f$.
\end{proof}

The theorem is used 
in many situations to extend holomorphic functions.
We usually need to
translate, scale, 
rotate (apply a unitary matrix),
and even take more general biholomorphic mappings
of $H$, to place it wherever we need it.  The corresponding polydisc---or the image of
the polydisc under the appropriate biholomorphic mapping if one was
used---to which all holomorphic functions on $H$ extend is denoted
by $\widehat{H}$ and is called the \emph{hull} of $H$.%
\index{hull of a Hartogs figure}

Let us state a simple but useful case of the so-called
\emph{\myindex{Hartogs phenomenon}}.

\begin{cor}
Let $U \subset \C^n$, $n \geq 2$, be an open set and $p \in U$.
Then every $f \in \sO\bigl(U \setminus \{ p \} \bigr)$
extends holomorphically to $U$.
\end{cor}

\begin{proof}
Without loss of generality,
by translating and scaling (those operations are after all holomorphic),
we assume that $p = \bigl(0,\ldots,0,\frac{3}{4}\bigr)$
and the unit polydisc $\D^n$ is contained in $U$.  We fit a Hartogs figure $H$
in $U$
by letting $m=n-1$ and $k=1$, writing $\C^n = \C^{n-1} \times \C^{1}$,
and taking $a = b = \frac{1}{2}$.
Then $H \subset U$, and $p \in \D^n \setminus H$.
\thmref{thm:extensionhartogsfigure} says that
$f$ extends to be holomorphic at $p$.
\end{proof}

This result provides another reason why holomorphic functions in several
variables have no isolated zeros (or poles).  Suppose $U \subset \C^n$,
$n \geq 2$, and $f \in \sO(U)$ with $f$ being zero only at $p$, that
is $f^{-1}(0) = \{ p \}$.  Then $\frac{1}{f}$ would be holomorphic in
$U \setminus \{ p \}$.  It would not be possible to extend $f$
through $p$ (not even continuously let alone holomorphically),
and we obtain a contradiction.

The extension works in an even more surprising fashion.  We could
take out a very large set, for example, any geometrically
convex subset:

\begin{exbox}
\begin{exercise} \label{exercise:convexhartogs}
Suppose $U \subset \C^n$, $n \geq 2$, be an open set and $K \subset \subset U$
is a compact geometrically
convex subset.
If $f \in \sO(U \setminus K)$,
then $f$ extends to be holomorphic in $U$.
Hint: Find a nice point on $\partial K$ and try extending a little bit.
Then make sure your extension is single-valued.
\end{exercise}
\end{exbox}

Convexity of $K$ is not needed; we only need that $U\setminus K$
is connected, however, the proof is much harder.
The single-valuedness of the extension is the key point that makes the
general proof harder.

Notice the surprising fact that every holomorphic function on
the shell
\begin{equation*}
\bB_n \setminus \overline{B_{1-\epsilon}(0)} =
\bigl\{ z \in \C^n : 1-\epsilon < \snorm{z} < 1 \bigr\}
\end{equation*}
for any $\epsilon > 0$ automatically
extends to a holomorphic function of $\bB_n$.  We need $n > 1$.
The extension result decisively does not work in one dimension; for
example take $\nicefrac{1}{z}$.
There is a related fact about zero sets.
If $n \geq 2$ and $f \in \sO(\bB_n)$
the set of its zeros must 
``touch the boundary'' (in other words $f^{-1}(0)$ is not compact)
or be empty:  If the set of zeros were 
compact in $\bB_n$, then we could try to
extend the function $\nicefrac{1}{f}$.

\begin{exbox}
\begin{exercise}[\myindex{Hartogs triangle}]\label{exercise:hartogstriangle}
Let
\begin{equation*}
T = \bigl\{ (z_1,z_2) \in \D^2 : \sabs{z_2} < \sabs{z_1} \bigr\} .
\end{equation*}
Show that $T$ is a domain of holomorphy.  Then show that if
\begin{equation*}
\widetilde{T} = T \cup B_{\epsilon}(0)
\end{equation*}
for an arbitrarily small $\epsilon > 0$, then $\widetilde{T}$ is not a domain
of holomorphy.  In fact, every function holomorphic on $\widetilde{T}$
extends to a holomorphic function of $\D^2$.
\end{exercise}

\begin{exercise} \label{exercise:C2minusR2}
Take the natural embedding of $\R^2 \subset \C^2$.  Suppose 
$f \in \sO(\C^2 \setminus \R^2)$.  Show that $f$ extends holomorphically
to all of $\C^2$.  Hint: Change coordinates before using Hartogs.
\end{exercise}

\begin{exercise} \label{exercise:fatcylinder1}
Suppose 
\begin{equation*}
U = \bigl\{ (z,w) \in \D^2 : \nicefrac{1}{2} < \sabs{z} \bigr\} .
\end{equation*}
Draw $U$.  
Let $\gamma = \bigl\{ z \in \C : \sabs{z} = \nicefrac{3}{4} \bigr\}$ oriented positively.
If $f \in \sO(U)$, then show that the function
\begin{equation*}
F(z,w)
=
\frac{1}{2\pi i}
\int_\gamma \frac{f(\xi,w)}{\xi-z} \, d\xi
\end{equation*}
is well-defined in
$\bigl( (\nicefrac{3}{4}) \D \bigr) \times \D$, holomorphic where defined, yet
it is not necessarily true that $F = f$ on the intersections of their
domains.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set such that for every
$z \in \C^n \setminus \{ 0 \}$, there is a $\lambda \in \C$ such that
$\lambda z \in U$.  Let $f \colon U \to \C$ be holomorphic with
$f(\lambda z) = f(z)$ whenever $z \in U$, $\lambda \in \C$ and $\lambda z
\in U$.
\begin{exparts}
\item
(easy) Prove that $f$ is constant.
\item
(hard) Relax the requirement on
$f$ to being meromorphic: $f = \nicefrac{g}{h}$
for holomorphic $g$ and $h$.
Find a nonconstant example, and prove that such an $f$ must be rational (that
is, $g$ and $h$ must be polynomials).
\end{exparts}
\end{exercise}

\begin{exercise} \label{exercise:fatcylinder2}
Suppose 
\begin{equation*}
U = \bigl\{ z \in \D^3 :
\nicefrac{1}{2} < \sabs{z_1} \quad\text{or}\quad
\nicefrac{1}{2} < \sabs{z_2} \bigr\} .
\avoidbreak
\end{equation*}
Prove that every function $f \in \sO(U)$ extends to $\D^3$.
Compare to \exerciseref{exercise:fatcylinder1}.
\end{exercise}

\begin{exercise} \label{exercise:codim2extends}
Suppose $U = \C^n \setminus \{ z \in \C^n : z_1 = z_2 = 0 \}$, $n
\geq 2$.  Show that every $f \in \sO(U)$ extends holomorphically to
$\C^n$.
\end{exercise}
\end{exbox}

\begin{example}
By
\exerciseref{exercise:C2minusR2},
$U_1 = \C^2 \setminus \R^2$
is not a domain of holomorphy.  On the other hand,
$U_2 = \C^2 \setminus \{ z \in \C^2 : z_2 = 0 \}$ is a domain of holomorphy;
simply
use $f(z) = \frac{1}{z_2}$ as the function that cannot extend.
Therefore, $U_1$ and $U_2$ are rather different as far as complex variables are
concerned, yet they are the same set if we ignore the complex structure.
They are both simply a 4-dimensional real vector space minus a 2-dimensional
real vector subspace.  That is, $U_1$ is the set
where either $\Im z_1 \not= 0$ or $\Im z_2 \not= 0$,
while $U_2$ is the set
where either $\Re z_2 \not= 0$ or $\Im z_2 \not= 0$.

The condition of being a domain of holomorphy,
requires something more than just some real geometric condition on the
set.  Namely, we have shown that the image of a domain of holomorphy
via an orthonormal
real-linear mapping 
(so preserving distances, angles, straight lines,
etc.)\ need not be a domain of holomorphy.  Therefore, when we want to
``rotate'' in complex analysis we need to use a complex linear mapping,
so a unitary matrix.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tangent vectors, the Hessian, and convexity}

An exercise in the previous section showed that every convex domain is a
domain of holomorphy.  However, classical convexity is too strong.

\begin{exbox}
\begin{exercise}
Show that if $U \subset \C^m$ and $V \subset \C^k$ are both domains of
holomorphy, then $U \times V$ is a domain of holomorphy.
\end{exercise}
\end{exbox}

In particular, the exercise says that
for any domains $U \subset \C$ and $V \subset \C$, the set
$U \times V$ is a domain of holomorphy in $\C^2$.  The domains
$U$ and $V$, and hence $U \times V$, can be spectacularly nonconvex.
But we should not discard convexity completely, there is a notion of
\emph{pseudoconvexity}, which vaguely means ``convexity in the 
complex directions'' and is the correct notion to distinguish 
domains of holomorphy.
Let us figure out what classical convexity means locally for a smooth boundary.

\begin{defn} \label{def:hypersurface}
A set $M \subset \R^n$ is a
\glsadd{not:Ck}%
$C^k$-smooth \emph{\myindex{hypersurface}}%
\index{Ck-smooth hypersurface@$C^k$-smooth hypersurface}
if at each point
$p \in M$, there exists a $k$-times continuously
differentiable function $r \colon V \to \R$
with nonvanishing derivative, defined in a neighborhood $V$ of $p$ 
such that $M \cap V = \bigl\{ x \in V : r(x) = 0 \bigr\}$.  The function $r$ is
called the \emph{\myindex{defining function}} of $M$ (at $p$).

An open set (or domain) $U \subset \R^n$ with
\emph{$C^k$-smooth boundary}%
\index{Ck-smooth boundary@$C^k$-smooth boundary}
is a set where
$\partial U$ is a $C^k$-smooth hypersurface,
and at every $p \in \partial U$ there is a defining
function $r$ such that
$r < 0$ for points in $U$ and $r > 0$
for points not in $U$.
See \figureref{fig:deffun}.

If we say simply
\index{domain with smooth boundary}%
\index{open set with smooth boundary}%
\emph{smooth}\index{smooth boundary},
\glsadd{not:Cinfty}%
we mean $C^\infty$-smooth,\index{Cinfinity-smooth@$C^\infty$-smooth}
that is, the $r$ above is infinitely differentiable.
\end{defn}

\begin{myfig}
\subimport*{figures/}{deffun.pdf_t}
\caption{Local defining function for a domain.\label{fig:deffun}}
\end{myfig}

What we really defined is an \emph{\myindex{embedded hypersurface}}.  In
particular, in this book the topology on the set $M$ will be the subset
topology.  Furthermore, in this book we generally deal with smooth
(that is, $C^\infty$) functions and hypersurfaces.  Dealing with
$C^k$-smooth functions for finite $k$ introduces technicalities that make
certain theorems and arguments unnecessarily difficult.

As the derivative of $r$ is nonvanishing, a
hypersurface $M$ is locally the graph of one variable over the rest
using the implicit function theorem.  That is, $M$ is a smooth 
hypersurface if it is locally a set defined by
$x_j = \varphi(x_1,\ldots,x_{j-1},x_{j+1},\ldots,x_n)$ for some $j$ and some
smooth function $\varphi$.

The definition of an open set with smooth boundary is not just that the
boundary is a smooth hypersurface, that is not enough.  It also says that 
one side of that hypersurface is in $U$ and one side is not in $U$.  That is
because if the derivative of $r$ never vanishes, then $r$ must have
different signs on different sides of $\bigl\{ x \in V : r(x) = 0 \bigr\}$.  The
verification of this fact is left to the reader (Hint: Look at where the
gradient points to).

Same definition works for $\C^n$, where we treat $\C^n$ as $\R^{2n}$.
For example, the ball $\bB_n$ is a domain with smooth boundary with defining
function $r(z,\bar{z}) = \snorm{z}^2-1$.  We can, in fact, find a
single global defining function for every open set with smooth boundary,
but we have no need of this.
In $\C^n$
a hypersurface defined as above is a \emph{\myindex{real hypersurface}},
to distinguish it from a complex hypersurface that would be the zero set of
a holomorphic function, although we may at times leave out the word ``real''
if it is clear from context.

\begin{defn}
For a point $p \in \R^n$, the set of \emph{tangent vectors}\index{vector} $T_p \R^n$ is given by
\glsadd{not:realtangentspace}%
\begin{equation*}
T_p \R^n = \operatorname{span}_{\R} \left\{
\frac{\partial}{\partial x_1}\Big|_p,
\ldots,
\frac{\partial}{\partial x_n}\Big|_p \right\} .
\end{equation*}
\end{defn}

That is, a vector $X_p \in T_p \R^n$ is an object of the form
\begin{equation*}
X_p = \sum_{j=1}^n a_j 
\frac{\partial}{\partial x_j}\Big|_p ,
\end{equation*}
for real numbers $a_j$.  For computations, $X_p$ could be represented
by an $n$-vector $a = (a_1,\ldots,a_n)$.  However, if $p \not= q$, then
$T_p \R^n$ and
$T_q \R^n$ are distinct spaces.
\glsadd{not:evalpartial}%
An object
$\frac{\partial}{\partial x_j}\big|_p$
is a linear functional
on the space of smooth functions:
When applied to a smooth function $g$, it gives
$\frac{\partial g}{\partial x_j} \big|_p$.  Therefore, $X_p$ is also such a
functional.  It is the directional derivative from calculus;
it is computed as $X_p f = \nabla f|_p \cdot (a_1,\ldots,a_n)$.

\begin{defn}
Let $M \subset \R^n$ be a smooth hypersurface,
$p \in M$, and $r$ is a defining function at $p$,
then a vector $X_p \in T_p \R^n$ is \emph{tangent}\index{tangent vector}
to $M$ at $p$ if
\begin{equation*}
X_p r = 0, \qquad \text{or in other words} \qquad
\sum_{j=1}^n a_j \frac{\partial r}{\partial x_j} \Big|_p = 0 .
\end{equation*}
\glsadd{not:realtangentspace}%
The space of tangent vectors to $M$ is denoted by $T_p M$, and
is called the \emph{\myindex{tangent space}} to $M$ at $p$.
\end{defn}

The space $T_pM$ is an $(n-1)$-dimensional real vector space---it is a subspace
of an $n$-dimensional $T_p\R^n$ given by a single linear equation.  
Recall from calculus that the gradient $\nabla r|_p$ is
``normal'' to $M$ at $p$, and
the tangent space is given by all the $n$-vectors $a$
that are orthogonal to the normal, that is $\nabla r|_p \cdot a = 0$.

\pagebreak[1]
We cheated in the terminology, and assumed without justification that $T_pM$
depends only on $M$, not on $r$.
Fortunately, the definition of $T_pM$ is independent of the choice of $r$ by the next two
exercises.

\begin{exbox}
\begin{exercise} \label{exercise:smoothdivision}
Suppose $M \subset \R^n$ is a smooth hypersurface and
$r$ is a smooth defining function for $M$ at $p$.
\begin{exparts}
\item
Suppose $\varphi$ is another
smooth defining function of $M$ on a neighborhood of $p$.
Show that there exists a smooth nonvanishing function $g$ such that
$\varphi = g r$ (in a neighborhood of $p$).
\item
Now suppose $\varphi$ is an arbitrary smooth function that vanishes on $M$ (not
necessarily a defining function).
Again show that $\varphi = g r$, but now $g$ may possibly vanish.
\end{exparts}
\nopagebreak
Hint: First suppose $r=x_n$ and
find a $g$ such that $\varphi = x_n g$.  Then find
a local change of variables to make $M$ into the set given by $x_n = 0$.
A useful calculus fact:
If $f(0) = 0$ and $f$ is smooth, then
$s \int_0^1 f'(ts) \,dt = f(s)$,
and $\int_0^1 f'(ts) \,dt$ is a smooth function of $s$.
\end{exercise}

\begin{exercise}
Show that $T_pM$ is independent of which defining function we take.  That
is,
prove that if $r$ and $\tilde{r}$ are defining functions for $M$ at $p$, then
$\sum_j a_j \frac{\partial r}{\partial x_j} \big|_p = 0$
if and only if
$\sum_j a_j \frac{\partial \tilde{r}}{\partial x_j} \big|_p = 0$.
\end{exercise}
\end{exbox}

The tangent space $T_p M$ is the set of derivatives
\emph{along} $M$ at $p$.  If $r$ is a defining function of $M$, and $f$ and $h$
are two smooth functions such that $f=h$ on $M$, then
\exerciseref{exercise:smoothdivision}
says that
\begin{equation*}
f-h = g r, \qquad \text{or} \qquad
f = h + g r,
\end{equation*}
for some smooth $g$.  Applying $X_p$ we find
\begin{equation*}
X_p f =
X_p h + X_p (gr) = 
X_p h + (X_p g)r + g(X_p r) = 
X_p h + (X_p g)r.
\end{equation*}
So $X_p f = X_p h$ on $M$ (where $r=0$).  In other words, $X_p f$ only
depends on the values of $f$ on $M$.

\begin{example}
If $M \subset \R^n$ is given by $x_n = 0$, then $T_p M$ is given by
derivatives of the form
\begin{equation*}
X_p =  \sum_{j=1}^{n-1} a_j \frac{\partial}{\partial x_j} \Big|_p .
\end{equation*}
That is, derivatives along the first $n-1$ variables only.
\end{example}

\begin{defn}
The disjoint union
\glsadd{not:realtangentbundle}%
\begin{equation*}
T\R^n = \bigcup_{p \in \R^n} T_p \R^n
\end{equation*}
is called the \emph{\myindex{tangent bundle}}.  There is a
natural identification $\R^n \times \R^n \cong T\R^n$:
\begin{equation*}
(p,a) \in \R^n \times \R^n
\quad
\mapsto
\quad
\sum_{j=1}^n a_j \frac{\partial}{\partial x_j} \Big|_p \in T\R^n .
\end{equation*}
The topology and smooth structure on $T\R^n$ comes from this identification.
The wording ``bundle'' (a bundle of fibers)
comes from the natural projection $\pi \colon T\R^n
\to \R^n$, where fibers are $\pi^{-1}(p) = T_p\R^n$.

A smooth
\emph{\myindex{vector field}} in $T\R^n$ is an object of the form 
\begin{equation*}
X = \sum_{j=1}^n a_j
\frac{\partial}{\partial x_j} ,
\end{equation*}
where $a_j$ are smooth
functions.
That is, $X$ is a smooth function
$X \colon V \subset \R^n \to T\R^n$ such that $X(p) \in T_p \R^n$.
Usually we write $X_p$ rather than $X(p)$.
To be more fancy,
say $X$ is a \emph{\myindex{section}} of $T \R^n$.

Similarly, the tangent bundle of $M$ is
\glsadd{not:realtangentbundle}%
\begin{equation*}
TM = \bigcup_{p \in M} T_p M .
\end{equation*}
A vector field $X$ in $TM$ is a vector field
such that $X_p \in T_p M$ for all $p \in M$.
\end{defn}

Before we move on, let us note how smooth maps transform tangent spaces.
Given a smooth mapping 
$f \colon U \subset \R^n \to \R^m$,
the derivative at $p$ is a linear mapping of
the tangent spaces: $Df(p) \colon T_p \R^n \to T_{f(p)} \R^m$.
If $X_p \in T_p \R^n$, then 
$Df(p) X_p$ should be in $T_{f(p)} \R^m$.
The vector 
$Df(p) X_p$ is defined by how it acts on
smooth functions $\varphi$ of a neighborhood
of $f(p)$ in $\R^m$:
%then
%$Df(p) X_p$ acts on $\varphi$ as
\glsadd{not:Df}%
\begin{equation*}
Df(p) X_p \varphi = X_p (\varphi \circ f) .
\end{equation*}
It is the only reasonable way to put those three objects together.
When the spaces are $\C^n$ and $\C^m$, we denote this
derivative as
\glsadd{not:DRf}%
$D_\R f$
to distinguish it from the holomorphic derivative.
As far as calculus computations are concerned,
the linear mapping $Df(p)$ is
the Jacobian matrix acting on vectors in the standard basis of the tangent space
as given above.
This is why we use the same notation for the Jacobian
matrix and the derivative acting on tangent spaces.
To verify this claim, it is enough to see where the basis element
$\frac{\partial}{\partial x_j}\big|_p$ goes, and the form of $Df(p)$
as a matrix
follows by the chain rule.
For example, the derivative of the mapping $f(x_1,x_2) =
(x_1+2x_2+x_1^2,3x_1+4x_2+x_1x_2)$ at the origin is given by the matrix
$\left[ \begin{smallmatrix} 1 & 2 \\ 3 & 4 \end{smallmatrix} \right]$,
and so the vector
$X_p = a\frac{\partial}{\partial x_1}\big|_0
+
b\frac{\partial}{\partial x_2}\big|_0$
gets taken to
$Df(0) X_0 = (a+2b)\frac{\partial}{\partial y_1}\big|_0
+
(3a+4b)\frac{\partial}{\partial y_2}\big|_0$, where we let $(y_1,y_2)$ be the
coordinates on the target.  You should check on some test
function, such as
$\varphi(y_1,y_2) = \alpha y_1 + \beta y_2$, that the definition above is
satisfied.

\medskip

Now that we know what tangent vectors are and how they transform,
let us define convexity
for domains with smooth boundary.

\begin{defn}
Suppose $U \subset \R^n$ is an open set with
smooth boundary, %$C^k$-smooth boundary, $k \geq 2$,
and 
$r$ is a defining function for $\partial U$ at $p \in \partial
U$ such that $r < 0$ on $U$.
If
\begin{equation*}
\sum_{j=1,\ell=1}^n
a_j a_\ell \frac{\partial^2 r}{\partial x_j \partial x_\ell} \Big|_p \geq 0 ,
\qquad \text{for all} \qquad
X_p = \sum_{j=1}^n a_j 
\frac{\partial}{\partial x_j}\Big|_p \quad \in \quad T_p \partial U,
\end{equation*}
then $U$ is said to be \emph{\myindex{convex}} at $p$.  If the inequality
above is strict for all nonzero $X_p \in T_p \partial U$, then $U$ is said to be
\emph{\myindex{strongly convex}} at $p$.

A domain $U$ is \emph{convex} if it is convex at all $p \in \partial U$.
If $U$ is bounded\footnote{Matters are a little more complicated
with the ``strong'' terminology if $U$ is unbounded.}, we say
$U$ is \emph{strongly convex} if it is strongly convex at all $p \in
\partial U$.
\end{defn}

The matrix
\begin{equation*}
\left[ \frac{\partial^2 r}{\partial x_j \partial x_\ell} \Big|_p
\right]_{j\ell}
\end{equation*}
is the
\emph{\myindex{Hessian}} of $r$ at $p$.
So, $U$ is convex at $p \in \partial U$ if
the Hessian
of $r$ at $p$ as a bilinear form is positive semidefinite
when restricted to $T_p \partial U$.
In the language of vector calculus, let $H$ be the Hessian of $r$ at $p$, and treat
$a \in \R^n$ as a column vector.
Then 
$\partial U$ is convex at $p$ whenever
\begin{equation*}
a^t H a \geq 0 , \qquad \text{for all $a \in \R^n$ such that} \quad \nabla r|_p
\cdot a = 0 .
\end{equation*}
This bilinear form given by the Hessian is the second fundamental form from Riemannian
geometry in mild disguise (or perhaps it is the other way around).

We cheated a little bit, since we have not proved that
the notion of convexity is well-defined.  In particular, there are many possible
defining functions.

\begin{exbox}
\begin{exercise}
Show that the definition of convexity is independent of the defining
function.  Hint: If $\tilde{r}$ is another defining function near $p$,
then there is a smooth function $g > 0$ such that $\tilde{r} = g r$.
\end{exercise}
\end{exbox}

\begin{example}
The unit disc in $\R^2$ is
convex (actually strongly convex).
Proof:
Let $(x,y)$ be the coordinates and
let $r(x,y) = x^2+y^2-1$ be the 
defining function.
The tangent space of the circle is one-dimensional, so we simply need to
find a single nonzero tangent vector at each point.
Consider the gradient
\glsadd{not:gradient}%
$\nabla r = (2x,2y)$ to check that
\begin{equation*}
X = y \frac{\partial}{\partial x} - x \frac{\partial}{\partial y}
\end{equation*}
is tangent to the circle, that is,
$Xr = X(x^2+y^2-1) = (2x,2y) \cdot (y,-x) = 0$ on the circle
(by chance $Xr=0$ everywhere).
The vector field $X$ is nonzero on the circle, so at each point it gives a
basis of the tangent space.  See \figureref{fig:circle-tangent}
\begin{myfig}
\subimport*{figures/}{circle-tangent.pdf_t}
\caption{Tangent vector to a circle.\label{fig:circle-tangent}}
\end{myfig}

The Hessian matrix of $r$ is
\begin{equation*}
\begin{bmatrix}
\frac{\partial^2 r}{\partial x^2} &
\frac{\partial^2 r}{\partial x \partial y} \\
\frac{\partial^2 r}{\partial y \partial x} &
\frac{\partial^2 r}{\partial y^2}
\end{bmatrix}
=
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix} .
\end{equation*}
Applying the vector $(y,-x)$ gets us
\begin{equation*}
\begin{bmatrix}
y & -x
\end{bmatrix}
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix}
\begin{bmatrix}
y \\ -x
\end{bmatrix}
=
2y^2+2x^2 = 2 > 0 .
\end{equation*}
So the domain given by $r < 0$ is strongly convex at all points.
\end{example}

In general, to construct a tangent vector field for
a curve in $\R^2$,
consider
$r_y \frac{\partial}{\partial x} - r_x \frac{\partial}{\partial y}$.  In
higher dimensions, running through enough pairs of variables gets
a basis of $TM$.

\begin{exbox}
\begin{exercise}
Show that if an open set with smooth boundary is strongly convex at a point, then it is
strongly convex at all nearby points.  On the other hand find an example of
an open set with smooth boundary that is convex at one point $p$, but not convex at points
arbitrarily near $p$.
\end{exercise}

\begin{exercise}
Show that the domain in $\R^2$ defined by $x^4+y^4 < 1$ is convex, but not strongly convex.
Find all the points where the domain is not strongly convex.
\end{exercise}

\begin{exercise}
Show that the domain in $\R^3$ defined by ${(x_1^2+x_2^2)}^2 < x_3$ is
strongly convex at all points except the origin, where it is just convex
(but not strongly).
\end{exercise}
\end{exbox}

In the following, \glsadd{not:Ol}%
we use the \emph{\myindex{big-oh notation}},
although we use a perhaps less standard shorthand\footnote{%
The standard notation for $O(\ell)$ is $O(\snorm{x}^{\ell})$ and
it means that
$\abs{\frac{f(x)}{\snorm{x}^\ell}}$ is bounded as $x \to p$.}.
A smooth function is $O(\ell)$ at a point $p$ (usually the origin),
if all its derivatives of order $0, 1, \ldots,  \ell-1$ vanish at $p$.
For example, if $f$ is $O(3)$ at the origin,
then $f(0)=0$, and its first and second derivatives vanish at the origin.

For computations it is often useful to use a more convenient
defining function, that is, it is convenient to write $M$ as a graph.

\begin{lemma} \label{lemma:realgraphcoords}
Suppose $M \subset \R^n$ is a smooth hypersurface, 
and $p \in M$.  Then after a rotation and translation, 
$p$ is the origin, and near the origin $M$ is given by
\begin{equation*}
y = \varphi(x) ,
\end{equation*}
where $(x,y) \in \R^{n-1} \times \R$ are our coordinates and
$\varphi$ is a smooth %$C^k$
function that is $O(2)$ at the origin,
that is $\varphi(0) = 0$ and $d\varphi(0) = 0$.

If $M$ is the boundary of an open set $U$ with smooth boundary and
$r < 0$ on $U$,
then the rotation can be chosen
such that 
$y > \varphi(x)$
for points in $U$.
See \figureref{fig:deffun-graph}.
\end{lemma}

\begin{myfig}
\subimport*{figures/}{deffun-graph.pdf_t}
\caption{Defining a domain as a graph.\label{fig:deffun-graph}}
\end{myfig}

\begin{proof}
Let $r$ be a defining function at $p$.  Take $v = \nabla r|_p$.
By translating $p$ to zero, and applying a rotation (an orthogonal matrix),
we assume $v = (0,0,\ldots,0,v_n)$, where $v_n < 0$.  Denote our
coordinates by $(x,y) \in \R^{n-1} \times \R$.  As $\nabla r|_0 =
v$, then $\frac{\partial r}{\partial y}(0) \not= 0$.  We apply 
the implicit function theorem to find a
smooth function $\varphi$ such that
$r\bigl(x,\varphi(x)\bigr) = 0$ for all $x$ in a neighborhood of the
origin, and $\bigl\{ (x,y) : y=\varphi(x) \bigr\}$ are all the
solutions to $r = 0$ near the origin.

What is left is to show that the derivative at 0 of $\varphi$ vanishes.
As
$r\bigl(x,\varphi(x)\bigr) = 0$ for all $x$ in a neighborhood of the
origin, we differentiate.
For every $j=1,\ldots,n-1$,
\begin{equation*}
0 = 
\frac{\partial}{\partial x_j} \Bigl[
r\bigl(x,\varphi(x)\bigr) 
\Bigr]
=
\left(
\sum_{\ell=1}^{n-1}
\frac{\partial r}{\partial x_\ell}
\frac{\partial x_\ell}{\partial x_j}
\right)
+
\frac{\partial r}{\partial y}
\frac{\partial \varphi}{\partial x_j}
=
\frac{\partial r}{\partial x_j}
+
\frac{\partial r}{\partial y}
\frac{\partial \varphi}{\partial x_j} .
\end{equation*}
At the origin,
$\frac{\partial r}{\partial x_j}(0,0) = 0$ and
$\frac{\partial r}{\partial y}(0,0) = v_n \not= 0$, and therefore
$\frac{\partial \varphi}{\partial x_j}(0) = 0$.

To prove the final statement, note that
$r < 0$ on $U$.  It is enough to check that $r$ is
negative for $(0,y)$ if $y > 0$ is small, which follows as $\frac{\partial
r}{\partial y}(0,0) = v_n < 0$.
\end{proof}

The advantage of this representation is that the tangent space at $p$
can be identified with the $x$ coordinates for the purposes of computation.
Considering $x$ as a column vector, the Taylor expansion of a smooth function $\varphi$ at the origin is
\begin{equation*}
\varphi(x) = \varphi(0) + \nabla \varphi|_0 \cdot x + \frac{1}{2}\, x^t H x + E(x) ,
\end{equation*}
where $H$ is the Hessian matrix of $\varphi$ at the origin, that is $H = \left[
\frac{\partial^2 \varphi}{\partial x_j \partial x_k} \big|_{0} \right]_{jk}$,
and $E$ is $O(3)$.  That is, $E(0) = 0$, and
all first and second derivatives of $E$ vanish at $0$.
For the situation from the lemma above,
the $\varphi$ is $O(2)$ at the origin, i.e.\ $\varphi(0) = 0$
and $\nabla \varphi|_0 = 0$.  So we write the hypersurface
$M$ as
\begin{equation*}
y = \frac{1}{2}\, x^t H x + E(x) .
\end{equation*}
If $M$ is the boundary
$\partial U$ of an open set, then
we pick the rotation so that $y > \frac{1}{2}\,x^t H x + E(x)$ on $U$.
It is an easy exercise to show that $U$ is convex at $p$ if
$H$ positive semidefinite, and $U$ is strongly convex at $p$ if $H$ is positive definite.

\begin{exbox}
\begin{exercise}
Prove the statement above about $H$ and convexity at $p$.
\end{exercise}

\begin{exercise}
Let $r$ be a defining function at $p$ for a smooth hypersurface
$M \subset \R^n$.
We say $M$ is convex from both sides at $p$ if both the set given by
$r > 0$ and the set given by $r < 0$ are convex at $p$.
Prove that if a hypersurface $M \subset \R^n$ is convex from both sides at all
points, then it is locally just a hyperplane (the zero set of a real affine
function).
\end{exercise}
\end{exbox}

Recall that $U$ is \emph{\myindex{geometrically convex}} if for every $p,q \in U$ the
line between $p$ and $q$ is in $U$, or in other words
$tp +(1-t)q \in U$ for all $t \in [0,1]$.
In particular, geometric convexity is a \emph{global} condition.  You need 
to know all of $U$.  On the other hand, the notion of convex for a smooth boundary is
\emph{local} in that you only need to know $\partial U$ in a small
neighborhood.  For domains with
smooth boundaries the two notions are equivalent.  Proving
one direction is easy.

\begin{exbox}
\begin{exercise}
Suppose a domain $U \subset \R^n$ with smooth
boundary is geometrically
convex.  Show that it is convex.
\end{exercise}
\end{exbox}

The other direction is considerably more complicated, and we will not worry
about it here.  Proving a global condition from a local one is often
trickier, but also often more interesting.
Similar difficulties will be present once we move back to
several complex variables and try to relate pseudoconvexity with domains of
holomorphy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Holomorphic vectors, the Levi form, and pseudoconvexity}

As $\C^n$ is identified with $\R^{2n}$ using $z=x+iy$, we have
$T_p\C^n = T_p\R^{2n}$.  If we take the complex span instead of the real
span we get the \emph{\myindex{complexified tangent space}}
\glsadd{not:complexifiedtangentspace}%
\begin{equation*}
\C \otimes T_p\C^n
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial x_1}\Big|_p,
\frac{\partial}{\partial y_1}\Big|_p,
\ldots,
\frac{\partial}{\partial x_n}\Big|_p ,
\frac{\partial}{\partial y_n}\Big|_p \right\} .
\end{equation*}
We simply replace all the real coefficients with complex ones.  
The space $\C \otimes T_p\C^n$ is a $2n$-dimensional complex vector space.
Both
$\frac{\partial}{\partial z_j}\big|_p$ and
$\frac{\partial}{\partial \bar{z}_j}\big|_p$ are in $\C \otimes
T_p\C^n$, and in fact:
\begin{equation*}
\C \otimes T_p\C^n
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial z_1}\Big|_p,
\frac{\partial}{\partial \bar{z}_1}\Big|_p,
\ldots,
\frac{\partial}{\partial z_n}\Big|_p ,
\frac{\partial}{\partial \bar{z}_n}\Big|_p \right\} .
\end{equation*}
Define
\glsadd{not:holtangentspace}%
\glsadd{not:aholtangentspace}%
\begin{equation*}
T_p^{(1,0)} \C^n \overset{\text{def}}{=}
\operatorname{span}_{\C} \left\{
\frac{\partial}{\partial z_1}\Big|_p,
\ldots,
\frac{\partial}{\partial z_n}\Big|_p  \right\} 
\quad \text{and} \quad
T_p^{(0,1)} \C^n \overset{\text{def}}{=}
\operatorname{span}_{\C} \left\{
\frac{\partial}{\partial \bar{z}_1}\Big|_p,
\ldots,
\frac{\partial}{\partial \bar{z}_n}\Big|_p \right\} .
\end{equation*}
The vectors in $T_p^{(1,0)} \C^n$ are the
\emph{\myindex{holomorphic vectors}} and vectors in 
$T_p^{(0,1)} \C^n$ are the
\emph{\myindex{antiholomorphic vectors}}.
We decompose the full tangent space as the direct sum
\begin{equation*}
\C \otimes T_p\C^n
=
T_p^{(1,0)} \C^n \oplus
T_p^{(0,1)} \C^n .
\end{equation*}
A holomorphic function is one that vanishes on $T_p^{(0,1)} \C^n$.

Let us note what holomorphic functions do to these spaces.  Given a smooth
mapping $f$ from $\C^n$ to $\C^m$, its derivative at $p \in \C^n$
is a real-linear mapping $D_\R f(p) \colon T_p\C^n \to T_{f(p)} \C^m$.
Given the basis above, this mapping is represented by
the standard real Jacobian matrix, that is, a real $2m \times 2n$ matrix
that we wrote before as $D_\R f(p)$.

\begin{prop} \label{prop:holvectmap}
Let $f \colon U \subset \C^n \to \C^m$ be a holomorphic mapping with
$p \in U$.
Suppose 
$D_\R f(p) \colon T_p\C^n \to T_{f(p)} \C^m$
is the real derivative of $f$ at $p$.
\glsadd{not:DCf}%
We naturally
extend the derivative to $D_\C f(p) \colon \C \otimes T_p\C^n \to \C \otimes  T_{f(p)}
\C^m$.  Then
\begin{equation*}
D_\C f(p)\Bigl(T_p^{(1,0)} \C^n\Bigr) \subset T_{f(p)}^{(1,0)} \C^m
\qquad \text{and} \qquad
D_\C f(p)\Bigl(T_p^{(0,1)} \C^n\Bigr) \subset T_{f(p)}^{(0,1)} \C^m .
\end{equation*}

If $f$ is a biholomorphism, then $D_\C f(p)$ restricted to $T_p^{(1,0)} \C^n$
is a vector space isomorphism.  Similarly for $T_p^{(0,1)} \C^n$.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove the proposition.
Hint: Start with $D_\R f(p)$ as a real $2m \times 2n$ matrix to show it
extends (it is the same matrix if you think of it as a matrix
and use the same basis vectors).
Think of $\C^n$ and $\C^m$ in terms of the $z$s and the
$\bar{z}$s and think of $f$ as a mapping
\begin{equation*}
(z,\bar{z}) \mapsto \bigl( f(z) , \bar{f}(\bar{z}) \bigr) .
\end{equation*}
Write the derivative as a matrix in terms of the $z$s and the $\bar{z}$s
and $f$s and $\bar{f}$s and the result will follow.  That is just changing
the basis.
\end{exercise}
\end{exbox}

When talking about only holomorphic functions and holomorphic vectors,
when we say derivative of $f$, we mean the holomorphic part of the
derivative, which we write as
\glsadd{not:Df}%
\begin{equation*}
D f(p) \colon T_p^{(1,0)} \C^n \to T_{f(p)}^{(1,0)} \C^m .
\end{equation*}
That is, $Df(p)$ is the restriction of $D_\C f(p)$ to $T_p^{(1,0)} \C^n$.
In other words, let $z$ be the coordinates on $\C^n$ and
$w$ be coordinates on $\C^m$.
Using the bases
$\left\{ \frac{\partial}{\partial z_1} \big|_p,\ldots,
\frac{\partial}{\partial z_n} \big|_p \right\}$
on $\C^n$ and 
$\left\{ \frac{\partial}{\partial w_1} \big|_{f(p)},\ldots,
\frac{\partial}{\partial w_m} \big|_{f(p)} \right\}$ on $\C^m$,
the holomorphic derivative of $f \colon \C^{n} \to \C^m$ is represented
as the $m \times n$ Jacobian matrix
\begin{equation*}
\left[
\frac{\partial f_j}{\partial z_k} \Big|_p
\right]_{jk} ,
\end{equation*}
which we have seen before and for which we also used the notation 
$Df(p)$.

As before, define the tangent bundles
\begin{equation*}
\C \otimes T\C^n,
\quad
T^{(1,0)} \C^n,
\quad \text{and} \quad
T^{(0,1)} \C^n ,
\end{equation*}
by taking the disjoint unions.
One can also define vector fields in these bundles.

Let us describe $\C \otimes T_pM$
for a smooth real hypersurface $M \subset \C^n$.
Let $r$ be a real-valued defining function of
$M$ at $p$.  A vector
$X_p \in \C \otimes T_p\C^n$ is in
$\C \otimes T_pM$ whenever $X_p r = 0$.  That is,
\begin{equation*}
X_p = \sum_{j=1}^n
\left(
a_j 
\frac{\partial}{\partial z_j} \Big|_p
+
b_j
\frac{\partial}{\partial \bar{z}_j} \Big|_p
\right) \in \C \otimes T_p M
\quad
\text{whenever}
\quad
 \sum_{j=1}^n
\left(
a_j 
\frac{\partial r}{\partial z_j} \Big|_p
+
b_j
\frac{\partial r}{\partial \bar{z}_j} \Big|_p
\right)
= 0 .
\end{equation*}
Therefore, $\C \otimes T_p M$ is a $(2n-1)$-dimensional complex vector space.
We decompose 
$\C \otimes T_p M$ as
\begin{equation*}
\C \otimes T_pM = 
T_p^{(1,0)} M \oplus T_p^{(0,1)} M \oplus B_p ,
\end{equation*}
where
\begin{equation*}
T_p^{(1,0)} M \overset{\text{def}}{=} \bigl( \C \otimes T_pM \bigr) \cap
\bigl( T_p^{(1,0)} \C^n \bigr),  \qquad \text{and}
\qquad
T_p^{(0,1)} M \overset{\text{def}}{=} \bigl( \C \otimes T_pM \bigr) \cap
\bigl( T_p^{(0,1)} \C^n \bigr) .
\end{equation*}
The $B_p$ is just the ``leftover'' and must
be included, otherwise the dimensions will not work out.

Make sure that you understand what all the objects are.  The space
$T_pM$ is a real vector space; $\C \otimes T_pM$, $T_p^{(1,0)}M$,
$T_p^{(0,1)} M$, and $B_p$ are complex vector spaces.  To see that
these give vector bundles,
we must first show that their dimensions do not vary
from point to point.  The easiest way to see this fact is to write down
convenient local coordinates.  First, let us see what a biholomorphic map
does to the holomorphic and antiholomorphic vectors.  A biholomorphic map
$f$ is a diffeomorphism.  And if a real hypersurface $M$ is defined
by a function $r$ near $p$, then the image $f(M)$ is also
a real hypersurface is given by the defining function
$r \circ f^{-1}$ near $f(p)$.


\begin{prop}
Suppose $M \subset \C^n$ is a smooth real hypersurface, $p \in M$,
and $U \subset \C^n$ is an open set such that $M \subset U$.
Let $f \colon U \to \C^n$ be a holomorphic mapping such
that $D f(p)$ is invertible (a biholomorphism near $p$).  Let $D_\C f(p)$ be
the complexified real derivative as before.  Then
\begin{equation*}
D_\C f(p)\Bigl(T_p^{(1,0)}M\Bigr) = T_{f(p)}^{(1,0)}f(M), \qquad
D_\C f(p)\Bigl(T_p^{(0,1)}M\Bigr) = T_{f(p)}^{(0,1)}f(M).
\end{equation*}
That is, the spaces are isomorphic as complex vector spaces.
\end{prop}

The proposition is local, if $U$ is only a neighborhood of $p$,
replace $M$ with $M \cap U$.

\begin{proof}
The proof is an application of \propref{prop:holvectmap}.
As the map is a biholomorphism at $p$,
\begin{multline*}
D_\C f(p)\Bigl(T_p^{(1,0)}\C^n\Bigr) = T_{f(p)}^{(1,0)}\C^n, \quad 
D_\C f(p)\Bigl(T_p^{(0,1)}\C^n\Bigr) = T_{f(p)}^{(0,1)}\C^n, \quad
\text{and} \\
D_\C f(p)\bigl(\C \otimes T_pM\bigr) =\C \otimes  T_{f(p)}f(M) .
\end{multline*}
Then $D_\C f(p)$ must take
$T_p^{(1,0)}M$ to $T_{f(p)}^{(1,0)}f(M)$ and
$T_p^{(0,1)}M$ to $T_{f(p)}^{(0,1)}f(M)$.
\end{proof}

In the next proposition it is important to note that a translation and
applying a unitary matrix are biholomorphic changes of coordinates.

\begin{prop} \label{prop:graphcoordinatesCn}
\pagebreak[2]
Let $M \subset \C^n$ be a smooth real hypersurface, $p \in M$.
After a translation and a rotation by a unitary
matrix, $p$ is the origin, and near the origin,
$M$ is written in variables $(z,w) \in \C^{n-1}
\times \C$ as
\begin{equation*}
\Im w = \varphi(z,\bar{z},\Re w) ,
\end{equation*}
with the $\varphi(0)$  and $d\varphi(0) = 0$.  Consequently
\begin{gather*}
T_0^{(1,0)} M
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial z_1}\Big|_0,
\ldots,
\frac{\partial}{\partial z_{n-1}}\Big|_0 \right\} ,
\qquad
T_0^{(0,1)} M
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial \bar{z}_1}\Big|_0,
\ldots,
\frac{\partial}{\partial \bar{z}_{n-1}}\Big|_0 \right\} ,
\\
B_0 = \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial (\Re w)}\Big|_0 \right\} .
\end{gather*}
In particular,
$\dim_\C T_p^{(1,0)} M = \dim_\C T_p^{(0,1)} M = n-1$ and 
$\dim_\C B_p = 1$.

\nopagebreak
If $M$ is the boundary of a open set $U$ with smooth boundary,
the rotation can be chosen so that
$\Im w > \varphi(z,\bar{z},\Re w)$ on $U$.
\end{prop}

\begin{proof}
We apply a translation to put $p=0$ and in the
same manner as in \lemmaref{lemma:realgraphcoords}
apply a unitary matrix to make sure that $\nabla r$ is
in the direction
$-\frac{\partial}{\partial (\Im w)}\big|_0$.  That $\varphi(0) = 0$
and $d\varphi(0) = 0$ follows as before.
As a translation and a unitary matrix are holomorphic
and in fact biholomorphic, then
via \propref{prop:holvectmap} we obtain that the tangent spaces are all
transformed correctly.

The rest of the proposition follows at once as
$\frac{\partial}{\partial (\Im w)}\big|_0$ is the normal vector to $M$
at the origin.
\end{proof}

\begin{remark}
When $M$ is of smaller dimension than $2n-1$ (no longer a hypersurface, but
a submanifold of higher codimension), then the proposition above does not hold.
That is, we would still have $\dim_\C T_p^{(1,0)} M = \dim_\C T_p^{(0,1)}
M$, but this number need not be constant from point to point.  Fortunately,
when talking about domains with smooth boundaries, the boundaries are
hypersurfaces, and this complication does not arise.
\end{remark}

\begin{defn}
Suppose $U \subset \C^n$ is an open set with
smooth boundary, %$C^k$-smooth boundary, $k \geq 2$,
and $r$ is a defining function for $\partial U$ at $p \in \partial
U$ such that $r < 0$ on $U$.
If
\begin{equation*}
\sum_{j=1,\ell=1}^n
\bar{a}_j a_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_p \geq 0 
\qquad
\text{for all}
\qquad
X_p = \sum_{j=1}^n a_j 
\frac{\partial}{\partial z_j}\Big|_p  \quad \in \quad 
T^{(1,0)}_p \partial U,
\end{equation*}
then $U$ is said to be \emph{\myindex{pseudoconvex}} at $p$
(or \emph{\myindex{Levi pseudoconvex}}).  If the inequality
above is strict for all nonzero $X_p \in T^{(1,0)}_p \partial U$, then $U$ is said to be
\emph{\myindex{strongly pseudoconvex}}.
If $U$ is pseudoconvex, but not strongly pseudoconvex at $p$, then we 
say that $U$ is \emph{\myindex{weakly pseudoconvex}}.

A domain $U$ is \emph{pseudoconvex} if it is pseudoconvex at all $p \in \partial U$.
For a bounded\footnote{The definition for unbounded domains is not
consistent in the literature.
Sometimes \emph{\myindex{strictly pseudoconvex}} is used.} $U$, we say $U$
is \emph{strongly pseudoconvex} if it is strongly pseudoconvex at all $p \in \partial U$.

For $X_p \in T^{(1,0)}_p\partial U$, the sesquilinear form
\glsadd{not:Leviform}%
\begin{equation*}
\sL(X_p,X_p)
=
\sum_{j=1,\ell=1}^n
\bar{a}_j a_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_p 
\end{equation*}
is called the \emph{\myindex{Levi form}} at $p$.  So $U$ is pseudoconvex
(resp.\ strongly pseudoconvex) at
$p \in \partial U$ if the Levi form is positive semidefinite
(resp.\ positive definite) at $p$.
The Levi form can be defined for any real hypersurface $M$,
although one has to decide which side of $M$ is ``the inside.''
\end{defn}

The matrix
\begin{equation*}
\left[ \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_p
\right]_{j \ell}
\end{equation*}
is called the
\emph{\myindex{complex Hessian}}\index{Hessian!complex} of $r$ at
$p$.\footnote{%
People sometimes call the complex Hessian the ``Levi form of $r$,''
which is incorrect.  The Levi form is something defined for a boundary
or a submanifold, not for $r$.}
So, $U$ is pseudoconvex at $p \in \partial U$ if
the complex Hessian
of $r$ at $p$ as a sesquilinear form is positive (semi)definite
when restricted to tangent vectors in $T^{(1,0)}_p \partial U$.
For example, the unit ball $\bB_n$ is
strongly pseudoconvex as can be seen by
computing the 
Levi form directly from $r(z,\bar{z}) = \snorm{z}^2-1$, that is,
the complex Hessian of $r$ is the identity matrix.

We remark that the complex Hessian is not the full Hessian.
Let us write down the full Hessian, using the
basis of $\frac{\partial}{\partial z}$s and
$\frac{\partial}{\partial \bar{z}}$s.  It is
the symmetric matrix
\begin{equation*}
\begin{bmatrix}
\frac{\partial^2 r}{\partial z_1 \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial z_1 \partial z_n}
&
\frac{\partial^2 r}{\partial z_1 \partial \bar{z}_1} 
& \cdots &
\frac{\partial^2 r}{\partial z_1 \partial \bar{z}_n} 
\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots
\\
\frac{\partial^2 r}{\partial z_n \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial z_n \partial z_n}
&
\frac{\partial^2 r}{\partial z_n \partial \bar{z}_1} 
& \cdots &
\frac{\partial^2 r}{\partial z_n \partial \bar{z}_n} 
\\
%%%%%
\frac{\partial^2 r}{\partial \bar{z}_1 \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_1 \partial z_n}
&
\frac{\partial^2 r}{\partial \bar{z}_1 \partial \bar{z}_1} 
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_1 \partial \bar{z}_n} 
\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots
\\
\frac{\partial^2 r}{\partial \bar{z}_n \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_n \partial z_n}
&
\frac{\partial^2 r}{\partial \bar{z}_n \partial \bar{z}_1} 
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_n \partial \bar{z}_n} 
\end{bmatrix}
.
\end{equation*}
The complex Hessian is the lower left,
or the transpose of the upper right, block.  That is,
if you write the full Hessian as
$\left[ \begin{smallmatrix} X & L^t \\ L & \widebar{X} \end{smallmatrix}
\right]$, then $L$ is the complex Hessian.
In particular, $L$ is a smaller matrix. And we also apply it only to a subspace 
of the complexified tangent space.

Let us illustrate the change of basis on one dimension.  The change of
variables is left to student for higher dimensions.  Let $z =
x+iy$ be in $\C$, and we denote by $T$ the change of basis matrix:
\begin{equation*}
T = 
\begin{bmatrix}
\nicefrac{1}{2} & \nicefrac{1}{2} \\
\nicefrac{-i}{2} & \nicefrac{i}{2}
\end{bmatrix}
,
\qquad
T^t
\begin{bmatrix}
\frac{\partial^2 r}{\partial x \partial x} &&
\frac{\partial^2 r}{\partial x \partial y}
\\
\frac{\partial^2 r}{\partial y \partial x} &&
\frac{\partial^2 r}{\partial y \partial y}
\end{bmatrix}
T
=
\begin{bmatrix}
\frac{\partial^2 r}{\partial z \partial z} &&
\frac{\partial^2 r}{\partial z \partial \bar{z}}
\\
\frac{\partial^2 r}{\partial \bar{z} \partial z} &&
\frac{\partial^2 r}{\partial \bar{z} \partial \bar{z}}
\end{bmatrix}
.
\end{equation*}
The relationship between the eigenvalues of the real Hessian and the complex
Hessian is not perhaps as straightforward as may at first seem, but there is
a relationship there nonetheless.

\begin{exbox}
\begin{exercise}[Easy]
If $r$ is real-valued, then the complex Hessian of $r$ is Hermitian, that
is, the matrix is equal to its conjugate transpose.
\end{exercise}

\begin{exercise}
Consider one dimension and $z = x+iy$.
Consider
\begin{equation*}
H=\begin{bmatrix}
\frac{\partial^2 r}{\partial x\partial x} &&
\frac{\partial^2 r}{\partial x \partial y}
\\
\frac{\partial^2 r}{\partial y \partial x} &&
\frac{\partial^2 r}{\partial y\partial y}
\end{bmatrix} ,
\end{equation*}
the real Hessian in terms of $x$ and $y$.
Prove that the complex Hessian $L$ (just a number now) is $\nicefrac{1}{4}$
of the trace of $H$.
Thus, if $H$ is positive definite, then $L > 0$, and if $H$ is negative
definite, then $L < 0$.  However, show by example that
if $H$ has mixed eigenvalues (positive
and negative), then $L$ can be positive, negative, or zero.
\end{exercise}

\begin{exercise}
For any dimension,
find the change of variables $T^t H T$ to go from the real Hessian in terms
of $x$ and $y$ to the Hessian in terms of $z$ and $\bar{z}$.
Hint: If you figure it out for $n=2$, it
will be easy to do in general.
\end{exercise}

\begin{exercise}
Prove in any dimension that if the real Hessian (in terms of $x$ and $y$) is
positive (semi)definite, then the complex Hessian is positive (semi)definite.
Hint: A Hermitian matrix $L$ is positive definite if $v^*Lv > 0$ for all
nonzero vectors $v$ and semidefinite if $v*Lv \geq 0$ for all $v$.
\end{exercise}
\end{exbox}

Let us also see how a complex linear change of variables
acts on the Hessian matrix.  A complex linear change of variables is not an
arbitrary $2n \times 2n$ matrix.  If the Hessian is in the 
basis of $\frac{\partial}{\partial z}$s and
$\frac{\partial}{\partial \bar{z}}$s, an $n \times n$ complex linear matrix $A$
acts on the Hessian as $A \oplus \widebar{A}$, that is
$\left[ \begin{smallmatrix} A & 0 \\ 0 & \widebar{A} \end{smallmatrix}
\right]$.
Write the full Hessian as
$\left[ \begin{smallmatrix} X & L^t \\ L & \widebar{X} \end{smallmatrix}
\right]$, where $L$ is the complex Hessian.  Then the complex linear change
of variables $A$ transforms
the Hessian as
\begin{equation*}
{\begin{bmatrix} A & 0 \\ 0 & \widebar{A} \end{bmatrix}}^t
\begin{bmatrix} X & L^t \\ L & \widebar{X} \end{bmatrix}
\begin{bmatrix} A & 0 \\ 0 & \widebar{A} \end{bmatrix}
=
\begin{bmatrix} A^tXA & {(A^*LA)}^t \\ A^*LA & \overline{A^tXA} \end{bmatrix} ,
\end{equation*}
\glsadd{not:star}%
where $A^* = \widebar{A}^t$ is the conjugate
transpose of $A$.
So $A$
transforms the complex Hessian $L$ as $A^* L A$, that is, by $*$-congruence.
Star-congruence preserves the \emph{\myindex{inertia}}
(the number of positive, negative, and zero eigenvalues)
of a Hermitian matrix
by the Sylvester's law of inertia from linear algebra.

The Levi form itself does depend on the defining function, but the signs of
the eigenvalues do not.  It is common to say ``the
Levi form'' without mentioning a specific defining function
even though that is not completely correct.
The proof of the following proposition is left as an exercise.

\begin{prop} \label{prop:inertiainvariant}
Let $U \subset \C^n$ be an open set with smooth boundary and $p \in \partial
U$.  The inertia  of the Levi form at $p$
does not depend on the defining function at $p$.
In particular, the definition of pseudoconvexity and strong pseudoconvexity is
independent of the defining function.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove \propref{prop:inertiainvariant}.
\end{exercise}

\begin{exercise}
Show that a convex domain with smooth boundary
is pseudoconvex, and show that (a bounded) strongly convex
domain with smooth boundary is strongly pseudoconvex.
\end{exercise}

\begin{exercise}
Show that if an open set with smooth boundary is strongly pseudoconvex at a point, it is strongly
pseudoconvex at all nearby points.
\end{exercise}
\end{exbox}

We are generally interested what happens under a holomorphic change of
coordinates, that is, a biholomorphic mapping.  And as far as pseudoconvexity
is concerned we are interested in local changes of coordinates as
pseudoconvexity is a local property.  Before proving 
that pseudoconvexity is a biholomorphic invariant, let us note where the
Levi form appears in the graph coordinates from 
\propref{prop:graphcoordinatesCn}, that is, when our boundary (the
hypersurface) is given near the origin by
\begin{equation*}
\Im w = \varphi(z,\bar{z},\Re w) ,
\end{equation*}
where $\varphi$ is $O(2)$.
Let $r(z,\bar{z},w,\bar{w}) = \varphi(z,\bar{z},\Re w) - \Im w$ be our
defining function.  The complex Hessian of $r$ has the form
\begin{equation*}
\begin{bmatrix}
L & 0 \\
0 & 0
\end{bmatrix}
\qquad \text{where} \quad
L = \left[
\frac{\partial^2 \varphi}{\partial \bar{z}_j \partial z_{\ell}}\Big|_0
\right]_{j \ell} .
\end{equation*}
Note that $L$ is an $(n-1) \times (n-1)$ matrix.
The vectors in $T_0^{(1,0)} \partial U$ are the span
of $\left\{
\frac{\partial}{\partial z_1}\big|_0,
\ldots,
\frac{\partial}{\partial z_{n-1}}\big|_0 \right\}$.
That is, as an $n$-vector,
a vector in $T_0^{(1,0)} \partial U$ is represented by $(a,0) \in \C^n$ for
some
$a \in \C^{n-1}$.  The Levi form at the origin is then $a^* L a$,
in other words, it is given by the $(n-1) \times (n-1)$ matrix $L$.
If this matrix $L$ is positive
semidefinite, then $\partial U$ is pseudoconvex at $0$.

\begin{example}
Let us change variables to write the ball $\bB_n$ in different
local holomorphic coordinates where the Levi form is displayed nicely.
The sphere $\partial \bB_n$ is defined in the variables $Z = (Z_1,\ldots,Z_n) \in \C^n$
by $\snorm{Z} = 1$.

Let us change variables 
to $(z_1,\ldots,z_{n-1},w)$ where
\begin{equation*}
z_j = \frac{Z_j}{1-Z_n} \quad \text{ for all $j=1,\ldots,n-1$}, \qquad
w = i\frac{1+Z_n}{1-Z_n} .
\end{equation*}
This change of variables is a biholomorphic mapping from the set where $Z_n \not= 1$
to the set where $w\not= -i$ (exercise).  For us it is 
sufficient to notice that the map is invertible near
$(0,\ldots,0,-1)$, which follows by simply computing the derivative.
Notice
that the last component is the inverse of the Cayley transform (that takes
the disc to the upper half-plane).

We claim that the mapping takes the unit sphere given by $\snorm{Z} = 1$
(without the point $(0,\ldots,0,1)$), to
the set defined by
\begin{equation*}
\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 ,
\end{equation*}
and that it takes $(0,\ldots,0,-1)$ to the origin (this part is trivial).
Let us check:
\begin{equation*}
\begin{split}
\sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 - \Im w
& =
\abs{\frac{Z_1}{1-Z_n}}^2
+ \cdots +
\abs{\frac{Z_{n-1}}{1-Z_n}}^2
-
\frac{
i\frac{1+Z_n}{1-Z_n} -
\overline{
i\frac{1+Z_n}{1-Z_n}}
}{2i}
\\
& =
\frac{\sabs{Z_1}^2}{\sabs{1-Z_n}^2}
+ \cdots +
\frac{\sabs{Z_{n-1}}^2}{\sabs{1-Z_n}^2}
-
\frac{1+Z_n}{2(1-Z_n)} -
\frac{1+\bar{Z}_n}{2(1-\bar{Z}_n)}
\\
& = 
\frac{\sabs{Z_1}^2
+ \cdots +
\sabs{Z_{n-1}}^2
+
\sabs{Z_n}^2-1}{\sabs{1-Z_n}^2} .
\end{split}
\end{equation*}
Therefore, $\sabs{Z_1}^2 + \cdots + \sabs{Z_n}^2 = 1$ if and only if $\Im w =
\sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2$.
As the map takes the point $(0,\ldots,0,-1)$ to the origin, we can
think of the set given by
\begin{equation*}
\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 
\end{equation*}
as the sphere in local holomorphic coordinates at $(0,\ldots,0,-1)$ (by symmetry
of the sphere we could have done this at any point by rotation).
In the coordinates $(z,w)$, the ball (the inside of the sphere) is the set given by
\begin{equation*}
\Im w > \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 .
\end{equation*}

In these new coordinates, the Levi form is just the identity matrix at the
origin.  In particular, the domain is strongly pseudoconvex.
We have not yet proved that pseudoconvexity is a biholomorphic invariant,
but when we do, it will also mean that the ball is strongly pseudoconvex.

Not the entire sphere gets transformed, the points where $Z_n=1$
get ``sent to infinity.''
The hypersurface $\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2$
is sometimes called the \emph{\myindex{Lewy hypersurface}}, and in the
literature some even say it \emph{is} the sphere\footnote{That is not, in
fact, completely incorrect.
If we think of the sphere in the complex projective space,
we are simply looking at the sphere in a different coordinate patch.}.
Pretending $z$ is just one real direction, see \figureref{fig:lewy-hyper}.

\begin{myfig}
\subimport*{figures/}{lewy-hyper.eepic}
\caption{Lewy hypersurface.\label{fig:lewy-hyper}}
\end{myfig}

As an aside,
the hypersurface 
$\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2$ is also called the
\emph{\myindex{Heisenberg group}}.  The group in this case
is the group defined on the parameters $(z,\Re w)$ of this hypersurface with the
group law defined by $(z,\Re w)(z',\Re w') =
(z+z',\Re w + \Re w' + 2 \Im z \cdot z')$.
\end{example}

\begin{exbox}
\begin{exercise}
Prove the assertion in the example about the mapping being biholomorphic
on the sets described above.
\end{exercise}
\end{exbox}

Let us see how the Hessian of $r$ changes under a biholomorphic change
of coordinates.  That is, let $f \colon V \to V'$ be a biholomorphic map
between two domains in $\C^n$, and let $r \colon V' \to \R$ be a smooth
function with nonvanishing derivative.  Let us compute the Hessian of
$r \circ f \colon V \to \R$.
We first compute what happens to the nonmixed derivatives.
As we have to apply chain rule twice, to keep track better track of things,
we write where the derivatives are being evaluated, as they are, after all,
functions.
For clarity, let $z$ be the coordinates in $V$
and $\zeta$ the coordinates in $V'$.
That is, $r$ is a function of $\zeta$ and $\bar{\zeta}$,
$f$ is a function of $z$, and $\bar{f}$ is a function of $\bar{z}$.
Therefore, $r \circ f$ is a function of $z$ and $\bar{z}$.

\begin{equation*} %\label{eq:nonmixedderschangeofcoords}
\begin{split}
\frac{\partial^2 (r \circ f)}{\partial z_j \partial z_k}
%\bigg|_{(z,\bar{z})}
& =
\frac{\partial}{\partial z_j }
\sum_{\ell=1}^n 
\biggl(
\frac{\partial r}{\partial \zeta_\ell}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial f_\ell}{\partial z_k} \bigg|_{z}
+
\frac{\partial r}{\partial \bar{\zeta}_\ell}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\cancelto{0}{\frac{\partial \bar{f}_\ell}{\partial z_k} \bigg|_{\bar{z}}}
\biggr)
\\
& =
\sum_{\ell,m=1}^n
\biggl(
\frac{\partial^2 r}{\partial \zeta_m \partial \zeta_\ell}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial f_m}{\partial z_j} \bigg|_z 
\frac{\partial f_\ell}{\partial z_k} \bigg|_z 
+
\frac{\partial^2 r}{\partial \bar{\zeta}_m \partial \zeta_\ell}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\smash{\cancelto{0}{\frac{\partial \bar{f}_m}{\partial z_j} \bigg|_{\bar{z}} }}
\frac{\partial f_\ell}{\partial z_k} \bigg|_z 
\biggr)
\\
& \qquad +
\sum_{\ell=1}^n 
\frac{\partial r}{\partial \zeta_\ell} \bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial^2 f_\ell}{\partial z_j \partial z_k} \bigg|_z
\\
&=
\sum_{\ell,m=1}^n
\frac{\partial^2 r}{\partial \zeta_m \partial \zeta_\ell}
\frac{\partial f_m}{\partial z_j}
\frac{\partial f_\ell}{\partial z_k}
+
\sum_{\ell=1}^n 
\frac{\partial r}{\partial \zeta_\ell}
\frac{\partial^2 f_\ell}{\partial z_j \partial z_k} .
\end{split}
\end{equation*}
The matrix 
$\left[ \frac{\partial^2 (r \circ f)}{\partial z_j \partial z_k} \right]$
can have different eigenvalues than the matrix
$\left[ \frac{\partial^2 r}{\partial \zeta_j \partial \zeta_k} \right]$.
If $r$ has nonvanishing gradient, then
using the second term, we can (locally) choose $f$ in such a way as to make
the matrix
$\left[ \frac{\partial^2 (r \circ f)}{\partial z_j \partial z_k} \right]$
be the zero matrix (or anything else) at a certain point by choosing the second
derivatives of $f$ arbitrarily at that point.  See the exercise below.  Nothing about the matrix
$\left[ \frac{\partial^2 r}{\partial \zeta_j \partial \zeta_k} \right]$ is
preserved under a biholomorphic map.  And that is precisely why it does not
appear in the definition of pseudoconvexity.
The story for
$\left[ \frac{\partial^2 (r \circ f)}{\partial \bar{z}_j \partial \bar{z}_k} \right]$
and
$\left[ \frac{\partial^2 r}{\partial \bar{\zeta}_j \partial \bar{\zeta}_k} \right]$ is
exactly the same.

\begin{exbox}
\begin{exercise}
Given a real function $r$ with nonvanishing gradient at $p \in \C^n$.  Find
a local change of coordinates $f$ at $p$ (so $f$ ought to be a holomorphic
mapping with an invertible derivative at $p$) such that 
$\left[ \frac{\partial^2 (r \circ f)}{\partial z_j \partial z_k} \Big|_p \right]$
and
$\left[ \frac{\partial^2 (r \circ f)}{\partial \bar{z}_j \partial \bar{z}_k}
\Big|_p \right]$
are just the zero matrices.
\end{exercise}
\end{exbox}

Let us look at the mixed derivatives:
\begin{equation*}
\begin{split}
\frac{\partial^2 (r \circ f)}{\partial \bar{z}_j \partial z_k}
%\bigg|_{(z,\bar{z})}
& =
\frac{\partial}{\partial \bar{z}_j }
\sum_{\ell=1}^n 
\left(
\frac{\partial r}{\partial \zeta_\ell} \bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial f_\ell}{\partial z_k} \bigg|_z 
\right)
\\
& =
\sum_{\ell,m=1}^n 
\frac{\partial^2 r}{\partial \bar{\zeta}_m \partial \zeta_\ell }
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial \bar{f}_m}{\partial \bar{z}_j} \bigg|_{\bar{z}} 
\frac{\partial f_\ell}{\partial z_k} \bigg|_z 
+
\sum_{\ell=1}^n 
\frac{\partial r}{\partial \zeta_\ell} \bigg|_{(f(z),\bar{f}(\bar{z}))}
\smash{\cancelto{0}{\frac{\partial^2 f_\ell}{\partial \bar{z}_j \partial z_k} \bigg|_z}}
\\
&=
\sum_{\ell,m=1}^n 
\frac{\partial^2 r}{\partial \bar{\zeta}_m \partial \zeta_\ell}
\frac{\partial \bar{f}_m}{\partial \bar{z}_j} 
\frac{\partial f_\ell}{\partial z_k} .
\end{split}
\end{equation*}
The complex Hessian of $r \circ f$ is the complex Hessian $H$ of $r$
conjugated as $D^*HD$, where $D$ is the holomorphic
derivative matrix of $f$ at $z$ and
$D^*$ is its conjugate transpose.  Sylvester's law of inertia 
says that the number of positive, negative, and zero
eigenvalues of $D^*HD$ is the same as that for $H$.  The
eigenvalues may change, but their sign do not.
We are only considering $H$ and $D^*HD$ on a subspace.  In linear algebra
language, consider an invertible $D$, a subspace $T$, and its image $DT$.
Then the inertia of $H$ restricted to $DT$ is the same
as the inertia of $D^*HD$ restricted to $T$.

Let $M$ be a smooth real hypersurface given by $r=0$, then $f^{-1}(M)$ is
a smooth real hypersurface given by $r \circ f = 0$.
The holomorphic derivative $D = Df(p)$ 
takes
$T_{p}^{(1,0)}f^{-1}(M)$ isomorphically to $T_{f(p)}^{(1,0)}M$.
So $H$ is positive (semi)definite
on $T_{f(p)}^{(1,0)}M$ if and only if $D^*HD$ is positive (semi)definite
on $T_{p}^{(1,0)} f^{-1}(M)$.
We have almost proved the following theorem.  In short, pseudoconvexity is a
biholomorphic invariant.

\begin{thm}
Suppose $U, U' \subset \C^n$ are open sets with smooth boundary,
$p \in \partial U$, $V \subset \C^n$ a neighborhood of $p$,
$q \in \partial U'$, $V' \subset \C^n$ a neighborhood of $q$,
and $f \colon V \to V'$ a biholomorphic map with $f(p) = q$, such that
$f(U \cap V) = U' \cap V'$.
See \figureref{fig:bndry-bihol}.

Then the inertia of the Levi form of $U$ at $p$ is the same as the inertia of
the Levi form of $U'$ at $q$.
In particular, $U$ is pseudoconvex at $p$ if and only if $U'$ is pseudoconvex at $q$.
Similarly,
$U$ is strongly pseudoconvex at $p$ if and only if $U'$ is strongly pseudoconvex at $q$.
\end{thm}

\begin{myfig}
\subimport*{figures/}{bndry-bihol.pdf_t}
\caption{Local boundary biholomorphism.\label{fig:bndry-bihol}}
\end{myfig}

To finish proving the theorem, the only thing left is to observe that if
$f(U \cap V) = U' \cap V'$, then $f(\partial U \cap V) = \partial U' \cap
V'$, and to note that if $r$ is a defining function for $U'$ at $q$,
then $f \circ r$ is a defining function for $U$ at $p$.

\begin{exbox}
\begin{exercise}
Find an example of a bounded domain with smooth boundary that is not convex,
but that is pseudoconvex.
\end{exercise}
\end{exbox}

While the Levi form is not invariant under holomorphic changes of coordinates,
its inertia is.
Putting this together with the other observations we made above,
we will find the normal form for the
quadratic part of the defining equation for a smooth real hypersurface
under biholomorphic transformations.
It is possible to do better than the following lemma, but it is not possible
to always get rid of the dependence on $\Re w$ in the higher order terms.

\begin{lemma} \label{lemma:normformquad}
Let $M$ be a smooth real hypersurface in $\C^n$ and $p \in M$.  Then there
exists a local biholomorphic change of coordinates taking $p$ to the origin
and $M$ to the hypersurface given by
\begin{equation*}
\Im w = \sum_{j=1}^\alpha \sabs{z_j}^2 - \sum_{j=\alpha+1}^{\alpha+\beta}
\sabs{z_j}^2 +
E(z,\bar{z},\Re w) ,
\end{equation*}
where $E$ is $O(3)$ at the origin.
Here $\alpha$ is the number of positive eigenvalues of the Levi form at $p$,
$\beta$ is the number of negative eigenvalues, and $\alpha+\beta \leq
n-1$.
\end{lemma}

Recall that $O(k)$ at the origin means
a function that together with its derivatives up to order $k-1$ vanish
at the origin.

\begin{proof}
Change coordinates so that $M$ is given by
$\Im w = \varphi(z,\bar{z},\Re w)$,  where $\varphi$ is $O(2)$.
Apply Taylor's theorem to $\varphi$ up to the second order:
\begin{equation*}
\varphi(z,\bar{z},\Re w) = q(z,\bar{z}) + (\Re w) (Lz + \overline{Lz}) +
a {(\Re w)}^2 +
O(3) ,
\end{equation*}
where $q$ is quadratic, $L \colon \C^{n-1} \to \C$ is linear, and $a \in \R$.
If $L
\not= 0$, do
a linear change of coordinates in the $z$ only to make $Lz = z_1$.  So
assume $Lz = \epsilon z_1$ where $\epsilon = 0$ or $\epsilon =
1$. 

Change coordinates
by leaving $z$ unchanged and letting $w = w'+bw'^2+cw'z_1$.  Ignore
$q(z,\bar{z})$ for a moment, as
this change of coordinates does not affect it.  Also, only work up to
second order.
\begin{equation*}
\begin{split}
-\Im w +
& \epsilon (\Re w) (z_1+\bar{z}_1)
+
a {(\Re w)}^2
\\
& =
-\frac{w-\bar{w}}{2i} +
\epsilon \frac{w+\bar{w}}{2}(z_1+\bar{z}_1)
+
a{\left(\frac{w+\bar{w}}{2}\right)}^2
\\
& =
-\frac{w'+bw'^2+cw'z_1-\bar{w}'-\bar{b}\bar{w}'^2-\bar{c}\bar{w}'\bar{z}_1}{2i} 
\\
& \phantom{=}~
+\epsilon \frac{w'+bw'^2+cw'z_1+\bar{w}'+\bar{b}\bar{w}'^2+\bar{c}\bar{w}'\bar{z}_1}{2}(z_1+\bar{z}_1)
\\
& \phantom{=}~
+ a \frac{{(w'+bw'^2+cw'z_1+\bar{w}'+\bar{b}\bar{w}'^2+\bar{c}\bar{w}'\bar{z}_1)}^2}{4}
%\\
%& =
%-\frac{w'-\bar{w}'}{2i} 
%+\frac{(i-c)w'z_1+(i+\bar{c})\bar{w}'\bar{z}_1 
%+iw'\bar{z}_1+i\bar{w}'z_1}{2i}
%+O(3)
\\
& =
-\frac{w'-\bar{w}'}{2i} 
+\frac{
\bigl((\epsilon i-c)w'
+\epsilon i\bar{w}'\bigr)z_1
+\bigl((\epsilon i+\bar{c})\bar{w}'
+\epsilon iw'\bigr)\bar{z}_1
}{2i}
\\
& \phantom{=}~
+ \frac{(ia-2b)w'^2+(ia+2\bar{b})\bar{w}'^2+2iaw'\bar{w}'}{4i}
+O(3) .
\end{split}
\end{equation*}
We cannot quite get rid of all the quadratic terms in $\varphi$, but we 
choose $b$ and $c$ to make the second order terms not depend on $\Re w'$.
Set $b=ia$ and $c=2i\epsilon$, and add $q(z,\bar{z}) + O(3)$ into the mix
to get
\begin{equation*}
\begin{split}
-\Im w + \varphi(z,\bar{z}, & \Re w)  =
-\Im w +
q(z,\bar{z}) +
\epsilon (\Re w) (z_1+\bar{z}_1)
+ a{(\Re w)}^2
+O(3)
\\
%& =
%-\frac{w'-\bar{w}'}{2i} 
%+
%q(z,\bar{z})
%+\frac{
%\bigl(-iw'
%+i\bar{w}'\bigr)z_1
%+\bigl(-i\bar{w}'
%+iw'\bigr)\bar{z}_1
%}{2i}
%+O(3)
%\\
& =
-\frac{w'-\bar{w}'}{2i} 
+
q(z,\bar{z}) 
%\\
%& \phantom{=}~
- \epsilon i
\frac{w' -\bar{w}'}{2i}
( z_1 -\bar{z}_1)
+ a {\left(\frac{w'-\bar{w}'}{2i}\right)}^2
+O(3)
\\
& =
-\Im w'
+ q(z,\bar{z})
-
\epsilon i
(\Im w')
( z_1 -\bar{z}_1)
+
a {(\Im w')}^2
+O(3) .
\end{split}
\end{equation*}
The right-hand side is the defining equation in the $(z,w')$ coordinates.
However, it is no longer written as a graph of $\Im w'$ over the
rest, so we apply the implicit
function theorem to solve for $\Im w'$
and write the hypersurface as a graph again.
The expression for $\Im w'$
is $O(2)$, and therefore $-i\epsilon (\Im w')(z_1-\bar{z}_1)+a{(\Im w')}^2$
is $O(3)$.
So if we write $M$ as a graph,
\begin{equation*}
\Im w' = q(z,\bar{z}) + E(z,\bar{z},\Re w'),
\end{equation*}
then $E$ is $O(3)$.

We write the quadratic polynomial $q$ as
\begin{equation} \label{eq:generalquadraticq}
q(z,\bar{z}) =
\sum_{j,k=1}^{n-1}
a_{jk} z_jz_k
+
b_{jk} \bar{z}_j\bar{z}_k
+
c_{jk} \bar{z}_jz_k .
\end{equation}
As $q$ is real-valued, it is left as an exercise to show that $a_{jk} =
\overline{b_{jk}}$ and $c_{jk} = \overline{c_{kj}}$.  That is, the
matrix $[b_{jk}]$ is the complex conjugate of $[a_{jk}]$ and
$[c_{jk}]$ is Hermitian.

We make another change of coordinates.  We fix the $z$s again, and we set
\begin{equation} \label{eq:wprimeexpression}
w' = w'' + i
\sum_{j,k=1}^{n-1}
a_{jk} z_jz_k .
\end{equation}
In particular,
\begin{equation*}
\Im w'
= \Im w''
+ \Im \biggl(
i
\sum_{j,k=1}^{n-1}
a_{jk} z_jz_k 
\biggr)
%=
%\Im w''
%-
%\sum_{j,k=1}^{n-1}
%\bigl(
%a_{jk} z_jz_k 
%+
%\overline{a_{jk}} \bar{z}_j\bar{z}_k 
%\bigr)
=
\Im w''
+
\sum_{j,k=1}^{n-1}
\bigl(
a_{jk} z_jz_k 
+
b_{jk} \bar{z}_j\bar{z}_k 
\bigr) ,
\end{equation*}
as $\overline{a_{jk}} = b_{jk}$.  Plugging
\eqref{eq:wprimeexpression} into $\Im w' = q(z,\bar{z}) + E(z,\bar{z},\Re w')$
and solving for $\Im w''$ cancels the
holomorphic and antiholomorphic terms in $q$, and leaves $E$ as $O(3)$.
After this change of coordinates we may assume
\begin{equation*}
q(z,\bar{z}) = \sum_{j,k=1}^{n-1} c_{jk} z_j \bar{z}_k .
\end{equation*}
That is, $q$ is a sesquilinear form.  Since $q$ is real-valued, the matrix
$C = [ c_{jk} ]$ is Hermitian.  In linear algebra notation,
$q(z,\bar{z}) = z^*Cz$,
where %the $*$ denotes the conjugate transpose, and
we think of $z$ as a column vector.
If $T$ is a linear transformation on the $z$ variables, say $z'=Tz$, we
obtain ${z'}^*Cz' = {(Tz)}^*CTz = z^* ( T^*CT) z$.  Thus, we normalize $C$
up to $*$-congruence.  A Hermitian matrix
is $*$-congruent to a diagonal matrix with only $1$s, $-1$s, and $0$s on the
diagonal, again by Sylvester's law of inertia.  Writing out what that means is precisely the conclusion of the
proposition.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the assertion in the proof, that is, if $q$ is a quadratic as in
\eqref{eq:generalquadraticq} that is real-valued, then
 $a_{jk} = \overline{b_{jk}}$ and $c_{jk} = \overline{c_{kj}}$.
\end{exercise}
\end{exbox}

\begin{lemma}[Narasimhan's lemma\index{Narasimhan's lemma}%
\footnote{A statement essentially of Narasimhan's lemma was already used by Helmut
Knesser in 1936.}]
Let $U \subset \C^n$ be an open set with smooth boundary
that is strongly pseudoconvex at $p \in \partial U$.  Then there exists a local biholomorphic change of
coordinates fixing $p$ such that in these new coordinates, $U$ is strongly
convex at $p$ and hence strongly convex at all points near $p$.
\end{lemma}

\begin{exbox}
\begin{exercise}
Prove Narasimhan's lemma.  Hint: See the proof of \lemmaref{lemma:normformquad}.
\end{exercise}

\begin{exercise}
Prove that an open $U \subset \C^n$ with smooth boundary is pseudoconvex at
$p$ if and only if there exist local holomorphic coordinates at $p$ such that 
$U$ is convex at $p$.
\end{exercise}
\end{exbox}

Narasimhan's lemma only works at points of strong
pseudoconvexity.  For weakly pseudoconvex points the situation is far more
complicated.  The difficulty is that we cannot make a $U$ that is
weakly pseudoconvex at all points near $p$ to be convex at all points near $p$.

\medskip


Let us prove the easy direction of the famous 
\emph{\myindex{Levi problem}}.  The Levi problem was a long-standing
problem%
\footnote{E.\ E.\ Levi stated the problem in 1911, but it was not completely
solved until the 1950s, by Oka and others.}
in several complex variables to classify domains of holomorphy in
$\C^n$.  The answer is that a domain is a domain of holomorphy if and only
if it is pseudoconvex.  Just as the problem of trying to show that
the classical geometric convexity is the same as convexity as we have
defined it, 
the Levi problem has an easier direction and a harder direction.
The easier direction is to show that a domain of holomorphy is pseudoconvex, and
the harder direction is to show that a pseudoconvex domain is a domain of
holomorphy.  See H\"ormander's book~\cite{Hormander} for the proof
of the hard direction.

\begin{thm}[Tomato can principle\index{tomato can principle}] \label{thm:tomatocan}
Suppose
$U \subset \C^n$ is an open set with smooth boundary and at some point $p \in
\partial U$ the Levi form has a negative eigenvalue.
Then every holomorphic function on $U$
extends to a neighborhood of $p$.
In particular, $U$ is not
a domain of holomorphy.
\end{thm}

Pseudoconvex at $p$ means that all eigenvalues of the Levi form are
nonnegative.
The theorem says that a domain of holomorphy must be pseudoconvex.
The theorem's name comes from the proof, and sometimes other theorems using a
similar proof of a ``tomato can'' of analytic discs are called
tomato can principles.
The general statement of proof of the principle is that ``an
analytic function holomorphic in a neighborhood of the sides and the bottom
of a tomato can extends to the inside.''  And the theorem we named after
the principle states that ``if the Levi form at $p$ has a negative
eigenvalue, we can fit a tomato can from inside the domain over $p$.''

\begin{proof}
We change variables so that $p = 0$, and
near $p$, $U$ is given by
\begin{equation*}
\Im w > -\sabs{z_1}^2 + \sum_{j=2}^{n-1} \epsilon_j \sabs{z_j}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) ,
\end{equation*}
where $z' = (z_2,\ldots,z_{n-1})$, $\epsilon_j = -1,0,1$, and $E$ is $O(3)$.
We embed an analytic disc via the map
$\xi \overset{\varphi}{\mapsto} (\lambda \xi, 0, 0, \ldots, 0)$
for some small $\lambda > 0$.
Clearly $\varphi(0) = 0 \in \partial U$.  For $\xi \not= 0$ near the origin
\begin{equation*}
-\lambda^2 \sabs{\xi}^2 + \sum_{j=2}^{n-1} \epsilon_j \sabs{0}^2 + E(\lambda
\xi,0,\lambda \bar{\xi},0,0)
=
-\lambda^2 \sabs{\xi}^2 + E(\lambda
\xi,0,\lambda \bar{\xi},0,0)
%=
%-\sabs{\xi}^2 + E(\xi,0,\bar{\xi},0,0)
< 0 .
\end{equation*}
That is because
the function above has a strict maximum at $\xi = 0$
by the second derivative test.
Therefore, for $\xi \not= 0$ near the origin,
$\varphi(\xi) \in U$.  By picking $\lambda$ small enough,
$\varphi(\overline{\D}\setminus\{0\}) \subset U$.

As $\varphi(\partial \D)$ is compact we can ``wiggle it a little'' and
find discs in $U$.  In particular, for all small enough $s > 0$,
the closed disc given by
\begin{equation*}
\xi \overset{\varphi_s}{\mapsto} (\lambda \xi, 0, 0, \ldots, 0, i s) 
\end{equation*}
is entirely inside $U$ (that is, for slightly positive $\Im w$).
Fix such a small $s > 0$.
Suppose $\epsilon > 0$ is small and $\epsilon < s$.
Define the Hartogs figure
\begin{equation*}
\begin{split}
H =
& \bigl\{ (z,w) : \lambda - \epsilon < \sabs{z_1} < \lambda + \epsilon,
\sabs{z_j} < \epsilon \text{ for } j=2,\ldots,n-1, \text{ and }
\sabs{w-is} < s+\epsilon \bigr\} 
\\
&
\cup 
\bigl\{ (z,w) : \sabs{z_1} < \lambda + \epsilon, 
\sabs{z_j} < \epsilon \text{ for } j=2,\ldots,n-1, \text{ and }
\sabs{w-is} < \epsilon \bigr\} .
\end{split}
\end{equation*}
The set where $\sabs{z_1} = \lambda$, $z' = 0$
and $\sabs{w} \leq s$, is inside $U$ for all small enough $s$, so
an $\epsilon$-neighborhood of that is in $U$.
For
$w = is$ the whole disc where $\sabs{z_1} \leq \lambda$ is in $U$,
so an $\epsilon$-neighborhood of that is in $U$.
Thus, for small enough $\epsilon >0$, $H \subset U$.
We are really just taking a Hartogs figure in the $z_1,w$ variables, and then
``fattening it up'' to the $z'$ variables.
In \figureref{fig:hartogs-figure-tomato}, we picture the Hartogs figure in the $\sabs{z_1}$ and $\sabs{w-is}$
variables.  The boundary $\partial U$ and $U$ are only pictured diagrammatically.
Also, we make a ``picture'' the analytic discs giving the ``tomato can.''
In the picture, the $U$ is below its boundary $\partial U$, unlike usually.

\begin{myfig}
\subimport*{figures/}{hartogs-figure-tomato.pdf_t}
\caption{Tomato can principle.\label{fig:hartogs-figure-tomato}}
\end{myfig}

The origin is in the hull of $H$, and so
every function holomorphic in $U$, and so in $H$, extends through the origin.
Hence $U$ is not a domain of holomorphy.
\end{proof}

\begin{exbox}
\begin{exercise}
\pagebreak[2]
For the following domains in $U \subset \C^2$,
find all the
points in $\partial U$ where $U$ is weakly pseudoconvex, all the points
where it is strongly pseudoconvex, and all the points where it is
not pseudoconvex.  Is $U$ pseudoconvex?
\begin{exparts}
\item
$\Im w > \sabs{z}^4$
\item
$\Im w > \sabs{z}^2(\Re w)$
\item
$\Im w > (\Re z)(\Re w)$
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $U \subset \C^n$ be an open set with smooth boundary that is
strongly pseudoconvex at $p \in \partial U$.  Show that
$p$ is a so-called \emph{\myindex{peak point}}: There
exists a neighborhood $W$ of $p$ and a holomorphic $f \colon
W \to \C$ 
such that $f(p)=1$ and $\sabs{f(z)} < 1$ for all
$z \in W \cap \widebar{U} \setminus \{ p \}$.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set with smooth boundary.  Suppose
for $p \in \partial U$, there is a neighborhood $W$ of $p$
and a holomorphic function $f \colon W \to
\C$ such that $df(p) \not= 0$, $f(p) = 0$, but
$f$ is never zero on $W \cap U$.  Show that $U$ is pseudoconvex
at $p$.  Hint: You may need the holomorphic implicit function theorem, see
\thmref{thm:ift}.
Note: The result does not require the derivative of $f$ to not vanish, but is
much harder to prove without that hypothesis.
\end{exercise}
\end{exbox}

A hyperplane is the ``degenerate'' case of normal convexity.
There is also a flat case of pseudoconvexity.  A smooth real hypersurface
$M \subset \C^n$ is \emph{\myindex{Levi-flat}} if the Levi form
vanishes at every point of $M$.  The zero matrix is positive semidefinite
and negative semidefinite, so both sides of $M$ are pseudoconvex.
Conversely, the only hypersurface pseudoconvex from both sides is a
Levi-flat one.

\begin{exbox}
\begin{exercise}
Suppose $U = V \times \C^{n-1} \subset \C^n$, where $V \subset \C$ is an
open set with smooth boundary.  Show that $U$ is has a smooth Levi-flat boundary.
\end{exercise}

\begin{exercise}
Prove that a real hyperplane is Levi-flat.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be open, $f \in \sO(U)$, and
$M = \bigl\{ z \in U : \Im f(z) = 0 \bigr\}$.  Show that
if $df(p) \not=0$ for some $p \in M$, then near $p$,
$M$ is a Levi-flat hypersurface.
\end{exercise}


\begin{exercise}
Suppose $M \subset \C^n$ is a smooth Levi-flat hypersurface,
$p \in M$, and
a complex line $L$ is tangent to $M$ at $p$.
Prove that $p$ is not an isolated point of $L \cap M$.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set with smooth boundary
and $\partial U$ is Levi-flat.
Show that $U$ is unbounded.
Hint: If $U$ were bounded, consider the point on $\partial U$ farthest from
the origin.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Harmonic, subharmonic, and plurisubharmonic functions}
\label{sec:harmonic}

\begin{defn}
Let $U \subset \R^n$ be an open set.
A $C^2$-smooth
function $f \colon U \to \R$ is
\emph{\myindex{harmonic}} if\footnote{The operator $\nabla^2$,
sometimes also written $\Delta$,
is the \emph{\myindex{Laplacian}}.
It is the trace of the Hessian matrix.}
\begin{equation*}
\nabla^2 f =
\frac{\partial^2 f}{\partial x_1^2} +
\cdots +
\frac{\partial^2 f}{\partial x_n^2} = 0
\quad \text{ on $U$.}
\end{equation*}

A function $f \colon U \to \R \cup \{ -\infty \}$ is 
\emph{\myindex{subharmonic}} if it is upper-semicontinuous%
\footnote{Recall $f$ is \emph{\myindex{upper-semicontinuous}}
if $\limsup_{t\to x} f(t) \leq f(x)$ for all $x \in U$.}
and for every
ball $B_r(a)$ with $\overline{B_r(a)} \subset U$,
and every function $g$
continuous on $\overline{B_r(a)}$ and
harmonic on $B_r(a)$,
such that $f(x) \leq g(x)$ for $x \in
\partial B_r(a)$, we have
\begin{equation*}
f(x) \leq g(x), \quad \text{ for all } x \in
B_r(a) .
\end{equation*}
\end{defn}

In other words, a subharmonic function is a function that is less than every
harmonic function on every ball.
We remark that when $n=1$ in the definition of a subharmonic function,
it is the same as the standard definition of a
convex function of one real variable, where affine linear functions play the role of harmonic
functions: A function of one real variable is
\emph{convex}\index{convex function} if 
for every interval it is less than the affine linear function with the same
end points.
After all, a function of one real variable is harmonic if the second
derivative vanishes, and it is therefore affine linear.
In one real dimension it is also easier to picture.
The function $f$ is convex if on
every interval $[\alpha,\beta]$, $f \leq g$ for every affine linear $g$
bigger than $f$ at the endpoints $\alpha$ and $\beta$.  In particular, we can
take the $g$ that is equal to $f$ at the endpoints.  See
\figureref{fig:convexfunc}.
The picture is analogous for subharmonic functions for $n > 1$,
but it is harder to draw.

\begin{myfig}
\subimport*{figures/}{convexfunc.pdf_t}
\caption{Convex function.\label{fig:convexfunc}}
\end{myfig}

We will consider harmonic and subharmonic functions in $\C \cong
\R^2$.
Let us go through some basic results on harmonic and subharmonic
functions in $\C$ that you have seen in detail in your one-variable class.
Consequently, we leave some of these results as exercises.
In this section (and not just here)
we often write $f(z)$ for a function
even if it is not holomorphic.

\begin{exbox}
\begin{exercise}
An upper-semicontinuous function achieves a maximum on compact sets.
\end{exercise}

\begin{exercise}
Let $U \subset \C$ be open.
Show that for a $C^2$ function $f \colon U \to \R$,
\begin{equation*}
\frac{\partial^2}{\partial \bar{z}\partial z} f = \frac{1}{4} \nabla^2 f .
\end{equation*}
Use this to show that $f$ is harmonic if and only if it is
(locally) the real or imaginary part
of a holomorphic function.
Hint: The key is finding an antiderivative of a holomorphic function.
\end{exercise}

\begin{exercise}
Prove the identity theorem.  Let $U \subset \C$ be a domain
and $f \colon U \to \R$ harmonic such that $f=0$ on a nonempty open subset
of $U$.  Then $f\equiv 0$.
\end{exercise}
\end{exbox}

It follows from the exercise that a harmonic function is infinitely differentiable.
It is useful to find a harmonic function given boundary values.
This problem is called the \emph{\myindex{Dirichlet problem}}, and it is
solvable for many (though not all) domains.
The proof of the following special case is contained in the exercises
following the theorem.
The
\emph{\myindex{Poisson kernel}} for the unit disc $\D \subset \C$ is
\glsadd{not:Pr}%
\begin{equation*}
P_r(\theta)
= \frac{1}{2\pi} \frac{1-r^2}{1+r^2-2r \cos \theta}
= \frac{1}{2\pi}
\operatorname{Re} \left( \frac{1+re^{i\theta}}{1-re^{i\theta}}\right) ,
\qquad \text{for $0 \leq r < 1$.}
\end{equation*}

\begin{thm} \label{thm:dirichsol}
Let $u \colon \partial \D \to \R$ be a continuous function.
The function
$Pu \colon \overline{\D} \to \R$, defined by
\begin{equation*}
Pu(re^{i\theta})
=
\int_{-\pi}^\pi u(e^{it}) P_r(\theta-t) \, dt
\quad \text{if $r < 1$} \qquad \text{and} \qquad
Pu(e^{i\theta}) = u(e^{i\theta}),
\avoidbreak
\end{equation*}
is harmonic in $\D$ and continuous on $\overline{\D}$.
\end{thm}

In the proof, it is useful to consider how
the graph of $P_r$ as a function of $\theta$ looks for a fixed $r$.  See
\figureref{fig:poisson-kernel}.

\begin{myfig}
\includegraphics[width=0.5\textwidth]{figures/poisson-kernel.pdf}
\caption{Graph of $P_r$ for $r=0.5$,
$r=0.7$, and $r=0.85$ on $[-\pi,\pi]$.\label{fig:poisson-kernel}}
\end{myfig}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Prove $P_r(\theta) > 0$ for all $0 \leq r < 1$ and all $\theta$.
\item
Prove $\int_{-\pi}^{\pi} P_r(\theta) \, d\theta = 1$.
\item
Prove for any given $\delta > 0$,
$\sup \{P_r(\theta) : \delta \leq \abs{\theta} \leq \pi \} \to 0$ as
$r \to 1$.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[1]%
Prove \thmref{thm:dirichsol} using the following guideline:
\begin{exparts}
\item
Poisson kernel is harmonic
as a function of $z=re^{i\theta} \in \D$, and hence
$Pu$ is harmonic.
\item
$P$ acts like an
approximate identity: Prove that
$Pu(re^{i\theta}) \to u(e^{i\theta})$ uniformly as
$r \to 1$ (Hint: Split the integral to $[-\delta,\delta]$ and the rest
and use the previous exercise).
\item
Prove that $Pu(z)$ tends to $u(z_0)$ as
$z \in \D \to z_0 \in \partial \D$.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]%
State and prove a version of \thmref{thm:dirichsol} for an arbitrary disc
$\Delta_r(a)$.
\end{exercise}

\begin{exercise}
Prove that the Dirichlet problem is not solvable in the punctured disc $\D
\setminus \{ 0 \}$.
Hint: Let $u = 0$ on $\partial \D$ and $u(0)=1$.
The solution would be
less than $- \epsilon \log \sabs{z}$ for every $\epsilon > 0$.
\end{exercise}
\end{exbox}

The Poisson kernel is a reproducing kernel for
holomorphic functions, as (the real and imaginary parts of) holomorphic functions are harmonic.
Poisson kernel exists for higher dimensions as well.
Solving the Dirichlet problem using the Poisson kernel leads to
the following result.

\begin{prop}[Mean-value property and sub-mean-value property]
\pagebreak[2]%
\label{prop:meansubmeanprop}%
\index{mean-value property}%
\index{sub-mean-value property}%
Let $U \subset \C$ be an open set.
\begin{enumerate}[(i)]
\item
A continuous function
$f \colon U \to \R$
is harmonic if and only if 
\begin{equation*}
f(a) = \frac{1}{2\pi} \int_0^{2\pi} f(a+re^{i\theta})\, d\theta
\qquad \text{whenever} \quad
\overline{\Delta_r(a)} \subset U .
\end{equation*}
\item
An upper-semicontinuous function $f \colon U \to \R \cup \{ -\infty \}$
is subharmonic if and only if
\begin{equation*}
f(a) \leq \frac{1}{2\pi} \int_0^{2\pi} f(a+re^{i\theta})\, d\theta
\qquad \text{whenever} \quad
\overline{\Delta_r(a)} \subset U .
\end{equation*}
\end{enumerate}
\end{prop}

For the sub-mean-value property you may have to use 
the Lebesgue integral to integrate an upper-semicontinuous function,
and to use the version of the Poisson integral above, you need to
approximate by continuous functions on the boundary in the right way.
On first reading, feel free to think of continuous subharmonic
functions and not too much will be lost.

\begin{exbox}
\begin{exercise}
Fill in the details of the proof of \propref{prop:meansubmeanprop}.
\end{exercise}

\begin{exercise}
Let $U \subset \C$ be open.
Show that if $f \colon U \to \R \cup\{- \infty \}$ is subharmonic,
then 
\begin{equation*}
\limsup_{w \to z} f(w) = f(z) 
\qquad \text{for all $z \in U$.}
\end{equation*}
\end{exercise}

\begin{exercise} \label{exercise:fminusgsubharmonic}
Suppose $U \subset \C$ is open and $g \colon U \to \R$ is harmonic.
Then $f \colon U \to \R \cup \{ -\infty \}$ is subharmonic if and only if $f-g$
is subharmonic.
\end{exercise}
\end{exbox}

\begin{prop}[Maximum principle]
\index{maximum principle!subharmonic functions}
Suppose $U \subset \C$ is a domain and $f \colon U \to \R \cup \{ -\infty \}$
is subharmonic.  If $f$ attains a maximum in $U$, then $f$ is constant.
\end{prop}

\begin{proof}
Suppose $f$ attains a maximum at $a \in U$.
If
$\overline{\Delta_r(a)} \subset U$, then
\begin{equation*}
f(a) \leq \frac{1}{2\pi} \int_0^{2\pi} f(a+re^{i\theta})\, d\theta \leq f(a)
.
\end{equation*}
Hence, $f = f(a)$ almost everywhere on $\partial \Delta_r(a)$.
By upper-semicontinuity, $f = f(a)$ everywhere on $\partial \Delta_r(a)$.
This was true for all $r$
with $\overline{\Delta_r(a)} \subset U$, so $f=f(a)$ on $\Delta_r(a)$,
and so the set where $f=f(a)$ is open.  The set where an upper-semicontinuous
function attains a maximum is closed.  So $f=f(a)$ on $U$ as $U$ is
connected.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that subharmonicity is a local property.  That is, given an open set
$U \subset \C$, a function $f \colon U \to \R \cup \{ -\infty \}$ is subharmonic if
and only if for every $p \in U$ there exists a neighborhood $W$ of $p$,
$W \subset U$, such that $f|_{W}$ is subharmonic.  Hint: Perhaps try to use
the maximum principle and \exerciseref{exercise:fminusgsubharmonic}.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C$ is a bounded open set, $f \colon \widebar{U} \to \R
\cup \{-\infty\}$ is an upper-semicontinuous function, such that $f|_U$
is subharmonic, $g \colon \widebar{U} \to \R$ is a continuous function
such that $g|_U$ is harmonic and
$f(z) \leq g(z)$ for all $z \in \partial U$.  Prove that
$f(z) \leq g(z)$ for all $z \in U$.
\end{exercise}

\begin{exercise} \label{exercise:onlyniceuneededforsubharmonic}
Let $g$ be a function
harmonic on a disc $\Delta \subset \C$ and continuous on
$\overline{\Delta}$.  Prove that for every $\epsilon > 0$ there exists
a function $g_\epsilon$, harmonic in a neighborhood of $\overline{\Delta}$,
such that $g(z) \leq g_\epsilon(z) \leq g(z)+\epsilon$ for all $z \in
\overline{\Delta}$.
In particular, to test subharmonicity, we only need to consider those
$g$ that are harmonic a bit past the boundary of the disc.
\end{exercise}
\end{exbox}

\begin{prop}
Suppose $U \subset \C$ is an open set and $f \colon U \to \R$ is a $C^2$ function.
The function $f$ is subharmonic if and only if
$\nabla^2 f \geq 0$.
\end{prop}

\begin{proof}
Suppose $f$ is a $C^2$-smooth function on a subset of $\C \cong \R^2$
with $\nabla^2 f \geq 0$.  We wish to show that $f$ is subharmonic.
Take a disc $\Delta$ such that $\overline{\Delta} \subset U$.
Consider a function
$g$ continuous on $\overline{\Delta}$,
harmonic on $\Delta$, and such that
$f \leq g$ on the boundary $\partial \Delta$.  Because
$\nabla^2 (f-g) = \nabla^2 f \geq 0$, we assume $g = 0$ and $f \leq 0$
on the boundary $\partial \Delta$. 

Suppose $\nabla^2 f > 0$ at all points on $\Delta$.
Suppose $f$ attains a maximum in $\Delta$,
call this point $p$.  
The Laplacian $\nabla^2 f$ is the trace of the Hessian matrix, but for $f$ to have a
maximum, the Hessian must have only nonpositive eigenvalues at the critical
points, which is a
contradiction as the trace is the sum of the eigenvalues.  So $f$ has no
maximum inside, and therefore $f \leq 0$ on all of
$\overline{\Delta}$.

Next suppose $\nabla^2 f \geq 0$.
Let $M$ be the maximum of $x^2+y^2$ on $\overline{\Delta}$.
Take $f_n(x,y) = f(x,y) + \frac{1}{n}
( x^2+y^2 ) - \frac{1}{n}M$.  Clearly $\nabla^2 f_n > 0$ everywhere on
$\Delta$ and
$f_n \leq 0$ on the boundary, so $f_n \leq 0$ 
on all of $\overline{\Delta}$.  As $f_n \to f$, we obtain that
$f \leq 0$ on all of $\overline{\Delta}$.

The other direction is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
Finish the proof of the proposition above.
\end{exercise}
\end{exbox}

In analogy to convex functions, a $C^2$-smooth function $f$ of one
real variable is convex if and only if $f''(x) \geq 0$ for all $x$.

\begin{prop}
\pagebreak[2]%
Suppose $U \subset \C$ is an open set and $f_\alpha \colon U \to \R \cup \{ -\infty \}$
is a family of subharmonic functions.  Let
\begin{equation*}
\varphi(z) = \sup_\alpha\, f_\alpha(z) .
\avoidbreak
\end{equation*}
If the family is finite, then $\varphi$ is subharmonic.
If the family is infinite, $\varphi(z) \not= \infty$ for
all $z$, and $\varphi$
is upper-semicontinuous, then $\varphi$ is subharmonic.
\end{prop}

\begin{proof}
Suppose $\overline{\Delta_r(a)} \subset U$.  For any $\alpha$,
\begin{equation*}
\frac{1}{2\pi} \int_0^{2\pi} \varphi (a+re^{i\theta})\, d\theta 
\geq
\frac{1}{2\pi} \int_0^{2\pi} f_\alpha (a+re^{i\theta})\, d\theta 
\geq f_\alpha(a) .
\end{equation*}
Taking the supremum on the right over $\alpha$ obtains the results.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that if $\varphi \colon \R \to \R$ is a monotonically increasing
convex function, $U \subset \C$ is an open set, and $f \colon U \to \R$
is subharmonic, then $\varphi \circ f$ is subharmonic.
\end{exercise}

\begin{exercise}
Let $U \subset \C$ be open, $\{ f_n \}$ a sequence of 
subharmonic functions uniformly bounded above on compact subsets, and 
$\{ c_n \}$ a sequence of positive real numbers such that
$\sum_{n=1}^\infty c_n < \infty$.
Prove that $f = \sum_{n=1}^\infty c_n f_n$ is subharmonic.  Make sure to prove
the function is upper-semicontinuous.
\end{exercise}

\begin{exercise}
\pagebreak[2]%
Suppose $U \subset \C$ is a bounded open set, and $\{ p_n \}$ a sequence of points in
$U$. For $z \in U$, define
$f(z) = \sum_{n=1}^\infty 2^{-n} \log \sabs{z-p_n}$, possibly taking on the
value $-\infty$.
\begin{exparts}
\item
Show that $f$ is a subharmonic function in $U$.
\item
If $U = \D$ and $p_n = \frac{1}{n}$, show that $f$ is discontinuous at 0
(the natural topology on $\R \cup \{ -\infty \}$).
\item
If $\{ p_n \}$ is dense in $U$, show that $f$ 
is discontinuous on a dense set.
Hint: Prove that $f^{-1}(-\infty)$ is a small (but dense) set.
Another hint: Integrate the partial sums, and use polar coordinates.
\end{exparts}
\end{exercise}
\end{exbox}

\pagebreak[2]
There are too many harmonic functions in $\C^n \cong \R^{2n}$.
The real and imaginary parts of holomorphic functions in $\C^n$
form a smaller set when $n > 1$.  Notice that when a holomorphic
function is restricted to a complex line, we obtain a holomorphic
function of one variable.  So the real and imaginary parts
of a holomorphic function had better be harmonic on every complex line.
It turns out, this is precisely the right class of functions.

\begin{defn}
Let $U \subset \C^n$ be open.
A $C^2$-smooth $f \colon U \to \R$ is 
\emph{\myindex{pluriharmonic}} if for every $a,b \in \C^n$, the function
of one variable
\begin{equation*}
\xi \mapsto f(a+b\xi)
\end{equation*}
is harmonic (on the set of $\xi \in \C$ where $a+b\xi \in U$).
That is, $f$ is harmonic on every complex line.

A function $f \colon U \to \R \cup \{ -\infty \}$ is 
\emph{\myindex{plurisubharmonic}}, sometimes \emph{\myindex{plush}}
or \emph{\myindex{psh}} for short,
if it is upper-semicontinuous and for every 
$a,b \in \C^n$, the function of one variable
\begin{equation*}
\xi \mapsto f(a+b\xi)
\end{equation*}
is subharmonic (whenever $a+b\xi \in U$).
\end{defn}

A harmonic function of one complex variable is in some sense a
generalization of an affine linear function of one real variable.
Similarly, as far as several complex variables are concerned, a
pluriharmonic function is the right generalization to $\C^n$
of an affine linear function on $\R^n$.
In the same way plurisubharmonic functions are the correct complex variable generalizations
of convex functions.  A convex function of one real variable is like a
subharmonic function, and a convex function of several real variables is
a function that is convex when restricted to any real line.

\begin{exbox}
\begin{exercise}
Let $U \subset \C^n$ be open.
Prove that
a $C^2$-smooth $f \colon U \to \R$ is pluriharmonic if and
only if
\begin{equation*}
\frac{\partial^2 f}{\partial \bar{z}_j \partial z_k} = 0
\quad \text{ on $U$ for all $j,k=1,\ldots,n$.}
\end{equation*}
\end{exercise}

\begin{exercise}
Show that a pluriharmonic function is harmonic.  On the other hand, find an
example of a harmonic function that is not pluriharmonic.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be open.
Show that $f \colon U \to \R$ is pluriharmonic if and
only if it is locally the real or imaginary part of a holomorphic function.
Hint:  Using a previous exercise
$\frac{\partial f}{\partial z_k}$ is holomorphic for all $k$.
Assume that $U$ is simply connected, $p \in U$, and
$f(p) = 0$.
Consider the line integral from $p$ to a nearby $q \in U$:
\begin{equation*}
F(q) =
\int_{p}^q \sum_{k=1}^n
\frac{\partial f}{\partial z_k}(\zeta) \, d\zeta_k .
\avoidbreak
\end{equation*}
Prove that it is path independent, compute derivatives of $F$, and
find out what is $F+\bar{F}-f$.
\end{exercise}

\begin{exercise}
Prove the maximum
principle\index{maximum principle!plurisubharmonic functions}:
If $U \subset \C^n$ is a domain and $f \colon U \to \R \cup
\{-\infty\}$ is
plurisubharmonic and achieves a maximum at $p \in U$, then $f$ is constant.
\end{exercise}
\end{exbox}

\begin{samepage}
\begin{prop}
Let $U \subset \C^n$ be open.
A $C^2$-smooth $f \colon U \to \R$ is plurisubharmonic
if and only if the complex Hessian matrix
\begin{equation*}
\left[
\frac{\partial^2 f}{\partial \bar{z}_j \partial z_k}
\right]_{jk}
\end{equation*}
is positive semidefinite at every
point.
\end{prop}
\end{samepage}

\begin{proof}
First suppose that the complex Hessian has a negative eigenvalue at
a point $p \in U$.
After a translation assume $p=0$.
As $f$ is real-valued, the complex Hessian
$\left[
\frac{\partial^2 f}{\partial \bar{z}_j \partial z_k}
\Big|_0
\right]_{jk}$ is Hermitian.  A complex linear change of coordinates
acts on the complex Hessian by $*$-congruence, and therefore we can
diagonalize,
using Sylvester's Law of Inertia again.
So assume that 
$\left[
\frac{\partial^2 f}{\partial \bar{z}_j \partial z_k}
\Big|_0
\right]_{jk}$ is diagonal.  If the complex Hessian has a negative eigenvalue, then
one of the diagonal entries is negative.
Without loss of generality suppose
$\frac{\partial^2 f}{\partial \bar{z}_1 \partial z_1}\Big|_0 < 0$.
The function $z_1 \mapsto f(z_1,0,\ldots,0)$ has
a negative Laplacian and therefore is not subharmonic, and thus $f$ itself
is not plurisubharmonic.

For the other direction, suppose the complex Hessian is positive
semidefinite at all points.
Let $p \in U$.  After an affine change of coordinates assume that the
line $\xi \mapsto a+b\xi$ is simply setting all but the first variable to
zero, that is, $a=0$ and $b=(1,0,\ldots,0)$.
As the complex Hessian is positive semidefinite,
$\frac{\partial^2 f}{\partial \bar{z}_1 \partial z_1} \geq 0$ for all
points $(z_1,0,\ldots,0)$.  We proved above that $\nabla^2 g \geq 0$
implies $g$ is subharmonic, and we are done.
\end{proof}

\begin{exbox}
\begin{exercise} \label{exercise:modholplush}
Suppose $U \subset \C^n$ is open and $f \colon U \to \C$ is holomorphic.
\begin{exparts}
\item
Show $\log \sabs{f(z)}$ is plurisubharmonic.  In fact, it is pluriharmonic away from the zeros of $f$.
\item
Show $\sabs{f(z)}^{\eta}$ is plurisubharmonic for all $\eta > 0$.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that the set of plurisubharmonic functions on an open set $U \subset \C^n$
is a cone in the sense that if $a,b > 0$ are constants and
$f, g \colon U \to \R \cup \{ -\infty \}$ are plurisubharmonic, then
$a f + b g$ is plurisubharmonic.
\end{exercise}
\end{exbox}

\begin{thm} \label{thm:subharlim}
\pagebreak[2]%
Suppose $U \subset \C^n$ is an open set and $f \colon U \to \R \cup \{
-\infty \}$ is plurisubharmonic.  For every $\epsilon > 0$,
let $U_\epsilon \subset U$
be the set of points further than $\epsilon$ away from $\partial U$.
Then there exists a smooth plurisubharmonic function
$f_\epsilon \colon U_\epsilon \to \R$ such that $f_\epsilon(z) \geq
f(z)$, and
\begin{equation*}
f(z) = \lim_{\epsilon \to 0} f_\epsilon(z) \qquad \text{for all $z \in U$}.
\end{equation*}
\end{thm}

That is, $f$ is a limit of smooth plurisubharmonic functions.
The idea of the proof is important and useful in many other
contexts.

\begin{proof}
We smooth $f$ out by convolving with so-called
\emph{mollifiers}\index{mollifier}, or
\emph{approximate delta functions}\index{approximate delta function}.
Many different mollifiers 
work, but let us use a specific one for concreteness.
For $\epsilon > 0$, define 
\begin{equation*}
g(z) = 
\begin{cases}
C e^{-1/(1-\snorm{z}^2)} & \text{ if $\snorm{z} < 1$,}
\\
0 & \text{ if $\snorm{z} \geq 1$,}
\end{cases}
\qquad
\text{and}
\qquad
g_\epsilon(z) = \frac{1}{\epsilon^{2n}} g(z/\epsilon) .
\end{equation*}
It is left as an exercise that $g$, and so $g_\epsilon$, is smooth.
The function $g$ has compact
support as it is only nonzero inside the unit ball.  The support of
$g_\epsilon$ is the $\epsilon$-ball.  Both are nonnegative.  Choose $C$ so that
\begin{equation*}
\int_{\C^n} g\, dV = 1 ,
\qquad \text{ and therefore } \qquad
\int_{\C^n} g_\epsilon\, dV = 1 .
\end{equation*}
\glsadd{not:dV}%
Here $dV$ is the volume measure.
The function $g$ only depends on $\snorm{z}$.
To get an idea of
how these functions work,
see \figureref{fig:graph-of-mollifier}.

\begin{myfig}
\includegraphics[width=0.5\textwidth]{figures/graph-of-mollifier.pdf}
\caption{Graphs of $e^{-1/(1-x^2)}$,
$\frac{1}{0.5}e^{-1/\bigl(1-{(x/0.5)}^2\bigr)}$, and
$\frac{1}{0.25}e^{-1/\bigl(1-{(x/0.25)}^2\bigr)}$.\label{fig:graph-of-mollifier}}
\end{myfig}

Compare the graphs to the graphs of the Poisson kernel as a function of
$\theta$, which is also a type of mollifier.  In fact, the idea of
integrating against the right approximate delta function with the desired properties
is similar to the solution of the Dirichlet
problem using the Poisson kernel.

The function $f$ is bounded above on compact sets as it is upper semicontinuous.
If $f$ is not bounded below, we replace $f$ with $\max \bigl\{ f ,
\frac{-1}{\epsilon}
\bigr\}$, which is still plurisubharmonic.  Therefore, without loss of generality
we assume that $f$ is locally bounded.

For $z \in U_\epsilon$, we define $f_\epsilon$ as
the convolution with $g_\epsilon$:
\glsadd{not:convolution}%
\begin{equation*}
f_\epsilon(z) = (f * g_\epsilon)(z) =
\int_{\C^n} f(w) g_\epsilon (z-w) \, dV(w) =
\int_{\C^n} f(z-w) g_\epsilon (w) \, dV(w) .
\end{equation*}
The two forms of the integral follow easily via change of variables.
We are perhaps abusing notation a bit as $f$ is only defined on $U$,
but it is not a problem as long as $z \in
U_\epsilon$, because $g_\epsilon$ is then zero when $f$ is undefined.
By differentiating the first form under the integral, we find that
$f_\epsilon$ is smooth.

Let us show that $f_\epsilon$ is plurisubharmonic.  We restrict to a
line $\xi \mapsto a+b\xi$.
We wish to test subharmonicity by the sub-mean-value property using a circle
of radius $r$ around $\xi = 0$:
\begin{equation*}
\begin{split}
\frac{1}{2\pi} \int_0^{2\pi} f_\epsilon(a+bre^{i\theta})\, d\theta & =
\frac{1}{2\pi} \int_0^{2\pi}
\int_{\C^n}
f\bigl(a+bre^{i\theta}-w\bigr) g_\epsilon (w) \, dV(w) 
\,d\theta
\\
& =
\int_{\C^n}
\left(
\frac{1}{2\pi} \int_0^{2\pi}
f\bigl(a-w+bre^{i\theta}\bigr) \, d\theta \right) g_\epsilon (w) \, dV(w) 
\\
& \geq 
\int_{\C^n}
f(a-w) g_\epsilon (w) \, dV(w)  = f_\epsilon(a).
\end{split}
\end{equation*}
For the inequality we used $g_\epsilon \geq 0$.
So $f_\epsilon$ is plurisubharmonic.

Let us show that $f_\epsilon(z) \geq f(z)$ for all $z \in U_\epsilon$.
As $g_\epsilon(w)$ only depends on $\sabs{w_1},\ldots,\sabs{w_n}$, we notice
that
$g_\epsilon(w_1,\ldots,w_n) =
g_\epsilon(\sabs{w_1},\ldots,\sabs{w_n})$.
Without loss of generality we consider $z=0$, and we use polar coordinates
for the integral.
\pagebreak[1]
\begin{equation*}
\begin{split}
f_\epsilon(0)
& =
\int_{\C^n} f(-w) g_\epsilon (\sabs{w_1},\ldots,\sabs{w_n})
\, dV(w)
\\
& =
\int_0^\epsilon \cdots
\int_0^\epsilon
\left(
\int_0^{2\pi}
\cdots
\int_0^{2\pi}
 f(-r_1e^{i\theta_1},\ldots,
-r_ne^{i\theta_n}) \,
d\theta_1 \cdots d\theta_n \right)
\\
%& \phantom{=}\qquad
& \hspace{2.8in}
g_\epsilon (r_1,\ldots,r_n) \,
 r_1 \cdots r_n \,d r_1 \cdots d r_n
\\
& \geq
\int_0^\epsilon \cdots
\int_0^\epsilon
\left(
\int_0^{2\pi}
\cdots
\int_0^{2\pi}
(2\pi)
 f(0,-r_2e^{i\theta_2},\ldots,
-r_ne^{i\theta_n}) \,
d\theta_2 \cdots d\theta_n \right)
\\
%& \phantom{=}\qquad
& \hspace{2.8in}
g_\epsilon (r_1,\ldots,r_n) \,
 r_1 \cdots r_n \,d r_1 \cdots d r_n
\\
& \geq
f(0)
\int_0^\epsilon \cdots
\int_0^\epsilon
{(2\pi)}^n
g_\epsilon (r_1,\ldots,r_n) \,
 r_1 \cdots r_n \,d r_1 \cdots d r_n
\\
& = f(0) \int_{\C^n} g_\epsilon (w) \, dV(w)
%\\
%&
= f(0) .
\end{split}
\end{equation*}
The second equality above
follows as $g_\epsilon$ is zero
outside the polydisc of radius $\epsilon$.
For the inequalities, we again needed that $g_\epsilon \geq 0$.
The penultimate equality follows from the fact that
$2\pi = \int_0^{2\pi}d \theta$.

Finally, we show $\lim_{\epsilon \to 0} f_\epsilon (z) = f(z)$.
For subharmonic, and so for plurisubharmonic,
functions, $\limsup_{\zeta\to z} f(\zeta) = f(z)$.
So given $\delta >0$ find an $\epsilon >0$ such that
$f(\zeta)-f(z) \leq \delta$ for all $\zeta \in B_\epsilon(z)$.
\begin{equation*}
\begin{split}
f_\epsilon(z) - f(z)
& =
\int_{B_\epsilon(0)} f(z-w) g_\epsilon (w)
\, dV(w)
- f(z) 
\int_{B_\epsilon(0)} g_\epsilon (w)
\, dV(w)
\\
& =
\int_{B_\epsilon(0)} \bigl(f(z-w)-f(z)\bigr)\, g_\epsilon (w)
\, dV(w)
\\
& \leq
\delta
\int_{B_\epsilon(0)} g_\epsilon (w)
\, dV(w)
= \delta .
\end{split}
\end{equation*}
Again we used that $g_\epsilon \geq 0$.
We find $0 \leq f_\epsilon - f(z) \leq \delta$, and so $f_\epsilon(z) \to
f(z)$.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that $g$ in the proof above is smooth on all of $\C^n$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Show that for a subharmonic function, $\int_0^{2\pi} f(a+re^{i\theta}) \,
d\theta$ is a monotone function of $r$ (Hint: Try a $C^2$ function first and
use Green's theorem).
\item
Use this
fact to show that the $f_\epsilon(z)$ from \thmref{thm:subharlim} is monotone
decreasing in $\epsilon$.
\end{exparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$
and $V \subset \C^m$ be open.
Prove that
if $g \colon U \to V$ is holomorphic and $f
\colon V \to \R$ is a $C^2$ plurisubharmonic function, then 
$f \circ g$ is plurisubharmonic.
Then use it to prove the same fact
for all plurisubharmonic functions (Hint: monotone convergence).
\end{exercise}

\begin{exercise}
Show that plurisubharmonicity is a local property, that is,
$f$ is plurisubharmonic if and only if $f$ is plurisubharmonic in
some neighborhood of each point.
\end{exercise}

\begin{exercise}
Using the computation from
\thmref{thm:subharlim} show that if $f$ is pluriharmonic, then
$f_\epsilon = f$ (where it makes sense), obtaining another proof that 
a pluriharmonic function is $C^\infty$.
\end{exercise}

\begin{exercise}
Let the $f$ in \thmref{thm:subharlim} be continuous and suppose $K \subset
\subset U$.  For small enough $\epsilon >0$, $K \subset U_\epsilon$.
Show that $f_\epsilon$ converges uniformly to $f$ on $K$.
\end{exercise}

\begin{exercise}
Let the $f$ in \thmref{thm:subharlim} be $C^k$-smooth for some $k \geq 0$.
Show that all derivatives of $f_\epsilon$ up to order $k$ converge uniformly
on compact sets to the corresponding derivatives of $f$.  See also previous
exercise.
\end{exercise}
\end{exbox}

Let us prove the theorem of 
Rad\'o, which is a complementary result to the Riemann extension theorem.
Here on the one hand the function is
continuous and vanishes on the set you wish to extend across, but on the
other hand you know nothing about this set.
It is sometimes covered in a one-variable course,
and in several variables it follows directly from
the one-variable result.

\begin{thm}[Rad\'o] \index{Rad\'o's theorem}\label{thm:rado}
Let $U \subset \C^n$ be open and $f \colon U \to \C$ a continuous
function that is holomorphic on the set
\begin{equation*}
U' = \bigl\{ z \in U : f(z) \not= 0 \bigr\} .
\avoidbreak
\end{equation*}
Then $f \in \sO(U)$.
\end{thm}

\begin{proof}
First assume $n=1$.  As the theorem is local, it is
enough to prove it for a small disc $\Delta$ such that $f$ is continuous
on the closure $\overline{\Delta}$, let $\Delta'$ be the part of the disc
where $f$ is nonzero.  If $\Delta'$ is empty, then we are done as
$f$ is just identically zero and hence holomorphic.

Let $u$ be the real part of $f$.  On $\Delta'$, $u$ is a harmonic function.
Let $Pu$ be the Poisson integral of $u$ on $\Delta$.  Hence $Pu$
equals $u$ on $\partial \Delta$, and $Pu$ is harmonic in all of $\Delta$.
Consider the function
$Pu(z) - u(z)$ on $\overline{\Delta}$.  The function is zero
on $\partial \Delta$ and it is harmonic on $\Delta'$.  By rescaling $f$
we can without loss of generality assume that $\abs{f(z)} < 1$ for all $z
\in \overline{\Delta}$.  For any $t >0$, the function 
$z \mapsto t \log \abs{f(z)}$ is subharmonic on $\Delta'$ and
upper-semicontinuous on $\overline{\Delta'}$.  Further, it is negative
on $\partial \Delta$.  The function $z \mapsto -t \log \abs{f(z)}$ is
superharmonic (minus a subharmonic function) on $\Delta'$,
lower-semicontinuous on $\overline{\Delta'}$ and positive on $\partial
\Delta$.  On the set $\Delta \setminus \Delta'$ where $f$ is zero, the two functions are $-\infty$ and
$\infty$ respectively.
See \figureref{fig:radosthm}.
Therefore, for all $t > 0$ and 
$z \in \partial \Delta \cup (\Delta \setminus \Delta')$,
we have
\begin{equation} \label{eq:radobound}
t \log \abs{f(z)} \leq Pu(z)-u(z) \leq -t \log \abs{f(z)}  .
\end{equation}
Applying the maximum principle to the subharmonic functions
$z \mapsto t \log \abs{f(z)} - \bigl(Pu(z)-u(z)\bigr)$
and
$z \mapsto t \log \abs{f(z)} - \bigl(u(z)-Pu(z)\bigr)$
shows that 
\eqref{eq:radobound} holds for all $z \in \Delta'$ and all $t > 0$.

\begin{myfig}
\subimport*{figures/}{radosthm.pdf_t}
\caption{Proof of Rad\'o's theorem.\label{fig:radosthm}}
\end{myfig}


Taking the limit
$t \to 0$ shows that $Pu = u$ on $\Delta'$.
Let $W = \Delta \setminus \overline{\Delta'}$.
On $W$, $u=0$ and so $Pu-u$ is harmonic on $W$
and continuous on $\widebar{W}$.  Furthermore,
$Pu-u=0$ on $\overline{\Delta'} \cup \partial \Delta$,
and so $Pu-u=0$ on $\partial W$.  By the maximum principle, $Pu=u$ on $W$
and therefore on all of $\overline{\Delta}$.
Similarly, if $v$ is the imaginary part of $f$, then $Pv = v$ on
$\overline{\Delta}$.
In other words, $u$ and $v$ are harmonic on $\Delta$.
As $\Delta$ is simply connected,
let $\tilde{v}$ be the harmonic conjugate of $u$ that equals $v$ at
some point of $\Delta'$.  As $f$ is holomorphic on $\Delta'$,
the harmonic functions $\tilde{v}$ and $v$
are equal the nonempty open subset $\Delta'$ of $\Delta$ and so
they are equal everywhere.  Consequently, $f = u +iv$ is holomorphic on
$\Delta$.

The extension of the proof to several variables is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
Use the one-variable result to extend the theorem to several variables.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hartogs pseudoconvexity}

It is worth mentioning explicitly that by the
exercises of the previous section,
plurisubharmonicity is preserved under holomorphic mappings.
That is, if $g$ is holomorphic and $f$ is plurisubharmonic, then
$f \circ g$ is plurisubharmonic.  In particular, if $\varphi \colon \D \to
\C^n$ is an analytic disc and $f$ is plurisubharmonic in a neighborhood of
$\varphi(\D)$,
then $f \circ \varphi$ is subharmonic.

\begin{defn}
Let $\sF$ be a class of (extended\footnote{%
By extended reals we mean $\R \cup \{ -\infty,\infty\}$.})-real-valued
functions defined on an open $U \subset \R^n$.  If $K
\subset U$, define $\widehat{K}$, the \emph{\myindex{hull}} of $K$ with
respect to $\sF$, as the set
\glsadd{not:Khat}%
\begin{equation*}
\widehat{K} \overset{\text{def}}{=} \Bigl\{ x \in U : f(x) \leq \sup_{y\in K} f(y)
\text{ for all $f \in \sF$ } \Bigr\} .
\end{equation*}

An open set $U$ is said to be \emph{convex with respect to $\sF$}\index{convex!with respect to $\sF$}
if for every $K \subset \subset U$, the hull $\widehat{K} \subset \subset U$.%
\footnote{Recall that $\subset \subset$ means relatively compact, that is,
the closure in the relative (subspace) topology is compact.}
\end{defn}

Clearly $K \subset \widehat{K}$.  The key is to show that $\widehat{K}$
is not ``too large'' for $U$.
Keep in mind that the functions in $\sF$ are defined on $U$, so $\widehat{K}$
depends on $U$ not just on $K$.  An easy mistake is to consider functions defined
on a larger set, obtaining a smaller $\sF$ and hence a larger
$\widehat{K}$.  Sometimes it is useful to write $\widehat{K}_{\sF}$ to
denote the dependence on $\sF$, especially when talking about several different
hulls.

For example, if $U=\R$ and $\sF$ is the set of real-valued smooth
$f \colon \R \to \R$ with $f''(x) \geq 0$, then for any $a,b \in \R$
we have $\widehat{\{ a, b \}} = [a,b]$.
In general, if $\sF$ was the set of convex functions, then 
a domain $U \subset \R^n$ is geometrically convex if and only if it is
convex with respect to convex functions, although let us not define
what that means except for smooth functions in exercises below.

\begin{exbox}
\begin{exercise} \label{exercise:geomconvexfuncs}
Suppose $U \subset \R^n$ is a domain.
\begin{exparts}
\item
Show that $U$
is geometrically convex if and only if it is
convex with respect to the affine linear functions.
\item
Suppose $U$
has smooth boundary.
Show that $U$ is 
convex if and only if it is
convex with respect to the smooth convex functions on $U$,
that is smooth functions with positive semidefinite Hessian.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that every open set $U \subset \R^n$ is convex with respect to real
polynomials.
\end{exercise}
\end{exbox}

\begin{thm}[Kontinuit\"atssatz---Continuity
principle\index{Kontinuit\"atssatz}\index{continuity principle}]
\label{thm:contprinciple}
Suppose an open set $U \subset \C^n$ is convex with respect to plurisubharmonic
functions,
then given any collection of closed analytic discs $\Delta_\alpha \subset U$
such that $\bigcup_\alpha \partial \Delta_\alpha \subset \subset U$,
we have
$\bigcup_\alpha \Delta_\alpha \subset \subset U$.
\end{thm}

Various similar theorems are named the \emph{continuity principle}.
Generally what they have in common is the family of analytic discs whose
boundaries stay inside a domain, and whose conclusion has to do
with extension of holomorphic functions, or with domains of holomorphy.

\begin{proof}
Let $f$ be a plurisubharmonic function on $U$.  If $\varphi_\alpha \colon
\overline{\D} \to U$ is the holomorphic (in $\D$) mapping giving the closed
analytic disc, then $f \circ \varphi_\alpha$ is subharmonic.
By the maximum principle,
$f$ on $\Delta_\alpha$ must be less than or equal to the supremum
of $f$ on $\partial \Delta_\alpha$, so $\overline{\Delta_\alpha}$
is in the hull of 
$\partial \Delta_\alpha$.
In other words,
$\bigcup_\alpha \Delta_\alpha$ is in the hull of
$\bigcup_\alpha \partial \Delta_\alpha$ and therefore 
$\bigcup_\alpha \Delta_\alpha \subset \subset U$ by convexity.
\end{proof}

Let us illustrate the failure of the continuity principle.
If you have discs (denoted by straight line segments) that approach the boundary as in
\figureref{fig:contprinc}, then the domain is not
not convex with respect to plurisubharmonic functions.
In the diagram, the boundaries of the discs are
denoted by the dark dots at the end of the segments.
In fact, if we replace discs with line segments, this is the standard
convexity, see the exercises below.

\begin{myfig}
\subimport*{figures/}{contprinc.pdf_t}
\caption{Failure of the continuity principle.\label{fig:contprinc}}
\end{myfig}

\begin{exbox}
\begin{exercise}
Suppose $U \subset \C^n$ is a domain and $K \subset \subset U$ is a nonempty
compact subset.  Prove that $U \setminus K$ is not convex with respect to
plurisubharmonic functions.
\end{exercise}

\begin{exercise} \label{exercise:affinedisctouchingsmooth}
Suppose $U \subset \C^n$ is a domain with smooth boundary,
$p \in \partial U$,
and $\Delta$ is an affine linear analytic disc with $p \in \Delta$, but
$\Delta \setminus \{ p \} \subset U$.  Prove that $U$ is not convex with
respect to the plurisubharmonic functions.
\end{exercise}

\begin{exercise}
Prove the corresponding
Kontinuit\"atssatz, and its converse, for geometric convexity:
Prove that a domain $U \subset \R^n$ is geometrically convex if and only if
whenever $[x_\alpha,y_\alpha] \subset U$
is a collection of straight line segments such that
$\bigcup_{\alpha} \{ x_\alpha,y_\alpha \} \subset \subset U$
implies
$\bigcup_{\alpha} [ x_\alpha,y_\alpha ] \subset \subset U$.
\end{exercise}
\end{exbox}

\begin{defn}
Let $U \subset \C^n$ be open.  An $f \colon U \to \R$ is an \emph{\myindex{exhaustion function}} for $U$ if
\begin{equation*}
\bigl\{ z \in U : f(z) < r \bigr\} \subset \subset U
\qquad \text{for every $r \in \R$.}
\end{equation*}

A domain $U \subset \C^n$ is
\emph{\myindex{Hartogs pseudoconvex}}\index{pseudoconvex}
if
there exists a continuous plurisubharmonic exhaustion function.
The set
$\{ z \in U : f(z) < r \}$ is called the \emph{\myindex{sublevel set}} of
$f$,
or the $r$-\emph{sublevel set}.
\end{defn}

\begin{example}
The unit ball $\bB_n$ is Hartogs pseudoconvex.  The continuous
function
\begin{equation*}
z \mapsto - \log \bigl( 1-\snorm{z} \bigr) 
\end{equation*}
is an exhaustion function, and it is easy to
check directly that it is plurisubharmonic.
\end{example}

\begin{example}
The entire $\C^n$ is Hartogs pseudoconvex as $\snorm{z}^2$ is
a continuous plurisubharmonic exhaustion function.
Also, because $\snorm{z}^2$ is plurisubharmonic, then given any $K \subset \subset
\C^n$, the hull $\widehat{K}$ with respect to plurisubharmonic functions must 
be bounded.  In other words, $\C^n$ is convex with respect to
plurisubharmonic functions.
\end{example}

\begin{samepage}
\begin{thm}
Suppose $U \subsetneq \C^n$ is a domain.  The following are equivalent:
\begin{enumerate}[(i)]
\item \label{thm:pscvx:itemi}
$-\log \rho(z)$ is plurisubharmonic, where $\rho(z)$ is the distance from $z$
to $\partial U$.
\item \label{thm:pscvx:itemii}
$U$ has a continuous plurisubharmonic exhaustion function,
that is, $U$ is Hartogs pseudoconvex.
\item \label{thm:pscvx:itemiii}
$U$ is convex with respect to plurisubharmonic functions defined on $U$.
\end{enumerate}
\end{thm}
\end{samepage}

\begin{proof}
\ref{thm:pscvx:itemi}
$\Rightarrow$
\ref{thm:pscvx:itemii}:
If $U$ is bounded,
the function $-\log \rho(z)$ is clearly a continuous exhaustion function.
If $U$ is unbounded, take 
$z \mapsto \max \{ -\log \rho(z) , \snorm{z}^2 \}$.

\ref{thm:pscvx:itemii}
$\Rightarrow$
\ref{thm:pscvx:itemiii}:
Suppose $f$ is a continuous plurisubharmonic exhaustion function.
If $K \subset \subset U$, then for some $r$ we have
$K \subset \{ z \in U : f(z) < r \} \subset \subset U$.
But then by definition of the hull $\widehat{K}$ we have
$\widehat{K} \subset \{ z \in U : f(z) < r \} \subset \subset U$.

\ref{thm:pscvx:itemiii}
$\Rightarrow$
\ref{thm:pscvx:itemi}:
For $c \in \C^n$ with $\snorm{c}=1$, let
$\rho_c(z)$ be the radius of the largest affine disc centered at $z$
in the direction $c$ that still lies in $U$.  That is,
\begin{equation*}
\rho_c(z) =
\sup \bigl\{ \lambda > 0 :
z+ \zeta c \in U \text{ for all $\zeta \in \lambda\D$} \bigr\} .
\end{equation*}
As $\rho(z) = \inf_c \rho_c(z)$,
\begin{equation*}
- \log \rho(z) = \sup_{\snorm{c}=1} \bigl(-\log \rho_c(z)\bigr) .
\end{equation*}
If we prove that for any $a, b \in \C^n$ the function $\xi \mapsto -\log \rho_c(a+b\xi)$ is
subharmonic, then $\xi \mapsto - \log \rho(a+b\xi)$ is subharmonic,
and we are done.
See \figureref{fig:distfun-hartogs} for the setup:
\begin{myfig}
\subimport*{figures/}{distfun-hartogs.pdf_t}
\caption{Largest disc in the direction of $c$.
The disc is drawn as a line.\label{fig:distfun-hartogs}}
\end{myfig}

Suppose $\Delta \subset \C$ is a disc such that
$a+b\xi \in U$
for all $\xi \in
\overline{\Delta}$.
If $u$ is a harmonic function on $\Delta$ continuous on $\overline{\Delta}$
such that
$- \log \rho_c(a+b\xi) \leq u(\xi)$ on $\partial \Delta$, we must
show that 
the inequality holds on $\Delta$.
By \exerciseref{exercise:onlyniceuneededforsubharmonic}
we may assume $u$ is harmonic on a neighborhood of $\overline{\Delta}$
and so let $u = \Re f$ for a holomorphic function $f$.
Fix a $\xi \in \partial \Delta$.  We have  $- \log \rho_c(a+b\xi) \leq \Re
f(\xi)$,
or in other words
\begin{equation*}
\rho_c(a+b\xi) \geq e^{-\Re f(\xi)} = \babs{e^{-f(\xi)}}.
\end{equation*}
Using $\zeta = t e^{-f(\xi)}$ in the definition of $\rho_c(a+b\xi)$, the statement above is equivalent
to saying that 
\begin{equation*}
(a+b\xi)+te^{-f(\xi)}c \in U \quad \text{for all $t \in \D$}.
\end{equation*}
This statement holds whenever $\xi \in \partial \Delta$.  We must prove that
it also holds for all $\xi \in \Delta$.

The function $\varphi_t(\xi) = 
(a+b\xi)+te^{-f(\xi)}c$ gives a closed analytic disc with boundary inside
$U$.  We have a family of analytic discs, parametrized by $t$, whose boundaries are in
$U$ for all $t$ with $\sabs{t} < 1$.  For $t=0$ the entire disc is
inside $U$.  As $\varphi_t(\xi)$ is continuous in both $t$ and $\xi$ and
$\overline{\Delta}$ is compact,
$\varphi_t(\Delta) \subset U$ for $t$ in some neighborhood of $0$.
Take $0 < t_0 < 1$ such that
$\varphi_t(\Delta) \subset U$ for all $t$ with $\sabs{t} < t_0$.
Then
\begin{equation*}
\bigcup_{\sabs{t} < t_0} \varphi_t(\partial \Delta)
\subset
\bigcup_{\sabs{t} \leq t_0} \varphi_t(\partial \Delta)
\subset \subset U ,
\end{equation*}
because continuous functions take compact sets to compact sets.
\hyperref[thm:contprinciple]{Kontinuit\"atssatz}
implies 
\begin{equation*}
\bigcup_{\sabs{t} < t_0} \varphi_t(\Delta)
\subset \subset U  .
\end{equation*}
By continuity again
$\bigcup_{\sabs{t} \leq t_0} \varphi_t(\Delta)
\subset \subset U$, and so
$\bigcup_{\sabs{t} < t_0+\epsilon} \varphi_t(\Delta)
\subset \subset U$
for some $\epsilon > 0$.
Consequently $\varphi_t(\Delta) \subset U$
for all $t$ with $\sabs{t} < 1$.  Thus
$(a+b\xi)+te^{-f(\xi)}c \in U$ for all $\xi \in \Delta$ and all $\sabs{t} <
1$.  This implies $\rho_c(a+b\xi) \geq e^{-\Re f(\xi)}$ for all $\xi \in
\Delta$, which in
turn implies $-\log \rho_c(a+b\xi) \leq \Re f(\xi) = u(\xi)$ for all $\xi
\in \Delta$. 
Therefore, $-\log \rho_c(a+b\xi)$ is subharmonic.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that if $U_1 \subset \C^n$ and $U_2 \subset \C^n$ are Hartogs
pseudoconvex domains, then so are all the topological components of $U_1 \cap U_2$.
\end{exercise}

\begin{exercise}
Show that if $U \subset \C^n$ and $V \subset \C^m$ are Hartogs
pseudoconvex domains, then so is $U \times V$.
\end{exercise}

\begin{exercise}
Show that every domain $U \subset \C$ is Hartogs pseudoconvex.
\end{exercise}

\begin{exercise} \label{exercise:nestedunions}
Consider the union $U = \bigcup_k U_k$ of a nested sequence of Hartogs pseudoconvex
domains, $U_{k-1} \subset U_k \subset \C^n$.  Show that $U$ is Hartogs pseudoconvex.
\end{exercise}

\begin{exercise}
Let $\R^2 \subset \C^2$ be naturally embedded (that is, it is the
set where $z_1$ and $z_2$ are real).  Show that the set $\C^2 \setminus
\R^2$ is not Hartogs pseudoconvex.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain and $f \in \sO(U)$.  Prove that
$U' = \bigl\{ z \in U : f(z) \not= 0 \bigr\}$ is a Hartogs pseudoconvex
domain.  Hint: See also \exerciseref{exercise:connectedcomplement}.
\end{exercise}

\begin{exercise} \label{exercise:biholHartogs}
Suppose $U,V \subset \C^n$ are biholomorphic domains.
Prove that $U$ is Hartogs pseudoconvex if and only if $V$ is
Hartogs pseudoconvex.
\end{exercise}

\begin{exercise}
Let $U = \bigl\{ z \in \C^2 : \sabs{z_1} > \sabs{z_2} \bigr\}$.
\begin{exparts}
\item
Prove that $U$ is a Hartogs pseudoconvex domain.
\item
Find a closed analytic disc $\Delta$ in $\C^2$ such that $0 \in \Delta$ ($0
\notin U$)
and $\partial \Delta \subset U$.
\item
What do you think would happen if you tried to move $\Delta$ a
little bit to avoid the intersection with the complement?
Think about the \hyperref[thm:contprinciple]{continuity principle}.
Compare with \exerciseref{exercise:affinedisctouchingsmooth}.
\end{exparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain, and $f \colon \widebar{U} \to \R$ be a
continuous function, plurisubharmonic and negative on $U$, and $f=0$ on $\partial
U$.  Prove that $U$ is Hartogs pseudoconvex.
\end{exercise}
\end{exbox}

The statement corresponding to \exerciseref{exercise:nestedunions} on nested unions
for domains of holomorphy is
the \emph{\myindex{Behnke--Stein theorem}}, which follows using this exercise and the solution
of the Levi problem.  Although historically Behnke--Stein was proved
independently and used to solve the Levi problem.

\exerciseref{exercise:biholHartogs} says that (Hartogs) pseudoconvexity is a
biholomorphic invariant.  That is a good indication that we are looking at a
correct notion.  It also allows us to change variables to more convenient
ones when proving a specific domain is (Hartogs) pseudoconvex.

It is not immediately clear from the definition, but Hartogs pseudoconvexity
is a local property.

\begin{lemma}
A domain $U \subset \C^n$ is Hartogs pseudoconvex if and only if
for every point $p \in \partial U$ there exists a neighborhood $W$ of $p$
such that $W \cap U$ is Hartogs pseudoconvex.
\end{lemma}

\begin{proof}
One direction is trivial, so consider the other.
Suppose $p \in \partial U$, and let
$W$ be such that $U \cap W$
is Hartogs pseudoconvex.  Intersection of
Hartogs pseudoconvex domains is Hartogs pseudoconvex, so
assume $W = B_r(p)$.
Let $B = B_{r/2}(p)$.  If $q \in B \cap U$, the distance from $q$ to the boundary of $W \cap U$ is the same as
the distance to $\partial U$.  The setup is illustrated in
\figureref{fig:hartogs-pseudoconvex-local}.

\begin{myfig}
\subimport*{figures/}{hartogs-pseudoconvex-local.pdf_t}
\caption{Local Hartogs pseudoconvexity.\label{fig:hartogs-pseudoconvex-local}}
\end{myfig}

The part of the boundary $\partial U$ in $W$ is marked by a thick
black line, the part of the boundary of $\partial (W \cap U)$ that arises as
the boundary of $W$ is marked by a thick gray line.  A point $q \in B$ is
marked and a ball of radius $\nicefrac{r}{2}$ around $q$ is dotted.
No point of distance $\nicefrac{r}{2}$ from $q$ is in $\partial W$, and 
the distance of $q$ to $\partial U$ is at most $\nicefrac{r}{2}$ as $p \in \partial U$
and $p$ is the center of $B$.
\glsadd{not:dist}%
Let $\operatorname{dist}(x,y)$ denote the
euclidean distance function\footnote{If either $x$ and/or $y$ are sets
of points, we take the infimum of the euclidean distance over all the points.}.
Then for $z \in B \cap U$
\begin{equation*}
- \log \, \operatorname{dist}(z, \partial U) = 
- \log \, \operatorname{dist}\bigl(z, \partial (U \cap W)\bigr).
\end{equation*}
The right-hand side is plurisubharmonic as $U \cap W$ is Hartogs
pseudoconvex.  Such a ball $B$ exists around every $p \in \partial U$, so near
the boundary, $- \log \, \operatorname{dist}(z, \partial U)$ is
plurisubharmonic.

If $U$ is bounded, then $\partial U$ is compact.  So there is some
$\epsilon > 0$ such that $- \log \, \operatorname{dist}(z, \partial U)$
is plurisubharmonic if $\operatorname{dist}(z, \partial U) < 2\epsilon$.
The function
\begin{equation*}
\varphi(z) = \max \bigl\{
- \log \, \operatorname{dist}(z, \partial U) , - \log \epsilon \bigr\} 
\end{equation*}
is a continuous plurisubharmonic exhaustion function.
The proof for
unbounded $U$ is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
Finish the proof of the lemma for unbounded domains.
See~\exerciseref{exercise:nestedunions}.
\end{exercise}
\end{exbox}



It may seem that we defined a totally different concept, but it turns
out that Levi and Hartogs pseudoconvexity are one and the same on domains
where both concepts make sense.
As a consequence of the following theorem we say simply ``pseudoconvex'' and there
is no ambiguity.

\begin{thm}
Let $U \subset \C^n$ be a domain with smooth boundary. 
Then $U$ is Hartogs pseudoconvex if and only if $U$ is Levi pseudoconvex.
\end{thm}


\begin{proof}
Suppose
$U \subset \C^n$ is a domain with smooth boundary that is not
Levi pseudoconvex at $p \in \partial U$.
As in
\thmref{thm:tomatocan}, change coordinates so that $p=0$ and $U$ is defined
by
\begin{equation*}
\Im z_n > - \sabs{z_1}^2 + \sum_{j=2}^{n-1} \epsilon_j \sabs{z_j}^2 + O(3) .
\end{equation*}
For a small fixed $\lambda > 0$, the
closed analytic discs defined by $\xi \in \overline{\D} \mapsto (\lambda \xi, 0, \cdots, 0, is)$
are in $U$ for all small enough $s > 0$.  The origin
is a limit point of the insides, but not a limit point of their boundaries.
\hyperref[thm:contprinciple]{Kontinuit\"atssatz} is not satisfied, and $U$ is not 
convex with respect to the plurisubharmonic functions.  Therefore,
$U$ is not Hartogs pseudoconvex.

Next suppose $U$ is Levi pseudoconvex.  Take any $p \in \partial U$.
After translation and rotation by a unitary, assume $p=0$ and
write a defining function $r$ as
\begin{equation*}
r(z,\bar{z}) = \varphi(z',\bar{z}',\Re z_n) - \Im z_n ,
\end{equation*}
where $z' = (z_1,\ldots,z_{n-1})$.
Levi pseudoconvexity says 
\begin{equation} \label{eq:psconvcond}
\sum_{j=1,\ell=1}^n
\bar{a}_j a_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q \geq 0 
\quad \text{whenever} \quad
\sum_{j=1}^n
a_j \frac{\partial r}{\partial z_j} \Big|_q = 0 ,
\end{equation}
for all $q \in \partial U$ near $0$.
Let $s$ be a small real constant,
and let $\widetilde{q} = (q_1,\ldots,q_{n-1},q_n + is)$.
None of the derivatives of $r$ depends on $\Im z_n$, and therefore
$\frac{\partial r}{\partial z_\ell} \big|_{\widetilde{q}} =
\frac{\partial r}{\partial z_\ell} \big|_{q}$ and
$\frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \big|_{\widetilde{q}} =
\frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \big|_{q}$
for all $j$ and $\ell$.
So condition \eqref{eq:psconvcond} holds for all $q \in U$ near $0$.
We will use $r$ to manufacture a plurisubharmonic exhaustion function, that
is one with a semidefinite Hessian.  Starting with $r$, we already
have what we need in all but one direction.

Let $\nabla_z r|_q =
\bigl(
\frac{\partial r}{\partial z_1}\big|_q,\ldots,
\frac{\partial r}{\partial z_n}\big|_q \bigr)$ denote the gradient of $r$ in
the holomorphic directions only.
Given $q \in U$ near $0$,
decompose an arbitrary $c \in \C^n$ as $c = a+b$, where $a = (a_1,\ldots,a_n)$
satisfies
\begin{equation*}
\sum_{j=1}^n
a_j \frac{\partial r}{\partial z_j} \Big|_q = 
\blinnprod{a}{\overline{\nabla_z r|_q}}
=
0 .
\end{equation*}
Taking the orthogonal decomposition, $b$ is a scalar multiple of
$\overline{\nabla_z r|_q}$.
By Cauchy--Schwarz,
\begin{equation*}
\BBabs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j} \Big|_q}
=
\BBabs{\sum_{j=1}^n b_j \frac{\partial r}{\partial z_j} \Big|_q}
=
\Babs{\blinnprod{b}{\overline{\nabla_z r|_q}}}
=
\snorm{b} \snorm {\nabla_z r|_q} .
\end{equation*}
As $\nabla_z r|_0 = (0,\ldots,0,-\nicefrac{1}{2i})$, then for $q$ sufficiently near $0$ we have that
$\snorm{\nabla_z r|_q} \geq \nicefrac{1}{3}$, and
\begin{equation*}
\snorm{b} =
\frac{1}{\snorm
{\nabla_z r|_q}}
\BBabs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j} \Big|_q}
%\frac{\abs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j} \Big|_q}}{\snorm
%{\nabla_z r|_q}}
\leq
3 \BBabs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j} \Big|_q}
.
\end{equation*}
As $c = a+b$ is the orthogonal decomposition, $\snorm{c} \geq \snorm{b}$.

\pagebreak[1]
The complex Hessian matrix of $r$ is continuous, and so let
$M \geq 0$ be an upper bound on its operator norm for $q$ near the origin.
Again using Cauchy--Schwarz
\begin{equation*}
\begin{split}
\sum_{j=1,\ell=1}^n
\bar{c}_j c_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
& =
\sum_{j=1,\ell=1}^n
( \bar{a}_j + \bar{b}_j )  (a_\ell + b_\ell) \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
\\
& =
\smashoperator{\sum_{j=1,\ell=1}^n}
\bar{a}_j a_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
\\
& \phantom{=} \quad
+
\smashoperator{\sum_{j=1,\ell=1}^n}
\bar{b}_j c_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
+
\smashoperator{\sum_{j=1,\ell=1}^n}
\bar{c}_j  b_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
+
\smashoperator{\sum_{j=1,\ell=1}^n}
\bar{b}_j  b_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
\\
& \geq
\smashoperator{\sum_{j=1,\ell=1}^n}
\bar{a}_j a_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
-
M\snorm{b}\snorm{c}
-
M\snorm{c}\snorm{b}
-
M\snorm{b}^2
\\
& \geq
-
3 M\snorm{c}\snorm{b} .
\end{split}
\end{equation*}
Together with what we know about
$\snorm{b}$, for $q \in U$ near the origin
\begin{equation*}
\sum_{j=1,\ell=1}^n
\bar{c}_j c_\ell \frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
\geq -3M \snorm{c} \snorm{b} %=
%-M \frac{\snorm{c}\Babs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j}
%\big|_q}}{\snorm{\nabla_z r |_q}}
\geq
-3^2 M \snorm{c}\BBabs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j}
\Big|_q} .
\end{equation*}
For $z \in U$ sufficiently close to $0$ define
\begin{equation*}
f(z) = -\log \bigl(-r(z)\bigr) + A \snorm{z}^2 ,
\end{equation*}
where $A > 0$ is some constant we will choose later.
The log is there to make $f$ blow up as we approach the boundary.
The $A \snorm{z}^2$ is there to add a constant diagonal matrix to the complex
Hessian of $f$, which we hope is enough to make it positive semidefinite at
all $z$ near $0$.
Compute:
\begin{equation*}
\frac{\partial^2 f}{\partial \bar{z}_j \partial z_\ell}
=
\frac{1}{r^2}
\frac{\partial r}{\partial \bar{z}_j}
\frac{\partial r}{\partial z_\ell}
-
\frac{1}{r}
\frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} 
+
A\delta_{j}^{\ell} ,
\end{equation*}
where $\delta_j^\ell$ is the Kronecker delta\footnote{%
Recall $\delta_j^\ell = 0$ if $j\not= \ell$ and $\delta_j^\ell = 1$ if $j =
\ell$.}.
Apply the complex Hessian of $f$ to $c$ at $q \in U$ near the origin
(recall that $r$ is negative on $U$ and so for $q \in U$, $-r = \sabs{r}$):
\begin{equation*}
\begin{split}
\sum_{j=1,\ell=1}^n
\bar{c}_j c_\ell \frac{\partial^2 f}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
& =
\frac{1}{r^2}
\BBabs{
\sum_{\ell=1}^n
c_\ell
\frac{\partial r}{\partial z_\ell} \Big|_q
}^2
+
\frac{1}{\sabs{r}}
\sum_{j=1,\ell=1}^n
\bar{c}_j
c_\ell
\frac{\partial^2 r}{\partial \bar{z}_j \partial z_\ell} \Big|_q
+
A \snorm{c}^2
\\
& \geq
\frac{1}{r^2}
\BBabs{
\sum_{\ell=1}^n
c_\ell
\frac{\partial r}{\partial z_\ell} \Big|_q
}^2
-
\frac{3^2 M}{\sabs{r}}
\snorm{c}\BBabs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j} \Big|_q} 
+
A \snorm{c}^2 .
\end{split}
\end{equation*}
Now comes a somewhat funky trick.
As a quadratic polynomial in $\snorm{c}$, the right-hand side of the
inequality
is always nonnegative if $A > 0$ and if the discriminant is negative or zero.
Let us set the discriminant to zero:
\begin{equation*}
0 = 
{\Biggl(
\frac{3^2 M}{\sabs{r}}
\BBabs{\sum_{j=1}^n c_j \frac{\partial r}{\partial z_j} \Big|_q}
\Biggr)}^2
- 4A 
\frac{1}{r^2}
\BBabs{
\sum_{\ell=1}^n
c_\ell
\frac{\partial r}{\partial z_\ell} \Big|_q
}^2 .
\end{equation*}
All the nonconstant terms go away and 
$A=\frac{3^4 M^2}{4}$ makes the discriminant zero.  Thus for that $A$,
\begin{equation*}
\sum_{j=1,\ell=1}^n
\bar{c}_j c_\ell \frac{\partial^2 f}{\partial \bar{z}_j \partial z_\ell} \Big|_q 
\geq 0.
\end{equation*}
In other words, the complex Hessian
of $f$ is positive semidefinite at all points $q \in U$ near $0$.
The function $f(z)$ goes to infinity as $z$ approaches $\partial U$.
So for every $t \in \R$, the $t$-sublevel set
(the set where $f(z) < t$) is a positive
distance away from $\partial U$ near $0$.

We have constructed a local continuous plurisubharmonic
exhaustion function for $U$ near $p$.  If we intersect
with a small ball $B$ centered at $p$, then we get that $U \cap B$ is
Hartogs pseudoconvex.  This is true at all
$p \in \partial U$, so $U$ is Hartogs pseudoconvex.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Holomorphic convexity}

\begin{defn}
Let $U \subset \C^n$ be a domain.  For a set $K \subset U$,
define the
\emph{\myindex{holomorphic hull}}
\glsadd{not:KhatU}%
\begin{equation*}
\widehat{K}_U \overset{\text{def}}{=} \Bigl\{ z \in U : \sabs{f(z)} \leq
\sup_{w\in K} \sabs{f(w)}
\text{ for all $f \in \sO(U)$}  \Bigr\} .
\end{equation*}
A domain $U$ is \emph{\myindex{holomorphically convex}} if whenever
$K \subset \subset U$, then $\widehat{K}_U \subset \subset U$.
In other words, $U$ is holomorphically convex if it is convex with
respect to moduli of holomorphic functions on $U$.\footnote{Sometimes simply
$\widehat{K}$ is used, but we use
$\widehat{K}_U$ to emphasize the dependence on $U$.}
\end{defn}


It is a simple exercise (see below) to show that a holomorphically convex
domain is Hartogs pseudoconvex.  We will prove that holomorphic convexity is
equivalent to being a domain of holomorphy.  That a Hartogs pseudoconvex
domain is holomorphically convex is the
Levi problem\index{Levi problem}
for Hartogs pseudoconvex domains and is considerably more
difficult.  The thing is, there are lots of plurisubharmonic functions,
and they are easy to construct; we can even construct them locally, and then
piece them together by taking maxima.  There are far fewer holomorphic functions, and 
we cannot just construct them locally and expect the pieces to somehow fit
together.  As it is so fundamental, let us state it as a theorem.

\begin{thm}[Solution of the Levi problem]
A domain $U \subset \C^n$ is holomorphically convex
if and only it is Hartogs pseudoconvex.
\end{thm}

\begin{proof}
The forward direction follows from an exercise below.
We skip the proof of the backward direction
in order to save some hundred pages or so.
See H\"ormander's book~\cite{Hormander} for the proof.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that a holomorphically convex domain is Hartogs pseudoconvex.
See \exerciseref{exercise:modholplush}.
\end{exercise}

\begin{exercise}
Prove that every domain $U \subset \C$ is holomorphically convex by 
giving a topological description of $\widehat{K}_U$ for any
compact $K \subset \subset U$.  Hint: Runge may be useful.
\end{exercise}

\begin{exercise}
Suppose $f \colon \C^n \to \C$ is holomorphic and $U$ is a topological
component of $\bigl\{ z \in \C^n : \sabs{f(z)} < 1 \bigr\}$.  Prove
that $U$ is a holomorphically convex domain.
\end{exercise}

\begin{exercise}
Compute the hull
$\widehat{K}_{\D^n}$ of the set $K = \bigl\{ z \in \D^n : \sabs{z_j} =
\lambda_j \text{ for } j=1,\ldots,n \bigr\}$, where $0 \leq \lambda_j < 1$.
Prove that the unit polydisc is holomorphically convex.
\end{exercise}

\begin{exercise}
Prove that a geometrically convex domain $U \subset \C^n$
is holomorphically convex.
\end{exercise}

\begin{exercise}
Prove that the Hartogs figure (see \thmref{thm:extensionhartogsfigure})
is not holomorphically convex.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain, $f \in \sO(U)$, and $f$ is not identically
zero.  Show that if
$U$ is holomorphically convex, then
$\widetilde{U} = \bigl\{ z \in U : f(z) \not= 0 \bigr\}$
is holomorphically convex.  
Hint: First see \exerciseref{exercise:connectedcomplement}.
\end{exercise}

\begin{exercise} \label{exercise:biholholconvex}
Suppose $U,V \subset \C^n$ are biholomorphic domains.
Prove that $U$ is holomorphically convex if and only if $V$ is
holomorphically convex.
\end{exercise}

\begin{exercise}
In the definition of holomorphic hull of $K$, replace $U$ with $\C^n$
and $\sO(U)$ with holomorphic polynomials on $\C^n$, to get the
\emph{\myindex{polynomial hull}} of $K$.  Prove that the polynomial hull of
$K \subset \subset \C^n$ is the same as the holomorphic hull $\widehat{K}_{\C^n}$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item Prove the Hartogs triangle $T$ (see \exerciseref{exercise:hartogstriangle})
is holomorphically convex.
\item Prove $T \cup B_{\epsilon}(0)$ (for a small enough $\epsilon > 0$) is
not holomorphically convex.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that if domains $U_1 \subset \C^n$ and $U_2 \subset \C^n$ are
holomorphically convex,
then so are all the topological components of $U_1 \cap U_2$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $n \geq 2$.
\begin{exparts}
\item
Let $U \subset \C^n$ be a domain and $K \subset \subset U$
a nonempty compact subset.  Show that $U \setminus K$ is not
holomorphically convex.
\item
Let $U \subset \C^n$ be a bounded holomorphically
convex domain.  Prove that $\C^n \setminus U$ is connected.
\item
Find an unbounded holomorphically convex domain $U \subset \C^n$ where
$\C^n \setminus U$ is disconnected.
\end{exparts}
\end{exercise}
\end{exbox}

The set $\C^n$ is both holomorphically convex and
a domain of holomorphy.  These two notions are equivalent also for all
other domains in $\C^n$.

\begin{thm}[Cartan--Thullen\index{Cartan--Thullen theorem}]
\pagebreak[2]
\label{thm:cartthul}
Let $U \subsetneq \C^n$ be a domain.  The following are equivalent:
\begin{enumerate}[(i)]
\item \label{thm:cartthul:domhol}
$U$ is a domain of holomorphy.
\item \label{thm:cartthul:disthull}
For all $K \subset \subset U$,
$\operatorname{dist}(K,\partial U) = \operatorname{dist}(\widehat{K}_U,\partial U)$.
\item \label{thm:cartthul:holconv}
$U$ is holomorphically convex.
\end{enumerate}
\end{thm}

\begin{proof}
Let us start with \ref{thm:cartthul:domhol} $\Rightarrow$
\ref{thm:cartthul:disthull}.
Suppose there is a $K \subset
\subset U$ with $\operatorname{dist}(K,\partial U) > \operatorname{dist}(\widehat{K}_U,\partial U)$.
After possibly a rotation by a unitary,
there exists a point $p \in \widehat{K}_U$ and a polydisc
$\Delta = \Delta_r(0)$ with polyradius $r = (r_1,\ldots,r_n)$ such that
$p + \Delta = \Delta_r(p)$ contains a point of $\partial U$, but
\begin{equation*}
K + \Delta = \bigcup_{q \in K} \Delta_r(q) \subset \subset U.
\end{equation*}
See \figureref{fig:cart-thul-fig}.

\begin{myfig}
\subimport*{figures/}{cart-thul-fig.pdf_t}
\caption{Point in the hull closer to the boundary than closest
point of $K$.\label{fig:cart-thul-fig}}
\end{myfig}

If $f \in \sO(U)$, then there is an $M > 0$ such that $\sabs{f} \leq M$ on
$K + \Delta$ as that is a relatively compact set.  By the Cauchy estimates
for each $q \in K$ we get
\begin{equation*}
\abs{\frac{\partial^\alpha f}{\partial z^\alpha}(q)} \leq \frac{M
\alpha!}{r^\alpha} .
\end{equation*}
This inequality therefore holds on $\widehat{K}_U$ and hence at $p$.
The series
\begin{equation*}
\sum_{\alpha}
\frac{1}{\alpha !}\frac{\partial^\alpha f}{\partial z^\alpha}(p) {(z-p)}^\alpha 
\end{equation*}
converges in $\Delta_r(p)$.  Hence $f$ extends to all of $\Delta_r(p)$ and
$\Delta_r(p)$ contains points outside of $U$, in other words,
$U$ is not a domain of holomorphy.

The implication \ref{thm:cartthul:disthull} $\Rightarrow$
\ref{thm:cartthul:holconv} is immediate.

Finally, we prove
\ref{thm:cartthul:holconv} $\Rightarrow$
\ref{thm:cartthul:domhol}.
Suppose $U$ is holomorphically convex.  Let $p \in \partial U$.
By convexity choose nested compact sets $K_{j-1} \subsetneq K_j \subset
\subset U$ such that $\bigcup_j K_j = U$, and $\widehat{(K_j)}_U = K_j$.
As the sets exhaust $U$, we can perhaps pass to a subsequence
to ensure that
there exists a 
sequence of points $p_j \in K_j \setminus K_{j-1}$ such that
$\lim_{j\to\infty} p_j = p$.

As $p_j$ is not in the hull of $K_{j-1}$, there is a function $f_j \in
\sO(U)$ such that $\sabs{f_j} < 2^{-j}$ on $K_{j-1}$, but
\begin{equation*}
\sabs{f_j(p_j)} > j + \abs{\sum_{k=1}^{j-1} f_k(p_j)} .
\end{equation*}
Finding such a function is left as an exercise below.
For any $j$, the series $\sum_{k=1}^\infty f_k(z)$ converges uniformly on $K_j$
as for all $k > j$, $\sabs{f_k} < 2^{-k}$ on $K_j$.
As the $K_j$ exhaust $U$, the series converges uniformly on compact
subsets of $U$.  Consequently,
\begin{equation*}
f(z) = \sum_{k=1}^\infty f_k(z)
\end{equation*}
is a holomorphic function on $U$.  We bound
\begin{equation*}
\sabs{f(p_j)} \geq
\sabs{f_j(p_j)}
-
\abs{\sum_{k=1}^{j-1} f_k(p_j)}
-
\abs{\sum_{k=j+1}^\infty f_k(p_j)}
\geq
j
-
\sum_{k=j+1}^\infty 2^{-k}
\geq j-1 .
\end{equation*}
So $\lim_{j\to\infty} f(p_j) = \infty$.
Clearly there cannot be any open $W \subset \C^n$
containing $p$ to which $f$ extends (see
\hyperref[defn:domainofhol]{definition of domain of holomorphy}).  As any
connected open $W$ such that $W \setminus U \not= \emptyset$ must contain a
point of $\partial U$, we are done.
\end{proof}

By \exerciseref{exercise:biholholconvex},
holomorphic convexity is a biholomorphic invariant.
Thus,
being a domain of holomorphy is also a biholomorphic invariant.  This
fact is not easy to prove from the definition of a domain of
holomorphy, as the
biholomorphism is defined only on the interior of our domains.

Holomorphic convexity is an intrinsic notion; it does not require
knowing anything about points outside of $U$.  It is a much
better way to think about domains of holomorphy.  Holomorphic
convexity generalizes easily to more complicated complex
manifolds\footnote{Manifolds with complex structure, that is, ``manifolds
with multiplication by $i$ on the tangent space.''}, while
the notion of a domain of holomorphy only makes sense for domains in $\C^n$.

\begin{exbox}
\begin{exercise}[Behnke--Stein again]
\index{Behnke--Stein theorem}
Show that the union $\bigcup_j U_j$ of a nested sequence of holomorphically
convex domains $U_{j-1} \subset U_j \subset \C^n$ is holomorphically convex.
\end{exercise}

\begin{exercise}
Prove the existence of the function $f_j \in \sO(U)$ as indicated in the proof
of Cartan--Thullen above.
\end{exercise}

\begin{exercise}
Show that if $U \subset \C^n$
is holomorphically convex, then there
exists a single function $f \in \sO(U)$ that does not extend through any
point $p \in \partial U$.
\end{exercise}

\begin{exercise}
We know $U = \C^2 \setminus \{ z \in \C^2 : z_1 = 0 \}$ is a domain of
holomorphy.  Use part 
\ref{thm:cartthul:disthull}
of the theorem to show that if $W \subset \C^2$ is a domain of holomorphy
and $U \subset W$, then either $W=U$ or $W = \C^2$.  Hint:  Suppose $L
\subset W$
is a complex line and $K$ is a circle in $L$.  What is $\widehat{K}_W$?
\end{exercise}
\end{exbox}

In the following series of exercises, which you should most definitely do in
order, you will solve the
Levi problem\index{Levi problem!for complete Reinhardt domains}
(and more) for complete Reinhardt domains.
Recall that a domain $U$ is a
\myindex{complete Reinhardt domain}\index{Reinhardt domain!complete}
if whenever $(z_1,\ldots,z_n)$ is in $U$ and $r_j = \sabs{z_j}$, then the
entire closed polydisc $\overline{\Delta_r(0)} \subset U$.
We say a complete Reinhardt domain $U$ is \emph{\myindex{logarithmically convex}}%
\index{Reinhardt domain!logarithmically convex}
if there exists a
(geometrically) convex $C \subset \R^n$ such that $z \in U$ if and only if
$(\log \sabs{z_1},\ldots,\log\sabs{z_n}) \in C$.

\begin{exbox}
\begin{exercise}
Prove that a logarithmically convex complete Reinhardt domain
is the intersection of sets of the form
\begin{equation*}
\bigl\{ z \in \C^n : \alpha_1 \log \sabs{z_1} + \cdots + \alpha_n \log
\sabs{z_n} < \beta  \bigr\} 
=
\bigl\{ z \in \C^n : \sabs{z_1}^{\alpha_1} \cdots \sabs{z_n}^{\alpha_n}
< e^\beta \bigr\} 
\avoidbreak
\end{equation*}
for some nonnegative $\alpha_1,\ldots,\alpha_n$, and $\beta \in \R$.
\end{exercise}

\begin{exercise}
Prove that a complete Reinhardt domain that is Hartogs
pseudoconvex is logarithmically convex.
\end{exercise}

\begin{exercise}
For each $k \in \N_0$, let $\ell_m^k \in \N_0$ be the smallest nonnegative integer such that
$\ell_m^k \geq k \alpha_m$.
Prove that the domain of convergence of the power series
\begin{equation*}
\sum_{k=0}^\infty e^{-k\beta}
z_1^{\ell_1^k}
\cdots
z_n^{\ell_n^k}
\end{equation*}
is precisely
$\bigl\{ z \in \C^n :
\sabs{z_1}^{\alpha_1} \cdots \sabs{z_n}^{\alpha_n}
< e^\beta \bigr\}$.
Hint: That it diverges outside is easy, what is hard is that it converges
inside.  Perhaps useful is to notice
$\frac{\ell_m^k}{k}-\alpha_m \leq \frac{1}{k}$, and 
that if $z$ is in the set, there is some $\epsilon > 0$ such that
$(1+\epsilon)\sabs{z_1}^{\alpha_1} \cdots \sabs{z_n}^{\alpha_n} =
e^{\beta}$.
\end{exercise}

\begin{exercise}
Prove that a logarithmically convex Reinhardt domain is holomorphically
convex and therefore it is a domain of holomorphy.
\end{exercise}

\begin{exercise}
Prove that a complete Reinhardt domain is
a domain of holomorphy if and only if it is the
domain of convergence of some power series at the origin.  Hint: There is a
function that does not extend past any boundary point of a holomorphically
convex domain.
\end{exercise}
\end{exbox}

\pagebreak[2]
We (you) have proved the following proposition.

\begin{prop}
\pagebreak[2]
Let $U \subset \C^n$ be a complete Reinhardt domain.  Then the following are
equivalent:
\begin{enumerate}[(i)]
\item
$U$ is logarithmically convex.
\item
$U$ is a domain of holomorphy.
\item
$U$ is a domain of convergence of some power series at the origin.
\item
$U$ is Hartogs pseudoconvex.
\end{enumerate}
\end{prop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{CR functions} \label{ch:crfunctions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Real-analytic functions and complexification}

\begin{defn}
Let $U \subset \R^n$ be open.
A function $f \colon U \to \C$ is 
\emph{\myindex{real-analytic}} (or simply \emph{analytic} if
clear from context) if at each point $p \in U$, the function $f$
has a convergent power series that converges (absolutely) to $f$ in some
neighborhood of $p$.
\glsadd{not:Comega}%
A common notation for real-analytic is $C^\omega$.
\end{defn}

Before we discuss the connection to holomorphic functions, we prove a simple
lemma.

\begin{lemma}
Let $\R^n \subset \C^n$ be the natural inclusion and 
$V \subset \C^n$ a domain such that $V \cap \R^n \not= \emptyset$.
Suppose $f,g \colon V \to \C$ are holomorphic functions such that
$f=g$ on $V \cap \R^n$.  Then $f=g$ on $V$.
\end{lemma}

\begin{proof}
Considering $f-g$ we may assume that $g=0$.
Let $z = x+iy$ as usual so that $\R^n$ is given by $y=0$.
Our assumption is that $f = 0$ when $y=0$,
so the derivative of $f$ with respect to $x_j$ is zero.
When $y=0$,
the Cauchy--Riemann equations say
\begin{equation*}
0 = \frac{\partial f}{\partial x_j} =
-i \frac{\partial f}{\partial y_j} .
\end{equation*}
Therefore, on $y=0$,
\begin{equation*}
\frac{\partial f }{\partial z_j} = 0 .
\end{equation*}
The derivative $\frac{\partial f }{\partial z_j}$ is holomorphic
and $\frac{\partial f }{\partial z_j} = 0$ on $y=0$.
By induction all holomorphic derivatives of $f$ at $p \in \R^n \cap V$ vanish,
and $f$ has a zero 
power series.  Hence $f$ is identically zero in a neighborhood of
$p$ in $\C^n$ and by the identity theorem it is zero on all of $V$.
\end{proof}


We return to $\R^n$ for a moment.
We write a power series in $\R^n$ in multinomial notation as usual.
Suppose that for some
$a \in \R^n$
and some polyradius
$r=(r_1,\ldots,r_n)$,
the series
\begin{equation*}
\sum_{\alpha} c_{\alpha} {(x-a)}^\alpha
\end{equation*}
converges whenever $\sabs{x_j-a_j} \leq r_j$ for all $j$.
Here convergence is absolute convergence.  That is,
\begin{equation*}
\sum_{\alpha} \sabs{c_{\alpha}}\, \sabs{x-a}^\alpha
\end{equation*}
converges.
If we replace $x_j \in \R$ with $z_j \in \C$ such that
$\sabs{z_j-a_j} \leq \sabs{x_j-a_j}$, then the series still converges.
Hence the series
\begin{equation*}
\sum_{\alpha} c_{\alpha} {(z-a)}^\alpha
\end{equation*}
converges absolutely in $\Delta_r(a) \subset \C^n$.

\begin{prop}[Complexification part I\index{complexification}]
Suppose $U \subset \R^n$ is a domain and
$f \colon U \to \C$ is real-analytic.
Let $\R^n \subset \C^n$ be the natural inclusion.
Then there exists a domain $V \subset \C^n$ such that $U \subset V$
and a unique holomorphic function $F \colon V \to \C$ such that $F|_U = f$.
\end{prop}

Among many other things that follow from this proposition, we
can now conclude that a real-analytic function is $C^\infty$ smooth.  Be careful
and notice that $U$ is a domain in $\R^n$, but it is not an open set
when considered as a subset of $\C^n$.  Furthermore, $V$ may be a very
``thin'' neighborhood around $U$.  There is no way of finding $V$ just from
knowing $U$.  You need to also know $f$.
As an example, consider
$f(x) = \frac{1}{\epsilon^2+x^2}$ for $\epsilon > 0$, which is
real-analytic on $\R$, but the complexification is not holomorphic at $\pm
\epsilon i$.

\begin{proof}
We proved the local version already.  But we must prove that if we
extend our $f$ near every point, we always get the same function.
That follows from the lemma above; any two such functions are
equal on $\R^n$, and hence equal.  There is a subtle topological technical
point in this, so let us elaborate.  A key topological fact is that we define
$V$ as a union of the polydiscs where the series converges.  If
a point $p$ is in two different such polydiscs, we need to show that
the two definitions of $F$ are the same at $p$.  But the intersection
of two polydiscs is always connected, and in this case contains a piece
of $\R^n$ as well, and we may apply the lemma above.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the identity theorem for real-analytic functions.  That is, if $U
\subset \R^n$ is a domain, $f \colon U \to \R$ a real-analytic function and
$f$ is zero on a nonempty open subset of $U$, then $f$ is identically zero.
\end{exercise}

\begin{exercise}
Suppose $U \subset \R^n$ is a domain and $f \colon U \to \R$ a real-analytic
function.  Suppose that $W \subset U$ is a nonempty open subset and
$f|_W$ is harmonic.  Prove that $f$ is harmonic.
\end{exercise}

\begin{exercise}
Let $(0,1) \subset \R$.  Construct a real-analytic function
on $(0,1)$ that does not complexify to the rectangle $(0,1) + i(-\epsilon,\epsilon)
\subset \C$ for any $\epsilon > 0$.  Why does this not contradict the
proposition?
\end{exercise}
\end{exbox}

A polynomial $P(x)$ in $n$ real variables $(x_1,\ldots,x_n)$ is homogeneous of degree $d$ if
$P(s x) = s^d P(x)$ for all $s \in \R$ and $x \in \R^n$.
A homogeneous polynomial of degree $d$ is a polynomial whose
every monomial
is of total degree $d$.
If $f$ is real-analytic near $a \in \R^n$, then
write the power series of $f$ at $a$ as
\begin{equation*}
\sum_{j=0}^{\infty} f_j(x-a) ,
\end{equation*}
where $f_j$ is a homogeneous polynomial of degree $j$.  The $f_j$ is 
called the
\emph{\myindex{degree $j$ homogeneous part}}\index{homogeneous part} of $f$
at $a$.

There is usually a better way to complexify 
real-analytic functions in $\C^n$.
Suppose $U \subset \C^n \cong \R^{2n}$, and $f \colon U \to
\C$ is real-analytic.  Assume $a=0 \in U$ for simplicity.
Writing $z = x+iy$, near $0$,
\begin{equation*}
f(x,y)
= 
\sum_{j=0}^\infty
f_j(x,y)
= 
\sum_{j=0}^\infty
f_j\left(
\frac{z+\bar{z}}{2},
\frac{z-\bar{z}}{2i}\right) .
\end{equation*}
The polynomial $f_j$ becomes a homogeneous polynomial of degree $j$
in the variables $z$ and $\bar{z}$.  The 
series becomes a power series in $z$ and $\bar{z}$.  As mentioned before,
we simply write the function as $f(z,\bar{z})$, and we consider the
power series representation in $z$ and $\bar{z}$ rather than
in $x$ and $y$.
In multinomial notation, we write a power series at $a \in \C^n$ as
\begin{equation*}
\sum_{\alpha,\beta} c_{\alpha,\beta} {(z-a)}^\alpha
{(\bar{z}-\bar{a})}^\beta .
\end{equation*}

A holomorphic function
is real-analytic, but not vice versa.  A holomorphic function
is a real-analytic function that does not depend on $\bar{z}$.

Before we discuss complexification in terms of $z$ and $\bar{z}$, we need
the following lemma.

\begin{lemma}
Let $V \subset \C^n \times \C^n$ be a domain, let the coordinates be $(z,\zeta) \in \C^n \times
\C^n$, let
\begin{equation*}
D = \bigl\{ (z,\zeta) \in \C^n \times \C^n : \zeta = \bar{z} \bigr\},
\end{equation*}
and suppose $D \cap V \not= \emptyset$.
Suppose $f,g \colon V \to \C$ are holomorphic functions such that
$f=g$ on $D \cap V$.  Then $f=g$ on all of $V$.
\end{lemma}

The set $D$ is sometimes called the \emph{\myindex{diagonal}}.

\begin{proof}
Again assume without loss of generality that $g=0$.
Whenever $(z,\bar{z}) \in V$, we have $f(z,\bar{z}) = 0$, which is really
$f$ composed with the map that takes $z$ to $(z,\bar{z})$.  Using the chain rule
\begin{equation*}
0 =
\frac{\partial}{\partial \bar{z}_j} \Bigl[f(z,\bar{z})\Bigr]
=
\frac{\partial f}{\partial \zeta_j}(z,\bar{z}) .
\end{equation*}
Let us do this again with the $z_j$
\begin{equation*}
0 =
\frac{\partial}{\partial z_j} \Bigl[f(z,\bar{z})\Bigr]
=
\frac{\partial f}{\partial z_j}(z,\bar{z}) .
\end{equation*}
Either way, we get another holomorphic function in $z$ and $\zeta$
that is zero on $D$.
By induction, for all $\alpha$ and $\beta$ we get
\begin{equation*}
0 =
\frac{\partial^{\sabs{\alpha}+\sabs{\beta}}}{\partial z^\alpha \partial \bar{z}^\beta} \Bigl[f(z,\bar{z})\Bigr]
=
\frac{\partial^{\sabs{\alpha}+\sabs{\beta}} f}{\partial z^\alpha \partial
\zeta^\beta}(z,\bar{z}) .
\end{equation*}
All holomorphic derivatives in $z$ and $\zeta$ of $f$ are zero on every point
$(z,\bar{z})$, so the power series is zero at every point $(z,\bar{z})$,
and so $f$ is identically zero in a neighborhood of any
point $(z,\bar{z})$.  The lemma follows by the identity
theorem.
\end{proof}

Let $f$ be a real-analytic function.  Suppose 
the series (in multinomial notation)
\begin{equation*}
f(z,\bar{z}) =
\sum_{\alpha,\beta} c_{\alpha,\beta} {(z-a)}^\alpha
{(\bar{z}-\bar{a})}^\beta
\end{equation*}
converges in a polydisc $\Delta_r(a) \subset \C^n$.
By convergence we mean absolute
convergence as we discussed before.  That is,
\begin{equation*}
\sum_{\alpha,\beta} \sabs{c_{\alpha,\beta}} \, \sabs{z-a}^\alpha
\sabs{\bar{z}-\bar{a}}^\beta
\end{equation*}
converges.
The series still converges if we replace $\bar{z}_j$  with
$\zeta_j$ where $\sabs{\zeta_j-\bar{a}} \leq \sabs{\bar{z}_j-\bar{a}}$.
So the series
\begin{equation*}
F(z,\zeta) =
\sum_{\alpha,\beta} c_{\alpha,\beta} {(z-a)}^\alpha
{(\zeta-\bar{a})}^\beta
\end{equation*}
converges for all $(z,\zeta) \in \Delta_r(a) \times \Delta_r(\bar{a})$.

Putting together the discussion above with the lemma we obtain:

\begin{prop}[Complexification part II\index{complexification}] \label{prop:complexificationpt2}
Suppose $U \subset \C^n$ is a domain and $f \colon U \to \C$ is
real-analytic.
Then there exists a domain $V \subset \C^n \times \C^n$ such that
\begin{equation*}
\bigl\{ (z,\zeta) : \zeta = \bar{z} \text{ and } z \in U \bigr\} \subset V ,
\end{equation*}
and a unique holomorphic function $F \colon V \to \C$ such that
$F(z,\bar{z}) = f(z,\bar{z})$ for all $z \in U$.
\end{prop}

The function $f$ can be thought of as the restriction of $F$ to the set
where $\zeta = \bar{z}$.  We will abuse notation and write
simply $f(z,\zeta)$ both for $f$ and its extension.
The reason for this abuse is evident from the computations above.
What we are calling $f$ is a function of $(z,\bar{z})$ if thinking
of it as a function on the diagonal where $\zeta=\bar{z}$, or it is a function of
$z$ if thinking of it as just the function $z \mapsto f(z,\bar{z})$, or
it is the function $(z,\zeta) \mapsto f(z,\zeta)$.  We have the
following commutative diagram:
\begin{equation*}
\begin{tikzcd}
U \subset \C^n \ar[rr, "{z \, \mapsto \, (z,\bar{z})}"] \ar[dr, "f"']
&
&
V \subset \C^n \times \C^n \ar[dl, "f ~~ {(=F)}"]
\\
&
\C
\end{tikzcd}
\end{equation*}
All three ways of going from one place to another in the diagram
we are calling $f$.  The arrow from $V$ was called $F$ in the proposition.
The notation plays well with differentiation and the Wirtinger operators.
Differentiating $f$ (really the $F$ in the proposition) in $\zeta_j$ and
evaluating at $(z,\bar{z})$ is the same thing as evaluating at
$(z,\bar{z})$ and then differentiating in $\bar{z}_j$ using the Wirtinger
operator:
\begin{equation*}
\frac{\partial F}{\partial \zeta_j}(z,\bar{z}) = 
\frac{\partial f}{\partial \zeta_j}(z,\bar{z}) = 
\frac{\partial}{\partial \bar{z}_j}\Bigl[ f(z,\bar{z}) \Bigr] = 
\frac{\partial f}{\partial \bar{z}_j}(z,\bar{z}) .
\end{equation*}
If we squint our mind's eye,
we can't quite see the difference between $\bar{z}$ and $\zeta$.
We have already used this idea for smooth functions, but for
real-analytic functions we can 
treat $z$ and $\bar{z}$ as truly independent variables.
The abuse of notation is entirely justified, at least once it is
understood well.

\begin{remark}
The domain $V$ in the proposition is not simply $U$ times the conjugate of $U$.
In general, it is much smaller.  For example, a real-analytic $f \colon \C^n \to
\C$ does not necessarily complexify to all of $\C^n \times \C^n$.  That is
because the domain of convergence for a real-analytic function on $\C^n$
is not necessarily all of $\C^n$.  In one dimension,
the function
\begin{equation*}
f(z,\bar{z})
= \frac{1}{1+\sabs{z}^2}
\end{equation*}
is real-analytic on $\C$, but it is not a restriction to the diagonal
of a holomorphic function on all of $\C^2$.  The problem is that the complexified
function
\begin{equation*}
f(z,\zeta)
= \frac{1}{1+z \zeta}
\end{equation*}
is undefined on the set where $z \zeta = -1$, which by a fluke
never happens when $\zeta = \bar{z}$.
\end{remark}

\begin{remark}
This form of complexification is sometimes called
\emph{\myindex{polarization}} due to its relation to the polarization
identities\footnote{Such as $4 \linnprod{z}{w} =
\snorm{z+w}^2-\snorm{z-w}^2 +i \bigl( \snorm{z+iw}^2 - \snorm{z-iw}^2 \bigr)$.}.  That is, suppose $A$ is a Hermitian matrix, we 
recover $A$ and therefore the sesquilinear form $\linnprod{Az}{w}$ for
$z,w\in \C^n$, by simply knowing the values of
\begin{equation*}
\linnprod{Az}{z} = z^*Az = \sum_{j,k=1}^n a_{jk} \, \bar{z}_j z_k 
\end{equation*}
for all $z \in \C^n$.  In fact, under the hood \propref{prop:complexificationpt2} is
polarization in an infinite-dimensional Hilbert space, but we digress.
\end{remark}

The idea of treating $\bar{z}$ as a separate variable is very powerful, and
as we have just seen it is completely natural when speaking about
real-analytic functions.  This is one of the reasons why real-analytic
functions play a special role in several complex variables.

\begin{exbox}
\begin{exercise}
Let $U \subset \C^n$ be an open set and $\varphi \colon U \to \R$ a
pluriharmonic function.  Prove that $\varphi$ is real-analytic.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be an open set, $z_0 \in U$.
Suppose $\varphi \colon U \to \R$ is a pluriharmonic function.
You know that $\varphi$ is real-analytic.
Using complexification, write down a formula for a holomorphic function near
$z_0$ whose real part is $\varphi$.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain, and suppose $f, g \in \sO(U)$.
Suppose that $f = \bar{g}$ on $U$.  Use complexification (complexify
$f-\bar{g}$) to show that both $f$ and $g$ are constant.
\end{exercise}
\end{exbox}

\begin{example}
Not every $C^\infty$ smooth function is real-analytic.  For $x \in \R$,
define
\begin{equation*}
f(x) =
\begin{cases}
e^{-1/x^2} & \text{if $x > 0$,} \\
0 & \text{if $x \leq 0$.}
\end{cases}
\end{equation*}
The function
$f \colon \R \to \R$ is $C^\infty$ and $f^{(k)}(0) = 0$ for all $k$.
The Taylor series of $f$ at the origin does
not converge to $f$ in any neighborhood of the origin; it converges to the
zero function but not to $f$.
Because of this, there is no neighborhood $V$ of the origin in $\C$ such that
$f$ is the restriction to $V \cap \R$ of a holomorphic function in $V$.
\end{example}

\begin{exbox}
\begin{exercise}
Prove the statements of the example above.
\end{exercise}
\end{exbox}

\begin{defn}
A real hypersurface $M \subset \R^n$ is said to be real-analytic
if locally at every point it is the graph of a real-analytic function.  That
is near every point (locally), after perhaps relabeling coordinates $M$ can be written as
a graph
\begin{equation*}
y = \varphi(x) ,
\end{equation*}
where $\varphi$ is real-analytic, $(x,y) \in \R^{n-1} \times \R = \R^n$.
\end{defn}

Compare this definition to \defnref{def:hypersurface}.  We could
define a real-analytic hypersurface as in 
\defnref{def:hypersurface} and then prove an analogue of
\lemmaref{lemma:realgraphcoords} to show that this would be identical to the
definition above.  The definition we gave is sufficient, and so we avoid
the complication and leave it to the interested reader.

\begin{exbox}
\begin{exercise}
Show that the definition above is equivalent to an analogue of
\defnref{def:hypersurface}.  That is, state the alternative definition of
real-analytic hypersurface and then prove the analogue of 
\lemmaref{lemma:realgraphcoords}.
\end{exercise}
\end{exbox}

A mapping to $\R^m$ is real-analytic if all the components are real-analytic
functions.  Via complexification we give a simple proof of the following
result.

\begin{prop}
Let $U \subset \R^n$, $V \subset \R^k$ be open and let
$f \colon U \to V$ and $g \colon V \to \R^m$ be real-analytic.
Then $g \circ f$ is real-analytic.
\end{prop}

\begin{proof}
Let $x \in \R^n$ be our coordinates in $U$ and $y \in \R^k$ be
our coordinates in $V$.  We complexify $f(x)$ and $g(y)$ by
allowing $x$ to be a complex vector in a small neighborhood of $U$ in
$\C^n$
and  $y$ to be a complex vector in a small neighborhood of $V$ in $\C^k$.
So we treat $f$ and $g$ as holomorphic functions.  On a certain
neighborhood of $U$ in $\C^n$, the composition $f \circ g$ makes sense
and it is holomorphic as composition of holomorphic mappings is holomorphic.
Restricting the complexified $f \circ g$ back to $\R^n$ we obtain a
real-analytic function.
\end{proof}

The proof demonstrates a simple application of complexification.  Many
properties of holomorphic functions are easy to prove because
holomorphic functions are solutions to certain PDE (the Cauchy--Riemann
equations).  There is no PDE
that defines real-analytic functions, so complexification provides a useful
tool to transfer certain properties of holomorphic functions to
real-analytic functions.  We must be careful, however.  Hypotheses on
real-analytic functions only give us hypotheses on certain points of the
complexified holomorphic functions.

\begin{exbox}
\begin{exercise}
Demonstrate the point about complexification we made just above.
Find a nonconstant bounded real-analytic $f \colon \R^n \to \R$,
that happens to complexify to $\C^n$.
\end{exercise}

\begin{exercise}
Let $U \subset \R^n$ be open.  Let $\varphi \colon (0,1) \to U$ be a
real-analytic function (curve), and let $f \colon U \to \R$ be
real-analytic.  Suppose that $(f \circ \varphi)(t) = 0$ for all $t \in
(0,\epsilon)$ for some $\epsilon > 0$.  Prove that $f$ is zero on the
image $\varphi\bigl((0,1)\bigr)$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CR functions}

We first need to know what it means for a function $f \colon X \to \C$
to be smooth if $X$ is not an open set, for example, if $X$ is a hypersurface.

\begin{defn}
Let $X \subset \R^n$ be a set.
The function $f \colon X \to \C$ is smooth (resp.\
real-analytic) if for each point $p \in X$ there is a
neighborhood $U \subset \R^n$ of $p$ and a smooth (resp.\ real-analytic) $F
\colon U \to \C$ such that $F(q) = f(q)$ for $q \in X \cap U$.
\end{defn}

For an arbitrary set $X$, issues surrounding this definition can be
rather subtle.  The definition
is easy to work with,
however, if $X$ is nice, such as a hypersurface, or if $X$ is
a closure of a domain with smooth boundary.

\begin{prop}
Suppose $M \subset \R^n$ is a smooth (resp.\ real-analytic) real hypersurface.
A function $f \colon M \to \C$
is smooth (resp.\ real-analytic) if and only if whenever near any point we write
$M$ in coordinates $(x,y) \in \R^{n-1} \times \R$ as
\begin{equation*}
y = \varphi(x) ,
\end{equation*}
for a smooth (resp.\ real-analytic) function $\varphi$, then
the function $f\bigl(x,\varphi(x)\bigr)$ is a smooth (resp.\ real-analytic) function of $x$.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove the proposition.
\end{exercise}

\begin{exercise}
Prove that if $M$ is a smooth or real-analytic
hypersurface, and $f \colon M \to \C$ is smooth or real-analytic, then the function $F$ from the definition is never unique,
even for a fixed neighborhood $U$.
\end{exercise}

\begin{exercise}
Suppose $M \subset \R^n$ is a smooth hypersurface, $f \colon M \to \C$
is a smooth function, $p \in M$, and $X_p \in T_p M$.  Prove that
$X_p f$ is well-defined.
That is, suppose
$U$ is a neighborhood of $p$,
$F \colon U \to \C$ and $G \colon U \to \C$
are smooth functions that both equal $f$ on $U \cap M$.
Prove that
$X_p F = X_p G$.
\end{exercise}
\end{exbox}

Due to the last exercise, we can apply vectors of $T_p M$ to
a smooth function on a hypersurface by simply applying
them to any smooth extension.  We can similarly apply vectors of
$\C \otimes T_p M$ to smooth functions on $M$, as 
$\C \otimes T_p M$ is simply the complex span of vectors in $T_p M$.

\begin{defn}
Let $M \subset \C^n$ be a smooth real hypersurface.  Then
a smooth function $f \colon M \to \C$ is a
\emph{\myindex{smooth CR function}}\index{CR function} if
\begin{equation*}
X_p f = 0
\avoidbreak
\end{equation*}
for all $p \in M$ and all vectors $X_p \in T^{(0,1)}_p M$.
\end{defn}

\begin{remark}
One only needs one derivative (rather than $C^\infty$) in the definition
above.
One can even
define a continuous CR function if the derivative is taken in the
distribution sense, but we digress.
\end{remark}

\begin{remark}
When $n=1$, a real hypersurface $M \subset \C$ is a curve and $T^{(0,1)}_p M$
is trivial.  Therefore, all functions $f \colon M \to \C$ are CR functions.
\end{remark}

\begin{prop}
Let $M \subset U$ be a smooth (resp.\ real-analytic) real hypersurface in an
open $U
\subset \C^n$.  Suppose $F \colon U \to \C$ is a holomorphic function,
then the restriction $f = F|_M$ is a smooth (resp.\ real-analytic) CR function.
\end{prop}

\begin{proof}
First let us prove that $f$ is smooth.  The function $F$ is smooth and
defined on a neighborhood of any point, and so it can be used in the
definition.
Similarly for real-analytic.

Let us show $f$ is CR at some $p \in M$.
Differentiating $f$ with vectors in $\C \otimes
T_pM$ is the same as differentiating $F$.
As $T_p^{(0,1)} M \subset T_p^{(0,1)} \C^n$, we have
\begin{equation*}
X_p f = X_p F = 0 \qquad \text{for all} \quad X_p \in T_p^{(0,1)} M.
\qedhere
\end{equation*}
\end{proof}

On the other hand, not every smooth CR function is a restriction of a holomorphic function.

\begin{example}
Take the smooth function $f \colon \R \to \R$ we defined before
that is not real-analytic at the origin.
Take $M \subset \C^2$ be the set defined by $\Im z_2 = 0$.
$M$ is a
real-analytic real hypersurface.  Clearly
$T_p^{(0,1)} M$ is one-complex-dimensional, and at each $p \in M$,
$\frac{\partial}{\partial \bar{z}_1}\big|_p$ is tangent and spans
$T_p^{(0,1)} M$.  Define $g \colon M \to \C$ by
\begin{equation*}
g(z_1,z_2,\bar{z}_1,\bar{z}_2) = f(\Re z_2) .
\end{equation*}
Then $g$ is CR as it is independent of $\bar{z}_1$.
If $G \colon U \subset \C^2 \to \C$ is a holomorphic
function where $U$ is some open set containing the origin, then $G$
restricted to $M$ must be real-analytic (a power series in $\Re z_1$, $\Im
z_1$, and $\Re z_2$) and therefore $G$ cannot equal to 
$g$ on $M$.
\end{example}

\begin{exbox}
\begin{exercise}
Suppose $M \subset \C^n$ is a smooth real hypersurface
and $f \colon M \to \C$ is a CR function that is a restriction
of a holomorphic function $F \colon U \to \C$ defined in
some neighborhood $U \subset \C^n$ of $M$.  Show that $F$ is unique,
that is if $G \colon U \to \C$ is another holomorphic function such that
$G|_M = f = F|_M$, then $G=F$.
\end{exercise}

\begin{exercise}
Show that there is no maximum principle of CR functions.  In fact, find a
smooth real hypersurface $M \subset \C^n$, $n \geq 2$, and a smooth CR function
$f$ on $M$ such that $\sabs{f}$ attains a strict maximum at a point.
\end{exercise}

\begin{exercise}
Suppose $M \subset \C^n$, $n \geq 2$, is the hypersurface given by $\Im z_n
= 0$.  Show that every smooth CR function on $M$ is holomorphic in the variables
$z_1,\ldots,z_{n-1}$.  Use this to show that for no smooth CR function $f$ on $M$ can
$\sabs{f}$ attain a \emph{strict} maximum on $M$.  But show that there do
exist nonconstant functions such that $\sabs{f}$ attains a (nonstrict) maximum $M$.
\end{exercise}
\end{exbox}

Real-analytic CR functions on a real-analytic
hypersurface $M$ always extend to holomorphic functions of a neighborhood of $M$.
To prove this we wish to complexify everything, that is treat the
$z$s and $\bar{z}$s as separate variables.  The standard way of
writing a hypersurface as a graph is not as convenient for this setting, so
let us prove that for a real-analytic hypersurface, we can write it as a
graph of a holomorphic function in the complexified variables.  That is,
using variables $(z,w)$,
we will write $M$ as a graph of $\bar{w}$ over $z$, $\bar{z}$, and $w$.
This allows us to easily eliminate $\bar{w}$ in any real-analytic
expression.

\begin{prop} \label{prop:complexificationofrasurface}
Suppose $M \subset \C^n$ is a real-analytic hypersurface and $p \in M$.
Then after a translation and rotation by a unitary matrix, $p=0$, and near
the origin in coordinates $(z,w) \in \C^{n-1} \times \C$,
the hypersurface $M$ is given by
\begin{equation*}
\bar{w} = \Phi(z,\bar{z},w) ,
\end{equation*}
where $\Phi(z,\zeta,w)$ is a holomorphic function defined on a neighborhood of the origin
in $\C^{n-1} \times \C^{n-1} \times \C$,
such that
$\Phi$,
$\frac{\partial \Phi}{\partial z_j}$,
$\frac{\partial \Phi}{\partial \zeta_j}$
vanish at the origin for all $j$,
and $w = \bar{\Phi}\bigl(\zeta,z,\Phi(z,\zeta,w)\bigr)$
for all $z$, $\zeta$, and $w$.

A local basis for $T^{(0,1)} M$ vector fields is given by
\begin{equation*}
\frac{\partial}{\partial \bar{z}_j}
+\frac{\partial \Phi}{\partial \bar{z}_j} \frac{\partial}{\partial \bar{w}} 
\quad
\left(
=
\frac{\partial}{\partial \bar{z}_j}
+\frac{\partial \Phi}{\partial \zeta_j} \frac{\partial}{\partial \bar{w}} 
\right)
,
\qquad j=1,\ldots,n-1.
\end{equation*}

Finally, let $\sM$ be the set in
$(z,\zeta,w,\omega) \in \C^{n-1} \times \C^{n-1} \times \C \times \C$
coordinates given near the origin 
by $\omega = \Phi(z,\zeta,w)$.
Then $\sM$ is the unique \emph{complexification}
of $M$\index{complexification of a real hypersurface}
near the origin in the sense that if $f(z,\bar{z},w,\bar{w})$
is a real-analytic function vanishing on $M$ near the origin, then 
$f(z,\zeta,w,\omega)$ vanishes on $\sM$ near the origin.
\end{prop}

Again as a slight abuse of notation $\Phi$ refers to both the function
$\Phi(z,\zeta,w)$ and $\Phi(z,\bar{z},w)$.

\begin{proof}
Translate and rotate so that $M$ is given by
\begin{equation*}
\Im w = \varphi(z,\bar{z},\Re w) ,
\end{equation*}
where $\varphi$ is $O(2)$.
Write the defining function
as $r(z,\bar{z},w,\bar{w}) = -\frac{w-\bar{w}}{2i}
+\varphi\bigl(z,\bar{z},\frac{w+\bar{w}}{2}\bigr)$.
Complexifying, consider
$r(z,\zeta,w,\omega)$ as
a holomorphic function of $2n$ variables,
and let $\sM$ be the set defined by
$r(z,\zeta,w,\omega) = 0$.
The derivative of $r$ in
$\omega$ (that is $\bar{w}$) does not vanish near the origin.
Use the implicit function theorem for holomorphic functions to write $\sM$
near the origin as
\begin{equation*}
\omega = \Phi(z,\zeta,w) .
\end{equation*}
Restrict to the diagonal, $\bar{w} = \omega$ and $\bar{z}=\zeta$,
to get
$\bar{w} = \Phi(z,\bar{z},w)$.  This is order 2 in the $z$ and the $\bar{z}$
since $\varphi$ is $O(2)$.

Because $r$ is real-valued, then
$r(z,\bar{z},w,\bar{w}) =
\overline{r(z,\bar{z},w,\bar{w})} = \bar{r}(\bar{z},z,\bar{w},w)$.
Complexify to obtain
$r(z,\zeta,w,\omega) =
\bar{r}(\zeta,z,\omega,w)$ for all $(z,\zeta,w,\omega)$ near the origin.
If $r(z,\zeta,w,\omega) = 0$,
then 
\begin{equation*}
0 = \overline{r(z,\zeta,w,\omega)} =
\overline{\bar{r}(\zeta,z,\omega,w)} =
r(\bar{\zeta},\bar{z},\bar{\omega},\bar{w}) =0.
\end{equation*}
So,
$(z,\zeta,w,\omega) \in \sM$
if and only if
$(\bar{\zeta},\bar{z},\bar{\omega},\bar{w}) \in \sM$.
Near the origin,
$(z,\zeta,w,\omega) \in \sM$ if and only if
$\omega = \Phi(z,\zeta,w)$, and hence
if and only if
$\bar{w} = \Phi(\bar{\zeta},\bar{z},\bar{\omega})$.
Conjugating, we get that $\sM$ is also given by
\begin{equation*}
w = \bar{\Phi}(\zeta,z,\omega).
\end{equation*}
As $\bigl(z,\zeta,w,\Phi(z,\zeta,w)\bigr) \in \sM$, then
for all $z$, $\zeta$, and $w$,
\begin{equation*}
w = \bar{\Phi}\bigl(\zeta,z,\Phi(z,\zeta,w)\bigr).
\end{equation*}

The vector field
$X_j = \frac{\partial}{\partial \bar{z}_j}
+\frac{\partial \Phi}{\partial \bar{z}_j} \frac{\partial}{\partial \omega}$
annihilates
the function $\Phi(z,\bar{z},w)-\bar{w}$, but that is not enough.
The vector field must annihilate a real defining function such as the
real part of $\Phi(z,\bar{z},w)-\bar{w}$.  So $X_j$ must also
annihilate the conjugate
$\bar{\Phi}(\bar{z},z,\bar{w})-w$, at least on $M$.  Compute, for $(z,w) \in M$,
\begin{equation*}
\begin{split}
X_j \bigl[\bar{\Phi}(\bar{z},z,\bar{w})-w\bigr]
&=
\frac{\partial \bar{\Phi}}{\partial \bar{z}_j}
%\Big|_
{(\bar{z},z,\bar{w})}
+
\frac{\partial \Phi}{\partial \bar{z}_j}
%\Big|_
{(z,\bar{z},w)}
\frac{\partial \bar{\Phi}}{\partial \bar{w}}
%\Big|_
{(\bar{z},z,\bar{w})}
\\
& =
\frac{\partial \bar{\Phi}}{\partial \bar{z}_j}
%\Big|_
{\bigl(\bar{z},z,\Phi(z,\bar{z},\bar{w})\bigr)}
+
\frac{\partial \Phi}{\partial \bar{z}_j}
%\Big|_
{(z,\bar{z},w)}
\frac{\partial \bar{\Phi}}{\partial \bar{w}}
%\Big|_
{\bigl(\bar{z},z,\Phi(z,\bar{z},\bar{w})\bigr)}
\\
& = 
\frac{\partial}{\partial \bar{z}_j}
\Bigl[
\bar{\Phi}\bigl(\bar{z},z,\Phi(z,\bar{z},w)\bigr)
\Bigr]
=
\frac{\partial}{\partial \bar{z}_j}
\Bigl[
w
\Bigr]
= 0 .
\end{split}
\end{equation*}

The last claim of the proposition is left as an exercise.
\end{proof}

Why do we say the last claim in the proposition
proves the ``uniqueness'' of the complexification?
Suppose we defined a complexification $\sM'$ by another holomorphic
equation $f=0$.
By the claim, $\sM \subset \sM'$, at least near the origin.
If the derivative $df$ is nonzero at the origin, then
$f\bigl(z,\zeta,w,\Phi(z,\zeta,w)\bigr) = 0$ implies that 
$\frac{\partial f}{\partial \omega}$ is nonzero at the origin.
Using the holomorphic implicit function theorem we can uniquely solve $f=0$
for $\omega$ near the origin, that unique solution is $\Phi$,
and hence $\sM' = \sM$ near the origin.

As an example, recall that the sphere (minus a point) in $\C^2$ is biholomorphic to the
hypersurface
given by $\Im w = \sabs{z}^2$.  That is, $\frac{w-\bar{w}}{2i} = z \bar{z}$.  Solving for
$\bar{w}$ and using $\zeta$ and $\omega$ obtains the equation for the
complexification $\omega = -2iz \zeta + w$.  Then
$\Phi(z,\zeta,w) = 
-2iz \zeta + w$, and
$\bar{\Phi}(\zeta,z,\omega) = 2i\zeta z + \omega$.  Let us check
that $\Phi$ is the right sort of function:
$\bar{\Phi}\bigl(z,\zeta,\Phi(z,\zeta,w)\bigr)
=
2i\zeta z + (-2i z \zeta + w) = w$.  The CR vector field is
given by
$\frac{\partial}{\partial \bar{z}}
+2i z \frac{\partial}{\partial \bar{w}}$.

\begin{exbox}
\begin{exercise}
Finish the proof of the proposition:
Let $M\subset \C^n$ be a real-analytic hypersurface given by
$\bar{w} = \Phi(z,\bar{z},w)$ near the origin, as in the proposition.  Let
$f(z,\bar{z},w,\bar{w})$ be a real-analytic function such that $f=0$
on $M$.  Prove that the complexified $f(z,\zeta,w,\omega)$ vanishes on
$\sM$.
\end{exercise}

\begin{exercise}
In the proposition we only rotated and translated.  Sometimes the following
change of coordinates is also done.  Prove that one can change coordinates
(no longer linear) so that the $\Phi$ in the proposition is
such that $\Phi(z,0,w) = \Phi(0,\zeta,w) = w$ for all $z$, $\zeta$, and $w$.
These coordinates are called \emph{\myindex{normal coordinates}}.
\end{exercise}

\begin{exercise}
Suppose
$\Phi$ is a holomorphic function defined on a neighborhood of the origin
in $\C^{n-1} \times \C^{n-1} \times \C$.
\begin{exparts}
\item
Show that $\bar{w} = \Phi(z,\bar{z},w)$ defines a real-analytic hypersurface
near the origin if and only 
$w = \bar{\Phi}\bigl(\zeta,z,\Phi(z,\zeta,w)\bigr)$
for all $z$, $\zeta$, and $w$.  Hint: One direction was proved already.
\item
As an example, show that $\bar{w} = z\bar{z}$ does not satisfy the 
condition above, nor does it define a real hypersurface.
\end{exparts}
\end{exercise}
\end{exbox}

Let us prove that real-analytic CR functions on real-analytic
hypersurfaces are restrictions
of holomorphic functions.  To motivate the proof, consider
a real-analytic function $f$ on the circle $\sabs{z}^2 = z \bar{z} = 1$
($f$ is vacuously CR).  This $f$
is a restriction of a real-analytic function on a 
neighborhood of the circle, that we write $f(z,\bar{z})$.
On the circle
$\bar{z} = \nicefrac{1}{z}$.  Thus,
$F(z) = f\bigl(z,\nicefrac{1}{z}\bigr)$ is a holomorphic function
defined on a neighborhood of the circle and
equal to $f$ on the circle.
Our strategy then is to solve for one of the barred variables via
\propref{prop:complexificationofrasurface}, and hope
the CR conditions take care of the rest of the barred variables
in more than one dimension.

\begin{thm}[Severi] \label{thm:severi}\index{Severi's theorem}
Suppose $M \subset \C^n$ is a real-analytic hypersurface and $p \in M$.
For every real-analytic CR function $f \colon M \to \C$, there exists
a holomorphic function $F \in \sO(U)$ for a neighborhood $U$ of $p$
such that $F(q) = f(q)$ for all $q \in M \cap U$.
\end{thm}

\begin{proof}
Write $M$ near $p$ as $\bar{w} = \Phi(z,\bar{z},w)$.
Let $\sM$ be the set in the $2n$ variables $(z,w,\zeta,\omega)$ given by
$\omega = \Phi(z,\zeta,w)$.
Take $f$ and consider any real-analytic extension of $f$
to a neighborhood of $p$ and write it
$f(z,w,\bar{z},\bar{w})$.  Complexify\footnote{At this point
$f$ stands for three distinct objects:
the function on $M$, its real-analytic extension to
a neighborhood in $\C^n$, and its complexification
to a neighborhood of $(p,\bar{p})$ in $\C^n \times \C^n$.}
as before to
$f(z,w,\zeta,\omega)$.  On $\sM$ we have
$f(z,w,\zeta,\omega) = f\bigl(z,w,\zeta,\Phi(z,\zeta,w)\bigr)$.  Let
\begin{equation*}
F(z,w,\zeta) = f\bigl(z,w,\zeta,\Phi(z,\zeta,w)\bigr).
\end{equation*}
Clearly $F(z,w,\bar{z})$ equals $f$ on $M$.  
As $f$ is a CR function, it is annihilated by
$\frac{\partial}{\partial \bar{z}_j}
+\frac{\partial \Phi}{\partial \bar{z}_j} \frac{\partial}{\partial
\bar{w}}$ on $M$.  So
\begin{equation*}
\frac{\partial F}{\partial \zeta_j}
+\frac{\partial \Phi}{\partial \zeta_j} \frac{\partial F}{\partial
\omega}
=
\frac{\partial F}{\partial \zeta_j} = 0
\end{equation*}
on $M \subset \sM$.  We have a real analytic function
$\frac{\partial F}{\partial \zeta_j}(z,w,\bar{z})$
that is zero on $M$, so 
$\frac{\partial F}{\partial \zeta_j}(z,w,\zeta) = 0$
on $\sM$
(\propref{prop:complexificationofrasurface} again).
As $\frac{\partial F}{\partial \zeta_j}$ is a function only of
$z$, $w$, and $\zeta$ (and not of $\omega$),
$\frac{\partial F}{\partial \zeta_j} = 0$
for all
$(z,w,\zeta)$ in a neighborhood of the origin.  Consequently,
$F$ does not depend on $\zeta$, and 
$F$ is actually a holomorphic function of $z$ and $w$ only
and $F = f$ on $M$.
\end{proof}

The most important place where we find CR functions that aren't necessarily
real-analytic is as boundary values of holomorphic functions.

\begin{prop} \label{prop:boundaryvaluesCR}
Suppose $U \subset \C^n$ is an open set with smooth boundary.  Suppose
$f \colon \widebar{U} \to \C$ is a smooth function, holomorphic on $U$.
Then $f|_{\partial U}$ is a smooth CR function.
\end{prop}

\begin{proof}
The function $f|_{\partial U}$ is clearly smooth.

Suppose $p \in \partial U$.
If $X_p \in T_p^{(0,1)} \partial U$ is such that
\begin{equation*}
X_p = \sum_{j=1}^n a_j \frac{\partial}{\partial \bar{z}_j} \Big|_p ,
\end{equation*}
take $\{ q_k \}$ in $U$ that approaches $p$, then take
\begin{equation*}
X_{q_k} = \sum_{j=1}^n a_j \frac{\partial}{\partial \bar{z}_j} \Big|_{q_k} .
\end{equation*}
Then $X_{q_k} f = 0$ for all $k$ and by continuity $X_p f = 0$.
\end{proof}

\pagebreak[2]
The boundary values of a holomorphic function define the function uniquely.
That is, if two holomorphic functions continuous up to the (smooth) boundary
are equal on an open set of the boundary, then they are equal in the domain:

\begin{prop} \label{prop:boundaryvaluesdeterminef}
Suppose $U \subset \C^n$ is a domain with smooth boundary and $f \colon
\widebar{U} \to \C$ is 
a continuous function, holomorphic on $U$.  If $f=0$ on a nonempty open subset of $\partial
U$, then $f=0$ on all of $U$.
\end{prop}

\begin{proof}
Take $p \in \partial U$ such that $f=0$ on a neighborhood of $p$ in
$\partial U$.  Consider a small neighborhood $\Delta$ of $p$ such
that $f$ is zero on $\partial U \cap \Delta$.  Define $g \colon \Delta \to
\C$ by setting $g(z) = f(z)$ if $z \in U$ and $g(z) = 0$ otherwise.
See \figureref{fig:zero-onbound}.
It is not hard to see that $g$ is continuous, and it is clearly holomorphic
where it is not zero.  Rad{\'o}'s theorem
(\thmref{thm:rado}) says that $g$ is holomorphic, and as it is zero on a
nonempty open subset of $\Delta$, it is identically zero on $\Delta$,
meaning $f$ is zero on a nonempty open subset of $U$, and we are done by
identity.

\begin{myfig}
\subimport*{figures/}{zero-onbound.pdf_t}
\caption{Extending a function zero on the boundary.\label{fig:zero-onbound}}
\end{myfig}
\end{proof}

\begin{exbox}
\begin{exercise}
Find a domain $U \subset \C^n$, $n \geq 2$, with smooth boundary and a smooth
CR function $f \colon \partial U \to \C$ such that there is no holomorphic function
on $U$ or $\C^n \setminus U$ continuous up to the boundary and whose boundary values are $f$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Suppose $U \subset \C^n$ is a bounded open set with smooth boundary,
$f \colon \widebar{U} \to \C$ is a continuous function, holomorphic in
$U$, and $f|_{\partial U}$ is real-valued.  Show that $f$ is
constant.
\item
Find a counterexample to the statement if you allow $U$
to be unbounded.
\end{exparts}
\end{exercise}

\begin{exercise}
Find a smooth CR function on the sphere $S^{2n-1} \subset \C^n$ that is not
a restriction of a holomorphic function of a neighborhood of $S^{2n-1}$.
\end{exercise}

\begin{exercise}
Show a global version of Severi.  Given a real-analytic hypersurface $M
\subset \C^n$ and a real-analytic CR function $f \colon M \to \C$,
show that there exists a neighborhood $U$ of $M$, and an $F \in \sO(U)$
such that $F|_U = f$.
\end{exercise}
\end{exbox}

A problem we tackle next is to try to extend
a smooth CR function from the boundary of a domain to a holomorphic
function inside.  This is a PDE problem where the PDE are
the Cauchy--Riemann equations, and the function on the boundary is
the boundary condition.
Cauchy--Riemann equations are
\emph{\myindex{overdetermined}}, that is, there are too many
equations.  Not every data on the boundary gives a solution.
\propref{prop:boundaryvaluesCR} says that the data being CR is a necessary condition
for a solution (it is not sufficient in general).
\propref{prop:boundaryvaluesdeterminef} says the solution is unique
if it exists.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Approximation of CR functions}

The following theorem (proved circa 1980) holds in much more generality, but
we state its simplest version.  One of the simplifications we make is that
we consider only smooth CR functions here, although the theorem holds even
for continuous CR functions where the CR conditions are interpreted in the
sense of distributions.

\begin{thm}[Baouendi--Tr{\`e}ves]%
\pagebreak[2]
\index{Baouendi--Tr{\`e}ves approximation theorem}
Suppose $M \subset \C^n$ is a smooth real hypersurface,
$p \in M$ is a point,
and
$z=(z_1,\dots,z_n)$ are the holomorphic coordinates of $\C^n$.
Then there exists a compact
neighborhood $K \subset M$ of $p$, such that for every smooth CR function $f \colon M \to \C$,
there exists a sequence $\{ p_j \}$ of polynomials in $z$ such that
\begin{equation*}
p_j(z) \to f(z)
\qquad \text{uniformly in $K$.}
\end{equation*}
\end{thm}

A key point is that $K$ cannot be chosen arbitrarily, it depends on $p$ and $M$.
On the other hand it does not depend on $f$.  Given $M$ and $p \in M$
there is a $K$ such that \emph{every} CR function on $M$ is approximated
uniformly on $K$ by holomorphic polynomials.  The theorem applies in one
dimension, although in that case the 
theorem of Mergelyan
(see \thmref{thm:mergelyan}) is much more general.

\begin{example}
Let us show that $K$ cannot possibly be arbitrary.  For simplicity $n=1$.
Let $S^1 \subset \C$ be the unit circle (boundary of the disc),
then every smooth function on $S^1$ is a smooth CR function.  Let $f$
be a nonconstant real function such as $\Re z$.  Suppose for
contradiction that we could take $K = S^1$ in the theorem.  Then $f(z) = \Re
z$ could be uniformly
approximated on $S^1$ by holomorphic polynomials.
By the maximum principle, the
polynomials would converge on $\D$ to a holomorphic function on $\D$
continuous on $\overline{\D}$.  This function would have nonconstant real boundary values,
which is impossible.  Clearly $K$ cannot be the entire circle.

The example is easily extended to $\C^n$
by considering 
$M = S^1 \times \C^{n-1}$, then $\Re z_1$ is a smooth CR function on $M$ that cannot be
approximated uniformly on $S^1 \times \{ 0 \}$ by holomorphic polynomials.
\end{example}

The technique of the example above will be used later in a more general
situation, to extend CR functions using Baouendi--Tr{\`e}ves.

\begin{remark}
It is important to note the difference
between Baouendi--Tr{\`e}ves (and similar theorems
in complex analysis)
and the Weierstrass approximation theorem.  In Baouendi--Tr{\`e}ves we obtain
an approximation by holomorphic polynomials, while Weierstrass gives us
polynomials in the real variables, or in $z$ and $\bar{z}$.  For example,
via Weierstrass, every continuous function is uniformly approximable on $S^1$ via polynomials
in $\Re z$ and $\Im z$, and therefore by polynomials in $z$ and $\bar{z}$.
These polynomials do not in general converge anywhere but on $S^1$.
\end{remark}

\begin{exbox}
\begin{exercise}
Let $z=x+iy$ as usual in $\C$.
Find a sequence of polynomials in $x$ and $y$ that converge uniformly to $e^{x-y}$ on $S^1$,
but diverge everywhere else.
\end{exercise}
\end{exbox}


The proof is an ingenious 
use of the standard technique used to prove the Weierstrass approximation
theorem.  Also, as we have seen mollifiers before, the technique will not be
completely foreign even to the reader who does not know the Weierstrass
approximation theorem.  Basically what we do is use the standard convolution
argument, this time against a holomorphic function.  Letting $z=x+iy$
we only do the convolution in the
$x$ variables keeping $y=0$.  Then we use the fact that the
function is CR to show that we get an approximation even for other $y$.

In the formulas below, given a vector $v = (v_1,\ldots,v_n)$,
it will be useful to write
\glsadd{not:bracketsquare}%
\begin{equation*}
[v]^2 \overset{\text{def}}{=} v_1^2 + \cdots + v_n^2 .
\end{equation*}

The following lemma is a neat application of ideas from several complex
variables to solve a problem that does not at first seems to involve
holomorphic functions.

\begin{lemma} \label{lemma:matrixint}
Let $W$ be the set of $n \times n$ complex matrices $A$ such that
\begin{equation*}
\snorm{(\Im A)x} < \snorm{(\Re A)x}
\end{equation*}
for all nonzero $x \in \R^n$ and $\Re A$ is positive definite.
Then for all $A \in W$,
\begin{equation*}
\int_{\R^n} e^{-{[Ax]}^2} \det A \, dx  = \pi^{n/2} .
\end{equation*}
\end{lemma}

\begin{proof}
Suppose $A$ has real entries and $A$ is positive definite (so
$A$ is also invertible).  By a
change of coordinates
\begin{equation*}
\int_{\R^n} e^{-{[Ax]}^2} \det A \, dx  =
\int_{\R^n} e^{-{[x]}^2} \, dx  =
\left(\int_\R e^{-x_1^2} \, dx_1 \right)
\cdots
\left(\int_\R e^{-x_n^2} \, dx_n \right)
=
{(\sqrt{\pi})}^n .
\end{equation*}
Next suppose $A$ is any matrix in $W$.
There is some $\epsilon > 0$
such that $\snorm{(\Im A) x}^2 \leq
(1-\epsilon^2) \snorm{(\Re A) x}^2$ for all $x \in \R^n$.  That is because
we only need to check this for $x$ in the unit sphere, which is compact
(exercise).  By reality of $\Re A$, $\Im A$, and $x$
we get 
${[(\Re A)x]}^2 = \snorm{(\Re A)x}^2$ and
${[(\Im A)x]}^2 = \snorm{(\Im A)x}^2$.  So
\begin{equation*}
\abs{e^{-{[Ax]}^2}}
=
e^{-\Re {[Ax]}^2}
\leq
e^{-{[(\Re A)x]}^2 + {[(\Im A)x]}^2}
\leq
e^{-\epsilon^2 {[(\Re A)x]}^2} .
\end{equation*}
Therefore, the integral exists for all $A$ in $W$ by a similar computation as
above.

The expression
\begin{equation*}
\int_{\R^n} e^{-{[Ax]}^2} \det A \, dx
\end{equation*}
is a well-defined holomorphic function in the entries of $A$, thinking of
$W$ as a domain (see exercises below) in $\C^{n^2}$.  We have a holomorphic function that is
constantly equal to $\pi^{n/2}$ on $W \cap \R^{n^2}$ and hence it is equal
to $\pi^{n/2}$ everywhere on $W$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the existence of $\epsilon > 0$ in the proof above. 
\end{exercise}

\begin{exercise}
Show that $W \subset \C^{n^2}$ in the proof above is a domain (open and connected).
\end{exercise}

\begin{exercise}
Prove that we can really differentiate under the integral to show that the
integral is holomorphic in the entries of $A$.
\end{exercise}

\begin{exercise}
Show that some hypotheses are needed for the lemma.  In particular, take
$n=1$ and find the exact set of $A$ (now a complex number) for which
the conclusion of the lemma is true.
\end{exercise}
\end{exbox}

Given an $n \times n$ matrix $A$, let $\snorm{A}$ denote the operator norm,
\begin{equation*}
\snorm{A} = \sup_{\snorm{v}=1} \snorm{Av} = \sup_{v \in \C^n, v\not= 0}
\frac{\snorm{Av}}{\snorm{v}} .
\end{equation*}

\begin{exbox}
\begin{exercise}
Let $W$ be as in \lemmaref{lemma:matrixint}.  Let $B$ be an $n \times n$
real matrix such that $\snorm{B} < 1$.   Show that $I + iB \in W$.
\end{exercise}
\end{exbox}

We will be using differential forms, and the following lemma says
that as far as the exterior
derivative is concerned, all CR functions behave as
restrictions of holomorphic functions.

\begin{lemma} \label{lemma:crdf}
Let $M \subset \C^n$ be a smooth real hypersurface, $f \colon M \to \C$
be a smooth CR function, and $(z_1,\ldots,z_n)$ be the holomorphic
coordinates of $\C^n$.  Then at each point $p \in M$,
the exterior derivative
$df$ is a linear combination of $dz_1,\ldots,dz_n$,
thinking of $z_1,\ldots,z_n$ as functions on $M$.\linebreak[1]
Namely,
\begin{equation*}
d(f \, dz) = df \wedge dz = 0.
\end{equation*}
\end{lemma}

Recall the notation $dz = dz_1 \wedge dz_2 \wedge \cdots \wedge dz_n$.

\begin{proof}
After a complex affine change of coordinates, we simply
need to prove the result at the origin.  Let
$\xi_1,\ldots,\xi_n$ be
the new holomorphic coordinates
and suppose the $T^{(1,0)}_0 M$ tangent space is spanned
by
$\frac{\partial}{\partial \xi_1}\big|_0,
\ldots,
\frac{\partial}{\partial \xi_{n-1}}\big|_0$,
and such that $\frac{\partial}{\partial \Re \xi_n}\big|_0$ is tangent
and $\frac{\partial}{\partial \Im \xi_n}\big|_0$ is normal.
At the origin, the CR conditions are
$\frac{\partial f}{\partial \bar{\xi}_k}(0) = 0$ for all $k$, so
\begin{equation*}
df(0) =
\frac{\partial f}{\partial \xi_1}(0) \, d\xi_1(0) + \cdots +
\frac{\partial f}{\partial \xi_{n-1}}(0) \, d\xi_{n-1}(0)  +
\frac{\partial f}{\partial \Re \xi_{n}}(0) \, d(\Re \xi_{n})(0) 
.
\end{equation*}
Also, at the origin $d\xi_n(0) = d(\Re \xi_n)(0) + i d(\Im \xi_n)(0) = d(\Re \xi_n)(0)$.
So $df(0)$ is a linear combination of $d\xi_1(0),\ldots,d\xi_n(0)$.
As $\xi$ is a complex affine function of $z$, then each $d\xi_k$ is a linear
combination of $dz_1$ through $dz_n$, and the claim follows.
So if $f$ is a CR function, then
$d(f\,dz) = df \wedge dz = 0$ since $dz_k \wedge dz_k = 0$.
\end{proof}

\begin{proof}[Proof of the theorem of Baouendi--Tr{\`e}ves]
Suppose $M \subset \C^n$ is a smooth real hypersurface, and without loss
of generality suppose $p=0 \in M$.
Let $z=(z_1,\ldots,z_n)$ be the holomorphic coordinates, write $z=x+iy$,
$y=(y',y_n)$, and
suppose $M$ is given by
\begin{equation*}
y_n = \psi(x,y') ,
\end{equation*}
where $\psi$ is $O(2)$.
The variables $(x,y')$ parametrize $M$ near 0:
\begin{equation*}
z_j = x_j+iy_j , \quad \text{ for $j = 1,\ldots,n-1$,} \quad \text{and} \quad
z_n = x_n + i \psi(x,y') .
\end{equation*}
Define
\begin{equation*}
\varphi(x,y') = \bigl(y_1,\ldots,y_{n-1},\psi(x,y')\bigr) .
\end{equation*}
Write $(x,y') \mapsto z = x + i\varphi(x,y')$ as the parametrization.
That is, think of $z$ as a function of $(x,y')$.

Let $r > 0$ and $d > 0$ be small numbers to be determined later.
Assume they are small enough so
that $f$ and $\varphi$ are defined and smooth on some neighborhood of the
set where $\snorm{x} \leq r$ and $\snorm{y'} \leq d$.
There exists a smooth $g \colon \R^n \to [0,1]$ such that $g \equiv 1$ on
$B_{r/2}(0)$ and $g \equiv 0$ outside of $B_{r}(0)$.
See \figureref{fig:cutoff-bt}.
Explicit formula
can be given.  Alternatively we obtain such a $g$ by use of
mollifiers on a function that is identically one on
$B_{3r/4}(0)$ and zero elsewhere.  Such a $g$ is commonly called a
\emph{\myindex{cutoff function}}.

\begin{myfig}
\subimport*{figures/}{cutoff-bt.pdf_t}
\caption{Cutoff function.\label{fig:cutoff-bt}}
\end{myfig}

\begin{exbox}
\begin{exercise}
Find an explicit formula for $g$ without using mollifiers.
\end{exercise}
\end{exbox}

Let
\begin{equation*}
K' = \bigl\{ (x,y') : \snorm{x} \leq \nicefrac{r}{4} , \snorm{y'} \leq d
\bigr\} .
\end{equation*}
Let $K = z(K')$, that is the image of $K'$ under the mapping $z(x,y')$.

Consider the CR function $f$ a function of $(x,y')$
and write $f(x,y')$.
For $\ell \in \N$,
let $\alpha_{\ell}$ be a differential $n$-form defined (thinking
of $w \in \C^n$ as a constant parameter) by
\begin{equation*}
\begin{split}
\alpha_{\ell}(x,y')
& =
{\left(\frac{\ell}{\pi}\right)}^{n/2}
e^{-\ell [w - z]^2} g(x) f(x,y')
\,
dz
\\
& =
{\left(\frac{\ell}{\pi}\right)}^{n/2}
e^{-\ell [w - x-i\varphi(x,y')]^2} g(x) f(x,y')
\\
& \qquad \qquad
(dx_1 + idy_1)  \wedge
\cdots \wedge
(dx_{n-1} + i dy_{n-1})
\wedge
\bigl(dx_{n} + i d \psi (x,y') \bigr) .
\end{split}
\end{equation*}

The key is the exponential, which looks like the bump function
mollifier, except that now we have $w$ and $z$
possibly complex.  The exponential is also holomorphic in $w$, and that will
give us entire holomorphic approximating functions.

Fix $y'$ with $0 < \snorm{y'} < d$ and let $D$ be defined by
\begin{equation*}
D = \bigl\{ (x,s) \in \R^n \times \R^{n-1} : \snorm{x} < r \text{ and } s = t y' \text{ for
$t \in (0,1)$} \bigr\} .
\end{equation*}
$D$ is an $(n+1)$-dimensional ``cylinder.''  We take a ball in the
$x$ directions, then take a single fixed point $y'$ in the $s$ variables and make a
cylinder.  See \figureref{fig:cylinder-bt}.

\begin{myfig}
\subimport*{figures/}{cylinder-bt.pdf_t}
\caption{Cylinder $D$.\label{fig:cylinder-bt}}
\end{myfig}

Orient $D$ in
the standard way as if it sat in the $(x,t)$ variables in $\R^n \times \R$.
Stokes' theorem says
\begin{equation*}
\int_D d \alpha_{\ell} (x,s)
=
\int_{\partial D} \alpha_{\ell} (x,s) .
\end{equation*}
Since $g(x) = 0$ if $\snorm{x} \geq r$, $\alpha_\ell$ is zero on the sides
of the cylinder $D$, so the integral over $\partial D$ only needs to 
consider the top and bottom of the cylinder.  And because of $g$, the integral over the top and bottom
can be taken over $\R^n$.
As is usual in these sorts of arguments, we do the slight abuse of notation
where
we ignore that $f$ and $\varphi$ are undefined
where $g$ is identically zero:
\begin{multline} \label{eq:BTstokes}
\int_{\partial D} \alpha_{\ell}(x,s)
\\
\begin{aligned}
=
&
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
\!\!\!
e^{-\ell [w - x-i\varphi(x,y')]^2 } g(x) f(x,y')
\,
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,y') \bigr) 
\\
& 
-
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
\!\!\!
e^{  -\ell [w - x-i\varphi(x,0)]^2 } g(x) f(x,0)
\,
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,0) \bigr) ,
\end{aligned}
\end{multline}
where $d_x$ means the derivative in the $x$ directions only.
That is, $d_x \psi =
\frac{\partial \psi}{\partial x_1} dx_1
+ \cdots +
\frac{\partial \psi}{\partial x_n} dx_n$.


We will show that as $\ell \to \infty$, the left-hand side of \eqref{eq:BTstokes}
goes to zero uniformly for $w \in K$
and the first term on the right-hand side goes to $f(\tilde{x},y')$
if $w = z(\tilde{x},y')$ is in $M$.  Hence,
we define entire functions that we will show approximate $f$:
\begin{equation*}
f_\ell(w)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
e^{  -\ell [w - x-i\varphi(x,0)]^2 } g(x) f(x,0)
\,
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,0) \bigr) .
\end{equation*}
Clearly each $f_\ell$ is holomorphic and defined for all $w \in \C^n$.

In the next claim it is important that $f$ is a CR function.

\begin{claim}
\pagebreak[2]
We have
\begin{equation*}
%\int_D
d \alpha_\ell(x,s)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
%\int_D
e^{-\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x) 
\wedge
dz(x,s) ,
\end{equation*}
and for sufficiently small $r>0$ and $d>0$,
\begin{equation*}
\lim_{\ell\to\infty}
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{(x,s)\in D}
e^{-\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x) 
\wedge
dz(x,s)
= 0
\end{equation*}
uniformly as a function of $w \in K$ and $y' \in B_d(0)$ (recall that $D$ depends on
$y'$).
\end{claim}

\begin{proof}
The function
$(x,s) \mapsto e^{-\ell [w - z(x,s)]^2}$ is CR (as a function on $M$), and so
is $f(x,s)$.  Therefore, using \lemmaref{lemma:crdf},
\begin{equation*}
d \alpha_{\ell}(x,s)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
e^{-\ell [w - z(x,s)]^2 } f(x,s)
\,
dg(x) 
\wedge
dz(x,s) .
\end{equation*}
Since $dg$ is zero for $\snorm{x} \leq \nicefrac{r}{2}$, the integral
\begin{equation*}
\int_D
d \alpha_\ell(x,s)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_D
e^{ -\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x) 
\wedge
dz(x,s)
\end{equation*}
is only evaluated for the subset of $D$ where $\snorm{x} > \nicefrac{r}{2}$.

Suppose $w \in K$ and $(x,s) \in D$ with $\snorm{x} > \nicefrac{r}{2}$.
Let $w = z(\tilde{x},\tilde{s})$.  
We need to estimate
\begin{equation*}
\babs{e^{ -\ell {[w - z(x,s)]}^2 }} =
e^{ -\ell \Re {[w - z(x,s)]}^2 } .
\end{equation*}
Then
\begin{equation*}
-\Re {[w - z]}^2 =
-\snorm{\tilde{x}-x}^2
+
\snorm{\varphi(\tilde{x},\tilde{s})-\varphi(x,s)}^2 .
\end{equation*}
By the mean value theorem
\begin{equation*}
\snorm{\varphi(\tilde{x},\tilde{s})-\varphi(x,s)}
\leq
\snorm{\varphi(\tilde{x},\tilde{s})-\varphi(x,\tilde{s})}
+
\snorm{\varphi(x,\tilde{s})-\varphi(x,s)}
\leq
a \snorm{\tilde{x}-x}
+
A \snorm{\tilde{s}-s} ,
\end{equation*}
where $a$ and $A$ are 
\begin{equation*}
a = \sup_{\snorm{\hat{x}} \leq r, \snorm{\hat{y}'} \leq d}
\norm{\,\left[\frac{\partial \varphi}{\partial x}(\hat{x},\hat{y}')\right]\,},
\qquad
A = \sup_{\snorm{\hat{x}} \leq r, \snorm{\hat{y}'} \leq d}
\norm{\,\left[\frac{\partial \varphi}{\partial y'}(\hat{x},\hat{y}'))\right]\,}.
\end{equation*}
Here $\bigl[ \frac{\partial \varphi}{\partial x} \bigr]$ and 
$\bigl[ \frac{\partial \varphi}{\partial y'} \bigr]$ are
the derivatives (matrices) of $\varphi$ with respect to $x$ and $y'$
respectively, and the norm we are taking is the operator norm.
Because $\bigl[ \frac{\partial \varphi}{\partial x} \bigr]$ is zero
at the origin, we pick $r$ and $d$ small
enough (and hence $K$ small enough) so that $a \leq \nicefrac{1}{4}$.
We furthermore pick $d$ possibly even smaller to ensure
that $d \leq \frac{r}{32A}$.  We have that $\nicefrac{r}{2} \leq \snorm{x} \leq
r$, but $\snorm{\tilde{x}} \leq \nicefrac{r}{4}$ (recall $w \in K$), so
\begin{equation*}
\frac{r}{4} \leq \snorm{\tilde{x}-x} \leq \frac{5r}{4} .
\end{equation*}
Also, $\snorm{\tilde{s}-s} \leq 2d$ by triangle inequality.

Therefore,
\begin{equation*}
\begin{split}
-\Re {[w - z(x,s)]}^2 & \leq
- \snorm{\tilde{x}-x}^2
+
a^2 \snorm{\tilde{x}-x}^2
+
A^2 \snorm{\tilde{s}-s}^2
+
2aA \snorm{\tilde{x}-x}\snorm{\tilde{s}-s}
\\
& \leq
\frac{-15}{16} \snorm{\tilde{x}-x}^2
+
A^2 \snorm{\tilde{s}-s}^2
+
\frac{A}{2} \snorm{\tilde{x}-x}\snorm{\tilde{s}-s}
\\
& \leq \frac{-r^2}{64} .
%(
%(-15/16)(1/16)
%+
%(2^2)/(2^2 * 16^2)
%+
%(1/2)(5/4)2(1/(2*16))
%) r^2
\end{split}
\end{equation*}
In other words,
\begin{equation*}
\babs{
e^{-\ell[w-z(x,s)]^2}}
\leq
e^{-\ell r^2  / 64} ,
\end{equation*}
or
\begin{equation*}
\abs{
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{(x,s)\in D}
e^{-\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x) 
\wedge
dz(x,s)
}
\leq
C
\ell^{n/2}
e^{-\ell r^2  / 64} ,
\end{equation*}
for some constant $C$.  Note that $D$ depends on $y'$.  The set of
all $y'$ with $\snorm{y'} \leq d$,
is a compact set, so we can make $C$
large enough to not depend on the $y'$ that was chosen.
The claim follows.
\end{proof}

\begin{claim}
For the given $r>0$ and $d>0$,
\begin{multline*}
\lim_{\ell\to\infty}
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
e^{  -\ell [\tilde{x}+i\varphi(\tilde{x},y') - x-i\varphi(x,y')]^2 }
\\
g(x) f(x,y')
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,y') \bigr) 
%\\
= f(\tilde{x},y')
\end{multline*}
uniformly in $(\tilde{x},y') \in K'$.
\end{claim}

That is, we look at \eqref{eq:BTstokes} and we plug in $w = z(\tilde{x},y') \in K$.
The $g$ (as usual) makes sure we never evaluate $f$, $\psi$, or
$\varphi$ at
points where they are not defined.

\begin{proof}
The change of variables formula implies
\begin{equation*}
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,y') \bigr) 
=
d_x z(x,y')
=
\det \left[\frac{\partial z}{\partial x}(x,y')\right] dx ,
\end{equation*}
where $\bigl[\frac{\partial z}{\partial x}(x,y')\bigr]$ is the matrix
corresponding to the derivative of the mapping $z$ with respect to the $x$
variables evaluated at $(x,y')$.

Let us change variables of integration via $\xi = \sqrt{\ell} ( x-\tilde{x})$:
\begin{multline*}
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
e^{  -\ell [\tilde{x}+i\varphi(\tilde{x},y') - x-i\varphi(x,y')]^2 } g(x) f(x,y')
\det \left[\frac{\partial z}{\partial x}(x,y')\right] dx
=
\\
{\left(\frac{1}{\pi}\right)}^{n/2}
\int_{\xi \in \R^n}
e^{-{\left[\xi +
i\sqrt{\ell}\left(\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')\right)\right  ]}^2}
\\
g\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}}\right)
f\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right)
\det \left[\frac{\partial z}{\partial
x}\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right)\right] d\xi .
\end{multline*}
We now wish to take a limit as $\ell \to \infty$ and for this we apply
the dominated convergence theorem.
So we need to dominate the integrand.
The second half of the integrand is uniformly bounded independent of
$\ell$ as 
\begin{equation*}
x \mapsto g(x) f(x,y') \det \left[\frac{\partial z}{\partial x}(x,y')\right]
\end{equation*}
is a continuous function with compact support (because of $g$).
Hence it is enough to worry about the exponential term.
We also only consider those $\xi$ where the integrand is not zero.
Recall that $r$ and $d$ are small enough that
\begin{equation*}
\sup_{\snorm{\hat{x}} \leq r, \snorm{\hat{y}'} \leq d}
\norm{\,\left[
\frac{ \partial \varphi}{\partial  x}(\hat{x},\hat{y}')
\right]\,} \leq \frac{1}{4} ,
\end{equation*}
and as $\snorm{\tilde{x}} \leq \nicefrac{r}{4}$ (as
$(\tilde{x},y') \in K$) and
$\norm{\tilde{x}+\frac{\xi}{\sqrt{\ell}}} \leq r$ (because $g$ is
zero otherwise), then
\begin{equation*}
\norm{\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')}
\leq \frac{1}{4} \norm{\tilde{x}+\frac{\xi}{\sqrt{\ell}}-\tilde{x}} =
\frac{\snorm{\xi}}{4 \sqrt{\ell}} .
\end{equation*}

So under the same conditions we have
\begin{equation*}
\begin{split}
\bbabs{e^{-{\left[\xi +
i\sqrt{\ell}\left(\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')\right)\right]}^2}}
& =
e^{-\Re {\left[\xi +
i\sqrt{\ell}\left(\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')\right)\right]}^2}
\\
& =
e^{-\snorm{\xi}^2 + \ell
\norm{\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')}^2}
\\
& \leq
e^{-(15/16)\snorm{\xi}^2} .
\end{split}
\end{equation*}
And that is integrable.  Therefore, we take the pointwise limit under the integral to obtain 
\begin{equation*}
{\left(\frac{1}{\pi}\right)}^{n/2}
\int_{\xi \in \R^n}
e^{-{\left[\xi + i\left[ \frac{\partial \varphi}{\partial x}(\tilde{x},y') \right] \xi \right]}^2}
g(\tilde{x})
f(\tilde{x},y')
\det \left[\frac{\partial z}{\partial
x}(\tilde{x},y')\right] d\xi .
\end{equation*}
In the exponent, we have an expression for the derivative
in the $\xi$ direction with $y'$ fixed.  If $(\tilde{x},y') \in K'$, then
$g(\tilde{x}) = 1$, and so we can ignore $g$.

Let $A = I + i \bigl[ \frac{\partial \varphi}{\partial x}(\tilde{x},y')
\bigr]$.  \lemmaref{lemma:matrixint} says
\begin{equation*}
{\left(\frac{1}{\pi}\right)}^{n/2}
\int_{\xi \in \R^n}
e^{-{\left[\xi + i\left[ \frac{\partial \varphi}{\partial x}(\tilde{x},y') \right] \xi \right]}^2}
f(\tilde{x},y')
\det \left[\frac{\partial z}{\partial
x}(\tilde{x},y')\right] d\xi  = f(\tilde{x},y') .
\end{equation*}

That the convergence is uniform in
$(\tilde{x},y') \in K'$ is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
In the claim above, finish the proof that the convergence is 
uniform in $(\tilde{x},y') \in K'$.
Hint: It may be easier to
use the form of the integral before the change of variables
and prove that the sequence is uniformly Cauchy.
\end{exercise}
\end{exbox}

We are essentially done with the proof of the theorem.
The two claims together with \eqref{eq:BTstokes} show that $f_\ell$ are entire
holomorphic functions that approximate $f$ uniformly on $K$.  Entire holomorphic
functions can be approximated by polynomials uniformly on compact subsets;
simply take the partial sums of Taylor series at the origin.
\end{proof}

\begin{exbox}
\begin{exercise}
Explain why being approximable on $K$ by (holomorphic) polynomials does not
necessarily mean that
$f$ is real-analytic.
\end{exercise}

\begin{exercise}
Suppose $M \subset \C^n$ is given by $\Im z_n = 0$.  Use the standard
Weierstrass approximation theorem to show that given a $K \subset \subset M$,
and a smooth CR function $f \colon M \to \C$, then $f$ can be uniformly approximated
by holomorphic polynomials on $K$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extension of CR functions}

We will now apply the so-called ``technique of analytic discs'' together
with
Baouendi--Tr{\`e}ves to prove the
Lewy extension theorem.  Lewy's original proof was different
and predates Baouendi--Tr{\`e}ves.  A local extension theorem of this type
was first proved by Helmut Knesser in 1936.

\begin{thm}[Lewy]%
\index{Lewy extension theorem}
Suppose $M \subset \C^n$ is a smooth real hypersurface and $p \in M$.
There exists a neighborhood $U$ of $p$ with the following
property.
Suppose $r \colon U \to \R$ is
a smooth defining function for $M \cap U$, denote by $U_- \subset U$ the set where $r$
is negative and $U_+ \subset U$ the set where $r$ is positive.
Let $f \colon M \to \R$ be a smooth CR function.
Then:

\begin{enumerate}[(i)]
\item
If the Levi form with respect to $r$ has a positive eigenvalue at $p$, then
$f$ extends to a holomorphic function on $U_-$ continuous up to $M$
(that is, continuous on $\{ z \in U : r(z) \leq 0 \}$).
\item
If the Levi form with respect to $r$ has a negative eigenvalue at $p$, then
$f$ extends to a holomorphic function on $U_+$ continuous up to $M$
(that is, continuous on $\{ z \in U : r(z) \geq 0 \}$).
\item
If the Levi form with respect to $r$ has eigenvalues of both signs at $p$, then
$f$ extends to a function holomorphic on $U$.
\end{enumerate}
\end{thm}

So if the Levi form has eigenvalues of both signs,
then near $p$ all CR functions are restrictions of holomorphic
functions.
The function $r$ can be any defining
function for $M$.  Either we can extend it to all of $U$ or we could take a
smaller $U$ such that $r$ is defined on $U$.  As we noticed before,
once we pick sides (where $r$ is positive and where it is negative), then
the number of positive eigenvalues and the number of negative eigenvalues of
the Levi form is fixed.  A different $r$ may flip $U_-$
and $U_+$, but the conclusion of the theorem is exactly the same.

\begin{proof}
We prove the first item, and the second item follows by considering $-r$.
Suppose $p = 0$ and $M$ is given in some neighborhood
$\Omega$ of the origin as
\begin{equation*}
\Im w = \sabs{z_1}^2 + \sum_{j=2}^{n-1} \epsilon_j \sabs{z_j}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) ,
\end{equation*}
where $z' = (z_2,\ldots,z_{n-1})$, $\epsilon_j = -1,0,1$,
and $E$ is $O(3)$.
Let $\Omega_-$ be given by
\begin{equation*}
0 > r = \sabs{z_1}^2 + \sum_{j=2}^{n-1} \epsilon_j \sabs{z_j}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) - \Im w .
\end{equation*}
The (real) Hessian of the function
\begin{equation*}
z_1 \mapsto \sabs{z_1}^2 +
E(z_1,0,\bar{z}_1,0,0) 
\end{equation*}
is positive definite in a 
neighborhood of the origin and the function has a strict minimum at 0.
There is some small disc $D \subset \C$ such
that this function is strictly positive on $\partial D$.

Therefore,
for $(z',w) \in W$ in some small neighborhood $W \subset \C^{n-1}$ of the origin, the 
function
\begin{equation*}
z_1 \mapsto \sabs{z_1}^2 + \sum_{j=2}^n \epsilon_j \sabs{z_j}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) - \Im w
\end{equation*}
is still strictly positive on $\partial D$.

We wish to apply
Baouendi--Tr{\`e}ves and so let $K$ be the compact neighborhood of the
origin from the theorem.  Take $D$ and $W$ small enough such
that $(D \times W) \cap M \subset K$.
Find the polynomials $p_j$ that approximate $f$ uniformly on $K$.
Consider $z_1 \in D$ and $(z',w) \in W$ such that
$(z_1,z',w) \in \Omega_-$.
Let
$\Delta = \bigl( D \times \{ (z',w) \} \bigr) \cap \Omega_-$.
Denote by $\partial \Delta$ the boundary of $\Delta$ in the subspace topology
of $\C \times \{ (z',w) \}$.

The set $\Omega_+$ where $r > 0$ is open and it contains
$(\partial D) \times \{ (z',w) \}$.  Therefore,
$\partial \Delta$ contains no points of $(\partial D) \times \{ (z',w) \}$.
Consequently, $\partial \Delta$ contains only points where
$r = 0$, that is $\partial \Delta \subset M$, and
also
$\partial \Delta \subset D \times W$.
As $(D \times W) \cap M \subset K$, we have $\partial \Delta \subset K$.
See \figureref{fig:lewy-extension-figure}.

\begin{myfig}
\subimport*{figures/}{lewy-extension-figure.pdf_t}
\caption{Proof of Lewy extension.\label{fig:lewy-extension-figure}}
\end{myfig}

As $p_j \to f$ uniformly on $K$, then $p_j \to f$ uniformly on
$\partial \Delta$.  As  $p_j$ are holomorphic, then by the maximum
principle, $p_j$ converge uniformly on all of $\Delta$.  In fact, as $(z_1,z',w)$ was
an arbitrary point in $(D \times W) \cap \Omega_-$,
the polynomials $p_j$ converge uniformly on
$(D \times W) \cap \overline{\Omega_-}$.
Let $U = D \times W$, then $U_- = (D \times W) \cap \Omega_-$.  Notice 
$U$ depends on $K$, but not on $f$.
So $p_j$ converge to a continuous function $F$ on $\overline{U_-} \cap
U$ and $F$
is holomorphic on
$U_-$.  Clearly $F$ equals $f$ on $M \cap U$.

To prove the last item, pick a side, and then use one of the first two
items to extend the function to that side.  Via the tomato can
principle (\thmref{thm:tomatocan}) the function also extends across $M$ and
therefore to a whole neighborhood of $p$.
\end{proof}

If you were wondering what happened to the analytic discs we promised,
the $\Delta$ in the above is an analytic disc (simply connected) for a small enough $U$, but it was
not necessary to prove that fact.

We state the next corollary for a strongly convex domain, even though it
holds with far more generality.
It is a simpler version of the \emph{\myindex{Hartogs--Bochner}}\footnote{What is called
Hartogs--Bochner is 
the $C^1$ version of this theorem where the domain is only assumed to be bounded
and the boundary connected,
and it was proved by neither Hartogs nor
Bochner, but by Martinelli in 1961.}.
Later, in \exerciseref{exercise:HartogsBochnerSPCVX},
you will prove it for strongly pseudoconvex
domains.  However, the theorem is true for every bounded domain with connected smooth
boundary
with no assumptions on the Levi form, but 
a different approach would have to be taken.

\begin{cor} \label{cor:bochnerhartogsstrconvex}
Suppose $U \subset \C^n$, $n \geq 2$, is a bounded domain with smooth boundary that is
strongly convex 
and $f \colon \partial U \to \C$ is a smooth CR function, then
there exists a continuous function $F \colon \widebar{U} \to \C$
holomorphic in $U$
such that $F|_{\partial U} = f$.
\end{cor}

\begin{proof}
A strongly convex domain is strongly pseudoconvex, so $f$ must extend to the
inside locally near every point.  The extension is locally unique as any two
extensions have the same boundary values.  Therefore, there exists a set
$K \subset \subset U$ such that $f$ extends to $U \setminus K$.
Via an exercise below we can assume that $K$ is strongly convex and
therefore we can apply the special case of Hartogs phenomenon
that you proved in \exerciseref{exercise:convexhartogs} to find an
extension holomorphic in $U$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the existence of the strongly convex $K$ in the proof of
\corref{cor:bochnerhartogsstrconvex} above.
\end{exercise}

\begin{exercise}
Show by example that the corollary is not true when $n=1$.  Explain where in
the proof have we used that $n \geq 2$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \partial \bB_2 \to \C$ is a smooth CR function.
Write down an explicit formula for the extension $F$.
\end{exercise}

\begin{exercise}
A smooth real hypersurface $M \subset \C^3$ is defined by $\Im w = \sabs{z_1}^2-\sabs{z_2}^2 + O(3)$
and $f$ is a real-valued smooth CR function on $M$.  Show
that $\sabs{f}$ does not attain a maximum at the origin.
\end{exercise}

\begin{exercise}
A real-analytic hypersurface
$M \subset \C^n$, $n \geq 3$, is
such that the Levi form at $p \in M$ has eigenvalues of both signs.
Show that every smooth CR function $f$ on $M$ is, in fact, real-analytic in
a neighborhood of $p$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $M \subset \C^3$ be defined by $\Im w = \sabs{z_1}^2-\sabs{z_2}^2$.
\begin{exparts}
\item
Show that for this $M$,
the conclusion of Baouendi--Tr{\`e}ves holds with
an arbitrary compact subset $K \subset \subset M$.
\item
Use this to show that every
smooth CR function $f \colon M \to \C$ is a restriction of an entire holomorphic function
$F \colon \C^3 \to \C$.
\end{exparts}
\end{exercise}

\begin{exercise}
Find an $M \subset \C^n$, $n \geq 2$, such that near some $p \in M$,
for every neighborhood $W$ of $p$ in $M$, there is a CR function $f \colon
W \to \C$ that does not extend holomorphically to either side of $M$ at $p$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \partial \bB_n \to \C$ is a smooth function and $n \geq 2$.
Prove that $f$ is a CR function if and only if
\begin{equation*}
\int_0^{2\pi} f(e^{i\theta}v) \, e^{ik\theta} d\theta = 0
\qquad
\text{for all $v \in \partial \bB_n$ and all $k \in \N$.}
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the third item in the Lewy extension theorem without the use
of the tomato can principle.  That is, prove in a more elementary
way that if $M \subset U \subset \C^n$ is a smooth real hypersurface
in an open set $U$ and $f \colon U \to \C$ is continuous
and holomorphic in $U \setminus M$, then $f$ is holomorphic.
\end{exercise}
\end{exbox}

\begin{remark}
Studying solutions to nonhomogeneous CR equations of the form $X f = \psi$
for a CR vector field $X$,
and the fact that such conditions can guarantee 
that a function must be real-analytic, led Lewy to a famous, very
surprising, and rather simple
example of a
linear partial differential equation with smooth coefficients
that has no solution on any open set\footnote{%
Lewy, Hans, \emph{An example of a smooth linear partial differential
equation without solution}, Annals of Mathematics, \textbf{66} (1957), 155--158.}.
The example is surprising because when a linear PDE has real-analytic
coefficients, a solution always exists by the theorem of Cauchy--Kowalevski.
Furthermore, if $X$ is a real vector field ($X$ is in $TM$ not in $\C
\otimes TM$),
then a solution to $Xf = \psi$ exists by the method of characteristics, even
if $X$ and $\psi$ are only smooth.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The \texorpdfstring{$\bar{\partial}$}{dbar}-problem} \label{ch:dbar}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The generalized Cauchy integral formula}
\index{generalized Cauchy integral formula}

Before we get into the $\bar{\partial}$-problem, let us prove a more general
version of Cauchy's formula using Stokes' theorem (really Green's theorem).
This version is called the \emph{\myindex{Cauchy--Pompeiu integral formula}}.
We only need the theorem for smooth functions, but as it is often 
applied in less regular contexts and it is just an application of Stokes'
theorem, let us state it more generally.  In applications, the boundary is often
only piecewise smooth, and again that is all we need for Stokes.

\begin{thm}[Cauchy--Pompeiu] \label{thm:generalizedcauchy}
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively (see \appendixref{ap:onevarresults}),
and let $f \colon \widebar{U} \to \C$ be continuous
with bounded continuous partial derivatives in $U$.
Then for $z \in U$:
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
+
\frac{1}{2\pi i}
\int_{U}
\frac{\frac{\partial f}{\partial \bar{\zeta}}(\zeta)}{\zeta-z}
\,
d\zeta \wedge d\bar{\zeta} .
\end{equation*}
\end{thm}

If $f$ is holomorphic, then the second term is zero, and we
obtain the standard Cauchy formula.
\glsadd{not:dA}%
If $\zeta = x+iy$, then the standard orientation on $\C$ is the one
corresponding to the area form $dA = dx \wedge dy$.
The form $d\zeta \wedge d\bar{\zeta}$ is the area form up to a scalar.
That is,
\begin{equation*}
d\zeta \wedge d\bar{\zeta}
=
(dx+i\,dy)\wedge (dx-i\,dy)
=
(- 2 i ) dx \wedge dy = (-2i) dA .
\end{equation*}

As we want to use Stokes, we need to write the standard
exterior derivative in terms of $z$ and $\bar{z}$.
For $z = x+iy$, we compute:
\begin{equation*}
d \psi
=
\frac{\partial \psi}{\partial x} dx 
+
\frac{\partial \psi}{\partial y} dy
=
\frac{\partial \psi}{\partial z} dz 
+
\frac{\partial \psi}{\partial \bar{z}} d\bar{z}.
\end{equation*}

\begin{exbox}
\begin{exercise}
Observe the singularity in the second term of the Cauchy--Pompeiu formula,
and prove that the integral still makes
sense (the function is integrable).  Hint: polar coordinates.
\end{exercise}

\begin{exercise}
Why can we not differentiate in $\bar{z}$ under the integral in the second
term of the Cauchy--Pompeiu formula?
Notice that it would lead to an impossible result.
\end{exercise}
\end{exbox}

\begin{proof}
Fix $z \in U$.  We wish to apply Stokes' theorem\footnote{%
We are really using Green's theorem, which is the generalized
Stokes' theorem in 2 dimensions, see \thmref{thm:greens}.},
but the integrand is not smooth at $z$.
Let $\Delta_r(z)$ be a small disc such that
$\Delta_r(z) \subset
\subset U$.  See \figureref{fig:cauchy-pompeiu}.
Stokes now applies on $U \setminus \Delta_r(z)$.

\begin{myfig}
\subimport*{figures/}{cauchy-pompeiu.pdf_t}
\caption{Proof of Cauchy--Pompeiu.\label{fig:cauchy-pompeiu}}
\end{myfig}

Via Stokes, we get
\begin{equation*}
\int_{\partial U} \frac{f(\zeta)}{\zeta-z}\,  d\zeta - 
\int_{\partial \Delta_r(z)} \frac{f(\zeta)}{\zeta-z}\,  d\zeta
=
\int_{U \setminus \Delta_r(z)} d\left( \frac{f(\zeta)}{\zeta-z} \, d\zeta \right)
=
\int_{U \setminus \Delta_r(z)} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta .
\end{equation*}
The second equality follows because holomorphic derivatives in $\zeta$
have a $d\zeta$ and when we wedge them with $d\zeta$ we just get zero.
We now wish to let the radius $r$ go to zero.
Via the exercise above,
$\frac{\frac{\partial f}{\partial \bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta$
is integrable over all of $U$.  Therefore,
\begin{equation*}
\lim_{r \to 0}
\int_{U \setminus \Delta_r(z)} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta
=
\int_{U} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta
=
-
\int_{U} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\zeta \wedge d\bar{\zeta} .
\end{equation*}
The second equality is simply swapping the order of the $d\zeta$ and
$d\bar{\zeta}$.
By continuity of $f$,
\begin{equation*}
\lim_{r \to 0}
\frac{1}{2\pi i}
\int_{\partial \Delta_r(z)} \frac{f(\zeta)}{\zeta-z}\,  d\zeta
=
\lim_{r \to 0}
\frac{1}{2\pi}
\int_0^{2\pi} f(z + r e^{i\theta})\, d\theta
=
f(z) .
\end{equation*}
The theorem follows.
\end{proof}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary and
suppose $f \colon \widebar{U} \to \C$ is a $C^1$ function such
that 
$\int_{U} \frac{\frac{\partial f}{\partial \bar{z}}(\zeta)}{\zeta-z} \,
dA(\zeta) =
0$ for every $z \in \partial U$.  Prove that $f|_{\partial U}$ are the boundary
values of a holomorphic function in $U$.
\item
Given arbitrary $\epsilon > 0$, find a $C^1$ function $f$ on the closed unit disc
$\overline{\D}$,
such that $\frac{\partial f}{\partial \bar{z}}$ is identically zero
outside an $\epsilon$-neighborhood of the origin, yet $f|_{\partial \D}$
are not the boundary values of a holomorphic function.
\end{exparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C$ and $f$ be as in the theorem, but let $z \notin
\widebar{U}$.  Show that
\begin{equation*}
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
+
\frac{1}{2\pi i}
\int_{U}
\frac{\frac{\partial f}{\partial \bar{\zeta}}(\zeta)}{\zeta-z}
\,
d\zeta \wedge d\bar{\zeta} 
= 0 .
\end{equation*}
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simple case of the \texorpdfstring{$\bar{\partial}$}{dbar}-problem}

For a smooth function $\psi$, consider the exterior
derivative in terms of $z$ and $\bar{z}$,
\glsadd{not:dpsi}%
\begin{equation*}
d \psi =
\frac{\partial \psi}{\partial z_1} dz_1 + \cdots +
\frac{\partial \psi}{\partial z_n} dz_n
+
\frac{\partial \psi}{\partial \bar{z}_1} d\bar{z}_1 + \cdots +
\frac{\partial \psi}{\partial \bar{z}_n} d\bar{z}_n .
\end{equation*}
Let us give a name to the two parts of the derivative:
\glsadd{not:d}%
\glsadd{not:dbar}%
\begin{equation*}
\partial \psi \overset{\text{def}}{=}
\frac{\partial \psi}{\partial z_1} dz_1 + \cdots +
\frac{\partial \psi}{\partial z_n} dz_n, \qquad
\bar{\partial} \psi \overset{\text{def}}{=}
\frac{\partial \psi}{\partial \bar{z}_1} d\bar{z}_1 + \cdots +
\frac{\partial \psi}{\partial \bar{z}_n} d\bar{z}_n .
\end{equation*}
Then $d \psi = \partial \psi + \bar{\partial} \psi$.
Notice $\psi$ is holomorphic if and only if $\bar{\partial} \psi = 0$.

The so-called
\emph{\myindex{inhomogeneous $\bar{\partial}$-problem}}\index{$\bar{\partial}$-problem}
($\bar{\partial}$ is pronounced ``dee bar'') is to
solve the equation
\begin{equation*}
\bar{\partial} \psi = g ,
\end{equation*}
for $\psi$, given a one-form
\begin{equation*}
g = g_1 d\bar{z}_1 + \cdots + g_n d\bar{z}_n .
\end{equation*}
Such a $g$ is called a \emph{\myindex{$(0,1)$-form}}.
The fact that the partial 
derivatives of $\psi$ commute, forces certain compatibility conditions
on $g$ for us to have any hope of getting a solution (see below).

\begin{exbox}
\begin{exercise}
Find an explicit example of a $g$ in $\C^2$ such that no corresponding
$\psi$ can exist.
\end{exercise}
\end{exbox}

On any open set where $g = 0$, $\psi$ is holomorphic.  So
for a general $g$, what we are doing is finding a function
that is not holomorphic in a 
specific way.

\begin{thm}
Suppose $g$ is a $(0,1)$-form
on $\C^n$, $n \geq 2$, given by
\begin{equation*}
g = g_1 d\bar{z}_1 + \cdots + g_n d\bar{z}_n ,
\end{equation*}
where $g_j \colon \C^n \to \C$ are compactly supported smooth functions
satisfying the \emph{\myindex{compatibility conditions}}
\begin{equation} \label{eq:compatconds}
\frac{\partial g_k}{\partial \bar{z}_\ell} =
\frac{\partial g_\ell}{\partial \bar{z}_k}  \qquad \text{for all $k,\ell =
1,2,\ldots,n$.}
\end{equation}
Then there exists a unique compactly supported smooth function $\psi \colon
\C^n \to \C$ such that
\begin{equation*}
\bar{\partial} \psi = g .
\end{equation*}
\end{thm}

The compatibility conditions on $g$ are necessary, but the compactness is not.
Without compactness, the boundary of the set where the equation lives
would come into play.  Let us not worry about this, and prove that this simple
compactly supported version always has a solution.
The compactly supported solution 
is unique:  Given any holomorphic
$f$, $\bar{\partial}(\psi+f) = g$.  But since the difference of
any two solutions $\psi_1$ and $\psi_2$ is holomorphic, and
the only holomorphic compactly supported function is 0, then the compactly
supported solution $\psi$ is unique.

\begin{proof}
We really have $n$ 
smooth functions, $g_1,\ldots,g_n$, so the equation $\bar{\partial} \psi = g$
is the $n$ equations
\begin{equation*}
\frac{\partial \psi}{\partial \bar{z}_k} = g_k ,
\end{equation*}
where the functions $g_k$ satisfy the compatibility conditions
\eqref{eq:compatconds}.

We claim that the following is an explicit solution:
\begin{equation*}
\begin{split}
\psi(z)
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 g_1(\zeta,z_2,\ldots,z_n)
}{\zeta - z_1}
d\zeta \wedge d\bar{\zeta}
\\
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 g_1(\zeta+z_1,z_2,\ldots,z_n)
}{\zeta}
d\zeta \wedge d\bar{\zeta} .
\end{split}
\end{equation*}
To show that the singularity does not matter for integrability is the same
idea as for the generalized Cauchy formula.

Let us check we have the solution.
We use the generalized Cauchy formula on the $z_1$
variable.
Take $R$ large enough so that 
$g_j(\zeta,z_2,\ldots,z_n)$ is zero when $\sabs{\zeta}\geq R$ for all $j$.
For every $j$,
\begin{equation*}
\begin{split}
g_j(z_1,\ldots,z_n) & =
\frac{1}{2\pi i}
\int_{\abs{\zeta}=R}
\frac{g_j(\zeta,z_2,\ldots,z_n)}{\zeta-z_1}
d \zeta
+
\frac{1}{2\pi i}
\int_{\abs{\zeta} \leq R}
\frac{\frac{\partial g_j}{\partial \bar{z}_1}(\zeta,z_2,\ldots,z_n)}{\zeta-z_1}
d\zeta \wedge d\bar{\zeta} 
\\
& =
\frac{1}{2\pi i}
\int_{\C}
\frac{\frac{\partial g_j}{\partial \bar{z}_1}(\zeta,z_2,\ldots,z_n)}{\zeta-z_1}
d\zeta \wedge d\bar{\zeta}  .
\end{split}
\end{equation*}

Using the second form of the definition of $\psi$, the
compatibility conditions \eqref{eq:compatconds}, and the computation above we get
\begin{equation*} 
\begin{split}
\frac{\partial\psi}{\partial \bar{z}_j}(z)
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 \frac{\partial g_1}{\partial \bar{z}_j}(\zeta+z_1,z_2,\ldots,z_n)
}{\zeta}
d\zeta \wedge d\bar{\zeta} 
\\
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 \frac{\partial g_j}{\partial \bar{z}_1}(\zeta+z_1,z_2,\ldots,z_n)
}{\zeta}
d\zeta \wedge d\bar{\zeta} 
\\
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 \frac{\partial g_j}{\partial \bar{z}_1}(z_1,z_2,\ldots,z_n)
}{\zeta-z_1}
d\zeta \wedge d\bar{\zeta} 
=
g_j(z) .
\end{split}
\end{equation*}

\begin{exbox}
\begin{exercise}
Show that we were allowed to differentiate under the integral in the
computation above.
\end{exercise}
\end{exbox}

That $\psi$ has compact support follows because $g_1$ has compact
support together with the identity theorem.  In particular, $\psi$ is
holomorphic for large $z$ since $\bar{\partial} \psi = g = 0$ when $z$
is large.  When at least one of $z_2,\ldots,z_n$ is large,
then $\psi$ is identically zero
simply from its definition.  See \figureref{fig:dbarcpt-fig}.

\begin{myfig}
\subimport*{figures/}{dbarcpt-fig.pdf_t}
\caption{Far enough, $\partial \psi = 0$.\label{fig:dbarcpt-fig}}
\end{myfig}

As $\bar{\partial} \psi = 0$ on the light gray and white areas in the
diagram, $\psi$ is holomorphic there. As $\psi$ is zero on the light
gray region, it is zero also on the white region by the identity theorem.
That is, $\psi$ is zero on the unbounded component of the set where $g=0$,
and so $\psi$ has compact support.
\end{proof}

The first part of the proof still works when $n=1$, we get a solution
$\psi$.  However, the last bit of the proof does not work in one dimension, so
$\psi$ does not have compact support.

\begin{exbox}
\begin{exercise} \label{exercise:supportofpsi}
\begin{exparts}
\item
Show that if $g$ is supported in $K \subset \subset \C^n$, $n \geq 2$,
then $\psi$ is supported in the complement of the unbounded component
of $\C^n \setminus K$.  In particular, show that if $K$ is the support of
$g$ and $\C^n \setminus K$ is connected, then the support of
$\psi$ is $K$.
\item
Find an explicit example where the support of $\psi$ is strictly larger
than the support of $g$.
\end{exparts}
\end{exercise}

\begin{exercise}
Find an example of a smooth function $g \colon \C \to \C$ with compact
support, such that no solution $\psi \colon \C \to \C$ to
$\frac{\partial \psi}{\partial \bar{z}} = g$ (at least one of which always exists) is
of compact support.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The general Hartogs phenomenon}

We can now prove the general Hartogs phenomenon as an application of the
solution of the compactly supported
inhomogeneous $\bar{\partial}$-problem.  We proved special
versions of this phenomenon using Hartogs figures before.
The proof of the theorem has a complicated history as
Hartogs' original proof from 1906 contained gaps.
A fully working proof was finally supplied by Fueter in 1939 for $n=2$
and independently by Bochner and Martinelli for higher $n$
in the early 40s. 
The proof we give is the
standard one given nowadays due to Leon Ehrenpreis from 1961.

\begin{thm}[Hartogs phenomenon]\index{Hartogs phenomenon}
Let $U \subset \C^n$ be a domain, $n \geq 2$, and let
$K \subset \subset U$ be a compact set such that
$U \setminus K$ is connected.  Every holomorphic $f \colon U \setminus K \to \C$
extends uniquely to a holomorphic function on $U$.
See \figureref{fig:hartogs-fig}.
\end{thm}

\begin{myfig}
\subimport*{figures/}{hartogs-fig.pdf_t}
\caption{Hartogs phenomenon.\label{fig:hartogs-fig}}
\end{myfig}

The idea of the proof is extending in any way whatsoever and then using the solution
to the $\bar{\partial}$-problem to correct the result to make it
holomorphic.

\begin{proof}
First find a smooth function $\varphi$ that is 1 in a neighborhood of
$K$ and is compactly supported in $U$ (exercise below).  Let
$f_0 = (1-\varphi)f$ on $U \setminus K$ and $f_0 = 0$ on $K$.  The function $f_0$
is smooth on $U$ and it is holomorphic
and equal to $f$ near the boundary of $U$, where $\varphi$ is 0.
We let $g = \bar{\partial} f_0$ on $U$, that is $g_k = \frac{\partial
f_0}{\partial \bar{z}_k}$,
and we let $g=0$ outside $U$.
As $g_k$ are identically zero near $\partial U$, we find that each
$g_k$ is smooth on $\C^n$.
The compatibility conditions
\eqref{eq:compatconds} are satisfied
because partial derivatives commute.
Let us see why $g_k$ is compactly supported.  The
only place to check is on $U \setminus K$ as elsewhere we have $g_k = 0$
automatically.  Note that $f$ is holomorphic on $U \setminus K$ and compute
\begin{equation*}
\frac{\partial f_0}{\partial \bar{z}_k}
=
\frac{\partial }{\partial \bar{z}_k}
\bigl((1-\varphi)f\bigr)
=
\frac{\partial f}{\partial \bar{z}_k}
- \varphi \frac{\partial f}{\partial \bar{z}_k}
- \frac{\partial \varphi}{\partial \bar{z}_k} f
=
- \frac{\partial \varphi}{\partial \bar{z}_k} f .
\end{equation*}
And 
$\frac{\partial \varphi}{\partial \bar{z}_k}$ is compactly supported in
$U \setminus K$ by construction.
Now apply the solution of the compactly supported $\bar{\partial}$-problem
to find a
compactly supported function $\psi$ such that $\bar{\partial}\psi = g$. 
Set $F = f_0 - \psi$.  Let us check that $F$ is the desired
extension.  It is holomorphic:
\begin{equation*}
\frac{\partial F}{\partial \bar{z}_k}
=
\frac{\partial f_0}{\partial \bar{z}_k}
-
\frac{\partial \psi}{\partial \bar{z}_k}
=
g_k
-
g_k
= 0 .
\end{equation*}
Next, \exerciseref{exercise:supportofpsi} and the fact that $U \setminus
K$ is connected reveals that $\psi$ must be compactly supported in $U$.
This means that $F$ agrees with $f$ near the boundary (in particular
on an open set) and thus everywhere in $U \setminus K$ since $U \setminus K$
is connected.
\end{proof}

The hypotheses on dimension and on connectedness of $U \setminus K$
are necessary.
No such theorem is true in one dimension.
If $U \setminus K$ is disconnected, a simple
counterexample can be constructed.
See the exercise below.

\begin{exbox}
\begin{exercise}
Show that $\varphi$ exists.  Hint: Use mollifiers.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is a domain and
$K \subset U$ is any compact set (perhaps $U \setminus
K$ is disconnected).
Prove that given $f \in \sO(U \setminus K)$ there exists an $F \in \sO(U)$ that equals
to $f$ on the intersection of $U$ and the unbounded component
of $\C^n \setminus K$.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is a domain and
$K \subset U$ is a compact set such that $U \setminus K$ is disconnected.
Find a counterexample to the conclusion to Hartogs.
\end{exercise}
\end{exbox}

One of many consequences of the Hartogs phenomenon is
that
the zero set of a holomorphic function $f$ is never compact in
dimension 2 or higher.  If it
were compact, $\frac{1}{f}$ would provide a contradiction, see also
\exerciseref{exercise:connectedcomplement}.

\begin{cor}
Suppose $U \subset \C^n$, $n \geq 2$, is a domain and $f \colon U \to \C$ is
holomorphic.  If the zero set $f^{-1}(0)$ is not empty, then it is not compact.
\end{cor}

Replacing $U \setminus K$ with a hypersurface
is usually called the Hartogs--Bochner theorem (when the hypersurface is
$C^1$ or smooth).
The real-analytic case was stated first by Severi in 1931.

\begin{cor}[Severi]\index{Severi's theorem}
Suppose $U \subset \C^n$, $n \geq 2$, is a bounded domain with connected real-analytic boundary and
$f \colon \partial U \to \C$ is a real-analytic CR function.  Then
there exists some neighborhood $U' \subset \C^n$ of $\widebar{U}$
and a holomorphic function $F \colon U' \to \C$ for which
$F|_{\partial U} = f$.
\end{cor}

\begin{proof}
By Severi's result (\thmref{thm:severi}), for every $p \in \partial U$,
there is a small ball $B_p$, such that $f$ extends to $B_p$.  Cover $\partial
U$ by finitely many such balls so that if $B_p$ intersects $B_q$, then 
the (connected) intersection $B_p \cap B_q$ contains points of $\partial U$.
The extension in $B_p$ and in $B_q$ then agree on a piece of a hypersurface
$\partial U$, and hence agree.  Taking a union of the $B_p$ we find
a unique extension in single neighborhood of $\partial U$.
We write this neighborhood as $U' \setminus K$ for some compact $K$
and a connected $U'$ such that $\widebar{U} \subset U'$.
Consider the topological components of $\C^n \setminus K$.  As
$\partial U$ is connected and $U$ is bounded,
the unbounded component of $\C^n \setminus K$ must contain all of $\partial U$.
By boundedness of $U$, all the other components are
relatively compact in $U$.  If we add them to $K$, then $K$
is still compact and $U' \setminus K$ is connected.
We apply the Hartogs phenomenon.
\end{proof}

\begin{exbox}
\begin{exercise}[Hartogs--Bochner again] \label{exercise:HartogsBochnerSPCVX}
Let $U \subset \C^n$, $n \geq 2$, be a bounded domain with connected strongly
pseudoconvex smooth boundary
and let $f \colon \partial U \to \C$ be a smooth CR function.  Prove
that there exists a continuous function $F \colon \widebar{U} \to \C$
holomorphic in $U$
such that $F|_{\partial U} = f$.
Note: Strong pseudoconvexity is not needed (``bounded with smooth boundary''
will do), but that is much more difficult to prove.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$, $n \geq 2$, is a bounded domain of
holomorphy.  Show that $\C^n \setminus U$ is connected using the
Hartogs phenomenon.
\end{exercise}

\begin{exercise}
Suppose $W \subset U \subset \C^n$, $n \geq 3$, are domains such that
for each fixed $z_3^0,z_4^0,\ldots,z_n^0$,
\begin{multline*}
\bigl\{ (z_1,z_2) \in \C^2 :
(z_1,z_2,z_3^0,\ldots,z_n^0) \in U \setminus W
\bigr\}
\\
\subset \subset
\bigl\{ (z_1,z_2) \in \C^2 :
(z_1,z_2,z_3^0,\ldots,z_n^0) \in U
\bigr\} .
\end{multline*}
Prove that every $f \in \sO(W)$ extends to a holomorphic function on
$U$.  Note: The fact that $W$ is connected is important.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Prove that if $n \geq 2$, no domain of the form $U = \C^n \setminus K$
for a compact $K$ is biholomorphic to a bounded domain.
\item
Prove that every domain of the form $U = \C \setminus K$
for a compact $K$ with nonempty interior is biholomorphic to a bounded domain.
\end{exparts}
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$, $n \geq 2$, is a domain such that
for some affine $A \colon \C^2 \to \C^n$
the set $A^{-1}\bigl(\C^n \setminus U\bigr)$ has a bounded
topological component.  Prove that $U$ is not a domain of holomorphy.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Integral kernels} \label{ch:integralkernels}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bochner--Martinelli kernel}

A generalization of
Cauchy's formula to several variables
is called the Bochner--Martinelli integral formula,
which in fact reduces 
to Cauchy's (Cauchy--Pompeiu) formula when $n=1$.
As for Cauchy's formula, we will prove the formula for all
smooth functions via Stokes' theorem.

First, let us define the \emph{\myindex{Bochner--Martinelli kernel}}:
\begin{equation*}
\omega(\zeta,z)
\overset{\text{def}}{=}
\frac{(n-1)!}{{(2\pi i)}^n}
\sum_{j=1}^n
\frac{\bar{\zeta}_j-\bar{z}_j}{\norm{\zeta-z}^{2n}}
\,
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_j } \wedge d\zeta_j \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n .
\end{equation*}
\glsadd{not:hatremove}%
The notation $\widehat{ d\bar{\zeta}_j }$ means that this term is
simply left out.

\begin{thm}[Bochner--Martinelli] \label{thm:bochnermartinelli}
\index{Bochner--Martinelli integral formula}
Let $U \subset \C^n$ be a bounded open set with smooth boundary and let
$f \colon \widebar{U} \to \C$ be a smooth function.
Then for $z \in U$,
\begin{equation*}
f(z) =
\int_{\partial U}
f(\zeta) \omega(\zeta,z)
-
\int_{U}
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z) .
\end{equation*}
In particular, if $f \in \sO(U)$, then
\begin{equation*}
f(z) =
\int_{\partial U}
f(\zeta) \omega(\zeta,z) .
\end{equation*}
\end{thm}

Recall that if $\zeta = x+iy$ are the coordinates in $\C^n$, the orientation that we assigned to $\C^n$ in
this book\footnote{Again, there is
no canonical orientation of $\C^n$, and
not all authors follow this (perhaps more prevalent) convention.}
is the one corresponding to the volume form
\glsadd{not:dV}%
\begin{equation*}
dV = dx_1 \wedge dy_1 \wedge dx_2 \wedge dy_2 \wedge \cdots \wedge dx_n \wedge dy_n .
\end{equation*}
With this orientation,
\begin{equation*}
d\zeta_1 \wedge d\bar{\zeta}_1 \wedge
d\zeta_2 \wedge d\bar{\zeta}_2 \wedge
\cdots \wedge
d\zeta_n \wedge d\bar{\zeta}_n = {(-2i)}^n dV ,
\end{equation*}
and hence
\begin{equation*}
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
d\bar{\zeta}_2 \wedge d\zeta_2 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n = {(2i)}^n dV .
\end{equation*}

\begin{exbox}
\begin{exercise}
Similarly to the Cauchy--Pompeiu formula,
note the singularity in the second term of the Bochner--Martinelli formula,
and prove that the integral still makes
sense (the function is integrable).
\end{exercise}

\begin{exercise}
Check that for $n=1$, the Bochner--Martinelli formula
reduces to the standard Cauchy--Pompeiu formula.
\end{exercise}
\end{exbox}

As usual, we split the derivatives into the holomorphic and
antiholomorphic parts.  We work with multi-indices.  For $\alpha$ and $\beta$
with $\abs{\alpha}=p$ and 
$\abs{\beta}=q$, a differential form
\begin{equation*}
\eta = 
\sum_{\substack{\abs{\alpha}=p \\ \abs{\beta}=q}}
\eta_{\alpha \beta} \, dz^\alpha \wedge d\bar{z}^\beta 
\end{equation*}
is called a \emph{\myindex{$(p,q)$-form}} or a differential form of
\emph{\myindex{bidegree}} $(p,q)$.
Define
\glsadd{not:d}%
\glsadd{not:dbar}%
\begin{equation*}
\partial \eta \overset{\text{def}}{=}
\sum_{\substack{\abs{\alpha}=p \\ \abs{\beta}=q}}
\,
\sum_{j=1}^n
\frac{\partial \eta_{\alpha \beta}}{\partial z_j} dz_j \wedge dz^\alpha
\wedge d\bar{z}^\beta ,
\qquad \text{and} \qquad
\bar{\partial} \eta \overset{\text{def}}{=}
\sum_{\substack{\abs{\alpha}=p \\ \abs{\beta}=q}}
\,
\sum_{j=1}^n
\frac{\partial \eta_{\alpha \beta}}{\partial \bar{z}_j} d\bar{z}_j \wedge dz^\alpha
\wedge d\bar{z}^\beta .
\end{equation*}
It is not difficult to see that $d \eta = \partial \eta +
\bar{\partial} \eta$ as before.

\begin{proof}[Proof of Bochner--Martinelli]
The structure of the proof is essentially the same as that of
the Cauchy--Pompeiu theorem for $n=1$, although some of the formulas are somewhat
more involved.

Let $z \in U$ be fixed.  Suppose $r > 0$ is small enough so that
$\overline{B_r(z)} \subset U$.  Orient both
$\partial U$ and $\partial B_r(z)$
positively.  Notice that 
$f(\zeta) \omega(\zeta,z)$ contains all the holomorphic $d\zeta_j$.
Therefore,
\begin{equation*}
\begin{split}
d \bigl( f(\zeta) \omega(\zeta,z) \bigr)
& =
\bar{\partial} \bigl( f(\zeta) \omega(\zeta,z) \bigr)
\\
& = 
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z)
\\
& \phantom{=} +
f(\zeta)
\frac{(n-1)!}{{(2\pi i)}^n}
\sum_{j=1}^n
\frac{\partial}{\partial \bar{\zeta}_j} \left[
\frac{\bar{\zeta}_j-\bar{z}_j}{\norm{\zeta-z}^{2n}}
\right]
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n .
\end{split}
\end{equation*}
We compute
\begin{equation*}
\sum_{j=1}^n
\frac{\partial}{\partial \bar{\zeta}_j}
\left[
\frac{\bar{\zeta}_j-\bar{z}_j}{\norm{\zeta-z}^{2n}}
\right]
=
\sum_{j=1}^n
\left(
\frac{1}{\norm{\zeta-z}^{2n}}
-n
\frac{\abs{\zeta_j-z_j}^2}{\norm{\zeta-z}^{2n+2}}
\right)
= 0 .
\end{equation*}
Therefore,
$d \bigl( f(\zeta) \omega(\zeta,z) \bigr) = \bar{\partial} f(\zeta) \wedge
\omega(\zeta,z)$.
We apply Stokes:
\begin{equation*}
\begin{split}
\int_{\partial U}
f(\zeta) \omega(\zeta,z)
-
\int_{\partial B_r(z)}
f(\zeta) \omega(\zeta,z)
& =
\int_{U \setminus \overline{B_r(z)}}
d \bigl( f(\zeta) \omega(\zeta,z) \bigr)
\\
%& =
%\int_{U \setminus \overline{B_r(z)}}
%\bar{\partial} \bigl( f(\zeta) \omega(\zeta,z) \bigr)
%\\
& =
\int_{U \setminus \overline{B_r(z)}}
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z) .
\end{split}
\end{equation*}
Again, due to the integrability, which you showed in an exercise above,
the right-hand side converges to the integral over $U$ as $r \to 0$.
Just as for the Cauchy--Pompeiu formula, we now need to show that the integral
over $\partial B_r(z)$ goes to $f(z)$ as $r \to 0$.

So
\begin{equation*}
\int_{\partial B_r(z)}
f(\zeta) \omega(\zeta,z)
=
f(z)
\int_{\partial B_r(z)}
\omega(\zeta,z)
+
\int_{\partial B_r(z)}
\bigl(f(\zeta)-f(z)\bigr) \omega(\zeta,z) .
\end{equation*}
To finish the proof,
we will show that
$\int_{\partial B_r(z)}
\omega(\zeta,z) = 1$, and that the second term goes to zero.
We apply Stokes again
and note that the volume of $B_r(z)$ is
$\frac{\pi^n}{n!}r^{2n}$.
\begin{multline*}
\int_{\partial B_r(z)}
\omega(\zeta,z)
\\
\begin{aligned}
&=
\int_{\partial B_r(z)}
\frac{(n-1)!}{{(2\pi i)}^n}
\sum_{j=1}^n
\frac{\bar{\zeta}_j-\bar{z}_j}{\norm{\zeta-z}^{2n}} \,
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_j } \wedge d\zeta_j \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{\partial B_r(z)}
\sum_{j=1}^n(\bar{\zeta}_j-\bar{z}_j)
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_j } \wedge d\zeta_j \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
\!
d\left(
\sum_{j=1}^n(\bar{\zeta}_j-\bar{z}_j)
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_j } \wedge d\zeta_j \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\right)
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
n~
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
n
{(2i)}^n dV
%\\
%&=
%\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
%n
%{(-2i)}^n
%\frac{\pi^n}{n!} r^{2n}
=
1 .
\end{aligned}
\end{multline*}

Next, we tackle the second term.
Via the same computation as above we find
\begin{multline*}
%\begin{split}
\int_{\partial B_r(z)}
\bigl(f(\zeta)-f(z)\bigr)
\omega(\zeta,z)
%&=
\\
=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\Biggl(
\int_{B_r(z)}
\bigl(f(\zeta)-f(z)\bigr)
n~
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
%&\phantom{=}\ +
%& +
+
%\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
\sum_{j=1}^n
\frac{\partial f}{\partial \bar{\zeta}_j}(\zeta)
(\bar{\zeta}_j-\bar{z}_j)
~
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n \Biggr).
%\end{split}
\end{multline*}
As $U$ is bounded, $\abs{f(\zeta)-f(z)} \leq M
\snorm{\zeta-z}$
and
$\abs{\frac{\partial f}{\partial \bar{\zeta}_j}(\zeta)
(\bar{\zeta}_j-\bar{z}_j)} \leq M \snorm{\zeta-z}$ for some $M$.
So 
for all $\zeta \in \partial B_r(z)$,
we have
$\abs{f(\zeta)-f(z)} \leq Mr$
and $\abs{\frac{\partial f}{\partial \bar{\zeta}_j}(\zeta)
(\bar{\zeta}_j-\bar{z}_j)} \leq Mr$.
Hence
\begin{multline*}
\abs{
\int_{\partial B_r(z)}
\bigl(f(\zeta)-f(z)\bigr)
\omega(\zeta,z)
}
\\
\leq
\frac{(n-1)!}{{(2\pi)}^n}\frac{1}{r^{2n}}
\left(
\int_{B_r(z)}
n 2^n Mr \, dV
+
\int_{B_r(z)}
n 2^n Mr \, dV
\right)
=
2 M r .
\end{multline*}
Therefore, this term goes to zero as $r \to 0$.
\end{proof}

One drawback of the Bochner--Martinelli formula $\int_{\partial U} f(\zeta)
\omega(\zeta,z)$ is that the kernel is not
holomorphic in $z$ unless $n=1$.  It does not simply produce
holomorphic functions.  If we differentiate in $\bar{z}$ underneath the
$\partial U$ integral, we do not necessarily obtain zero.
On the other hand, we have an explicit formula and this formula does not
depend on $U$.  This is not the case for the Bergman and Szeg\"o
kernels, which we will see next, although those are holomorphic in the
right way.

\begin{exbox}
\begin{exercise}
Prove that if $z \notin \widebar{U}$, then rather than $f(z)$ in the
formula you obtain
\begin{equation*}
\int_{\partial U}
f(\zeta) \omega(\zeta,z)
-
\int_{U}
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z) = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $f$ is holomorphic on a neighborhood of
$\overline{B_r(z)}$.
\begin{exparts}
\item
Using the Bochner--Martinelli formula, prove that
\begin{equation*}
f(z) = \frac{1}{V\bigl(B_r(z)\bigr)} \int_{B_r(z)} f(\zeta) \, dV(\zeta) ,
\end{equation*}
where $V\bigl(B_r(z)\bigr)$ is the volume of $B_r(z)$.
\item
Use part a) to prove the maximum principle for holomorphic functions.
\end{exparts}
\end{exercise}

\begin{exercise}
Use Bochner--Martinelli for the solution of $\bar{\partial}$ with compact
support.  That is, suppose $g = g_1 d\bar{z}_1 + \cdots + g_n d\bar{z}_n$
is a smooth compactly supported $(0,1)$-form
on $\C^n$, $n \geq 2$, and
$\frac{\partial g_k}{\partial \bar{z}_\ell} =
\frac{\partial g_\ell}{\partial \bar{z}_k}$ for all $k, \ell$.
Prove that
\begin{equation*}
\psi(z) = - \int_{\C^n} g(\zeta) \wedge \omega(\zeta,z)
\end{equation*}
is a compactly supported smooth solution to $\bar{\partial} \psi = g$.
Hint: Look at the previous proof.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bergman kernel}

Let $U \subset \C^n$ be a domain.  Define
\emph{\myindex{Bergman space}} of $U$:
\glsadd{not:A2}%
\glsadd{not:L2}%
\begin{equation*}
A^2(U) \overset{\text{def}}{=} \sO(U) \cap L^2(U) .
\end{equation*}
That is, $A^2(U)$ denotes the space
of holomorphic functions $f \in \sO(U)$ such that
\glsadd{not:L2norm}%
\begin{equation*}
\snorm{f}_{A^2(U)}^2 \overset{\text{def}}{=} \snorm{f}_{L^2(U)}^2
= \int_U \sabs{f(z)}^2 dV < \infty .
\end{equation*}
$A^2(U)$ is
an inner product space with the
$L^2(U)$ inner product
\glsadd{not:L2innprod}%
\begin{equation*}
\linnprod{f}{g} \overset{\text{def}}{=} \int_U f(z) \overline{g(z)} \, dV .
\end{equation*}
We will prove that $A^2(U)$ is
complete, in other words, it is a Hilbert space.  We first
prove that
the $A^2(U)$ norm bounds the
uniform norm on compact sets.

\begin{lemma} \label{lemma:bergmanKbound}
Let $U \subset \C^n$ be a domain and $K \subset \subset U$ compact.  Then
there exists a constant $C_K$, such that
\begin{equation*}
\snorm{f}_K
=
\sup_{z \in K} \sabs{f(z)} 
\leq C_K \snorm{f}_{A^2(U)} 
\qquad \text{ for all } f\in A^2(U) .
\end{equation*}
Consequently, $A^2(U)$ is complete.
\end{lemma}

\begin{proof}
As $K$ is compact there exists an $r > 0$ such that
$\overline{\Delta_r(z)} \subset U$
for all $z \in K$.
Take any $z \in K$, and
apply \exerciseref{exercise:averageDelta} and Cauchy--Schwarz:
\begin{equation*}
\begin{split}
\abs{f(z)} &=
\abs{\frac{1}{V\bigl(\Delta_r(z)\bigr)} \int_{\Delta_r(z)} f(\xi) \,
dV(\xi)}
\\
& \leq
\frac{1}{\pi^n r^{2n}}
\sqrt{\int_{\Delta_r(z)} 1^2 \, dV(\xi)}
\sqrt{\int_{\Delta_r(z)} \abs{f(\xi)}^2 \, dV(\xi)}
\\
& =
\frac{1}{\pi^{n/2} r^n}
\snorm{f}_{A^2(\Delta_r(z))} 
\leq
\frac{1}{\pi^{n/2} r^n}
\snorm{f}_{A^2(U)} .
\end{split}
\end{equation*}
Taking supremum over $z \in K$ proves the estimate.
Therefore, if $\{ f_j \}$ is a sequence of functions in $A^2(U)$
converging in $L^2(U)$ to some $f \in
L^2(U)$, then it converges uniformly on compact sets, and so $f \in \sO(U)$.
Consequently, $A^2(U)$ is a closed subspace of $L^2(U)$, and hence complete.
\end{proof}

For a bounded domain, $A^2(U)$ is always infinite-dimensional, see exercise
below.  There exist unbounded domains for which either 
$A^2(U)$ is trivial (just the zero function) or even finite-dimensional.
When $n=1$, $A^2(U)$ is either trivial, or infinite-dimensional.

\begin{exbox}
\begin{exercise}
Show that if a domain $U \subset \C^n$ is bounded, then $A^2(U)$ is
infinite-dimensional.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Show that $A^2(\C^n)$ is trivial (it is just the zero function).
\item
Show that $A^2(\D \times \C)$ is trivial.
\item
Find an example of an unbounded domain $U$ for which $A^2(U)$ is
infinite-dimensional.
Hint: Think in one dimension for simplicity.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that $A^2(\D)$ can be identified with $A^2(\D \setminus \{ 0 \})$,
that is, every function in the latter can be extended to a function in the
former.
\end{exercise}
\end{exbox}

The lemma says that 
point evaluation is a bounded linear
functional.
That is, fix $z \in U$ and take $K= \{ z \}$, then the linear operator
\begin{equation*}
f \mapsto f(z)
\end{equation*}
is a bounded linear functional.  By the Riesz--Fisher theorem, there exists
a $k_z \in A^2(U)$, such that
\begin{equation*}
f(z) = \linnprod{f}{k_z} .
\end{equation*}
Define the \emph{\myindex{Bergman kernel}} for $U$ as
\glsadd{not:Bergmanker}%
\begin{equation*}
K_U(z,\bar{\zeta}) \overset{\text{def}}{=} \overline{k_z(\zeta)} .
\end{equation*}
The function $K_U$ is defined as $(z,\bar{\zeta})$ vary over
\glsadd{not:Ustar}%
$U \times U^*$, where we write
\begin{equation*}
U^* = \{ \zeta \in \C^n : \bar{\zeta} \in U \}.
\end{equation*}


Then for all $f \in A^2(U)$ we have
\begin{equation} \label{eq:repropropBergman}
f(z)
=
\int_U f(\zeta) K_U(z,\bar{\zeta}) \, dV(\zeta) .
\end{equation}
This last equation is sometimes called
the \emph{\myindex{reproducing property}} of the kernel.

Note that the Bergman kernel depends on $U$, which is why we write it
as $K_U(z,\bar{\zeta})$.

\begin{prop}
The Bergman kernel $K_U(z,\bar{\zeta})$ is holomorphic in $z$, 
antiholomorphic in $\zeta$, and
\begin{equation*}
\overline{K_U(z,\bar{\zeta})} = K_U(\zeta,\bar{z}) .
\end{equation*}
\end{prop}

\begin{proof}
As each $k_z$ is in $A^2(U)$, it is holomorphic in $\zeta$.  Hence, $K_U$ is
antiholomorphic in $\zeta$.  If we prove
$\overline{K_U(z,\bar{\zeta})} = K_U(\zeta,\bar{z})$, then we prove $K_U$
is holomorphic in $z$.

As $\overline{K_U(z,\bar{\zeta})} = k_z(\zeta)$ is in $A^2(U)$, then
\begin{equation*}
\begin{split}
\overline{K_U(z,\bar{\zeta})}
& =
\int_{U} \overline{K_U(z,\bar{w})} K_U(\zeta,\bar{w}) dV(w)
\\
& =
\overline{
\left(
\int_{U} \overline{K_U(\zeta,\bar{w})} K_U(z,\bar{w}) dV(w)
\right)
}
=
\overline{
\overline{
K_U(\zeta,\bar{z})
}}
=
K_U(\zeta,\bar{z}) . \qedhere
\end{split}
\end{equation*}
\end{proof}

Therefore, thinking of
$\bar{\zeta}$ as the variable, $K_U$ is a holomorphic function of
$2n$ variables.

\begin{example} \label{example:bergmankerneldisc}
\index{Bergman kernel!unit disc}%
\index{Szeg{\"o} kernel!unit disc}%
Let us compute the Bergman kernel (and
the Szeg{\"o} kernel of the next section while we're at it)
explicitly for the
unit disc $\D \subset \C$.  Let $f \in \sO(\D) \cap C(\widebar{D})$, that
is, $f$ is holomorphic in $\D$ and continuous up to the boundary.
Let $z \in \D$.
Then
\begin{equation*}
f(z) = \frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{\zeta-z} \,
d\zeta .
\end{equation*}
On the unit circle $\zeta \bar{\zeta} = 1$.  Let $ds$ be
the arc-length measure on the circle, parametrized as $\zeta =
e^{is}$.
Then $d\zeta = i e^{is} ds$, and 
\begin{equation*}
f(z) = \frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{\zeta-z} \,
d\zeta
= \frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}}
\bar{\zeta} \, d\zeta
= \frac{1}{2\pi} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}} \, ds .
\end{equation*}
The integral is now a regular line integral of a function whose
singularity, which used to be inside the unit disc, disappeared (we 
``reflected it'' to the outside).  The kernel 
$\frac{1}{2\pi} \frac{1}{1-z\bar{\zeta}}$ is called the
\emph{Szeg{\"o} kernel}, which we
will briefly mention next.  We apply Stokes to the second integral above:
\begin{equation*}
\begin{split}
\frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}}
\bar{\zeta} \, d\zeta 
&=
\frac{1}{2\pi i} \int_{\D} f(\zeta)
\frac{\partial}{\partial \bar{\zeta}} \left[
\frac{\bar{\zeta}}{1-z\bar{\zeta}} \right] \,
d\bar{\zeta} \wedge d\zeta 
\\
&=
\frac{1}{\pi} \int_{\D} 
\frac{f(\zeta)}{{(1-z\bar{\zeta})}^2} \, dA(\zeta) .
\end{split}
\end{equation*}
The Bergman kernel in the unit disc is, therefore,
\begin{equation*}
K_{\D}(z,\bar{\zeta}) = \frac{1}{\pi} \frac{1}{{(1-z\bar{\zeta})}^2} .
\end{equation*}
It follows from the exercises below that
this function really is the Bergman kernel.
That is, $K_{\D}$ is the unique conjugate symmetric
reproducing function that is in
$A^2({\D})$ for a fixed $\zeta$.  
We have only shown the formula for functions continuous up to the boundary,
but those are dense in $A^2({\D})$.
\end{example}

\begin{example}
In an exercise you found that
$A^2(\C^n) = \{ 0 \}$.  Therefore, 
$K_{\C^n}(z,\bar{\zeta}) \equiv 0$.
\end{example}

The Bergman kernel for a more general domain is diffcult (usually impossible)
to compute explicitly.  We do have the following formula however.

\begin{prop}
Suppose $U \subset \C^n$ is a domain, and
$\{ \varphi_j (z) \}_{j\in I}$ is a complete orthonormal system
for $A^2(U)$.  Then
\begin{equation*}
K_U(z,\bar{\zeta})
=
\sum_{j \in I} \varphi_j(z) \overline{\varphi_j(\zeta)} ,
\end{equation*}
with uniform convergence on compact subsets of $U \times U^*$.
\end{prop}

\begin{proof}
For any fixed $\zeta \in U$, the function $z \mapsto K_U(z,\bar{\zeta})$ is
in $A^2(U)$ and so expand this function
in terms of the basis and use the reproducing property of $K_U$
\begin{equation*}
K_U(z,\bar{\zeta}) = 
\sum_{j \in I}
\left(\int_U K_U(w,\bar{\zeta}) \overline{\varphi_j(w)} \, dV(w) \right)
\varphi_j(z)
=
\sum_{j \in I}
\overline{\varphi_j(\zeta)}
\varphi_j(z) .
\end{equation*}
The convergence is in $L^2$ as a function of $z$, for a fixed $\zeta$.
Let $K \subset \subset U$ be a compact set.
Via \lemmaref{lemma:bergmanKbound}, $L^2$ convergence in $A^2(U)$ is uniform convergence on
compact sets.  Therefore, for a fixed $\zeta$ the convergence is uniform in
$z \in K$.  In particular, we get pointwise convergence.  So,
\begin{equation*}
\sum_{j \in I}
\abs{
\varphi_j(z) 
}^2
=
\sum_{j \in I}
\varphi_j(z) 
\overline{\varphi_j(z)}
=
K_U(z,\bar{z})
\leq C_K < \infty ,
\end{equation*}
where $C_K$ is the supremum of $K_U(z,\bar{\zeta})$ on $K \times K^*$.
Hence for $(z,\bar{\zeta}) \in K \times K^*$,
\begin{equation*}
\sum_{j \in I}
\abs{
\varphi_j(z)
\overline{\varphi_j(\zeta)}
}
\leq
\sqrt{
\sum_{j \in I}
\abs{
\varphi_j(z)
}^2
}
\sqrt{
\sum_{j \in I}
\abs{
\varphi_j(\zeta)
}^2
}
\leq
C_K < \infty .
\end{equation*}
And so the convergence is uniform on $K \times K^*$.
\end{proof}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Show that if $U \subset \C^n$ is bounded, then
$K_U(z,\bar{z}) > 0$
for all $z \in U$.
\item
Why can this fail if $U$ is unbounded?
Find a (trivial) counterexample.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that given a domain $U \subset \C^n$, the Bergman kernel is the unique
function $K_U(z,\bar{\zeta})$ such that
\begin{exnumparts}
\item
for a fixed $\zeta$, $K_U(z,\bar{\zeta})$ is in $A^2(U)$,
\item
$\overline{K_U(z,\bar{\zeta})} =
K_U(\zeta,\bar{z})$,
\item
the reproducing property \eqref{eq:repropropBergman}
holds.
\end{exnumparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be either the unit ball or the unit polydisc.
Show that 
$A^2(U) \cap C(\widebar{U})$ is dense in $A^2(U)$.  In particular, this
exercise says we
only need to check the reproducing property on functions continuous up to
the boundary to show we have the Bergman kernel.
\end{exercise}

\begin{exercise}
Let $U, V \subset \C^n$ be two domains and $f \colon U \to V$ a
biholomorphism.  Prove
\begin{equation*}
K_U(z,\bar{\zeta}) = \det D f(z) \, \overline{\det D f (\zeta)} \,
K_V\bigl(f(z),\overline{f(\zeta)}\bigr) .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that the Bergman kernel for the polydisc is
\begin{equation*}
K_{\D^n}(z,\bar{\zeta}) =
\frac{1}{\pi^n} \prod_{j=1}^n \frac{1}{{(1-z_j\bar{\zeta}_j)}^2}.
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Show that for
some constants $c_\alpha$,
the set of all monomials $\frac{z^\alpha}{c_\alpha}$ gives a complete orthonormal
system of $A^2(\bB_n)$.  Hint: To show orthonormality
compute the integral using polar coordinates in each variable
separately, that is, let $z_j = r_j e^{i\theta_j}$ where $\theta \in
[0,2\pi]^n$ and $\sum_j r_j^2 < 1$.  Then show completeness by showing that
if $f \in A^2(\bB_n)$ is orthogonal to all $z^\alpha$, then $f = 0$.
Finding $c_\alpha = \sqrt{\frac{\pi^n \alpha!}{(n+\abs{\alpha})!}}$ requires
the classical $\beta$ function of special function theory.
\end{exercise}

\begin{exercise}
Using the previous exercise, show that the Bergman kernel for the unit ball
is
\begin{equation*}
K_{\bB_n}(z,\bar{\zeta}) =
\frac{n!}{\pi^n}\frac{1}{{(1-\linnprod{z}{\zeta})}^{n+1}},
\end{equation*}
where $\linnprod{z}{\zeta}$ is the standard inner product on $\C^n$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Szeg{\"o} kernel}

We use the same technique to create a reproducing kernel on the
boundary by starting with $L^2(\partial U, d\sigma)$ instead of $L^2(U)$.
We obtain a
kernel where we integrate over the boundary rather than the domain itself.
Let us give a quick overview, but let us not get into the details.

Let $U \subset \C^n$ be a bounded domain with smooth boundary.  Let
$C(\widebar{U}) \cap \sO(U)$ be the holomorphic functions in $U$
continuous up to the boundary.  The restriction of
$f \in C(\widebar{U}) \cap \sO(U)$ to $\partial U$ is a continuous
function,
and hence $f|_{\partial U}$ is in $L^2(\partial U,d\sigma)$, where $d\sigma$
is the surface measure on $\partial U$.
Taking a closure
of these restrictions in $L^2(\partial U)$ obtains the Hilbert space
$H^2(\partial U)$,
\glsadd{not:H2}%
which is called the \emph{\myindex{Hardy space}}.
The inner product on $H^2(\partial U)$ is the $L^2(\partial U,
d\sigma)$ inner product:
\begin{equation*}
\linnprod{f}{g} \overset{\text{def}}{=} \int_{\partial U} f(z)
\overline{g(z)} \, d\sigma(z) .
\end{equation*}

\begin{exbox}
\begin{exercise}
Show that monomials $z^\alpha$ are a complete orthonormal system in
$H^2(\partial \bB_n)$.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a bounded domain with smooth boundary.
Prove that 
$H^2(\partial U)$ is infinite-dimensional.
\end{exercise}
\end{exbox}

Given an $f \in H^2(\partial U)$, write the Poisson integral
\begin{equation*}
Pf(z) = \int_{\partial U} f(\zeta) \, P(z,\zeta) \, d \sigma(\zeta) ,
\end{equation*}
where $P(z,\zeta)$ is the Poisson kernel.  The Poisson integral
reproduces harmonic functions.  As holomorphic functions are harmonic, we
find that if $f \in C(\widebar{U}) \cap \sO(U)$, then $Pf = f$.

Although $f \in H^2(\partial U)$ is only defined on the boundary,
through the Poisson integral, we have the values
$Pf(z)$ for $z \in U$.
For each $z \in U$,
\begin{equation*}
f \mapsto Pf(z)
\end{equation*}
defines a continuous linear functional.  Again we find a $s_z \in
H^2(\partial U)$ such that
\begin{equation*}
Pf(z) = \linnprod{f}{s_z} .
\end{equation*}
For $z \in U$ and $\zeta \in \partial U$, define
\glsadd{not:Szegoker}%
\begin{equation*}
S_U(z,\bar{\zeta}) \overset{\text{def}}{=} \overline{s_z(\zeta)} ,
\end{equation*}
although for a fixed $z$ this is a function only defined almost everywhere
as it is an element of $L^2(\partial U,d\sigma)$.
The function $S_U$ is the \emph{\myindex{Szeg{\"o} kernel}}.
If $f \in H^2(\partial U)$, then
\begin{equation*}
Pf(z) = \int_{\partial U} f(\zeta) \, S_U(z,\bar{\zeta}) \, d\sigma(\zeta) .
\end{equation*}

As functions in $H^2(\partial U)$ extend to $\widebar{U}$, then 
$f \in H^2(\partial U)$ may be considered a function on
$\widebar{U}$, where values in $U$ are given by $Pf$.  Similarly, we 
extend $S(z,\bar{\zeta})$ to a function on $U \times \widebar{U}^*$ (where
the values on the boundary are defined only almost everywhere).
We state without proof that if $\{ \varphi_j \}_{j\in I}$ is a complete
orthonormal system for $H^2(\partial U)$, then 
\begin{equation} \label{eq:formulaszego}
S_U(z,\bar{\zeta}) = \sum_{j \in I} \varphi_j(z)\overline{\varphi_j(\zeta)}
\end{equation}
for $(z,\bar{\zeta}) \in U \times U^*$, converging uniformly on compact subsets.
As before, this formula shows that $S$ is conjugate symmetric,
and so it extends to 
$(U \times \widebar{U}^*) \cup (\widebar{U} \times U^*)$.

\begin{example}
In \exerciseref{example:bergmankerneldisc}, we computed that
if $f \in C(\overline{\D}) \cap \sO(\D)$, then
\begin{equation*}
f(z) = \frac{1}{2\pi} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}} \, ds .
\avoidbreak
\end{equation*}
In other words, $S_{\D}(z,\zeta) = 
\frac{1}{\pi}
\frac{1}{1-z\bar{\zeta}}$ .
\end{example}

\begin{exbox}
\begin{exercise}
Using the formula \eqref{eq:formulaszego} compute $S_{\bB_n}$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Complex analytic varieties} \label{ch:analyticvarieties}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The ring of germs}

\begin{defn}
Let $p$ be a point in a topological space $X$.  Let $Y$ be a set and
$U, V \subset X$ be open neighborhoods of $p$.  We say that
two functions $f \colon U \to Y$ and
$g \colon V \to Y$ are equivalent if there exists a neighborhood
$W$ of $p$ such that $f|_W = g|_W$.

An equivalence class of functions defined in a neighborhood of $p$
is called a \emph{\myindex{germ of a function}}.
\glsadd{not:germf}%
Usually it is denoted by $(f,p)$, but we simply say $f$ when
the context is clear.
\end{defn}

The set of germs of complex-valued functions forms a
commutative ring, see exercise below to check the details.
For example, to multiply $(f,p)$ and $(g,p)$, take two representatives
$f$ and $g$ defined on a common neighborhood multiply them and
then consider the germ $(fg,p)$.  Similarly, $(f,p) + (g,p)$ is
defined as $(f+g,p)$.  It is easy to check that these operations are
well-defined.

\begin{exbox}
\begin{exercise}
Let $X$ be a topological space and $p \in X$.
Let $\sF$ be a class of complex-valued functions defined on open subsets
of such that whenever $f \colon U \to \C$ is in $\sF$ and $W \subset U$ is open,
then $f|_W \in \sF$, and such that whenever $f$ and $g$
are two functions in $\sF$, and $W$ is an open set where both are defined,
then $fg|_W$ and $(f+g)|_W$ are also in $\sF$.
Assume that all constant functions are in $\sF$.
Show that the ring operations defined above
on a set of germs at $p$ of functions from $\sF$ are well-defined,
and that the set of germs at $p$ of functions from $\sF$ is a commutative
ring.
\end{exercise}

\begin{exercise}
Let $X=Y=\R$ and $p=0$.  Consider the ring of germs of
continuous functions (or smooth functions if you wish).
Show that for every continuous $f \colon \R \to \R$ and every neighborhood
$W$ of $0$, there exists a $g \colon \R \to \R$ 
such that $(f,0) = (g,0)$, but $g|_W \not= f|_W$.
\end{exercise}
\end{exbox}

Germs are particularly useful for holomorphic functions because of the identity
theorem.  In particular, the behavior of the exercise above does not happen
for holomorphic functions.  Furthermore, for holomorphic functions, the
ring of germs is the same as the ring of convergent power series, see
exercise below.  No similar result is true for only smooth functions.

\begin{defn}
Let $p \in \C^n$.
\glsadd{not:ringofgerms}%
Write
${}_n\sO_p = \sO_p$ as the ring of germs at $p$ of holomorphic functions.
\end{defn}

The ring of germs $\sO_p$ has many nice properties,
and it is generally a ``nicer'' ring than the ring $\sO(U)$ for
some open $U$, and so it is easier to work with if we are interested
in local properties.

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Show that $\sO_p$ is an \myindex{integral domain} (has no zero divisors).
\item
Prove the ring of germs at $0 \in \R$ of smooth real-valued functions
is not an integral domain.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that the units (elements with multiplicative inverse)
of $\sO_p$ are the germs of functions which do not vanish at
$p$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item (easy)
Show that given a germ $(f,p) \in \sO_p$,
there exists a fixed open neighborhood $U$
of $p$ and a representative $f \colon U \to \C$ such that any other
representative $g$ can be analytically continued from $p$ to a holomorphic
function $U$.
\item
(easy) Given two representatives $f \colon U \to \C$
and $g \colon V \to \C$ of a germ $(f,p) \in \sO_p$,
let $W$ be the connected component of $U \cap V$
that contains $p$.  Then $f|_W = g|_W$.
\item
There exists a germ $(f,p) \in \sO_p$, such that for
any open neighborhood $U$ of $p$, and
any representative $f \colon U \to \C$ we can
find another representative of $g \colon V \to \C$
of that same germ such that $g|_{U \cap V} \not= f|_{U \cap V}$.
Hint: $n=1$ is sufficient.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that $\sO_p$ is isomorphic to the ring of convergent power series.
\end{exercise}
\end{exbox}


\begin{defn}
Let $p$ be a point in a topological space $X$.
We say that sets $A, B \subset X$ are equivalent
if there exists a neighborhood $W$ of $p$
such that $A \cap W = B \cap W$.
An equivalence class of sets 
is called a \emph{\myindex{germ of a set}} at $p$.
\glsadd{not:germA}%
It is denoted by $(A,p)$, but we may write $A$ when
the context is clear.
\end{defn}

The concept of $(X,p) \subset (Y,p)$ is defined in an obvious manner,
that is, there exist representatives $X$ and $Y$, and a neighborhood $W$
of $p$ such that $X \cap W \subset Y \cap W$.
Similarly,
if $(X,p)$, $(Y,p)$ are germs and $X$, $Y$
are any representatives of these germs, then
the intersection $(X,p) \cap (Y,p)$,
is the germ $(X \cap Y,p)$ and the union
$(X,p) \cup (Y,p)$ is the germ $(X \cup Y,p)$.

\begin{exbox}
\begin{exercise}
Check that the definition of
subset, union, and intersection of germs
of sets
is well-defined.
\end{exercise}
\end{exbox}


Let $R$ be some ring of germs of complex-valued
functions at $p \in X$ for some topological space $X$.
\glsadd{not:Zf}%
If $f$ is a complex-valued function,
let $Z_f$ be the zero set of $f$, that is $f^{-1}(0)$.
When $(f,p) \in R$ is a germ of a function
it makes sense to talk about the germ $(Z_f,p)$.  We take the zero
set of some representative and look at its germ at $p$.

\begin{exbox}
\begin{exercise}
Suppose $f$ and $g$ are two representatives of a germ $(f,p)$
show that the germs $(Z_f,p)$ and $(Z_g,p)$ are the same.
\end{exercise}

\begin{exercise}
Show that if $(f,p)$ and $(g,p)$ are in $R$
and $f$ and $g$ are some representatives, then
$(Z_f,p) \cup (Z_g,p) = (Z_{fg},p)$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Weierstrass preparation and division theorems} \label{sec:wpt}

Suppose
$f$ is (a germ of) a holomorphic function at a point $p \in \C^n$.
Write
\begin{equation*}
f(z) = \sum_{k=0}^\infty f_k(z-p),
\end{equation*}
where $f_k$ is a homogeneous polynomial of degree $k$,
that is, $f_k(tz) = t^k f_k(z)$.

\begin{defn}
Let $p \in \C^n$ and $f$ be a function holomorphic in a neighborhood of $p$.
If $f$ is not identically zero,
define
\glsadd{not:ord}%
\begin{equation*}
\ord_p f \overset{\text{def}}{=} \min \bigl\{ k \in \N_0 : f_k \not\equiv 0 \bigr\} .
\end{equation*}
If $f \equiv 0$, then define $\ord_p f = \infty$.
The number $\ord_p f$ is called the \emph{\myindex{order of vanishing}} of $f$ at $p$.
\end{defn}

In other words, the order of vanishing of $f$ at $p$ is $k$, whenever all
partial derivatives of order less than $k$ vanish at $p$, and there exists
at least one derivative of order $k$ that does not vanish at $p$.

In one complex variable, a holomorphic function $f$ with $\ord_0 f = k$
can be written
(locally) as $f(z) = z^k u(z)$ for a nonvanishing $u$.
In several variables, there is a similar theorem, or in fact a pair of theorems,
the so-called Weierstrass preparation and
division theorems.

\begin{defn}
Let $U \subset \C^{n-1}$ be open, and let $z' \in \C^{n-1}$ denote the
coordinates.
\glsadd{not:polyinOU}%
Suppose a polynomial $P \in \sO(U)[z_n]$ is monic of degree $k \geq 0$, that is,
\begin{equation*}
P(z',z_n) = z_n^k + \sum_{j=0}^{k-1} c_j(z') \, z_n^j ,
\end{equation*}
where $c_j$ are holomorphic functions defined on $U$, such that
$c_j(0) = 0$ for all $j$.  Then $P$ is
called a \emph{\myindex{Weierstrass polynomial}} of degree $k$.
\glsadd{not:polyinO0}%
If the $c_j$ are germs in $\sO_0 = {}_{n-1}\sO_0$, then $P \in \sO_0[z_n]$ and
$P$ is a \emph{\myindex{germ of a Weierstrass polynomial}}.
\end{defn}

The definition (and the theorem that follows) still holds for $n=1$.
If you read the
definition carefully, you will find that if $n=1$, then the only Weierstrass
polynomial of degree $k$ is $z^k$.
Note that for any $n$, if $k=0$, then $P = 1$.

The purpose of this section is to show that every holomorphic function
in $\sO_0$ is up to a unit 
and a possible small rotation a Weierstrass
polynomial, which carries the zeros of $f$.
Consequently the 
algebraic and geometric properties of 
${}_n\sO_0$ can be understood via algebraic and geometric properties of
${}_{n-1}\sO_0[z_n]$.

\begin{thm}[\myindex{Weierstrass preparation theorem}]
Suppose $f \in \sO(U)$ for an open $U \subset \C^{n-1} \times \C$,
where $0 \in U$, and $f(0)=0$.  Suppose
$z_n \mapsto f(0,z_n)$ is not identically zero near the origin
and its order of vanishing at the origin is $k \geq 1$.

Then there exists an open polydisc $V = V' \times D \subset \C^{n-1} \times \C$
with $0 \in V \subset U$,
a unique $u \in \sO(V)$, $u(z) \not=0$ for all $z \in V$, and a unique
Weierstrass polynomial $P$ of degree $k$
with coefficients holomorphic in $V'$ such that
\begin{equation*}
f(z',z_n) = u(z',z_n) \, P(z',z_n) ,
\end{equation*}
and such that all $k$ zeros (counting multiplicity)
of $z_n \mapsto P(z',z_n)$ lie in $D$ for all $z' \in V'$
\end{thm}

\begin{proof}
There exists a small disc $D \subset \C$ centered at zero such that
$\{0\} \times \widebar{D} \subset U$ and such that
$f(0,z_n) \not= 0$ for $z_n \in \widebar{D} \setminus \{ 0 \}$.
By continuity
of $f$,
there is a small polydisc $V = V' \times D$ such that
$\widebar{V} \subset U$ and $f$ is not zero on
$V' \times \partial D$.  We will consider the zeros of
$z_n \mapsto f(z',z_n)$ for $z'$ near zero.  See \figureref{fig:wprepdiv}.

\begin{myfig}
\subimport*{figures/}{wprepdiv.pdf_t}
\caption{The zeros of $z_n \mapsto f(z',z_n)$.\label{fig:wprepdiv}}
\end{myfig}

By the one-variable argument principle (\thmref{thm:onevarargprinc}) the number of zeros (with
multiplicity) of $z_n
\mapsto f(z',z_n)$ in $D$ is
\begin{equation*}
\frac{1}{2\pi i}
\int_{\partial D}
\frac{\frac{\partial f}{\partial z_n} (z',\zeta)}{f(z',\zeta)} ~d\zeta .
\end{equation*}
As $f(z',\zeta)$ does not vanish when $z' \in V'$ and $\zeta \in \partial
D$,
the expression above is a continuous integer-valued
function of $z' \in V'$.
The expression
is equal to $k$ when $z'=0$, and
so it is equal to $k$ for all $z' \in V'$.
Write
the zeros of $z_n \mapsto f(z',z_n)$ as $\alpha_1(z'),\ldots,\alpha_k(z')$, including
multiplicity.  The zeros are not ordered in any particular way.
Pick \emph{some} ordering for every $z'$.
Write
\begin{equation*}
P(z',z_n)
=
\prod_{j=1}^k \bigl(z_n-\alpha_j(z')\bigr)
=
z_n^k + c_{k-1}(z') \, z_n^{k-1} + \cdots + c_0 (z') .
\end{equation*}
For a fixed $z'$, $P$ is uniquely defined as the ordering of zeros does not
matter in its definition (see exercise below).  It is
clear that
$u$ and $P$ are unique if they exist (that is, if they exist as holomorphic functions).

The functions $\alpha_j$ are not even continuous in general (see
\exampleref{sqrt:example}).  However,
we will prove that the functions $c_j$ are holomorphic.  The functions
$c_j$ are
(up to sign)
the \emph{elementary symmetric functions}
of $\alpha_1,\ldots,\alpha_k$ (see below).  It is a standard
theorem in algebra
(see \exerciseref{exercise:powersums})
that the elementary symmetric functions are
polynomials in the so-called power sum functions in the $\alpha_j$s:
\begin{equation*}
s_m(z') = \sum_{j=1}^k \alpha_j{(z')}^m , \qquad m = 1,\ldots,k.
\end{equation*}
Therefore, if we show that the power sums $s_m$ are holomorphic, then
$c_\ell$ are 
holomorphic.

A refinement of the argument principle (see also \thmref{thm:onevarargprinc}) says:
If $h$ and $g$ are
holomorphic functions on a disc $D$, continuous on $\widebar{D}$,
such that $g$ has no zeros on $\partial D$, and $\alpha_1,\ldots,\alpha_k$
are the zeros of $g$ in $D$, then
\begin{equation*}
\frac{1}{2 \pi i}
\int_{\partial D} h(\zeta) \frac{g'(\zeta)}{g(\zeta)} ~d\zeta
= \sum_{j=1}^k h(\alpha_j) .
\end{equation*}

The formula above with $h(\zeta) = \zeta^m$ and $g(\zeta)=f(z',\zeta)$ says that
\begin{equation*}
s_m(z') = 
\sum_{j=1}^k \alpha_j{(z')}^m
=
\frac{1}{2\pi i}
\int_{\partial D}
\zeta^m
\frac{\frac{\partial f}{\partial \zeta} (z',\zeta)}{f(z',\zeta)} ~d\zeta .
\end{equation*}
The function $s_m$ is clearly continuous, and if we
differentiate under the integral
with $\frac{\partial}{\partial\bar{z}_\ell}$
for $\ell=1,\ldots,{n-1}$,
we find that $s_m$
is holomorphic.

Finally, we wish to show that $P$ divides $f$ as claimed.
For each fixed $z'$, one variable theory says that
$z_n \mapsto \frac{f(z',z_n)}{P(z',z_n)}$ has only removable singularities,
and in fact, it has no zeros as we defined $P$
to exactly cancel them all out.
The Cauchy
formula on $\nicefrac{f}{P}$ then says that the function
\begin{equation*}
u(z',z_n) =
\frac{1}{2\pi i}
\int_{\partial D} \frac{f(z',\zeta)}{P(z',\zeta)(\zeta-z_n)} \,
d\zeta
\end{equation*}
is equal to $\frac{f(z',z_n)}{P(z',z_n)}$.
The function $u$ is clearly continuous and holomorphic in $z_n$
for each fixed $z'$.  By differentiating under the integral, we
find that it is also holomorphic in $z'$.
\end{proof}

\begin{example} \label{sqrt:example}
A useful example to keep in mind is $f(z_1,z_2) = z_2^2 - z_1$,
a Weierstrass polynomial in $z_2$ of degree $k=2$.  So $z' = z_1$.
For all $z_1$ except the origin there are two zeros, $\pm \sqrt{z_1}$.
Call one of them $\alpha_1(z_1)$ and one of them $\alpha_2(z_1)$.  Recall
there is no continuous choice of a square root that works for all $z_1$,
so no matter how you choose, $\alpha_1$ and $\alpha_2$ will not be continuous.
At the origin there is only one zero of order two,
so $\alpha_1(0) = \alpha_2(0) = 0$.
On the other hand the
symmetric functions $c_1(z_1) = - \alpha_1(z_1) - \alpha_2(z_1) = 0$
and $c_0(z_1') = \alpha_1(z_1)\alpha_2(z_1) = -z_1$ are holomorphic.

The $k$ depends on the coordinates chosen.  If we do a
linear change of coordinates and consider $g(z_1,z_2) = -f(z_2,z_1)$,
then $g(z_1,z_2) = z_1^2 - z_2$,
which is a Weierstrass polynomial in $z_2$ of degree $k=1$.
After the change, there is only one zero,
$\alpha_1(z_1) = z_1^2$, and so $c_0(z_1) =
-z_1^2$.
\end{example}


A function $f(z_1,\ldots,z_n)$ is \emph{symmetric}\index{symmetric function}
if $f = f \circ p$ for all permutations of the variables $p$.
The \emph{\myindex{elementary symmetric functions}} of
$\alpha_1,\ldots,\alpha_k$ are the coefficients $\sigma_j$ of the polynomial
\begin{equation*}
\prod_{j=1}^k \bigl(t+\alpha_j\bigr)
=
t^k
+ \sigma_{1} \, t^{k-1} +
\cdots
+ \sigma_{k-2} \, t^2
+ \sigma_{k-1} \, t
+ \sigma_k .
\end{equation*}
In other words:
\begin{align*}
\sigma_{1} & = \alpha_1 + \alpha_2 + \cdots + \alpha_k, \\
\sigma_{2} & = \alpha_1 \alpha_2 \, + \, \alpha_1 \alpha_3 \, + \,
  \cdots \, + \, \alpha_{k-1} \alpha_k , \\
& \;\: \smash{\vdots} \\
\sigma_{k-1} & =
  \alpha_2 \alpha_3 \cdots \alpha_{k} \, + \,
  \alpha_1 \alpha_3 \alpha_4 \cdots \alpha_{k} \, + \, \cdots
  \,+\, \alpha_1 \alpha_2 \cdots \alpha_{k-1}, \\
\sigma_k & = \alpha_1 \alpha_2 \cdots \alpha_k.
\end{align*}
So for example when $k=2$, then $\sigma_2 = \alpha_1\alpha_2$ and
$\sigma_1 = \alpha_1 + \alpha_2$.  The function $\sigma_1$ happens to
already be a power sum.  We can write $\sigma_2$
as a polynomial in the power sums:
\begin{equation*}
\sigma_2
=
\frac{1}{2}
\left(
{\bigl(\alpha_1 + \alpha_2\bigr)}^2
-
\bigl(\alpha_1^2 + \alpha_2^2\bigr)
\right) .
\end{equation*}

\begin{exbox}
\begin{exercise} \label{exercise:powersums}
Show that elementary symmetric functions are polynomials in the power sums.
\end{exercise}

\begin{exercise} \label{exercise:symmetric}
Prove the \emph{\myindex{fundamental theorem of symmetric polynomials}}:
Every symmetric polynomial can be written as a polynomial in
the elementary symmetric functions.  Use the following procedure.
Using double induction, suppose the theorem is true if the number of
variables is less than $k$, and the theorem is true in $k$ variables
for degree less than $d$.
Consider a symmetric $P(z_1,\ldots,z_k)$ of degree $d$.
Write $P(z_1,\ldots,z_{k-1},0)$ by induction hypothesis as a polynomial
in the elementary symmetric functions of one less variable.  Use the
same coefficients, but plug in the elementary symmetric functions of $k$
variables except the symmetric polynomial in $k$ variables of degree $k$,
that is except the $z_1\cdots z_k$.
You will obtain a symmetric function $L(z_1,\ldots,z_k)$ and you need to
show 
$L(z_1,\ldots,z_{k-1},0) = P(z_1,\ldots,z_{k-1},0)$.  Now use symmetry to
prove that
\begin{equation*}
P(z_1,\ldots,z_k) =
L(z_1,\ldots,z_k) +
z_1\cdots z_k Q(z_1,\ldots,z_k) .
\end{equation*}
Then note that $Q$ has lower degree and finish by induction.
\end{exercise}

\begin{exercise} \label{exercise:symmetricseries}
Extend the previous exercise to power series.  Suppose $f(z_1,\ldots,z_k)$
is a convergent symmetric power series at 0, show that $f$ can be written
as a convergent power series in the elementary symmetric functions.
\end{exercise}

\begin{exercise} \label{exercise:symmetricseriesweier}
Suppose $P(z',z_n)$ is a Weierstrass polynomial of degree $k$,
and write the zeros as $\alpha_1(z'), \ldots, \alpha_k(z')$.
These are not holomorphic functions,
but suppose that $f$ is a symmetric convergent
power series at the origin in $k$ variables.  Show that
$f\bigl(\alpha_1(z'), \ldots, \alpha_k(z')\bigr)$ is a holomorphic function
of $z'$ near the origin.
\end{exercise}
\end{exbox}

The hypotheses of the preparation theorem are not an obstacle.  If a holomorphic
function $f$ is such that $z_n \mapsto f(0,z_n)$ vanishes identically,
then we can make a small linear change of
coordinates $L$ ($L$ can be a matrix arbitrarily close to the identity) such
that $f \circ L$ satisfies the hypotheses of the theorem.
For example, $f(z_1,z_2,z_3) = z_1z_3+z_2z_3$ does not satisfy the
hypotheses of the theorem as $f(0,0,z_3) \equiv 0$.  But for an arbitrarily
small $\epsilon \not= 0$, replacing
$z_2$ with $z_2 + \epsilon z_3$ leads to $\tilde{f}(z_1,z_2,z_3)
= f(z_1,z_2+\epsilon z_3,z_3) = 
z_1z_3+z_2z_3 + \epsilon z_3^2$, and $\tilde{f}(0,0,z_3) = \epsilon z_3^2$.
Thence $\tilde{f}$
satisfies the hypotheses of the theorem.

\begin{exbox}
\begin{exercise}
Prove the fact above about the existence of $L$ arbitrarily close to the
identity.
\end{exercise}

\begin{exercise}
Prove that a monic polynomial $P(\zeta)$ of one variable is
uniquely determined by its zeros up to multiplicity.  That is, suppose
$P$ and $Q$ are two monic polynomials with the same zeros
up to multiplicity,
then $P=Q$.  That proves the uniqueness of the Weierstrass polynomial.
\end{exercise}

\begin{exercise}
Suppose $D \subset \C$ is a bounded domain, $0 \in D$,
$U' \subset \C^{n-1}$ is a domain, $0 \in U'$,
and $P \in \sO(U')[z_n]$ is a Weierstrass polynomial
such that $P(z',z_n)$ is not zero on $U' \times \partial D$.
Then for any $z' \in U$, all zeros of $z_n \mapsto P(z',z_n)$ are in $D$.
\end{exercise}

\begin{exercise}
Let $D \subset \C$ be a bounded domain,
and
$U' \subset \C^{n-1}$ a domain.
Suppose
$f$ is a continuous function on
$U' \times \widebar{D}$ holomorphic on $U' \times D$,
where $f$ is zero on at least one point
of $U' \times D$, and $f$ is never zero on
$U' \times \partial D$.
Prove that
$z_n \mapsto f(z',z_n)$ has at least one zero in $D$ for every $z' \in U'$.
\end{exercise}
\end{exbox}

\pagebreak[2]
The order of vanishing of $f$ at the origin is a lower bound
on the number $k$ in the theorem.  The order of vanishing for a certain
variable may be larger than this lower bound.  If $f(z_1,z_2) =
z_1^2 + z_2^3$, then the $k$ we get is 3, but $\ord_0 f = 2$.
We can make a small
linear change of coordinates to ensure $k = \ord_0 f$.
With the $f$ as above, $f(z_1 + \epsilon z_2,z_2)$ gives $k = 2$
as expected.

The Weierstrass preparation theorem is a generalization of
the implicit function theorem.  When $k=1$ in the theorem, then we obtain
the Weierstrass polynomial $z_n + c_0(z')$.  That is, the zero set of 
$f$ is a graph of the holomorphic function $-c_0$.
Therefore, the Weierstrass theorem is a generalization of the
implicit function theorem to the case when $\frac{\partial f}{\partial z_n}$
is zero.  We can still ``solve'' for $z_n$, but we find $k$ solutions
given as the zeros of the obtained Weierstrass polynomial.

There is an obvious statement of the preparation theorem for germs.

\begin{exbox}
\begin{exercise}
State and prove a germ version of the preparation theorem.
\end{exercise}
\end{exbox}

\begin{thm}[\myindex{Weierstrass division theorem}]
Suppose $f$ is holomorphic near the origin, and suppose $P$
is a Weierstrass polynomial of degree $k \geq 1$ in $z_n$.  Then there exists
a neighborhood $V$ of the origin and unique $q,r \in \sO(V)$,
where $r$ is a polynomial in $z_n$ of degree less than $k$, and on $V$,
\begin{equation*}
f = qP + r .
\end{equation*}
\end{thm}

Note that $r$ need not be a Weierstrass polynomial; it need not be monic
nor do the coefficients need to vanish at the origin.  It is simply a
polynomial in $z_n$ with coefficients that are holomorphic functions
of the first $n-1$ variables.

\begin{proof}
Uniqueness is left as an exercise.  Consider
a connected neighborhood $V = V' \times D$ of the origin, where
$D$ is a disc,
$f$ and $P$ are continuous in $V' \times \widebar{D}$,
and 
$P$ is not zero on $V' \times \partial D$.
Let
\begin{equation*}
q(z',z_n) =
\frac{1}{2\pi i} \int_{\partial D} \frac{f(z',\zeta)}{P(z',\zeta)(\zeta-z_n)}
~d\zeta .
\end{equation*}
As $P$ is not zero on $V' \times \partial D$,
the function $q$ 
is holomorphic in $V$ (differentiate under the integral).
If $P$ did divide $f$, then $q$ would really be $\nicefrac{f}{P}$.
But if $P$ does not divide $f$, then
the Cauchy integral formula does not apply and $q$ is not equal to
$\nicefrac{f}{P}$.  Interestingly
the expression does give the quotient in the division with remainder.

Write $f$ using the Cauchy integral formula in $z_n$ and
subtract $qP$ to obtain $r$:
\begin{multline*}
r(z',z_n) = f(z',z_n) - q(z',z_n)P(z',z_n)
\\
=
\frac{1}{2\pi i}
\int_{\partial D} \frac{f(z',\zeta)P(z',\zeta) - f(z',\zeta)P(z',z_n)}{P(z',\zeta)(\zeta-z_n)}
~d\zeta .
\end{multline*}
We need to show $r$ is a polynomial in $z_n$ of degree less than
$k$.  In the expression inside the integral, the numerator is
of the form $\sum_j h_j(z',\zeta)(\zeta^j-z_n^j)$ and is therefore
divisible by $(\zeta-z_n)$.
The numerator is a polynomial of degree $k$ in
$z_n$.  After dividing by $(\zeta-z_n)$,
the integrand becomes
a polynomial in $z_n$ of degree $k-1$. 
Use linearity of the integral
to integrate the coefficients of the polynomial.  Each coefficient is a
holomorphic function in $V'$ and the proof is finished.  Some coefficients may have
integrated to zero, so we can only say that $r$ is a polynomial
of degree $k-1$ or less.
\end{proof}

For example, let $f(z,w) = e^z + z^4 e^w + z w^2 e^w + zw$ and $P(z,w)
= w^2 + z^3$.  Then $P$ is a Weierstrass polynomial in $w$ of degree $k=2$.  A bit
of computation shows
\begin{equation*}
\frac{1}{2\pi i}
\int_{\partial \D}
\frac{e^z + z^4 e^{\zeta} + z {\zeta}^2 e^{\zeta} + z \zeta}{(\zeta^2+z^3)(\zeta-w)}
d\zeta
=
z e^w
,
\quad \text{so} \quad
f(z,w) = \underbrace{\bigl( ze^w \bigr)}_{q} \underbrace{\bigl( w^2 + z^3 \bigr)}_{P} +
\underbrace{z w + e^z}_{r} .
\end{equation*}
Notice that $r$ is a polynomial of degree 1 in $w$, but it is neither monic,
nor do coefficients vanish at 0.

\begin{exbox}
\begin{exercise}
Prove the uniqueness part of the theorem.
\end{exercise}

\begin{exercise}
State and prove a germ version of the division theorem.
\end{exercise}
\end{exbox}

The Weierstrass division theorem is a generalization of the division
algorithm for polynomials with coefficients in a field, such as the
complex numbers:  If $f(\zeta)$ is a
polynomial, and $P(\zeta)$ is a nonzero polynomial of degree $k$, then there exist
polynomials $q(\zeta)$ and $r(\zeta)$ with degree of $r$ less than $k$ such
that $f = qP + r$.  If the coefficients are in a commutative ring,
we can divide as long as $P$ is monic.
The Weierstrass division theorem says that in the case of the ring $\sO_p$,
we can divide by a monic $P \in {}_{n-1}\sO_p[z_n]$,
even if $f$ is a holomorphic function (a ``polynomial of
infinite degree'') as long as $f(0,z_n)$ has
finite order.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The dependence of zeros on parameters} \label{sec:dependenceofzeros}

Let us prove that the zeros change holomorphically as long as they do not
come together.  We will prove shortly that the zeros come together
only on a small set; it is a zero set of a certain holomorphic function
called the discriminant.

A set of zeros are said to be
\emph{\myindex{geometrically distinct}} if they are distinct
points of $\C$.  A zero is called
\emph{\myindex{geometrically unique}} if it is a unique complex number.
For example, ${(z_n-1)}^2$ has a geometrically unique zero at $1$, and 
${(z_n-1)}^2(z_n+1)$ has two geometrically distinct zeros, $1$ and $-1$.


\begin{prop} \label{prop:zerohol}
Let $D \subset \C$ and $U' \subset \C^{n-1}$ be domains,
and
$f \in \sO(U' \times D)$.
Suppose that for each fixed $z' \in U'$ the function
$z_n \mapsto f(z',z_n)$ has a geometrically unique zero $\alpha(z') \in D$.  Then $\alpha$ is
holomorphic in $U'$.
\end{prop}

The proposition shows that the regularity conclusion of the implicit
function theorem holds under the hypothesis that there exists some local
solution for $z_n$.  This result holds only for holomorphic
functions and not for real-analytic functions.  For example, $x^2-y^3 = 0$ has a
unique real solution $y = x^{2/3}$, but that function is not even
differentiable.

\begin{proof}
We must show that $\alpha$ 
is holomorphic near any point, which, without loss
of generality, is the origin.
Apply the preparation
theorem to find $f = u P$,
where $P$ is a
Weierstrass polynomial in $\sO(V')[z_n]$ for some $V' \subset U'$
and all zeros of $z_n \mapsto P(z',z_n)$ are in $D$.
As $\alpha$ is a geometrically unique zero in $D$,
\begin{equation*}
P(z',z_n) = {\bigl(z_n-\alpha(z') \bigr)}^k = z_n^k - k \alpha(z') z_n^{k-1}
+ \cdots
\end{equation*}
The coefficients of $P$ are holomorphic, so $\alpha$ is holomorphic.
\end{proof}

\begin{prop} \label{prop:zeroshol}
Let $D \subset \C$ and $U' \subset \C^{n-1}$
be domains,
and
$f \in \sO(U' \times D)$.
Let $m \in \N$ be such that
for each $z' \in U'$, the function $z_n \mapsto f(z',z_n)$ has
precisely $m$ geometrically distinct zeros.
Then locally near each point in $U'$ there exist $m$ holomorphic functions
$\alpha_1(z'),\ldots,\alpha_m(z')$,
positive integers
$k_1,\ldots,k_m$,
and a nonvanishing holomorphic function $u$
such that
\begin{equation*}
f(z',z_n) = u(z',z_n) \prod_{j=1}^m {\bigl( z_n - \alpha_j(z') \bigr)}^{k_j}
.
\end{equation*}
\end{prop}

We can only define $\alpha_1$ through $\alpha_m$ locally (on a
smaller domain) as we do not know
how $\alpha_1$ through $\alpha_m$ are ordered, and the order could change 
as we move around 
$U'$ if it is not simply connected.  If $U'$ is simply connected, then
the functions can be defined globally by analytic continuation.
For an example where $U'$ is not simply connected,
recall \exampleref{sqrt:example}. Consider $U' = \C \setminus \{ 0 \}$ and
think $D=\C$ rather than a disc for simplicity.  Then $U'$ is not simply
connected, and there do not exist continuous functions
$\alpha_1(z_1)$ and
$\alpha_2(z_1)$ that are zeros
of the Weierstrass polynomial, that is
$z_2^2 - z_1 =
\bigl(z_2-\alpha_1(z_1) \bigr)
\bigl(z_2-\alpha_2(z_1) \bigr)$.
These would be the two square roots of $z_1$, and there is no continuous
(let alone holomorphic) square root defined in $\C \setminus \{ 0 \}$.
Such roots can be chosen to be holomorphic on any smaller
simply connected open subset of $U'$, for
example, on any disc $\Delta \subset U'$.


\begin{exbox}
\begin{exercise}
Let $D \subset \C$ be a bounded domain, $U' \subset \C^{n-1}$
a domain,
$f$ a continuous function on $U' \times \widebar{D}$ holomorphic
on $U' \times D$,
where $f$ is zero on at least one point
of $U' \times D$, and $f$ is never zero on
$U' \times \partial D$.
Suppose that for each fixed $z' \in U'$ the function
$z_n \mapsto f(z',z_n)$
has at most one zero in $D$.  Prove that for each $z' \in U'$
$z_n \mapsto f(z',z_n)$ has exactly one zero in $D$.
Note: And therefore by \propref{prop:zerohol}, that zero is
a holomorphic function.
\end{exercise}

\begin{exercise}
Prove \propref{prop:zeroshol}.  See 
the exercise above and \propref{prop:zerohol}.
\end{exercise}
\end{exbox}

\begin{thm} \label{thm:discrthm}
\pagebreak[2]
Let $D \subset \C$ be a bounded domain,
$U' \subset \C^{n-1}$ a domain,
and
$f \in \sO(U' \times D)$.
Suppose the zero set $f^{-1}(0)$ has no limit points
on $U' \times \partial D$.
Then there exists an $m \in \N$
and
a holomorphic function $\Delta \colon U' \to \C$, not identically zero, such
that for every $z' \in U' \setminus E$, where $E = \Delta^{-1}(0)$,
$z_n \mapsto f(z',z_n)$ has exactly $m$ geometrically distinct zeros in $D$,
and 
$z_n \mapsto f(z',z_n)$ has strictly less than $m$ geometrically distinct
zeros
for $z' \in E$.
\end{thm}

The complement of a zero set of a holomorphic function is connected, open
and dense.  The function $\Delta$ is 
called the \emph{\myindex{discriminant function}} and its zero set
$E$ is called the 
\emph{\myindex{discriminant set}}.  For the quadratic equation,
$\Delta$ is the discriminant we learned about in high school.

\begin{proof}
The zeros of $z_n \mapsto f(z',z_n)$ are isolated, and there are finitely
many for every $z'$ as $D$ is bounded and $f^{-1}(0)$ has no limit points on
$U' \times \partial D$.
For any $p' \in U'$ we define two useful paths.
Let $\gamma$ be the union of nonintersecting
small simple closed curves
in $D$, one around each geometrically distinct zero of $z_n \mapsto f(p',z_n)$.
Let $\lambda$ be a large closed path in $D$ going 
exactly once around all the zeros and such that the interior of $\lambda$ is
in $D$.  Suppose $\gamma$ and $\lambda$
intersect no zeros.
See \figureref{fig:curve-around-each-zero}.
By continuity,
the curves $\gamma$
and $\lambda$ do not intersect any zeros for $z'$ near $p'$.
Since the set $f^{-1}(0)$ is closed and the zeros
do not
accumulate on $U' \times \partial D$,
then
for $z'$ near $p'$ the zeros stay a positive distance away from
the boundary.  So $\lambda$ can be picked to go 
around all
the zeros exactly once for $z'$ near $p'$.

\begin{myfig}
\subimport*{figures/}{curve-around-each-zero.pdf_t}
\caption{Curve around each zero.\label{fig:curve-around-each-zero}}
\end{myfig}

The argument principle applied to $z_n \mapsto f(z',z_n)$ using $\lambda$ for $z'$ near
$p'$ shows that the number of zeros (with multiplicity) is bounded by some $M \in \N$
near $p'$.  The $M$ is locally well-defined (it does not depend on $\lambda$ as
long as it contains all the zeros), and it is a locally constant function of
$z' \in U'$.
As $U'$ is connected, and it is a fixed number.  The maximum number of
geometrically distinct zeros must be bounded by $M$.  Let $m$ be the maximal
number of geometrically distinct zeros and suppose that at some point
in $U'$, there are exactly $m$ geometrically distinct zeros.

Let $U_m' \subset U'$ be the set of $z' \in U'$ for which $z_n \mapsto f(z',z_n)$
has exactly $m$ zeros.
Write $U'$ as a union of disjoint sets $U' = U_m' \cup E$, where $E = U'
\setminus U_m'$.
By definition of $m$, $U_m'$ is nonempty.  Suppose $p' \in U_m'$ and $\gamma$
goes around the zeros as above.  Let $\gamma'$ be a single component curve
of the path $\gamma$ going around one of the zeros.
The argument principle with respect to $\gamma'$ says that
$\gamma'$ must contain at least one zero for all $z'$ near $p'$.
There are only finitely many zeros, and so for $z'$ in 
some neighborhood of $p'$,
$z_n \mapsto f(z',z_n)$ has at least $m$
zeros in $\gamma$, and as $m$ is the maximum, it has exactly $m$ zeros.
In other words, $U_m'$ is open.

Locally on $U_m'$, there exist $m$ holomorphic functions
$\alpha_1, \ldots, \alpha_m$ giving the zeros by the previous proposition.
We cannot define these on all of $U_m'$ as we do not know how they are
ordered.
The function
\begin{equation*}
\Delta(z') = \prod_{j \not= k} \bigl( \alpha_j(z') - \alpha_k(z') \bigr) 
\end{equation*}
defined for $z' \in U_m'$ does not depend on the order.
That means $\Delta$ is well-defined as a function on the open set $U_m'$,
and since $\alpha_j$ are locally holomorphic, $\Delta$ is holomorphic.

Let $p' \in E \cap \overline{U_m'}$,
so there are fewer than $m$ zeros at $p'$.
Suppose $\gamma$ and $\lambda$ are as above.
In each particular component $\gamma'$ of $\gamma$, there must
be at least one zero for all $z'$ near $p'$ by the same argument as above.
There exist $z'$ arbitrarily near $p'$ where there are $m$ zeros.
The region between $\lambda$ and $\gamma$ (including the curves) is compact,
and so by continuity,
if $z_n \mapsto f(p',z_n)$ was nonzero on it,
so is $z_n \mapsto f(z',z_n)$ for $z'$ near $p'$.
As no zeros of $z_n \mapsto f(z',z_n)$ lie outside $\lambda$,
we have that all
zeros lie in one of the components of $\gamma$.  So if $z' \in U_m'$ near
$p'$, there must be one
component $\gamma'$ that contains at least two zeros.  Let $\{ z'_\ell \}$
be an arbitrary sequence of points in $U_m'$ going to $p'$.
As the number of zeros is finite, $\{ z'_\ell \}$ has a
subsequence such that $z_n \mapsto f(z'_\ell,z_n)$ has at least two zeros in
some fixed component $\gamma'$ of $\gamma$ for $z'_\ell$.  Assume $\{ z'_\ell
\}$ is this subsequence.  Label the two distinct zeros at $z'_\ell$
as $\alpha_1(z'_\ell)$ and
$\alpha_2(z'_\ell)$.  At $p'$ there is only a single (geometrically) zero in $\gamma'$, let
us name it $\alpha_1(p')$.  As $f^{-1}(0)$ is closed it must be that
$\alpha_1(z'_\ell)$ and $\alpha_2(z'_\ell)$ both approach $\alpha_1(p')$ as
$\ell \to \infty$.
As all zeros are necessarily bounded,
$\lim_{\ell \to \infty} \Delta(z'_\ell) = 0$.  As the limit is zero for a 
subsequence of an arbitrary sequence,
\begin{equation*}
\lim_{z' \in U_m' \to p'} \Delta(z') = 0 .
\end{equation*}

We have already defined $\Delta$ on $U_m'$, so set $\Delta(z') = 0$ if $z' \in E$.
The function $\Delta$ is a continuous function on $U'$ that is zero precisely on
$E$ and holomorphic on $U_m'$.
Rad{\'o}'s theorem
(\thmref{thm:rado}) says that $\Delta$ is holomorphic in $U'$.
\end{proof}

The discriminant given above is really the discriminant of the set
$f^{-1}(0)$ rather than of the corresponding Weierstrass polynomial.  Often
for Weierstrass polynomials the discriminant is defined as 
$\prod_{j \not= k} \bigl( \alpha_j(z') - \alpha_k(z') \bigr)$ taking
multiple zeros into account, and therefore the ``discriminant'' could be
identically zero.  It will be clear from upcoming exercises that if
the Weierstrass polynomial is irreducible, then the two notions do in fact
coincide.

\begin{exbox}
\begin{exercise}
Prove that if $f \in \sO(U)$, then $U \setminus f^{-1}(0)$ is not simply
connected if $f^{-1}(0)$ is nonempty.  In particular, in the theorem, $U' \setminus E$ is not
simply connected if $E \not= \emptyset$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $D \subset \C$ be a bounded domain,
and
$U' \subset \C^{n-1}$ a domain.
Suppose
$f$ is a continuous function on
$U' \times \widebar{D}$ holomorphic on $U' \times D$,
and $f$ is never zero on
$U' \times \partial D$.
Suppose $\gamma \colon [0,1] \to U'$ is a continuous function
such that $f\bigl(\gamma(0),c\bigr) = 0$ for some $c \in D$.
Prove that there exists a continuous function $\alpha \colon [0,1] \to \C$
such that $\alpha(0) = c$ and
$f\bigl(\gamma(t),\alpha(t)\bigr) = 0$ for all $t \in [0,1]$.
Hint: Show it is possible for a path
arbitrarily close to $\gamma$, but one that stays away from
the discriminant.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Properties of the ring of germs} \label{sec:propertiesofringofgerms}

Let us prove some basic properties of the
ring of germs of holomorphic functions.  First some algebra terminology.
Given a commutative ring $R$,
an \emph{\myindex{ideal}} $I \subset R$ is
a subset such that $f g \in I$ whenever $f \in R$ and $g \in I$
and $g+h \in I$ whenever $g,h \in I$.
An intersection of ideals is again an ideal, and hence it makes sense to
talk about the smallest ideal containing a set of elements.
An ideal $I$ is generated by $f_1,\ldots,f_k$
if $I$ is the smallest ideal containing $\{ f_1,\ldots,f_k \}$.  We then 
\glsadd{not:ideal}%
write $I = (f_1,\ldots,f_k)$.
Every element in $I$ can be written as $c_1 f_1 + \cdots + c_k f_k$
where $c_1,\ldots,c_k \in R$.
A \emph{\myindex{principal ideal}}
is an ideal generated by a single element, that is, $(f)$.

For convenience, when talking about germs of functions we often identify a representative
with the germ when the context is clear.  So by abuse of notation, we often write
$f \in \sO_p$ instead of $(f,p) \in \sO_p$ and
$(f_1,\ldots,f_k)$ instead of $\bigl((f_1,p),\ldots,(f_k,p)\bigr)$.  As in
the following exercises.

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Suppose $f \in \sO_p$ is such that $f(p) \not= 0$, and 
$(f)$ is the ideal generated by $f$.  Prove $(f) = \sO_p$.
\item
Let $\mathfrak{m}_p = (z_1-p_1,\ldots,z_n-p_n) \subset \sO_p$ be the ideal generated by the coordinate
functions.  Show that if $f(p) = 0$, then $f \in
\mathfrak{m}_p$.
\item
Show that if $I \subsetneq \sO_p$ is a \emph{\myindex{proper ideal}}
(ideal such that $I
\not= \sO_0$), then
$I \subset \mathfrak{m}_p$, that is $\mathfrak{m}_p$ is a \emph{\myindex{maximal ideal}}.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]
Suppose $n=1$.  Show that ${}_1\sO_p$ is a
\emph{\myindex{principal ideal domain}}\index{PID} (PID)\@,
that is every ideal is a principal ideal.
More precisely, show that given an ideal $I \subset {}_1 \sO_p$, then there
exists a $k=0,1,2,\ldots$, such that $I = \bigl( (z-p)^k \bigr)$.
\end{exercise}

\begin{exercise}
If $U,V \subset \C^n$ are two neighborhoods of $p$
and $h \colon U \to V$ is a biholomorphism.  First
prove that it makes sense to talk about $f \circ h$
for any $(f,p) \in \sO_p$.  Then
prove that $f \mapsto f \circ h$ is a ring isomorphism.
\end{exercise}
\end{exbox}

A commutative ring $R$ is
\emph{\myindex{Noetherian}} if every ideal in $R$ is finitely generated.
That is, for every ideal $I \subset R$
there exist finitely many generators $f_1,\ldots,f_k \in I$:
Every $g \in I$ can be written as $g = c_1 f_1 + \cdots + c_k f_k$,
for some $c_1,\ldots,c_k \in R$.
In an exercise, you proved ${}_1\sO_p$ is a PID\@.
So ${}_1\sO_p$ is Noetherian.  In higher dimensions, the ring of germs may not
be a PID\@, but it is Noetherian.

\begin{thm}
$\sO_p$ is Noetherian.
\end{thm}

\begin{proof}
Without loss of generality $p=0$.  The proof is by
induction on dimension.  By an exercise above, ${}_1\sO_0$ is Noetherian.
By another exercise, we are allowed to do a change of coordinates at zero.

For induction suppose ${}_{n-1}\sO_0$ is Noetherian, and let $I \subset
{}_n \sO_0$ be an ideal.
If $I = \{ 0 \}$
or $I = {}_n \sO_0$, then the assertion is obvious.  Therefore, assume
that all elements of $I$ vanish at the origin ($I \not= {}_n \sO_0$), and
that there exist elements that are not identically zero
($I \not= \{ 0 \}$).  Let $g$
be such an element.  After perhaps a linear change of coordinates, 
assume $g$ is a Weierstrass polynomial in $z_n$
by the preparation theorem.

The ring ${}_{n-1}\sO_0[z_n]$ is a subring of ${}_n \sO_0$.
The set $J= {}_{n-1}\sO_0[z_n] \cap I$ is an ideal in the
ring ${}_{n-1}\sO_0[z_n]$.  By the Hilbert basis theorem (see
\thmref{thm:hilbertbasis} in the appendix for a proof), as
${}_{n-1}\sO_0$ is Noetherian, the ring
${}_{n-1}\sO_0[z_n]$ is also Noetherian.  Thus $J$ has finitely many
generators,
that is $J = (h_1,\ldots,h_k)$, in the ring ${}_{n-1}\sO_0[z_n]$.

By the division theorem,
every $f \in I$ is of the form $f = qg+r$, where $r \in {}_{n-1}\sO_0[z_n]$
and $q \in {}_n\sO_0$.
As $f$ and $g$ are in $I$, so is $r$.
As $g$ and $r$ are in ${}_{n-1}\sO_0[z_n]$,
they are both in $J$.
Write
$g = c_1 h_1 + \cdots + c_k h_k$ and
$r = d_1 h_1 + \cdots + d_k h_k$.  Then
$f = (qc_1 + d_1) h_1 + \cdots + (qc_k + d_k) h_k$.
So
$h_1,\ldots,h_k$ also generate $I$ in ${}_n \sO_0$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that every proper ideal $I \subset \sO_0$ where $I \not= \{ 0 \}$
is generated by Weierstrass polynomials.  As a technicality,
note that a
Weierstrass polynomial of degree 0 is just 1, so it works for $I = \sO_0$.
\end{exercise}

\begin{exercise}
We saw above that
${}_1\sO_p$ is a PID\@.  Prove that
if $n > 1$, then ${}_n\sO_p$ is not a PID\@.
\end{exercise}
\end{exbox}

\begin{thm}
$\sO_p$ is a \emph{\myindex{unique factorization domain}}\index{UFD}
(UFD)\@.  That is, up to a
multiplication by a unit, every element has a unique factorization into
irreducible elements of $\sO_p$.
\end{thm}

\begin{proof}
Again assume $p=0$ and induct on the dimension.
The one-dimensional statement is an exercise below.  If ${}_{n-1}\sO_0$ is a UFD\@, then
${}_{n-1}\sO_0[z_n]$ is a UFD by the Gauss lemma (see
\thmref{thm:gausslemma}).

Take $f \in {}_n\sO_0$.  After perhaps a linear change of coordinates
$f = qP$, for $q$ a unit in ${}_n\sO_0$,
and $P$ a Weierstrass polynomial in $z_n$.
As 
${}_{n-1}\sO_0[z_n]$ is a UFD\@, $P$ has a unique
factorization in 
${}_{n-1}\sO_0[z_n]$ into $P = P_1 P_2 \cdots P_k$.
So $f = q P_1 P_2 \cdots P_k$.  That $P_j$ are irreducible
in ${}_n\sO_0$ is left as an exercise.

Suppose 
$f = \tilde{q} g_1 g_2 \cdots g_m$ is another factorization. 
The preparation theorem applies to each $g_j$.  Therefore, write
$g_j = u_j \widetilde{P}_j$ for a unit $u_j$ and a Weierstrass polynomial
$\widetilde{P}_j$.  We obtain
$f = u \widetilde{P}_1 \widetilde{P}_2 \cdots \widetilde{P}_m$ for a unit $u$.  By
uniqueness part of the preparation theorem we obtain
$P = \widetilde{P}_1 \widetilde{P}_2 \cdots \widetilde{P}_m$.  Conclusion is 
obtained by noting that
${}_{n-1}\sO_0[z_n]$ is a UFD\@.
\end{proof}

\begin{exbox}
\begin{exercise}
Finish the proof of the theorem by proving 
${}_1\sO_p$ is a unique factorization domain.
\end{exercise}

\begin{exercise}
Show that if an element is irreducible in 
${}_{n-1}\sO_0[z_n]$, then it is irreducible in
${}_{n}\sO_0$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Varieties} \label{sec:varieties}

As before, if $f \colon U \to \C$
\glsadd{not:Zf}%
is a function, let $Z_f = f^{-1}(0) \subset U$ denote the zero set of $f$.

\begin{defn}
Let $U \subset \C^n$ be an open set.  Let $X \subset U$ be a set such that
near each point $p \in U$, there exists a neighborhood $W$ of $p$
and a family of holomorphic functions $\sF$ defined on $W$ such that
\begin{equation*}
W \cap X = \bigl\{ z \in W : f(z) = 0 \text{ for all } f \in \sF \bigr\} 
= \bigcap_{f \in \sF} Z_f .
\end{equation*}
Then $X$ is called a
(\emph{complex} or \emph{complex-analytic})
\emph{\myindex{variety}}\index{complex variety}\index{complex-analytic variety}
or a
\emph{\myindex{subvariety}}\index{complex
subvariety}\index{complex-analytic subvariety} of $U$.
Sometimes $X$ is called an \emph{\myindex{analytic set}}.
We say $X \subset U$ is a proper subvariety if $\emptyset \not= X \subsetneq
U$.
\end{defn}

We generally leave out the ``complex'' from ``complex subvariety'' as it is
clear from context.  But you should know that there are other types of
subvarieties, namely real subvarieties given by real-analytic functions.  We
will not cover those in this book.

\begin{example}
The set $X = \{ 0 \} \subset \C^n$ is a subvariety as it is the only common vanishing
point of functions $\sF = \{ z_1,\ldots,z_n \}$.  Similarly, $X= \C^n$
is a subvariety of $\C^n$, where we let $\sF = \emptyset$.
\end{example}

\begin{example}
The set defined by $z_2 = e^{1/z_1}$ is a subvariety of
$U = \bigl\{ z \in \C^2 : z_1 \not=0 \bigr\}$.  It is not a subvariety of
any open set larger than $U$.
\end{example}

It is useful to note what happens when
we replace ``near each point $p \in U$'' with ``near each point $p \in
X$.''  We get a slightly different concept, and $X$ is said to be a
\emph{\myindex{local variety}}.  A local variety $X$ is a subvariety of
some neighborhood of $X$, but it is not necessarily closed in $U$.  As a
simple example, the set
$X = \bigl\{ z \in \C^2 : z_1 = 0, \abs{z_2} < 1 \bigr\}$ is a
local variety, but not a subvariety of $\C^2$.  On the other hand $X$
is a subvariety of the unit ball $\bigl\{ z \in \C^2 : \norm{z} < 1 \bigr\}$.

Note that $\sF$ depends on $p$ and near each point may have a different set of
functions.  Clearly the family $\sF$ is not unique.  We will prove below
that we would obtain the same definition if we restricted to finite
families $\sF$.

We work with germs of functions.  Recall, that when $(f,p)$ is a germ of a function
the germ $(Z_f,p)$ is the germ of the zero set of some representative.
Let
\glsadd{not:idealfromset}%
\begin{equation*}
I_p(X) \overset{\text{def}}{=}
\bigl\{ (f,p) \in \sO_p : (X,p) \subset (Z_f,p) \bigr\} .
\end{equation*}
That is, $I_p(X)$ is the set of germs of holomorphic functions vanishing on
$X$ near $p$.  If a function vanishes on $X$, then any multiple of it also
vanishes on $X$, so $I_p(X)$ is an ideal.  Really $I_p(X)$ depends only on
the germ of $X$ at $p$, so define $I_p\bigl((X,p)\bigr) = I_p(X)$.

Every ideal in $\sO_p$ is finitely generated.
Let $I \subset \sO_p$ be an ideal generated by $f_1,f_2,\ldots,f_k$.
Write
\glsadd{not:vanishingset}%
\begin{equation*}
V(I) \overset{\text{def}}{=}
(Z_{f_1},p) \cap (Z_{f_2},p) \cap \cdots \cap (Z_{f_k},p) .
\end{equation*}
That is, $V(I)$ is the germ of the subvariety ``cut out'' by the elements of $I$,
since every
element of $I$ vanishes on the points where all the generators vanish.
Suppose representatives $f_1,\ldots,f_k$ of the generators are defined
in some neighborhood $W$ of $p$,
and a germ $(g,p) \in I$ has a representative $g$ defined in $W$
such that $g = c_1 f_1 + \cdots + c_k f_k$, where $c_k$ are also holomorphic
functions on $W$.  If $q \in Z_{f_1} \cap \cdots \cap Z_{f_k}$,
then $g(q) = 0$.  Thus,
$Z_{f_1} \cap \cdots \cap Z_{f_k} \subset Z_g$, or in terms of germs,
$V(I) \subset (Z_g,p)$.  The reason why we did not define $V(I)$ to be the
intersection of zero sets of all germs in $I$ is that this would be an
infinite intersection, and we did not define such an object for germs.

\begin{exbox}
\begin{exercise}
Show that $V(I)$ is independent of the choice of generators.
\end{exercise}

\begin{exercise}
Suppose $I_p(X)$ is generated by the functions $f_1, f_2, \ldots, f_k$.
Prove 
\begin{equation*}
(X,p) = (Z_{f_1},p) \cap (Z_{f_2},p) \cap \cdots \cap (Z_{f_k},p) .
\end{equation*}
\end{exercise}

\begin{exercise}
Given a germ $(X,p)$ of a subvariety at $p$, show
$V\bigl(I_p(X)\bigr) = (X,p)$ (see above).
Then given an ideal $I \subset \sO_p$, show
$I_p\bigl(V(I)\bigr) \supset I$.
\end{exercise}
\end{exbox}

As $\sO_p$ is Noetherian, $I_p(X)$ is finitely
generated.  Near each point $p$ only finitely many functions are
necessary to define a subvariety, that is, by an exercise above, those functions 
``cut out'' the subvariety.  When one says 
\emph{\myindex{defining functions}} for a germ of a subvariety, one generally
means that those functions generate the ideal, not just that their common
zero set happens to be the subvariety.  A theorem that we will not prove here
in full generality,
the \emph{\myindex{Nullstellensatz}}, says that if we take the germ of
a subvariety defined by functions
in an ideal $I \subset \sO_p$, and look at the ideal given by that subvariety,
we obtain the
radical\footnote{The radical of $I$ is defined as $\sqrt{I} \overset{\text{def}}{=} \{ f : f^m \in I, \text{ for
some } m \}$.} of $I$.  In more concise language, the Nullstellensatz says
$I_p\bigl(V(I)\bigr) = \sqrt{I}$.
Germs of subvarieties are in
one-to-one correspondence with radical ideals of $\sO_p$.

\begin{example}
The subvariety $X = \{ 0 \} \subset \C^2$ can be given by
$\sF = \bigl\{ z_1^2, z_2^2 \bigr\}$.
If $I = \bigl(z_1^2,z_2^2\bigr) \subset \sO_0$ is the ideal of germs
generated by these two functions, then $I_0(X) \not= I$.  We have
seen that the ideal $I_0(X)$ is the maximal ideal $\mathfrak{m}_0 =
(z_1,z_2)$.  If we prove that 
all the nonconstant monomials are in $\sqrt{I}$, then
$\sqrt{I} = (z_1,z_2) = \mathfrak{m}_0$.  The only nonconstant monomials
that are not in $I$ are $z_1$, $z_2$, and $z_1z_2$, but the square
of each of these is in $I$, so $\sqrt{I} = \mathfrak{m}_0$.
\end{example}

The local properties of a subvariety at $p$ are 
encoded in the properties of
the ideal $I_p(X)$.  Therefore, the study of subvarieties often
involves the study of
the various algebraic properties of the ideals of $\sO_p$.
Let us also mention in passing that the other object
that is studied is the so-called
\emph{\myindex{coordinate ring}}
$\sO_p / I_p(X)$, which represents the functions on $(X,p)$.  That is, we
identify two functions if they differ by something in the ideal, since then
they are equal on $X$.

\medskip

At most points a subvariety behaves like a piece of $\C^k$, more
precisely like a graph over $\C^k$.
A graph of a mapping
$f \colon U' \subset \C^k \to \C^{n-k}$
is the set $\Gamma_f \subset U' \times \C^{n-k} \subset \C^k \times
\C^{n-k}$ defined by
\glsadd{not:graph}%
\begin{equation*}
\Gamma_f \overset{\text{def}}{=}
\bigl\{ (z,w) \in U' \times \C^{n-k} : w=f(z) \bigr\} .
\end{equation*}

\begin{defn}
Let $X \subset U \subset \C^n$ be a subvariety of an open set $U$.  Let $p \in X$ be a
point.
Suppose that after a permutation of coordinates, near
$p$ the set $X$ is a graph of a holomorphic
mapping.
That is, after relabeling coordinates,
there is a neighborhood $U' \times U'' \subset \C^{k}
\times \C^{n-k}$ of $p$, where $k \in \N_0$, such that
\begin{equation*}
X \cap (U' \times U'') = \Gamma_f
\end{equation*}
for a holomorphic mapping $f \colon U' \to \C^{n-k}$.
Then $p$ is a \emph{\myindex{regular point}} (or
\emph{\myindex{simple point}}) of $X$ and
the \emph{dimension}\index{dimension at a regular point}
of $X$ at $p$ is $k$.  We write $\dim_p X = k$.
If all points of $X$ are regular points of dimension $k$, then $X$ is called
a \emph{\myindex{complex manifold}}, or
\emph{\myindex{complex submanifold}}, of (complex) dimension $k$.

As the ambient\footnote{The word \emph{\myindex{ambient}}
is used often to mean the set
that contains whatever object we are talking about.} dimension
is $n$, we say $X$ is of
\emph{\myindex{codimension}} $n-k$ at $p$.

\glsadd{not:Xreg}%
The set of regular points of $X$ is denoted by $X_{\mathit{reg}}$.  Any
point that is not regular is \emph{singular}\index{singular point}.
\glsadd{not:Xsing}%
The set of singular points of $X$ is denoted by $X_{\mathit{sing}}$.
\end{defn}

A couple of remarks are in order.
A subvariety $X$ can have
regular points of several different dimensions, although if a point
is a regular point of dimension $k$, then all nearby points are regular
points of dimension $k$ as the same $U'$ and $U''$ works.
Any isolated point of $X$ is automatically a regular point of dimension
0.
Finally remark is that dimension is well-defined.  We leave it as an exercise.
Sometimes the empty set is considered a complex manifold of dimension $-1$ (or
$-\infty$).

\begin{example}
The set $X = \C^n$ is a complex submanifold of dimension $n$ (codimension 0).
In particular, $X_{\mathit{reg}} = X$ and $X_{\mathit{sing}} = \emptyset$.

The set $Y = \bigl\{ z \in \C^3 : z_3 = z_1^2 - z_2^2 \bigr\}$ is a complex submanifold of
dimension $2$ (codimension 1).  Again,
$Y_{\mathit{reg}} = Y$ and $Y_{\mathit{sing}} = \emptyset$.

On the other hand, the so-called \emph{\myindex{cusp}},
$C = \bigl\{ z \in \C^2 : z_1^3-z_2^2 = 0 \bigr\}$ is not a complex
submanifold.  The origin is a singular point of $C$
(see exercise below).
At every other point we can write $z_2 = \pm z_1^{3/2}$,
so $C_{\mathit{reg}} = C \setminus \{0\}$, and so $C_{\mathit{sing}} = \{ 0
\}$.
The dimension at every regular point is 1.
See \figureref{fig:cusp} for a 
plot of $C$ in two real dimensions.

\begin{myfig}
\medskip
\subimport*{figures/}{cusp.eepic}
\bigskip
\caption{The cusp.\label{fig:cusp}}
\end{myfig}
\end{example}


\begin{exbox}
\begin{exercise}
Prove that if $p$ is a regular point of a subvariety $X \subset U \subset
\C^n$ of a domain $U$, then the dimension at $p$ is well-defined.  Hint: If there were two possible
$U'$ of different dimension (possibly different affine coordinates), construct a map
from one such $U'$ to another such $U'$ with nonvanishing derivative.
\end{exercise}

\begin{exercise}
Consider the cusp 
$C = \bigl\{ z \in \C^2 : z_1^3-z_2^2 = 0 \bigr\}$.  Prove that
the origin is not a regular point of $C$.
\end{exercise}

\begin{exercise}
Show that $p$ is a regular point of dimension $k$ of a subvariety $X$
if and only if there
exists a local biholomorphic change of coordinates that puts $p$ to the
origin and near $0$, $X$ is given by $w=0$, where $(z,w) \in \C^{k} \times
\C^{n-k}$.  In other words, if we allow a biholomorphic change of
coordinates, we can let $f=0$ in the definition.
\end{exercise}
\end{exbox}

We also define dimension at a singular point.
A fact that we will not prove in general is that
the set of regular points of a
subvariety is open and dense in the subvariety; a
subvariety is regular at most points.  Therefore, the 
following definition makes sense without resorting to
the convention that $\max \emptyset = -\infty$.

\begin{defn}
Let $X \subset U \subset \C^n$ be a (complex) subvariety of $U$.  Let $p \in
X$ be a point.  We define the \emph{\myindex{dimension}} of $X$ at $p$
to be
\glsadd{not:dimpX}%
\begin{equation*}
\dim_p X \overset{\text{def}}{=}
\max \bigl\{ k \in \N_0 : \text{ $\forall$ neighbhds.
$W$ of $p$, $\exists \, q \in W \cap X_{\mathit{reg}}$ with $\dim_q X = k$}
\bigr\} .
\end{equation*}
If $(X,p)$ is a germ and $X$ a representative,
the \emph{dimension} of $(X,p)$ is the dimension of
$X$ at $p$.
The dimension of the entire subvariety $X$ is defined to be
\glsadd{not:dimX}%
\begin{equation*}
\dim X \overset{\text{def}}{=}
\max_{p \in X} \dim_p X .
\end{equation*}
We say that $X$ is of \emph{\myindex{pure dimension}} $k$ if at
all points $p$, dimension of $X$ at $p$ is $k$.  
We say a germ $(X,p)$ is of pure dimension $k$ if there exists a representative
of $X$ that is of pure dimension $k$.
We define the word \emph{codimension} as before, that is, the
ambient dimension minus the dimension of $X$.
\end{defn}

\begin{example}
We saw that $C = \bigl\{ z \in \C^2 : z_1^3-z_2^2 = 0 \bigr\}$ is of
dimension 1 at all the regular points, and the only singular point is the
origin.  Hence $\dim_0 C = 1$, and so $\dim C = 1$.  The subvariety $C$ is
of pure dimension 1.
\end{example}


We have the following theorem, which we
state without proof, at least in the general setting.

\begin{thm}
Let $U \subset \C^n$ be open and connected and let $X \subset U$
be a subvariety, then the set of regular points $X_{\mathit{reg}}$
is open and dense in $X$.
In fact, $X_{\mathit{sing}} \subset X$ is a subvariety.
\end{thm}

\begin{exbox}
\begin{exercise}
Suppose that $X \subset U \subset \C^n$ is a subvariety
of a domain $U$, such that $X_{\mathit{reg}}$ is connected.  Show that $X$ is of
pure dimension.  Feel free to assume $X_{\mathit{reg}}$ is dense in $X$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypervarieties} \label{section:hypervarieties}


Pure codimension 1 subvarieties are particularly nice.
Sometimes pure codimension 1 subvarieties are called
\emph{hypervarieties}\index{hypervariety}.
Back in \sectionref{sec:riemannextzerosetsinjmaps} we proved
the following theorem (\thmref{thm:regptsdense}).  We restate it in the
language of varieties.

\begin{thm} \label{thm:regptsdense2}
Let $U \subset \C^n$ be a domain and
$f \in \sO(U)$.  Then $Z_f$ is empty, a subvariety of pure codimension 1, or
$Z_f = U$.
Furthermore, if $Z_f$ is a pure codimension 1 subvariety, then
$(Z_f)_{\mathit{reg}}$ is open and dense in $Z_f$.
\end{thm}

We can improve on that slightly.
Let us prove that $(Z_f)_{\mathit{sing}}$ is (locally) contained in the zero set of some holomorphic
function that is not zero on any nonempty open subset of $Z_f$.
It suffices to assume $0 \in Z_f$ and to prove this locally near the origin.
As usual, after possibly a linear change of coordinates, we assume
that near the origin $Z_f$ is contained in some $U' \times D \subset
\C^{n-1} \times \C$ for a disc $D$ and $Z_f$ does not
intersect $U' \times \partial D$.
Apply \thmref{thm:discrthm} to find
the discriminant function
$\Delta$ and the discriminant $E = \Delta^{-1}(0)$.
Above a neighborhood of each
point in $U' \setminus E$, the set $Z_f$ is union of $m$ distinct graphs
of holomorphic functions (see \propref{prop:zeroshol}), so those
points are regular.  Above
each point of $E$ there are only finitely many points of $Z_f$,
and $E$ is nowhere dense in $U'$.  So the
function $(z',z_n) \mapsto \Delta(z')$ does not vanish on any nonempty open
subset of $Z_f$.

\begin{thm} \label{thm:codim1var}
If $(X,p)$ is a germ of a pure codimension 1 subvariety, then
there is a germ of a holomorphic function $f$ at $p$
such that $(Z_f,p) = (X,p)$.  Further, $I_p(X)$ is generated by $(f,p)$.
\end{thm}

\begin{proof}
We already proved all the relevant pieces of the first part of this
theorem.

For the second part there has to exist a germ of a function that vanishes on
$(X,p)$.  Assume $p=0$, and after a linear change
of coordinates assume we can apply the Weierstrass preparation theorem
to the function.  Taking representatives of the germs, we assume
$X$ is a pure codimension 1 subvariety of a small enough
neighborhood
$U' \times D \subset \C^{n-1} \times \C$ of the origin, where $D$ is a disc,
and the function that vanishes on $X$ is a
Weierstrass polynomial $P(z',z_n)$ defined for $z' \in U'$, and
all zeros of $z_n \mapsto P(z',z_n)$ are in $D$ for $z' \in U$.

\thmref{thm:discrthm} applies. Let
$E \subset U'$ be the discriminant set, a zero set of a
holomorphic function.
On $U' \setminus E$, there are a certain number of geometrically
distinct zeros of $z_n \mapsto P(z',z_n)$.

Let $X'$ be a topological component of $X \setminus ( E \times D )$.
Above each point $z' \in U' \setminus E$, let 
$\alpha_1(z'),\ldots,\alpha_k(z')$ denote the distinct zeros that are in $X'$,
that is $\bigl(z',\alpha_j(z')\bigr) \in X'$.
If $\alpha_j$ is a holomorphic function in some small neighborhood and
$\bigl(z',\alpha_j(z')\bigr) \in X'$ at one point, then 
$\bigl(z',\alpha_j(z')\bigr) \in X'$ for all nearby points too.
Furthermore, this means that the set $X'$ contains only regular points of $X$ of
dimension $n-1$.

The number of
such geometrically distinct zeros above each point in
$U' \setminus E$ is locally constant, and as $U' \setminus E$ is connected
there exists a unique $k$.  Take
\begin{equation*}
F(z',z_n) = \prod_{j=1}^k \bigl( z_n-\alpha_j(z')\bigr)
= 
z_n^k + \sum_{j=0}^{k-1} g_j(z') z_n^j .
\end{equation*}
The coefficients $g_j$ are well-defined for $z \in U' \setminus E$
as they are independent of how $\alpha_1,\ldots,\alpha_k$ are ordered.
The $g_j$ are holomorphic for $z \in U' \setminus E$
as locally we can choose the order so that each $\alpha_j$ is 
holomorphic.
The coefficients $g_j$ are bounded
on $U'$ and therefore extend to holomorphic functions of $U'$.
Hence, the polynomial $F$ is a polynomial
in $\sO(U')[z_n]$.
The zeros of $F$
above $z' \in U' \setminus E$
are simple and give precisely $X'$.
By using the argument principle
again, we find that all zeros above points of $E$ are limits of zeros
above points in $U' \setminus E$.
Consequently,
the zero set of $F$ is the closure of $X'$ in $U' \times D$ by continuity.
It is left to the reader to check that %(using the argument principle)
all the functions $g_j$ vanish at the origin and $F$ is a Weierstrass
polynomial, a fact that will be useful in the exercises below.

If the polynomial $P(z',z_n)$ is of degree $m$,
then $z' \mapsto P(z',z_n)$ has at most $m$ zeros.  Together with
the fact that $U' \setminus E$ is connected, this means that
$X \setminus (E \times D)$ has at most finitely many components (at
most $m$).
So we can find an $F$ for every topological component of
$X \setminus ( E \times D )$.  Then we multiply those functions together
to get $f$.

The fact that this $f$ will generate $I_p(X)$ is left as an exercise below.
\end{proof}

In other words, local properties of a codimension 1 subvariety can be
studied by studying the zero set of a single Weierstrass polynomial.

\begin{example}
It is not true that
if a dimension of a subvariety in $\C^n$ is $n-k$ (codimension $k$),
there are $k$
holomorphic functions that ``cut it out.''  That only works for $k=1$.
The set defined by
\begin{equation*}
\rank
\begin{bmatrix}
z_1 & z_2 & z_3 \\
z_4 & z_5 & z_6
\end{bmatrix}
< 2
\end{equation*}
is a pure 4-dimensional subvariety of $\C^6$, so of codimension 2,
and the defining equations are
$z_1z_5-z_2z_4 = 0$,
$z_1z_6-z_3z_4 = 0$, and
$z_2z_6-z_3z_5 = 0$.  Let us state without proof that the unique singular point is the origin and there exist
no 2 holomorphic functions near the origin
that define this subvariety.  In more technical
language, the subvariety is not a \emph{\myindex{complete intersection}}.
\end{example}

\begin{example}
If $X$ is a hypervariety and $E$ the corresponding discriminant set,
it is tempting to say that the singular set of $X$ is the
set $X \cap (E \times \C)$, which is a codimension 2 subvariety.  It is true that
$X \cap (E \times \C)$ will contain the singular set, but in general the
singular set is smaller.
A simple example of this behavior is the set defined by
$z_2^2 - z_1 = 0$.  The defining function is a Weierstrass
polynomial in $z_2$ and the discriminant set is given by $z_1 = 0$.
However, the subvariety has no singular points.  A less trivial example
is given in an exercise below.
\end{example}

Interestingly we also proved the following theorem.  Same theorem is true
for higher codimension, but it is harder to prove.

\begin{cor}
Let $(X,p)$ is a germ of a subvariety of pure codimension 1.  Then there exists
a neighborhood $U$ of $p$, a representative $X \subset U$ of $(X,p)$
and subvarieties $X_1,\ldots,X_k \subset U$
of pure codimension 1 such that $(X_j)_{\mathit{reg}}$ is connected for
every $j$, and
$X = X_1 \cup \cdots \cup X_k$.
\end{cor}

\begin{proof}
A particular $X_j$ is defined by considering a topological component of $X \setminus
(E \times D)$ as in the proof of \thmref{thm:codim1var}, getting the $F$,
and setting $X_j = Z_F$.
The topological component is of course a connected set and it is dense in
$(X_j)_{\mathit{reg}}$, which proves the corollary.
\end{proof}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Prove that the hypervariety in $\C^n$, $n \geq 2$, given by $z_1^2 + z_2^2 + \cdots + z_n^2 = 0$
has an isolated singularity at the origin (that is, the origin is the only
singular point).
\item
For any $0 \leq k \leq n-2$, find a hypervariety $X$ of $\C^n$ whose set
of singular points is a subvariety of dimension $k$.
\end{exparts}
\end{exercise}

\begin{exercise}
Suppose $p(z',z_n)$ is a Weierstrass polynomial of degree $k$ such that
for an open dense set of $z'$ near the origin 
$z_n \mapsto p(z',z_n)$ has geometrically $k$ zeros, and such that the
regular points of $Z_p$ are connected.  Show that $p$ is
irreducible in the sense that if $p = rs$ for two Weierstrass polynomials
$r$ and $s$, then either $r=1$ or $s=1$.
\end{exercise}

\begin{samepage}
\begin{exercise}
Suppose $f$ is a function holomorphic in a neighborhood of the origin with
$z_n \mapsto f(0,z_n)$ being of finite order.  Show that
\begin{equation*}
f = u p_1^{d_1} p_2^{d_2} \cdots p_\ell^{d_\ell} ,
\end{equation*}
where $p_j$ are Weierstrass polynomials of degree $k_j$ that have
generically (that is, on an open dense set) $k_j$ distinct zeros
(no multiple zeros), the regular points of $Z_{p_j}$ are
connected, and $u$ is a nonzero holomorphic function
in a neighborhood of the origin.  See also the next section, these
polynomials will be the irreducible factors in the factorization of $f$.
\end{exercise}
\end{samepage}

\begin{exercise}
Suppose $(X,p)$ is a germ of a pure codimension 1 subvariety.  Show that
the ideal $I_p(X)$ is a principal ideal (has a single generator).
\end{exercise}

\begin{exercise}
Suppose $I \subset \sO_p$ is an ideal such that $V(I)$ is a germ of a pure codimension 1 subvariety.  Show that
the ideal $I$ is principal.
\end{exercise}

\begin{exercise}
Let $I \subset \sO_p$ be a principal ideal.  Prove the
\emph{\myindex{Nullstellensatz}} for hypervarieties: 
$I_p\bigl(V(I)\bigr) = \sqrt{I}$.  That is, show that if 
$(f,p) \in I_p\bigl(V(I)\bigr)$, then $(f^k,p) \in I$ for some integer $k$.
\end{exercise}

\begin{exercise}
Suppose $X \subset U$ is a subvariety of pure codimension 1 for an open set $U \subset \C^n$.
Let $X'$ be a topological component of $X_{\textit{reg}}$.  Prove that the
closure $\overline{X'}$ is a subvariety of $U$ of pure codimension 1.
\end{exercise}
\end{exbox}

\section{Irreducibility, local parametrization, and Puiseux}

\begin{defn}
A germ of a subvariety $(X,p) \subset (\C^n,p)$ is 
\emph{\myindex{reducible}} at $p$ if there exist
two germs $(X_1,p)$ and $(X_2,p)$ with
$(X_1,p) \not\subset (X_2,p)$ and
$(X_2,p) \not\subset (X_1,p)$ such that
$(X,p) = (X_1,p) \cup (X_2,p)$.
Otherwise, the germ $(X,p)$ is \emph{\myindex{irreducible}} at $p$.

Similarly globally, a subvariety $X \subset U$ is
\emph{reducible} in $U$ if there exist
two subvarieties
$X_1$ and $X_2$ of $U$ with
$X_1 \not\subset X_2$ and
$X_2 \not\subset X_1$ such that
$X = X_1 \cup X_2$.
Otherwise, the subvariety $X$ is \emph{irreducible} in $U$.
\end{defn}

\begin{example}
Local and global reducibility are different.
The subvariety given by
\begin{equation*}
z_2^2 = z_1{(z_1-1)}^2
\end{equation*}
is irreducible in $\C^2$ (the regular points are connected), but locally
at the point
$(1,0)$ it is reducible.  There, the subvariety is
a union of two graphs: $z_2 = \pm \sqrt{z_1}(z_1-1)$.
See \figureref{fig:locallyredcurve} for a plot in two real dimensions.

\begin{myfig}
\medskip
\subimport*{figures/}{locallyredcurve.eepic}
\bigskip
\caption{Locally reducible curve.\label{fig:locallyredcurve}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Prove a germ of a subvariety $(X,p)$ is irreducible
if and only if $I_p(X)$ is a prime ideal.
Recall an ideal $I$ is \emph{prime}\index{prime ideal}
if $ab \in I$ implies either $a \in I$ or $b
\in I$.
\end{exercise}

\begin{exercise}
Suppose a germ of a subvariety $(X,p)$ is of pure codimension 1.
Prove $(X,p)$ is irreducible if and only if there
exists a representative of $X$, such that $X_{\textit{reg}}$
is connected.  %Hint: See the proof of \thmref{thm:codim1var}.
\end{exercise}

\begin{exercise}
Let $X \subset U$ be a subvariety of pure codimension 1 of a domain $U
\subset \C^n$.
Prove $X$ is irreducible if and only if the set of regular points
is connected.  Hint: See previous exercise.
\end{exercise}
\end{exbox}

For complex subvarieties, a subvariety is irreducible if
and only if the set of regular points is connected.  We omit the proof
in the general case, and for hypervarieties it is an exercise above.
It then makes sense that we can split a subvariety into its irreducible parts.

\begin{prop}
Let $(X,p) \subset (\C^n,p)$ be a germ of a subvariety.  Then there exist
finitely many irreducible subvarieties $(X_1,p),\ldots,(X_k,p)$ such that
$(X_1,p) \cup \ldots \cup (X_k,p) = (X,p)$ and such that $(X_j,p)
\not\subset (X_\ell,p)$ for all $j$ and $\ell$.
\end{prop}

\begin{proof}
Suppose $(X,p)$ is reducible:
Find $(Y_1,p) \not\subset (Y_2,p)$ and $(Y_2,p) \not\subset (Y_1,p)$,
such that $(Y_1,p) \cup (Y_2,p) = (X,p)$.
As $(Y_j,p) \subsetneq (X,p)$, then
$I_p(Y_j) \supsetneq I_p(X)$ for both $j$.  If both
$(Y_1,p)$ and $(Y_2,p)$ are irreducible, then stop, we are done.  Otherwise
apply the same reasoning to whichever (or both) $(Y_j,p)$ that was
reducible.  After finitely many steps you must come to a stop as you cannot
have an infinite ascending chain of ideals since $\sO_p$ is Noetherian.
\end{proof}

These $(X_1,p),\ldots,(X_k,p)$ are the
\emph{\myindex{irreducible components}}.
We omit the proof in general
that they are unique.
For a germ of a hypervariety,
the UFD property of ${}_{n}\sO_p$ gives the irreducible
components.  You found this factorization in an exercise above,
and so this factorization is unique.

Each irreducible component has the following structure.
We give the theorem without proof in the general case, although we have
essentially proved it already for pure codimension 1 (to put it
together is left as an exercise).

\begin{thm}[\myindex{Local parametrization theorem}]
\pagebreak[2]
\label{localparthm}
Let $(X,0)$ be an irreducible germ of a subvariety of dimension $k$
in $\C^n$.  Let $X$ denote a representative of the germ.
Then after a linear change of coordinates, we let
$\pi \colon \C^n \to \C^k$ be the projection onto the first $k$
components, and obtain that there exists a neighborhood $U \subset \C^n$
of the origin, and a proper subvariety $E \subset \pi(U)$ such that
\begin{enumerate}[(i)]
\item $X' = X \cap U \setminus \pi^{-1}(E)$ is a connected
$k$-dimensional complex manifold that is dense in $X \cap U$.
\item $\pi \colon X' \to \pi(U) \setminus E$ is an $m$-sheeted covering map
for some integer $m$.
\item $\pi \colon X \cap U \to \pi(U)$ is a proper mapping.
\end{enumerate}
\end{thm}

\pagebreak[2]
The $m$-sheeted covering map in this case is a local biholomorphism
that is an $m$-to-1 map.

\begin{exbox}
\begin{exercise}
Use \thmref{thm:discrthm}
to prove the parametrization theorem if $(X,0)$ is
of pure codimension 1.
\end{exercise}
\end{exbox}

Let $(z_1,\ldots,z_n)$ be the coordinates.
The linear change of coordinates needed in the theorem is
to ensure that the set defined by $z_1=z_2=\cdots=z_k = 0$ intersected
with $X$ is an isolated point at the origin.  This is precisely
the same condition needed to apply Weierstrass preparation theorem in the case
when $X$ is the zero set of a single function.

\medskip

We saw hypersurfaces are the simpler cases of complex-analytic
subvarieties.  At the other end of the spectrum, complex-analytic
subvarieties of dimension 1 are also reasonably simple for different reasons.
Locally, complex-analytic subvarieties of dimension 1 are analytic discs.
Moreover, they are locally the one-to-one holomorphic images of discs,
and so they have a natural topological manifold structure even at singular
points.

\begin{example}
The image of the analytic disc
$\xi \mapsto (\xi^2,\xi^3)$ is the cusp subvariety
defined by $z_1^3-z_2^2 = 0$ in $\C^2$.
\end{example}


The following theorem is often stated only in $\C^2$ for zero sets of
a single function although it follows in
the same way from the local parametrization theorem in higher-dimensional
spaces.  Of course, we only
proved that theorem (or in fact you the reader did so in an exercise), for
codimension 1 subvarieties, and therefore, we also only have a complete
proof of the following in $\C^2$.

\begin{thm}[\myindex{Puiseux}]
Let $(z,w) \in \C \times \C^{n-1}$ be coordinates.
Suppose $f \colon U \subset \C \times \C^{n-1} \to \C^\ell$
is a holomorphic map such that
$f(z,w) = 0$ defines a dimension 1 subvariety $X$ of $U$,
$0 \in X$,
and $w \mapsto f(0,w)$ has an isolated zero at the origin.

Then there exists an integer $k$ and a holomorphic function $g$ defined near
the origin in $\C$ such that
for all $\xi$ near the origin
\begin{equation*}
f\bigl(\xi^k,g(\xi)\bigr) = 0 .
\end{equation*}
\end{thm}

\begin{proof}
Without loss of generality
assume $(X,0)$
is irreducible, so that
the local parametrization theorem applies.
We work in a small disc $D \subset \C$ centered at the origin, such that the
origin is the unique point of the discriminant set (the subvariety
$E$).  Let $N = \{ z \in D : \Im z = 0 , \Re z \leq 0 \}$.
As $D \setminus N$ is simply connected we have the well-defined functions
$\alpha_1(z),\ldots,\alpha_m(z)$ holomorphic on $D \setminus N$
that are solutions to $f\bigl(z,\alpha_j(z)\bigr) = 0$.
These functions continue analytically across $N$ away from the
origin.  The continuation equals one of the zeros, e.g.\ $\alpha_j(z)$
becomes $\alpha_\ell(z)$ (and by continuity it is the
same zero along the entire $N$).  So there is
a permutation $\sigma$ on $m$ elements such that as $z$ moves
counter-clockwise around the origin from the upper half-plane across $N$ to the
lower half-plane,
$\alpha_j(z)$ is continued as $\alpha_{\sigma(j)}(z)$.
There exists some number $k$ (e.g.\ $k=m!$) such that $\sigma^k$ is the identity.
As $\xi$ goes around
a circle around the origin, $\xi^k$ goes around the origin $k$ times.
Start at a positive real $\xi$ and start defining a
function $g(\xi)$ as
$\alpha_1(\xi^k)$.
Move $\xi$ around the origin counter-clockwise continuing $g$ analytically.
Divide the disc into sectors of angle $\nicefrac{2\pi}{k}$,
whose boundaries are where $\xi^k \in N$.
Transition to $\alpha_{\sigma(1)}(\xi^k)$ after we reach the boundary
of the first sector, then to
$\alpha_{\sigma(\sigma(1))}(\xi^k)$ after we reach the boundary of the next sector, and so on.
After $k$ steps, that is as
$\xi$ moved all the way around the circle,
we are back at $\alpha_1(\xi^k)$,
because
$\sigma^k$ is the identity.
So $g(\xi)$ is a well-defined holomorphic function outside the origin.  Let
$g(0) = 0$, and $g$ is holomorphic at 0 by the Riemann extension theorem.
See \figureref{fig:puiseux} for an example.
\begin{myfig}
\subimport*{figures/}{puiseux.pdf_t}
\caption{Proving Puiseux with $m = k = 4$.  The permutation $\sigma$ takes 1 to 2, 2 to 3, 3 to 4,
and 4 to 1.  As $\xi$ moves along the short circular arrow on the right, $\xi^4$
moves along the long circular arrow on the left.  The definition of $g$ is
then given in the right-hand diagram.\label{fig:puiseux}}
\end{myfig}
\end{proof}

\begin{exbox}
\begin{exercise}
Consider an irreducible germ
$(X,0) \subset (\C^2,0)$ defined
by an irreducible Weierstrass polynomial $f(z,w) = 0$ (polynomial in $w$)
of degree $k$.  Prove there exists a holomorphic $g$ such that
$f\bigl(z^k,g(z)\bigr) = 0$ and $z \mapsto \bigl(z^k,g(z)\bigr)$
is one-to-one and onto a neighborhood of 0 in $X$.
\end{exercise}

\begin{exercise}
Suppose $(X,0) \subset (\C^2,0)$ is a germ of a dimension 1 subvariety.
Show that after perhaps a linear change of coordinates,
there are natural numbers
$d_1,\ldots,d_k$
and
holomorphic functions $c_1(z),\ldots,c_k(z)$ vanishing at $0$,
such that $X$ can be defined near 0 by
\begin{equation*}
\prod_{j=1}^k {\bigl( w^{d_j} - c_j(z) \bigr)} = 0.
\end{equation*}
\end{exercise}

\begin{exercise}
Using the local parametrization theorem, prove that
if $(X,p)$ is an irreducible germ of a subvariety of dimension greater
than 1, then there exists a neighborhood $U$ of $p$ and a closed subvariety
$X \subset U$ (whose germ at $p$ is $(X,p)$), such that for every
$q \in X$ there exists an irreducible subvariety $Y \subset X$
of dimension 1 such that $p \in Y$ and $q \in Y$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Prove a stronger version of the exercise above.  Show that not only is there
a $Y$, but an analytic disc $\varphi \colon \D \to U$ such that
$\varphi(\D) \subset X$, $\varphi(0) = p$ and $\varphi(\nicefrac{1}{2}) =
q$.
\end{exercise}

\begin{exercise}
Suppose $X \subset U$ is a subvariety of a domain $U \subset \C^n$.
Suppose that for any two points $p$ and $q$ on $X$ there exists a finite sequence
of points $p_0 = p, p_1, \ldots, p_k = q$ in $X$, and a sequence of analytic discs
$\Delta_j \subset X$ such that $p_{j}$ and $p_{j-1}$ are in $\Delta_j$.
\end{exercise}

\begin{exercise}\label{exercise:maxprincsubvar}
\index{maximum principle!subvarieties}
Prove a \emph{maximum principle for subvarieties} using the exercises above:
Suppose $X \subset U$ is an irreducible subvariety of an open set $U$,
and suppose $f \colon U \to \R \cup \{ - \infty \}$
is a plurisubharmonic function.  If the modulus of the restriction $f|_X$
achieves a maximum
at some point $p \in X$, then the restriction $f|_X$ is constant.
\end{exercise}

\begin{exercise}
Prove that an analytic disc (namely the image of the disc) in $\C^2$
is a one-dimensional local variety (that is, a subvariety of some
open subset of $\C^2$).
\end{exercise}
\end{exbox}

Using the Puiseux theorem, we often simply parametrize germs
of complex one-dimensional subvarieties.  
And for larger-dimensional varieties, we can find
enough one-dimensional curves through any point and parametrize those.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Segre varieties and CR geometry} \label{sec:crgeomcr}

The existence of analytic discs (or subvarieties)
in boundaries of domains says a lot about the geometry of the boundary.

\begin{example}
Let $M \subset \C^n$ be a smooth real hypersurface containing
a complex hypersurface $X$ (zero set of a holomorphic function
with nonzero derivative), at $p \in X \subset M$.
Apply a local biholomorphic change of coordinates at $p$, so
that in the new coordinates
$(z,w) \in \C^{n-1} \times \C$,
$X$ is given by $w=0$, and $p$ is the origin.
The tangent hyperplane to $M$ at 0 contains $\{ w=0 \}$.
By rotating the $w$ coordinate (multiplying it by $e^{i\theta}$),
we assume $M$ is tangent to the set $\bigl\{ (z,w) : \Im w = 0
\bigr\}$.
In other words,
$M$ is given by
\begin{equation*}
\Im w = \rho(z,\bar{z},\Re w) ,
\end{equation*}
where $d\rho = 0$.
As $w = 0$ on $M$, then $\rho = 0$
when $\Re w = 0$. That is, $\rho$
is divisible by $\Re w$.  So $M$ is defined by
\begin{equation*}
\Im w = (\Re w) \widetilde{\rho}(z,\bar{z},\Re w), 
\end{equation*}
for a smooth function $\widetilde{\rho}$.
The Levi form at the origin vanishes.
As $p=0$ was an arbitrary point on $M \cap X$,
the Levi form of $M$ vanishes on $M \cap X$.
\end{example}

\begin{example}
The vanishing of the Levi form is not necessary if the complex varieties in
$M$ are small.  Consider $M\subset \C^3$ with a nondegenerate (but not definite)
Levi form:
\begin{equation*}
\Im w = \abs{z_1}^2-\abs{z_2}^2 .
\end{equation*}
For every $\theta \in \R$,
$M$ contains the complex line $L_\theta$,
given by $z_1 = e^{i\theta} z_2$ and
$w = 0$.  The union $\bigcup_\theta L_\theta$ of those
complex lines is not
contained in some single unique complex subvariety inside $M$.  Any complex
subvariety that contains all $L_\theta$ must contain the entire complex hypersurface given
by $w = 0$, which is not contained in $M$.
\end{example}

\begin{exbox}
\begin{exercise}
Let $M \subset \C^n$ be a smooth real hypersurface.
Show that if $M$ at $p$ contains a complex submanifold of (complex)
dimension more than
$\frac{n-1}{2}$, then the Levi form must be degenerate, that is, it must
have at least one zero eigenvalue.
\end{exercise}

\begin{exercise}
Let $M \subset \C^n$ be a smooth pseudoconvex real hypersurface
(one side of $M$ is pseudoconvex).
Suppose $M$ at $p$ contains a dimension $k$ complex submanifold $X$.
Show that the Levi form has at least $k$ zero eigenvalues.
\end{exercise}

\begin{exercise}
Find an example of a smooth real hypersurface $M \subset \C^n$ that contains a
germ of a singular complex-analytic subvariety $(X,p)$ through a point $p$,
which is unique in the sense that if $(Y,p)$ is another germ of a complex
analytic subvariety in $M$ then $(Y,p) \subset (X,p)$.
\end{exercise}
\end{exbox}

Let us discuss a tool, the \emph{\myindex{Segre variety}}, that allows us to
find such complex subvarieties inside $M$, and much more.  Segre varieties only
work in the real-analytic setting and rely on complexification.

Let $M \subset \C^n$ be a real-analytic hypersurface and $p \in M$.
Suppose $M \subset U$,
where $U \subset \C^n$ is a small domain with a defining function $r \colon
U  \to \R$ for $M$.  That is, $r$ is a real-analytic function in $U$ such that
$M = r^{-1}(0)$, but
$dr \not= 0$ on $M$.  Define
\glsadd{not:Ustar}%
\begin{equation*}
U^* = \bigl\{ z \in \C^n : \bar{z} \in U \bigr\} .
\end{equation*}
Suppose $U$ is small enough so that the Taylor series for $r$
converges in $U \times U^*$ when treating $z$ and $\bar{z}$ as separate
variables.  That is, $r(z,\zeta)$ is a well-defined function on
$U \times U^*$, and $r(z,\zeta) = 0$ defines a complexification $\sM$
in $U \times U^*$.  Assume also that $U$ is small enough that
the complexified
$dr$ does not vanish on $\sM$ and that $\sM$ is connected.
See also \propref{prop:complexificationofrasurface}.

Given $q \in U$,
define the \emph{Segre variety}
\glsadd{not:Segrevar}%
\begin{equation*}
\Sigma_q(U,r) =
\bigl\{ z \in U : r(z,\bar{q}) = 0 \bigr\} =
\bigl\{ z \in U : (z,\bar{q}) \in \sM \bigr\} .
\end{equation*}
A priory, the subvariety $\Sigma_p$ depends on $U$ and $r$.
However, if $\widetilde{r}$ is a 
real-analytic function that complexifies to $U \times U^*$
and vanishes on $M$, it must also vanish on the complexification $\sM$.
If $\widetilde{r}$ is a defining function as above,
that is, $d\widetilde{r}$ does not vanish on its zero set
and the zero set of the complexified $\widetilde{r}$ is connected
in $U \times U^*$, then $\widetilde{r}(z,\zeta) = 0$ also
defines $\sM$.
Hence the actual $r$ does not matter.
As long as $q \in M$, then
$q \in \Sigma_q(U,r)$, and furthermore the
Segre variety is a complex hypersurface for every $q$.
It is not hard to see that
if $\widetilde{U}$ is a small neighborhood of $q$, the same $r$ is
a defining function in
$\widetilde{U}$, and we 
get the same complexification in $\widetilde{U} \times \widetilde{U}^*$.
So the germ at $q \in U$ is well-defined, and we write
\begin{equation*}
\Sigma_q = \bigl( \Sigma_q(U,r) , q \bigr) .
\end{equation*}
The Segre variety is
well-defined as a germ, and so often when one talks about $\Sigma_q$
without mentioning the $U$ or $r$, then one means some small enough
representative of a Segre variety or the germ itself.

\begin{exbox}
\begin{exercise}
Let $r \colon U \to \R$ be a real-valued
real-analytic function that complexifies to
$U \times U^*$.  Show that
$r(z,\bar{\zeta}) = 0$
if and only if
$r(w,\bar{\zeta}) = 0$.  In other words,
$z \in \Sigma_{\zeta}(U,r)$ if and only if
$\zeta \in \Sigma_z(U,r)$.
\end{exercise}
\end{exbox}

\begin{example}
Suppose we start with the real-analytic hypersurface $M$ given by
\begin{equation*}
\Im w = (\Re w) \rho(z,\bar{z},\Re w) ,
\end{equation*}
with $\rho$ vanishing at the origin.
Rewriting in terms of $w$ and $\bar{w}$, we find
\begin{equation*}
\frac{w-\bar{w}}{2i} = \left(\frac{w+\bar{w}}{2}\right)
\rho\left(z,\bar{z},\frac{w+\bar{w}}{2}\right) .
\end{equation*}
Setting $\bar{z} = \bar{w} = 0$, we obtain 
\begin{equation*}
\frac{w}{2i} = \left(\frac{w}{2}\right)
\rho\left(z,0,\frac{w}{2}\right) .
\end{equation*}
As $\rho$ vanishes at the origin, then near the origin the equation 
defines the complex hypersurface given by $w=0$.
So $\Sigma_0$ is defined by $w = 0$.
This is precisely the complex hypersurface that lies inside $M$.
\end{example}

The last example is not a fluke.
The most important property of Segre varieties is that it locates complex
subvarieties in a real-analytic submanifold.
We will phrase it in terms of analytic discs, which is
enough as complex subvarieties can be filled with analytic discs,
as we have seen.

\begin{prop}
Let $M \subset \C^n$ be a real-analytic hypersurface and $p \in M$.
Suppose $\Delta \subset M$ is an analytic disc
through $p$.  Then as germs $(\Delta,p) \subset \Sigma_p$.
\end{prop}

\begin{proof}
Let $U$ be a neighborhood of $p$ where a representative
of $\Sigma_p$ is defined, that is, we assume that $\Sigma_p$ is
a closed subset of $U$, and suppose $r(z,\bar{z})$ is the corresponding
defining function.
Let $\varphi \colon \D \to \C^n$ be the parametrization of $\Delta$
with $\varphi(0) = p$.  We can restrict $\varphi$ to a smaller disc around the
origin, and since we are only interested in the germ of $\Delta$ at $p$ this
is sufficient (if there are multiple points of $\D$
that go to $p$, we repeat the argument for each one).
So let us assume without loss of generality that $\varphi(\D) = \Delta \subset U$.
Since $\Delta \subset M$ we have
\begin{equation*}
r\bigl(\varphi(\xi),\overline{\varphi(\xi)}\bigr) =
r\bigl(\varphi(\xi),\bar{\varphi}(\bar{\xi})\bigr) = 0 .
\end{equation*}
The function $\xi \mapsto
r\bigl(\varphi(\xi),\bar{\varphi}(\bar{\xi})\bigr)$ is a real-analytic
function of $\xi$, and therefore for some
small neighborhood of the origin, it complexifies.  In fact, it complexifies
to $\D \times \D$ as $\varphi(\xi) \in U$ for all $\xi \in \D$.
So we can treat $\xi$ and $\bar{\xi}$ as separate variables.  By
complexification, the equation holds for all such independent
$\xi$ and $\bar{\xi}$.  Set $\bar{\xi} = 0$ to obtain
\begin{equation*}
0 =
r\bigl(\varphi(\xi),\bar{\varphi}(0)\bigr) =
r\bigl(\varphi(\xi),\bar{p}\bigr) 
\qquad \text{for all $\xi \in \D$}.
\end{equation*}
In particular, $\varphi(\D) \subset \Sigma_p$ and the result follows.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that if a real-analytic real hypersurface $M \subset \C^n$ is strongly
pseudoconvex at $p \in M$ (one side of $M$ is strongly pseudoconvex at $p$),
then $\Sigma_p \cap
(M,p) = \{p\}$ (as germs).
\end{exercise}

\begin{exercise}
Use the proposition and the exercise above to show that if a real-analytic
real hypersurface $M$ is strongly 
pseudoconvex, then $M$ contains no analytic discs.
\end{exercise}
\end{exbox}

We end our discussion of Segre varieties by its perhaps most well-known
application, the so-called Diederich--Forn\ae ss lemma.  Although
we state and prove it only for real-analytic hypersurfaces it works in greater generality.
There are two parts to it, although it is generally the corollary
that is called the \emph{\myindex{Diederich--Forn\ae ss lemma}}.

First, for real-analytic hypersurfaces each point has a fixed neighborhood
such that germs of complex subvarieties contained in the hypersurface extend
to said fixed neighborhood.

\begin{thm}[Diederich--Forn\ae ss]
Suppose $M \subset \C^n$ is a real-analytic hypersurface.  For every
$p \in M$ there exists a neighborhood $U$ of $p$ with the following
property:
If $q \in M \cap U$ and
$(X,q)$ is a germ of a complex subvariety
such that $(X,q) \subset (M,q)$,
then there exists a complex subvariety $Y \subset U$ (in
particular a closed subset of $U$) such that $Y \subset M$ and $(X,q)
\subset (Y,q)$.
\end{thm}

\begin{proof}
Suppose $U$ is a polydisc centered at $p$,
small enough so that the defining function
$r$ of $M$ complexifies to $U \times U^*$ as above.
Suppose $q \in M \cap U$ is a point such that $(X,q)$ is a germ of a
positive-dimensional complex subvariety with $(X,q) \subset (M,q)$.
Most points of a subvariety are regular, so
without loss of generality assume $q$ is a regular point, that is,
$(X,q)$ is a germ of a complex submanifold.
Let $X$ be a representative of the germ $(X,q)$ such that $X \subset M$,
and $X \subset U$, although we do not assume it is closed.

Assume $X$ is an
image of an open subset $V \subset \C^k$ via a holomorphic surjective mapping $\varphi \colon V \to
X$.  Since $r\bigl(\varphi(\xi),\overline{\varphi(\xi)}\bigr) = 0$
for all $\xi \in V$, then we may treat $\xi$ and $\bar{\xi}$ separately.
In particular,
$r(z,\bar{\zeta}) = 0$ for all $z,\zeta \in X$.

Define complex subvarieties $Y', Y \subset U$ (closed in $U$) by
\begin{equation*}
Y' = \bigcap_{a \in X} \Sigma_a(U,r) 
\qquad \text{and} \qquad
Y = \bigcap_{a \in Y'} \Sigma_a(U,r) .
\end{equation*}
If $a \in Y'$ and $b \in X$, then $r(a,\bar{b}) = 0$.
Because $r$ is real-valued,
$r(b,\bar{a}) = 0$.  Therefore,
$X \subset Y \subset Y'$.  Furthermore, $r(z,\bar{z}) = 0$
for all $z \in Y$, and so $Y \subset M$.
\end{proof}

\begin{cor}[Diederich--Forn\ae ss]
Suppose $M \subset \C^n$ is a compact real-analytic hypersurface.
Then there does not exist any point $q \in M$ such that
there exists a germ of a positive-dimensional complex subvariety
$(X,q)$ such that $(X,q) \subset (M,q)$.
\end{cor}

\begin{proof}
Let $S \subset M$ be the set of points through which there exists 
a germ of a positive-dimensional complex subvariety contained in $M$.
As $M$, and hence $\widebar{S}$, is compact,
there must exist a point $p \in \widebar{S}$
that is furthest from
the origin.  After a rotation by a unitary and rescaling assume
$p=(1,0,\ldots,0)$.  Let $U$ be the neighborhood from the previous
theorem around $p$.  There exist germs of varieties in $M$ through points
arbitrarily close to $p$.  So for any distance $\epsilon > 0$,
there exists a subvariety $Y \subset U$ (in particular, $Y$ closed in $U$)
of positive dimension with $Y \subset M$ that contains points 
$\epsilon$ close to $p$.  Consider the function $\Re z_1$, whose modulus attains a
strict maximum on $\widebar{S}$ at $p$.  Because $\Re z_1$ achieves a maximum
strictly smaller than 1 on $\partial U \cap \widebar{S}$, for a small enough $\epsilon$,
we would obtain a pluriharmonic function with a strict
maximum on $Y$, which is impossible by the maximum principle for
varieties that you proved in \exerciseref{exercise:maxprincsubvar}.
The picture would look as in \figureref{fig:forndied}.
\begin{myfig}
\medskip
\subimport*{figures/}{forndied.pdf_t}
\caption{Contradicting the maximum principle at $p$.\label{fig:forndied}}
\end{myfig}
\end{proof}

\begin{example}
The results above do not work
in the smooth setting.  Let us disprove the theorem in the smooth
setting.  Disproving the corollary is an exercise.
Let $g \colon \R \to \R$ be a smooth function that is
strictly positive for $\sabs{t} > 1$, and $g(t) = 0$ for all $\sabs{t} \leq 1$.
Define $M$ in $(z,w) \in \C^{n-1} \times \C$ by
\begin{equation*}
\Im w = g\bigl(\snorm{z}^2\bigr) .
\end{equation*}
$M$ is a smooth real hypersurface.
Consider $p = (1,0,\ldots,0) \in M$.  For every $0 < s < 1$, let
$q_s = (s,0,\ldots,0) \in M$ and $X_s = \bigl\{ (z,w) \in M :
w = s \bigr\}$.  Each $X_s$ is a local complex subvariety of dimension $n-1$
and $(X_s,q_s) \subset (M,q_s)$.  The size of $X_s$ goes to
zero as $s \to 1$ and $X_s$ cannot extend to a
larger complex subvariety inside $M$.  So, no neighborhood $U$ at
$p$ (as in the theorem) exists.
\end{example}

\begin{exbox}
\begin{exercise}
Find a compact smooth real hypersurface $M \subset \C^n$ that contains a germ
of a positive dimensional complex subvariety.
\end{exercise}
\end{exbox}

\vspace{1in}

\ldots and that is how using sheep's bladders can prevent
earthquakes!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

% No sections in appendixes
\counterwithin{thm}{chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Basic notation and terminology} \label{ap:basicnotation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Is this overly too nitpicky to include?

Let us quickly review some basic notation that we use in this
book that is perhaps not described elsewhere.
We use $\C$, $\R$ for complex and real numbers, and $i$ for imaginary unit
(a square root of $-1$).  We use
$\N = \{ 1,2,3, \ldots \}$ for 
the natural numbers, 
$\N_0 = \{ 0,1,2,3, \ldots \}$ for the zero-based natural numbers,
and $\Z$ for all integers.
When we write $\C^n$ or $\R^n$ we implicitly mean that $n \geq 1$, unless
otherwise stated.

\glsadd{not:setminus}%
We denote the set subtraction by $A \setminus B$, meaning all elements of
$A$ that are not in $B$.
We denote the complement of a set by $X^c$.  The ambient set
should be clear.  So, for example, if $X \subset \C$ naturally,
then $X^c = \C \setminus X$.
\glsadd{not:closure}%
The topological closure of a set $S$ is denoted by $\widebar{S}$, its
boundary is denoted by
\glsadd{not:boundary}%
$\partial S$.  If $S$ is relatively compact subset of $X$
(its closure in $X$ is compact) or compact, we write $S \subset \subset X$.

\glsadd{not:function}%
The notation $f \colon X \to Y$ means a function with domain $X$ and
codomain $Y$.  By $f(S)$ we mean the direct image of $S$ by $f$.
We use the notation $f^{-1}$ for the inverse image of sets and
single points.  When $f$ is bijective (one-to-one and onto),
we use $f^{-1}$ for the inverse mapping.  So $f^{-1}(T)$ for
a set $T \subset Y$ denotes all the points of $X$ that $f$ maps to $T$.
Similarly, $f^{-1}(q)$ could denote all the points that map to $q$,
but if the mapping is bijective, then it means just the unique point
mapping to $q$.
If we wish to define a function without necessarily giving it a name, we use
the notation
\glsadd{not:mapsto}%
\begin{equation*}
x \mapsto F(x),
\end{equation*}
where $F(x)$ would generally be some formula giving the output.
The notation
\glsadd{not:restriction}%
\begin{equation*}
f|_S
\end{equation*}
means the restriction of $f$ to $S$:
it is a function
$f|_S \colon S \to Y$ such that $f|_S(x) = f(x)$ for all $x \in S$.
A function $f \colon U \to \C$ is said to be
\emph{\myindex{compactly supported}} if
the \emph{\myindex{support}}, that is the set
$\overline{\{ p \in U : f(p) \not= 0 \}}$, is compact.
If $f(x) = g(x)$ for all $x$ in the domain,
we write
\glsadd{not:identeq}%
\begin{equation*}
f \equiv g ,
\end{equation*}
and we say that $f$ and $g$ are identically equal.
The notation
\glsadd{not:composition}%
\begin{equation*}
f \circ g
\end{equation*}
denotes the composition defined by $x \mapsto
f\bigl(g(x)\bigr)$.

We write
\glsadd{not:definition}%
\begin{equation*}
X
\overset{\text{def}}{=}
Y
\end{equation*}
to define $X$ to be $Y$ rather than just show equality.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Results from one complex variable} \label{ap:onevarresults}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We review some results from one complex variable that may be useful for
reading this book.
The reader should first look through \sectionref{sec:motivation}
for basic notation and motivation, although we will review some of the
results again here.
Let $U \subset \C$ be open.
A function
$f \colon U \to \C$ is \emph{holomorphic} if it is complex
differentiable at every point, that is,
\begin{equation*}
f'(z) = \lim_{h \in \C \to 0} \frac{f(z+h) - f(z)}{h}
\end{equation*}
exists for all $z \in U$.  For example, polynomials and rational
functions in $z$ are holomorphic.  Perhaps the most important holomorphic
function is the solution to the differential equation $f'(z) = f(z)$, $f(0) = 1$, that is
the complex exponential, defined in the entire complex plane:
\begin{equation*}
f(z) = e^z = e^{x+iy} = e^x \bigl( \cos y + i \sin(y) \bigr) .
\end{equation*}


A piecewise-$C^1$ \emph{path} (or curve)\index{path}\index{curve} in $\C$
is a continuous function $\gamma \colon [a,b] \to \C$,
continuously differentiable except at finitely many points,
such that one-sided limits of $\gamma'$ exist at all points of $[a,b]$
and such that $\gamma'$ (and its one sided limits) is never zero.
By abuse of notation, when $\gamma$ is used as a set, we mean
the image
$\gamma\bigl([a,b]\bigr)$.
For a continuous function $f \colon \gamma \to \C$, define
\glsadd{not:pathint}%
\begin{equation*}
\int_\gamma f(z) \, dz 
\overset{\text{def}}{=}
\int_a^b f\bigl(\gamma(t)\bigr) \gamma'(t) \, dt .
\end{equation*}
As $\gamma'$ is defined at all but finitely many points and is otherwise
continuous, the integral is well-defined.  Similarly, one defines the more general
path integral in $dz = dx + i\,dy$ and $d\bar{z} = dx - i\, dy$.
Let
$z = \gamma(t) = \gamma_1(t) + i \, \gamma_2(t) = x + i\, y$
parametrize the path.  Then
\begin{equation*}
\begin{split}
\int_\gamma f(z) \, dz + g(z) \, d\bar{z}
& =
\int_\gamma
\Bigl(f\bigl(x+i\, y\bigr) + g\bigl(x+i\, y\bigr) \Bigr) \, dx
+
i \, \Bigl( f\bigl(x+i\, y\bigr) - g\bigl(x+i\, y\bigr) \Bigr) \, dy
\\
& =
\int_a^b
\Bigl(
f\bigl(\gamma(t)\bigr) \gamma'(t)
+
f\bigl(\gamma(t)\bigr) \overline{\gamma'(t)}
\Bigr) \, dt
\\
& =
\int_a^b
\biggl(
\Bigl(f\bigl(\gamma(t)\bigr) + g\bigl(\gamma(t)\bigr) \Bigr) \,
\gamma_1'(t)
+
i \, \Bigl( f\bigl(\gamma(t)\bigr) - g\bigl(\gamma(t)\bigr) \Bigr)
\gamma_2'(t)
\biggr)
\, dt .
\end{split}
\end{equation*}

A path is \emph{closed}\index{closed path}
if $\gamma(a) = \gamma(b)$,
and a path is \emph{simple}\index{simple path}
if $\gamma|_{(a,b]}$ is one-to-one
with the possible exception of $\gamma(a) = \gamma(b)$.

An open $U \subset \C$ has \emph{\myindex{piecewise-$C^1$ boundary}} if
for each $p \in \partial U$ there is an open neighborhood $W$ of $p$
such that
$\partial U \cap W = \gamma\bigl((a,b)\bigr)$
where $\gamma \colon [a,b] \to \C$ is an injective piecewise-$C^1$ path,
and such that each $p \in \partial U$ is in the closure of $\C \setminus
\widebar{U}$.  Intuitively the boundary is a piecewise-$C^1$ curve that
locally cuts the plane into two open pieces.
If at each point where the parametrization of $\partial U$ is differentiable
the domain is on the left ($\gamma'(t)$ rotated by $\frac{\pi}{2}$ points into the
domain), then the boundary is \emph{\myindex{positively oriented}}.
As in the introduction, we have the following version of Cauchy integral formula.

\begin{thm}[Cauchy integral formula]
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, and let
$f \colon \widebar{U} \to \C$ be a continuous function
holomorphic in $U$.
Then for $z \in U$,
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta .
\end{equation*}
\end{thm}

The theorem follows from Green's theorem, which is the Stokes' theorem
in two dimensions.  In the versions we state, one needs to
approximate the open set by smaller open sets from the inside to insure
the partial derivatives are bounded.  See
\thmref{thm:generalizedcauchy}.  Let us state Green's theorem using
the $dz$ and $d\bar{z}$ for completeness.  See \appendixref{ap:diffforms}
for an overview of differential forms.

\begin{thm}[Green's theorem]\index{Green's theorem} \label{thm:greens}
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, and let
$f \colon \widebar{U} \to \C$ be continuous
with bounded continuous partial derivatives in $U$.
Then
\begin{multline*}
\int_{\partial U} f(z) \, dz + g(z) \, d\bar{z}
=
\int_{U} d \Bigl( f(z) \, dz + g(z) \, d\bar{z} \Bigr)
=
\int_{U}
\left(
\frac{\partial g}{\partial z}
-
\frac{\partial f}{\partial \bar{z}}
\right)
\, dz \wedge d\bar{z}
\\
=
(-2i)
\int_{U}
\left(
\frac{\partial g}{\partial z}
-
\frac{\partial f}{\partial \bar{z}}
\right)
\, dx \wedge dy 
=
(-2i)
\int_{U}
\left(
\frac{\partial g}{\partial z}
-
\frac{\partial f}{\partial \bar{z}}
\right)
\, dA.
\end{multline*}
\end{thm}

The Cauchy integral formula is equivalent to
what is usually called just Cauchy's theorem:

\begin{thm}[Cauchy]
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, and let
$f \colon \widebar{U} \to \C$ be a continuous function
holomorphic in $U$.  Then
\begin{equation*}
\int_{\partial U}
f(z) \, dz = 0 .
\end{equation*}
\end{thm}

There is a converse to Cauchy as well.  A triangle $T \subset \C$ is
the convex hull of the three vertices (we include the inside of the
triangle), and $\partial T$ is the boundary of the triangle oriented
counter-clockwise.  Let us state the following theorem as an
``if and only if,'' even though, usually it is only the reverse direction that
is called Morera's theorem.

\begin{thm}[Morera] \label{thm:onevarmorera}
Suppose $U \subset \C$ is an open set, and $f \colon U \to \C$
is continuous.  Then $f$ is holomorphic
if and only if
\begin{equation*}
\int_{\partial T} f(z) \, dz = 0
\end{equation*}
for all triangles $T \subset U$.
\end{thm}

As we saw in the introduction, a holomorphic function has a power series.

\begin{prop}
If $U \subset \C$ is open and $f \colon U \to \C$ is holomorphic,
then $f$ is infinitely differentiable, and if $\Delta_\rho(p) \subset \C$
is a disc, then $f$ has a power series that
converges absolutely uniformly on compact subsets of $\Delta_\rho(p)$:
\begin{equation*}
f(z) = \sum_{k=0}^\infty c_k {(z-p)}^k ,
\end{equation*}
where given a simple closed (piecewise-$C^1$) path $\gamma$
going once counter-clockwise
around $p$ inside $\Delta_\rho(p)$,
\begin{equation*}
c_k = \frac{f^{(k)}(p)}{k!} =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-z)}^{k+1}}
\,
d \zeta  .
\end{equation*}
\end{prop}

\emph{\myindex{Cauchy estimates}} follow:  If $M$
is the maximum of $\sabs{f}$ on the circle $\partial \Delta_r(p)$, then
\begin{equation*}
\sabs{c_k} \leq \frac{M}{r^k} .
\end{equation*}
Conversely, if a power series satisfies such estimates,
it converges on $\Delta_r(p)$.

A holomorphic $f \colon \C \to \C$ that is
\emph{\myindex{entire}}.  An immediate application of Cauchy estimates
is Liouville's theorem:

\begin{thm}[Liouville]\index{Liouville's theorem}
If $f$ is entire and bounded, then $f$ is constant.
\end{thm}

And as a holomorphic function has a power series it satisfies the
identity theorem:

\begin{thm}[Identity]\index{identity theorem}
Suppose $U \subset \C$ is a domain and $f \colon U \to \C$ is holomorphic.
If the zero set $f^{-1}(0)$ has a limit point in $U$, then
$f \equiv 0$.
\end{thm}

Another consequence  of the Cauchy integral formula is that there is
a differential equation characterizing holomorphic functions.

\begin{prop}[Cauchy--Riemann equations]
Let $U \subset \C$ be open.
A function $f \colon U \to \C$ is holomorphic if and only if
$f$ is continuously differentiable and
\begin{equation*}
\frac{\partial f}{\partial \bar{z}}
=
\frac{1}{2}
\left(
\frac{\partial f}{\partial x} + i
\frac{\partial f}{\partial y}
\right)
 = 0 \qquad \text{on $U$.}
\end{equation*}
\end{prop}

Yet another consequence of the Cauchy formula (and one can make an argument
that everything in this appendix is a consequence of the Cauchy formula)
is the open mapping theorem.

\begin{thm}[Open mapping theorem]\index{open mapping theorem}
Suppose $U \subset \C$ is a domain and
$f \colon U \to \C$ is holomorphic and not constant.
Then $f$ is an open mapping, that is,
$f(V)$ is open whenever $V$ is open.
\end{thm}

The real and imaginary parts $u$ and $v$ of a holomorphic function $f =
u+iv$ are harmonic, that is
$\nabla^2 u = \nabla^2 v = 0$, where $\nabla^2$ is the
Laplacian.
A domain $U$ is \emph{\myindex{simply connected}} if every simple closed
path
is homotopic in $U$ to a constant, in other words, if the domain has no
holes.  For example a disc is simply connected.

\begin{prop}
If $U \subset \C$ is a simply connected domain and $u \colon U \to \R$
is harmonic, then there exists a harmonic function $v \colon U \to \R$
such that $f = u+iv$ is holomorphic.
\end{prop}

The function $v$ is called the \emph{\myindex{harmonic conjugate}} of
$u$.  For further review of harmonic functions see
\sectionref{sec:harmonic} on harmonic functions.
We have the following versions of the maximum principle.

\begin{thm}[Maximum principles]\index{maximum principle}
Suppose $U \subset \C$ is a domain.
\begin{enumerate}[(i)]
\item
If $f \colon U \to \C$ is holomorphic and $\sabs{f}$
achieves a local maximum in $U$, then $f$ is constant.
\item
If $U$ is bounded and $f \colon \widebar{U} \to \C$ is holomorphic in $U$
and continuous, then $\sabs{f}$ achieves its maximum on $\partial U$.
\item
If $f \colon U \to \R$ is harmonic 
achieves a local maximum or a minimum in $U$, then $f$ is constant.
\item
If $U$ is bounded and $f \colon \widebar{U} \to \R$ is harmonic in $U$
and continuous, then $f$ achieves its maximum and minimum on $\partial U$.
\end{enumerate}
\end{thm}

The first two items are sometimes called the \emph{\myindex{maximum modulus principle}}.
The maximum principle immediately implies the following lemma.

\begin{lemma}[Schwarz's lemma]\index{Schwarz's lemma}
\pagebreak[2]
Suppose $f \colon \D \to \D$ is holomorphic and $f(0) = 0$,
then 
\begin{enumerate}[(i)]
\item $\sabs{f(z)} \leq \sabs{z}$, and
\item $\sabs{f'(0)} \leq 1$.
\end{enumerate}
Furthermore, if $\sabs{f(z_0)} = \sabs{z_0}$ for some $z_0 \in \D \setminus
\{ 0 \}$
or $\sabs{f'(0)} = 1$, then
for some $\theta \in \R$ we have $f(z) =
e^{i\theta} z$ for all $z \in \D$.
\end{lemma}

The theorem above is actually quite general.

\begin{thm}[Riemann mapping theorem]\index{Riemann mapping theorem}
If $U \subset \C$ is a nonempty simply connected domain such that $U \neq \C$,
then $U$ is biholomorphic to $\D$.  Given $z_0 \in U$
there exists a unique biholomorphic $f \colon U \to \D$
such that $f(z_0) = 0$, $f'(z_0) > 0$, and $f$
maximizes $\sabs{f'(z_0)}$ among all injective holomorphic maps to $\D$ such
that $f(z_0) = 0$.
\end{thm}

Schwarz's lemma can also be used to classify the automorphisms of the disc
(and hence any simply connected domain).  Let
$\operatorname{Aut(\D)}$ denote the group of biholomorphic (both $f$ and
$f^{-1}$ are holomorphic) self maps of the
disc to itself.

\begin{prop}
If $f \in \operatorname{Aut}(\D)$, then there exists an $a \in \D$
and $\theta \in \R$ such that
\begin{equation*}
f(z) = e^{i\theta} \frac{z-a}{1-\bar{a}z} .
\end{equation*}
\end{prop}

Speaking of automorphisms.  We have the following
version of inverse function theorem.

\begin{thm}
Suppose $U$ and $V$ are open subsets of $\C$.
\begin{enumerate}[(i)]
\item
If $f \colon U \to V$ is holomorphic and bijective (one-to-one and onto),
then $f'(z) \not= 0$ for all $z \in V$, and $f^{-1} \colon V \to U$
is holomorphic.  If $f(p) = q$, then
\begin{equation*}
\left(f^{-1}\right)(q) = \frac{1}{f'(p)} .
\end{equation*}
\item
If $f \colon U \to V$ is holomorphic, $f(p) = q$,
and $f'(p) \not= 0$, then there exists a neighborhood $W$ of $q$
and a holomorphic function $g \colon W \to U$ that is
one-to-one and $f\bigl(g(z)\bigr) = z$ for all $z \in W$.
\end{enumerate}
\end{thm}

\pagebreak[2]
The Riemann mapping theorem actually follows from the following
theorem about existence of branches of the logarithm.

\begin{thm}
Suppose $U \subset \C$ is a simply connected domain, and $f \colon U \to \C$
is a holomorphic function without zeros in $U$.  Then there exists a
holomorphic function $L \colon U \to \C$ such that
\begin{equation*}
e^L = f .
\end{equation*}
In particular, we can take roots:
For every $k \in \N$, there exists a holomorphic function
$g \colon U \to \C$ such that
\begin{equation*}
g^k = f .
\end{equation*}
\end{thm}

In one complex variable, zeros of holomorphic functions
can be divided out.  Moreover, zeros
of holomorphic functions are of finite order unless
the function is identically zero.

\begin{prop}
Suppose $U \subset \C$ is a domain and
$f \colon U \to \C$ is holomorphic, not identically zero, and $f(p) = 0$
for some $p \in U$.  There exists a $k \in \N$ and
a holomorphic function $g \colon U \to \C$,
such that $g(p) \not= 0$ and
\begin{equation*}
f(z) = {(z-p)}^k g(z) \qquad \text{for all $z \in U$.}
\end{equation*}
\end{prop}

The number $k$ above is called the \emph{order}\index{order of a zero}
or \emph{multiplicity}\index{multiplicity of a zero}
of the zero at $p$.
We can use this fact and the existence of roots to show that every
holomorphic function is locally like $z^k$.  The function $\varphi$
below can be thought of as a local change of coordinates.

\begin{prop}
Suppose $U \subset \C$ is a domain and
$f \colon U \to \C$ is holomorphic, not identically zero, and $p \in U$.
Then there exists a $k \in \N$,
a neighborhood $V \subset U$ of $p$,  and
a holomorphic function $\varphi \colon V \to \C$ with 
$\varphi'(p) \not= 0$, such that
\begin{equation*}
{\bigl(\varphi(z)\bigr)}^k = f(z) - f(p)
\qquad \text{for all $z \in V$.}
\end{equation*}
\end{prop}

Convergence of holomorphic functions is the same as for continuous
functions: uniform convergence on compact subsets.
Sometimes this is called \emph{\myindex{normal convergence}}.

\begin{prop}
Suppose $U \subset \C$ is open and $f_k \colon U \to \C$ is
a sequence of holomorphic functions which converge uniformly
on compact subsets of $U$ to $f \colon U \to \C$.  Then $f$ is holomorphic,
and every derivative $f_k^{(\ell)}$ converges uniformly on compact subsets
to the derivative $f^{(\ell)}$.
\end{prop}

Holomorphic functions satisfy a Heine--Borel-like property:

\begin{thm}[Montel]\index{Montel's theorem}\label{thm:onevarmontel}
Suppose
$U \subset \C$ is open and
$f_n \subset U \to \C$ is a sequence of holomorphic functions.
If $\{ f_n \}$ is uniformly bounded on compact subsets of $U$,
then there exists a subsequence converging uniformly on compact subsets
of $U$.
\end{thm}

A sequence of holomorphic functions cannot create or delete zeros out of thin air:

\begin{thm}[Hurwitz]\index{Hurwitz's theorem}\label{thm:onevarhurwitz}
Suppose $U \subset \C$ is a domain and
$f_n \subset U \to \C$ is a sequence of holomorphic functions
converging uniformly on compact subsets of $U$ to $f \colon U \to \C$.
If $f$ is not identically zero and $z_0$ is a zero of $f$,
then there exists a disc $\Delta_r(z_0)$ and an $N$, such that
for all $n \geq N$, $f_n$ has the same number of zeros (counting
multiplicity) in $\Delta_r(z_0)$ as $f$ (counting multiplicity).
\end{thm}

A common application, and sometimes the way
the theorem is stated, is that if $f_n$ have no zeros in $U$, then
either the limit $f$ is identically zero, or it also has no zeros.


\pagebreak[2]
If $U \subset \C$ is open, $p \in U$, and
$f \colon U \setminus \{ p \} \to \C$ is holomorphic, we say that
$f$ has an \emph{\myindex{isolated singularity}} at $p$. 
An isolated singularity is \emph{removable}\index{removable singularity}
if 
there exists a holomorphic function $F \colon U \to \C$
such that $f(z) = F(z)$ for all $z \in U \setminus \{ p \}$.
An isolated singularity is a \emph{\myindex{pole}} if 
\begin{equation*}
\lim_{z \to p} f(z) = \infty \qquad \text{(that is
$\sabs{f(z)} \to \infty$ as $\sabs{z-p} \to 0$)}.
\end{equation*}
An isolated singularity that is neither removable nor a pole is said to be
\emph{essential}\index{essential singularity}.

At nonessential isolated singularities
the function blows up to a finite integral order.
The first part of the following proposition is
usually called the \emph{\myindex{Riemann extension theorem}}.

\begin{prop} \label{prop:onevarclassifysing}
Suppose $U \subset \C$ is an open set, $p \in U$,
and $f \colon U \setminus \{p\} \to \C$ holomorphic.
\begin{enumerate}[(i)]
\item \label{prop:onevarclassifysing:i}
If $f$ is bounded (near $p$ is enough), then $p$ is a removable singularity.
\item \label{prop:onevarclassifysing:ii}
If $p$ is a pole, there exists a $k \in \N$ such that
\begin{equation*}
g(z) = {(z-p)}^k f(z)
\end{equation*}
is bounded near $p$ and hence $g$ has a removable singularity at $p$.
\end{enumerate}
\end{prop}

The number $k$ above is called the \emph{order}\index{order of a pole}
of the pole.  There is a symmetry between zeros and poles:
If $f$ has a zero of order $k$, then $\frac{1}{f}$ has a pole of order $k$.
If $f$ has a pole of order $k$, then $\frac{1}{f}$
has a removable singularity, and the extended function has a zero of order
$k$.

Let $\bP^1 = \C \cup \{\infty\}$ be the \emph{\myindex{Riemann sphere}}.
The topology on $\bP^1$ is given by insisting that
the function $\frac{1}{z}$ is a homeomorphism
of $\bP^1$ to itself,
where $\frac{1}{\infty} = 0$ and $\frac{1}{0} = \infty$.
A function $f \colon U \to \bP^1$ is called \emph{\myindex{meromorphic}},
if it is not
identically $\infty$, is holomorphic on $U \setminus f^{-1}(\infty)$,
and has poles at $f^{-1}(\infty)$.
A holomorphic function with poles is meromorphic
by setting the value to be $\infty$ at the poles.
A meromorphic function is one that can locally be written as a quotient of
holomorphic functions.


At an isolated singularity we can expand a holomorphic function
via the so-called
\emph{\myindex{Laurent series}} by adding all negative powers.
The Laurent series also characterizes the type of the singularity.

\begin{prop}
If $\Delta \subset \C$ is a disc centered at $p \in \C$,
and $f \colon \Delta \setminus \{p\} \to \C$ holomorphic,
then there exists a double sequence $\{ c_{k} \}_{k = -\infty}^\infty$
such that
\begin{equation*}
f(z) = \sum_{k=-\infty}^\infty c_k {(z-p)}^k ,
\end{equation*}
converges absolutely uniformly on compact subsets of $\Delta$.  If $\gamma$
is a simple closed piecewise-$C^1$ path going once counter-clockwise around
$p$ in $\Delta$, then
\begin{equation*}
c_k = 
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-z)}^{k+1}}
\,
d \zeta  .
\end{equation*}
The singularity at $p$ is
\begin{enumerate}[(i)]
\item \emph{removable} if $c_k = 0$ for all $k < 0$.
\item \emph{pole} of order $\ell \in \N$ if $c_k = 0$ for all $k < -\ell$ and $c_{-\ell}
\not= 0$.
\item \emph{essential} if for every exist infinitely negative $k$
such that $c_k \not= 0$.
\end{enumerate}
\end{prop}

\pagebreak[3]
If $p$ is an isolated singularity of $f$, then call the corresponding
$c_{-1}$ the \emph{\myindex{residue}} of $f$ at $p$, and write it
as $\operatorname{Res}(f,p)$.  
The proposition says that for a small $\gamma$ around $p$ in the positive direction,
\begin{equation*}
\operatorname{Res}(f,p) = c_{-1} = \frac{1}{2\pi i} \int_\gamma f(z) \, dz
.
\end{equation*}
Combining this equation with Cauchy's theorem tells us that to compute
integrals of functions with isolated singularities we simply need to find
the residues,
which tend to be simpler to
compute.  For example, if $p$ is a simple pole (of order 1), then 
\begin{equation*}
\operatorname{Res}(f,p) = \lim_{z \to p} (z-p)f(z) .
\end{equation*}

\begin{thm}[Residue theorem]\index{residue theorem}\label{thm:residue}
Suppose $U \subset \C$ is an open set, and $\gamma$ is a piecewise-$C^1$
simple closed path in $U$ such that the interior of $\gamma$ is in $U$.
Suppose that $f \colon U \setminus S \to \C$ is a holomorphic function with isolated
singularities in a finite set $S$, and suppose $S$ lies in the interior of $\gamma$.
Then
\begin{equation*}
\int_{\gamma} f(z) \, dz = 2\pi i \sum_{p \in S} \operatorname{Res}(f,p) .
\end{equation*}
\end{thm}

The identity theorem says that zeros of a nonconstant holomorphic $f$
have no limit points, and so are isolated points.
Since $\frac{1}{f}$ is a meromorphic
function with zeros at the poles of $f$, poles are also
isolated.  Zeros and poles of  can be counted fairly easily.

\begin{thm}[Argument principle]\index{argument principle}\label{thm:onevarargprinc}
Suppose $U \subset \C$ is an open set, and $\gamma$ is a piecewise-$C^1$
simple closed path in $U$ such that the interior of $\gamma$ is in $U$.
Suppose that $f \colon U \to \bP^1$ is a meromorphic function with no zeros
or poles on $\gamma$.
Then
\begin{equation*}
\frac{1}{2\pi i}
\int_\gamma \frac{f'(z)}{f(z)} \, dz
= N - P ,
\end{equation*}
where $N$ is the number of zeros of $f$ inside $\gamma$ and $P$ is the
number of poles inside $\gamma$, both counted with multiplicity.

Furthermore, suppose $h \colon U \to \C$ is holomorphic.  
Let $z_1,\ldots,z_N$ be the zeros of $f$ inside $\gamma$ and
$w_1,\ldots,w_P$ be the poles of $f$ inside $\gamma$.
Then
\begin{equation*}
\frac{1}{2\pi i}
\int_\gamma h(z) \frac{f'(z)}{f(z)} \, dz
=
\sum_{j=1}^N h(z_j)
\quad
-
\quad
\sum_{j=1}^P h(w_j) .
\end{equation*}
\end{thm}

The proof is an immediate application of the residue theorem.  Simply
compute the residues at the zeros and poles of $f$.  In particular,
if $f$ has a zero at $p$ or multiplicity $k$, then $h(z) \frac{f'(z)}{f(z)}$
has a simple pole
at $p$ with residue $k\, h(p)$.  Similarly, if $f$ has a pole at $p$ of
order $k$,
then $h(z) \frac{f'(z)}{f(z)}$ has a simple pole with residue $-k\, h(p)$ at $p$.

In the couple of theorems above,
we avoided introducing winding numbers by making $\gamma$ a simple closed curve,
so the statements above may be slightly different from what you have seen in
your one-variable course.

Another useful way to count zeros is Rouch\'e's theorem.

\begin{thm}[Rouch\'e]\index{Rouch\'e's theorem}\label{thm:onevarrouche}
Suppose $U \subset \C$ is an open set, and $\gamma$ is a piecewise-$C^1$
simple closed path in $U$ such that the interior of $\gamma$ is in $U$.
Suppose that $f \colon U \to \C$ and $g \colon U \to \C$
are holomorphic functions such that
\begin{equation*}
\sabs{f(z)-g(z)} < \sabs{f(z)}+\sabs{g(z)}
\end{equation*}
for all $z \in \gamma$.  Then $f$ and $g$
have the same number of zeros
inside $\gamma$ (up to multiplicity).
\end{thm}

In the classical statement of the theorem the weaker
inequality $\sabs{f(z)-g(z)} < \sabs{f(z)}$ is used.
Notice that either inequality precludes any zeros on $\gamma$ itself.

A holomorphic function with an essential singularity achieves essentially
every value.  A weak version of this result (and an easy to prove one)
is the \emph{\myindex{Casorati--Weierstrass theorem}}:
If a holomorphic $f$ has an essential singularity at $p$,
then for every neighborhood $W$ of $p$, $f\bigl(W \setminus \{p\}\bigr)$ is dense in
$\C$.  Let us state the much stronger theorem of Picard:
A function with an essential singularity is very wild.
It achieves every value (except possibly one) infinitely often.

\begin{thm}[Picard's big theorem]\index{Picard's big theorem}
Suppose $U \subset \C$ is open, $f \colon U \setminus \{ p \} \to \C$
is holomorphic, and $f$ has an essential singularity at $p$.  Then for every
neighborhood $W$ of $p$, $f\bigl(W \setminus \{ p \}\bigr)$ is either $\C$
or $\C$ minus a point.
\end{thm}

For example, $e^{1/z}$ has an essential singularity at the origin
and the function is never 0.
Since we stated the big theorem, let us also state the little theorem.

\begin{thm}[Picard's little theorem]\index{Picard's little theorem}
If $f \colon \C \to \C$ is holomorphic,
then
$f(\C)$ is either $\C$ or $\C$ minus a point.
\end{thm}


One theorem from algebra that is important in complex analysis, and becomes
perhaps even more important in several variables is the fundamental theorem
of algebra.  It really is a theorem of complex analysis and its standard
proof is via the maximum principle.

\begin{thm}[Fundamental theorem of algebra]\index{fundamental theorem of algebra}
\label{thm:fundamentalthmalg}%
If $P \colon \C \to \C$ is a nonzero polynomial of degree $k$,\linebreak[2]
then $P$
has exactly $k$ zeros (roots) in $\C$ counted with multiplicity.
\end{thm}

The set of rational functions is dense in the space of holomorphic
functions, and we even have control over where the poles need to be.
Note that a nonconstant polynomial has a ``pole at infinity'' meaning 
$P(z) \to \infty$ as $z \to \infty$.  Letting $\bP^1$ again be the Riemann
sphere, we have Runge's approximation theorem.

\begin{thm}[Runge]\index{Runge's approximation theorem}
Suppose $U \subset \C$ is an open set and $A \subset \bP^1 \setminus U$
is a set containing at least one point from each component of
$\bP^1 \setminus U$.  Suppose $f \colon U \to \C$ is holomorphic.
Then for any $\epsilon > 0$ and any compact
$K \subset \subset U$, there exists a rational function $R$ with poles in $A$
such that
\begin{equation*}
\sabs{R(z) - f(z)} < \epsilon \qquad \text{for all $z \in K$}.
\end{equation*}
\end{thm}

Perhaps a surprising generalization of the
classical Weierstrass approximation theorem,
and one of my favorite one-variable theorems,
is Mergelyan's theorem.
It may be good to note that Mergelyan does not follow from Runge.

\begin{thm}[Mergelyan]\index{Mergelyan's theorem} \label{thm:mergelyan}
Suppose $K \subset \subset \C$ is a compact set such that $\C \setminus K$
is connected and
$f \colon K \to \C$ is a continuous function that is
holomorphic in the interior $K^\circ$.
Then for any $\epsilon > 0$ and any compact
$K \subset \subset U$, there exists a polynomial $P$
such that
\begin{equation*}
\sabs{P(z) - f(z)} < \epsilon \qquad \text{for all $z \in K$}.
\end{equation*}
\end{thm}

The reason why the theorem is perhaps
surprising is that $K$ may have only a
small or no interior.  Using a closed interval $K=[a,b]$ of the real line we
recover the Weierstrass approximation theorem.

\medskip

Given an open set $U \subset \C$, we say $U$ is
\emph{\myindex{symmetric with respect to the real axis}} if
$z \in U$ implies $\bar{z} \in U$.  We divide $U$ into
three parts
\begin{equation*}
U_+ = \{ z \in U : \Im z > 0 \}, \qquad
U_0 = \{ z \in U : \Im z = 0 \}, \qquad
U_- = \{ z \in U : \Im z < 0 \}.
\end{equation*}
We have the following theorem for extending (reflecting) holomorphic functions past
boundaries.

\begin{thm}[Schwarz Reflection Principle]
\index{Schwarz reflection principle}%
Suppose $U \subset \C$ is a domain symmetric with respect to the real axis, 
$f \colon U_+ \cup U_0 \to \C$ a continuous function holomorphic on $U_+$
and real valued on $U_0$.  Then the function $g \colon U \to \C$
defined by
\begin{equation*}
g(z) = f(z) \quad \text{if $z \in U_+ \cup U_0$},
\qquad
g(z) = 
\overline{f(\bar{z})} \quad \text{if $z \in U_-$},
\end{equation*}
is holomorphic on $U$.
\end{thm}

In fact, the reflection is really about harmonic functions.

\begin{thm}[Schwarz reflection principle for harmonic functions]
% this is probably not needed: \index{Schwarz reflection principle for harmonic functions}%
Suppose $U \subset \C$ is a domain symmetric with respect to the real axis, 
$f \colon U_+ \cup U_0 \to \R$ a continuous function harmonic on $U_+$
and zero on $U_0$.  Then the function $g \colon U \to \R$
defined by
\begin{equation*}
g(z) = f(z) \quad \text{if $z \in U_+ \cup U_0$},
\qquad
g(z) = 
-f(\bar{z}) \quad \text{if $z \in U_-$},
\end{equation*}
is harmonic on $U$.
\end{thm}

Functions may be defined locally, and continued along paths.
Suppose $p$ is a point and $D$ is a disc centered at $p \in D$.
A holomorphic function $f \colon D \to \C$ can be
\emph{analytically continued}\index{analytic continuation}
along a path
$\gamma \colon [0,1] \to \C$, $\gamma(0) = p$,
if for every $t \in [0,1]$ there exists
a disc $D_t$ centered at $\gamma(t)$, where $D_0=D$, and a holomorphic function
$f_t \colon D_t \to \C$, where $f_0 = f$, and for each $t_0 \in [0,1]$ there is an
$\epsilon > 0$ such that if $\sabs{t-t_0} < \epsilon$, then
$f_t = f_{t_0}$
in $D_t \cap D_{t_0}$.  The monodromy theorem says that as long as there are
no holes, analytic continuation defines a function uniquely.

\begin{thm}[Monodromy theorem]\index{Monodromy theorem}
If $U \subset \C$ is a simply connected domain, $D \subset U$ a disc and
$f \colon D \to \C$ a holomorphic function that can be analytically
continued from $p \in D$ to every $q \in U$ along any path from $p$ to $q$, then there exists
a unique holomorphic function $F \colon U \to \C$ such that $F|_D = f$.
\end{thm}

\medskip

An interesting and useful theorem getting an inequality in the opposite
direction from
Schwarz's lemma, and one which is often not covered in a one-variable
course is the Koebe $\frac{1}{4}$-theorem.
Think of why no such theorem could possibly hold for just smooth
functions.  At first glance the theorem should seem quite counterintuitive,
and at second glance, it should seem outright outrageous.

\begin{thm}[Koebe quarter theorem]\index{Koebe quarter theorem}\index{Koebe
$\frac{1}{4}$-theorem}
Suppose $f \colon \D \to \C$ is holomorphic and injective.  Then
$f(\D)$ contains a disc centered at $f(0)$ and radius $\frac{\sabs{f'(0)}}{4}$.
\end{thm}

The $\frac{1}{4}$ is sharp, that is, it is the best it can be.

\medskip

Finally, it is useful to factor out all the zeros of a holomorphic function,
not just finitely many.  Similarly, we can work with poles.

\begin{thm}[Weierstrass product theorem]\index{Weierstrass product theorem}
Suppose $U \subset \C$ is a domain, $\{ a_k \}$, $\{ b_k \}$ are
countable sets in $U$
with no limit points in $U$, and $\{ n_k \}$, $\{ m_k \}$ any countable sets of
natural numbers.
Then there exists a meromorphic function $f$ of $U$ whose
zeros are exactly at $a_k$, with orders given by $n_k$, and
poles are exactly at $b_k$, with orders given by $m_k$.
\end{thm}

For a more explicit statement, we need infinite products.  The product
$\prod_{j=1}^\infty (1+a_j)$
\emph{converges} if the sequence of partial products
$\prod_{j=1}^n (1+a_j)$ converges.  We say that the product
\emph{converges absolutely} if
$\prod_{j=1}^\infty (1+\sabs{a_j})$
converges, which is equivalent to $\sum_{j=1}^\infty \sabs{a_j}$ converging.

Define
\begin{equation*}
E_0(z) = (1-z), \qquad
E_m(z) = (1-z) \exp\left( z +\frac{z^2}{2} + \cdots + \frac{z^m}{m} \right)
.
\end{equation*}
The function $E_m\bigl(\nicefrac{z}{a}\bigr)$ has a zero of order 1 at $a$.

\begin{thm}[Weierstrass factorization theorem]\index{Weierstrass factorization theorem}
Let $f$ be an entire function with zeros (repeated according to multiplicity) at points of
the sequence $\{ a_k \}$ except the zero at
the origin, whose order is $m$ (possibly $m=0$).  Then there exists an
entire function $g$ and a sequence $\{ p_k \}$ such that
\begin{equation*}
f(z) = z^m e^{g(z)} \prod_{k=1}^\infty E_{p_k}\left(\frac{z}{a_k}\right) ,
\end{equation*}
converges uniformly absolutely on compact subsets.
\end{thm}

The $p_k$ are chosen such that
\begin{equation*}
\sum_{j=1}^\infty {\abs{\frac{r}{a_k}}}^{1+p_k}
\end{equation*}
converges for all $r > 0$.

\begin{center}
* * *
\end{center}

There are many other useful theorems in one complex variable, and we could
spend a lot of time listing them all.
However, hopefully the listing above is useful
as a refresher for the reader of the most common results, some of which are
used in this book, some of which are useful in the exercises, and some of
which are just too interesting not to mention.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Differential forms and Stokes' theorem} \label{ap:diffforms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Differential forms come up quite a bit in this book, especially
in \chapterref{ch:dbar} and \chapterref{ch:integralkernels}.
Let us overview their definition
and state the general Stokes' theorem.  No proofs are given, this appendix
is just a bare bones guide.
For a more complete introduction to differential forms,
see Rudin~\cite{Rudin:principles}.

The short story about differential forms is that a $k$-form is an object
that can be integrated (summed) over a $k$-dimensional object,
taking orientation into account.
For simplicity, as in most of this book,
everything in this appendix is stated for smooth
($C^\infty$) objects to avoid worrying about how much regularity is needed.

The main point of differential forms is to find the proper context for the
Fundamental Theorem of Calculus,
\begin{equation*}
\int_a^b f'(x) \, dx = f(b)-f(a) .
\end{equation*}
We interpret both sides as integration.  The left-hand side is an integral
of the $1$-form $f'\, dx$ over the $1$-dimensional interval $[a,b]$
and the right-hand side is an integral of the $0$-form, that is a  function,
$f$ over the $0$-dimensional (two-point) set $\{ a, b \}$.  Both sides
consider orientation, $[a,b]$ is integrated from $a$ to $b$,
$\{a\}$ is oriented negatively and 
$\{b\}$ is oriented positively.  The two-point set
$\{a,b\}$ is the boundary of $[a,b]$, and the orientation of $\{ a,b \}$
is induced by $[a,b]$.

Let us define the objects over which we integrate, that is,
smooth submanifolds of $\R^n$.
Our model for a $k$-dimensional submanifold-with-boundary
is the upper-half-space and its boundary:
\begin{equation*}
\bH^k
\overset{\text{def}}{=}
\{ x \in \R^k : x_k \geq 0 \} ,
\qquad
\partial \bH^k
\overset{\text{def}}{=}
\{ x \in \R^k : x_k = 0 \} ,
\end{equation*}

\begin{defn}
Let $M \subset \R^n$ have the induced subspace topology.
Let $k \in \N_0$.
Let $M$ have the property that
for each $p \in M$, there exists a neighborhood $W \subset \R^n$ of $p$,
a point $q \in \bH^k$, a neighborhood $U \subset \bH^k$ of $q$, 
and a smooth one-to-one open\footnote{By open, we mean that $\varphi(V)$ is
a relatively open set of $M$ for every open set $V \subset U$.}
mapping $\varphi \colon U \to M$ such that
$\varphi(q) = p$, 
the derivative $D\varphi$ has rank $k$ at all points, and $\varphi(U) = M \cap W$.
Then $M$ is an
\emph{\myindex{embedded submanifold-with-boundary}}\index{submanifold-with-boundary}
of dimension $k$.
The map $\varphi$ is called a
\emph{\myindex{local parametrization}}\index{parametrization}.
If $q$ is such that $q_k = 0$ (the last component is zero), then
$p = \varphi(q)$ is a \emph{boundary point}\index{boundary of a submanifold}.
\glsadd{not:boundary}%
Let $\partial M$ denote the set
of boundary points.  If $\partial M = \emptyset$, then we say
$M$ is simply an \emph{\myindex{embedded submanifold}}\index{submanifold}.
\end{defn}

The situation for a boundary point and an interior point is depicted in
\figureref{fig:maif-bound}.
Note that $W$ is a bigger neighborhood in $\R^n$ than
the image $\varphi(U)$.

\begin{myfig}
\subimport*{figures/}{manif-bound.pdf_t}
\caption{Parametrization at
an interior and a boundary point of a submanifold.\label{fig:maif-bound}}
\end{myfig}

Completely correctly, we should say \emph{submanifold of $\R^k$}.
Sometimes people (including me)
say \emph{\myindex{manifold}} when they mean \emph{submanifold}.
A manifold is a more abstract concept, but
all submanifolds are manifolds.
The word \emph{embedded} has to do with the topology on $M$, and this
has to do with the condition $\varphi(U) = M \cap W$ and $\varphi$ being open.
The condition means that $\varphi$ is a homeomorphism onto $M \cap W$.
It is important that $W$ is an open set in $\R^n$.  For our
purposes here, all submanifolds will be embedded.
We have also made some economy in the definition.  If $q$ is not on the
boundary of $\bH^k$, then we might as well have used $\R^k$ instead of
$\bH^k$.
A submanifold is something that is locally like $\R^k$, and if it has a
boundary, then near the boundary it is locally like $\bH^k$ near a point
of $\partial \bH^k$.

We also remark that submanifolds are often defined in reverse rather than by
parametrizations, that is, by starting with the (relatively) open sets $M \cap W$,
and the maps $\varphi^{-1}$, calling those \emph{\myindex{charts}},
and calling the entire set of charts an \emph{\myindex{atlas}}.
The particular version of the definition we have chosen makes 
it easy to evaluate integrals in the same way that parametrizing curves
makes it easy to evaluate integrals.

Examples of such submanifolds are domains with smooth boundaries
as in \defnref{def:hypersurface}, we can take the inclusion map $x \mapsto
x$ as our parametrization.  The domain is then the submanifold $M$
and $\partial M$ is the boundary of the domain.
Domains are the key application for our purposes.
Another example are smooth curves.

If $M$ is an embedded submanifold-with-boundary
of dimension $k$, then $\partial M$ is also an embedded submanifold of dimension
$k-1$.  Simply restrict the parametrizations to the boundary of $\bH^k$.

We also need to define an orientation.

\begin{defn}
Let $M \subset \R^n$ be an embedded submanifold-with-boundary of dimension $k
\geq 2$.  Suppose
a set of parametrizations can be chosen such that
each point of $M$ is in the image of one of the parametrizations,
and
if
$\varphi \colon U \to M$ and $\widetilde{\varphi} \colon \widetilde{U} \to
M$ are two parametrizations such
that $\varphi(U) \cap \widetilde{\varphi}(\widetilde{U}) \not= \emptyset$, then the 
\emph{\myindex{transition map}} (automatically smooth) defined by
\begin{equation*}
{\widetilde{\varphi}}^{~{-1}} \circ \varphi
\end{equation*}
on $\varphi^{-1}\bigl(\varphi(U) \cap \widetilde{\varphi}(\widetilde{U})\bigr)$ (in other
words, wherever it makes sense) is orientation preserving, that is 
\begin{equation*}
\det D\bigl({\widetilde{\varphi}}^{~{-1}} \circ \varphi \bigr) > 0 
\end{equation*}
at all points.
The set of such parametrizations is the \emph{\myindex{orientation}} on $M$,
and we usually take the maximal set of such parametrizations.

If $M$ is oriented, then the restrictions of the parametrizations
to $\partial \bH^k$ give an orientation on $\partial M$.  We say
this is the \emph{\myindex{induced orientation}} on $\partial M$.
\end{defn}

For dimensions $k=0$ (isolated points) and $k=1$ (curves) we must define
orientation differently.  For $k=0$, we simply give each point an
orientation of $+$ or $-$.  For $k=1$, we need to allow parametrization by
open subsets not only of $\bH^1 = [0,\infty)$, but also $-\bH^1 = (-\infty,0]$.
The definition is the same otherwise.  To define the orientation
of the boundary, if the boundary point corresponds to the $0$ in
$[0,\infty)$ we give this boundary point the orientation
$-$, and if it corresponds to the $0$ in $(-\infty,0]$, then
we give this point the orientation $+$.  The reason for this complication is
that unlike in $\R^k$ for $k \geq 2$, the set $\bH^1 = [0,\infty)$ cannot be
``rotated'' (in $\R^1$) or mapped via an orientation preserving map onto
$-\bH^1 = (-\infty,0]$, but in $\R^2$ the upper-half-plane $\bH^2$ can
be rotated to the lower-half-plane $-\bH^2 = \{ x \in \R^2 : x_2 \leq 0 \}$.
For computations, it is often useful for compact curves
with endpoints (boundary) to just give one
parametrization from $[0,1]$ or perhaps $[a,b]$, then $a$ corresponds to the
$-$ and $b$ corresponds to the $+$.

The fact that the transition map is smooth does require a proof, which is a
good exercise in basic analysis.  It requires a bit of care at boundary
points.

An orientation allows us to have a well-defined integral on $M$, just
like a curve needs to be oriented in order to define a line integral.
However, unlike for curves, not every submanifold of dimension
more than one is \emph{orientable}, that is, admits an orientation.
A classical nonorientable example is the M\"obius strip.

\medskip

Now that we know ``on'' what we integrate, let us figure out what ``it'' is
that we integrate.  Let us start with $0$-forms.
We define $0$-forms as smooth functions (possibly complex-valued).
Sometimes we need a function defined just on a submanifold.  A function $f$ defined
on a submanifold $M$ is smooth when $f \circ \varphi$ is smooth on $U$
for every parametrization $\varphi \colon U \to M$.  Equivalently,
one can prove that $f$ is the restriction of some smooth function
defined on some neighborhood of $M$ in $\R^n$.

A $0$-form $\omega$ defined on a $0$-dimensional oriented submanifold $M$
is integrated as
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{p \in M} \epsilon_p \omega(p) ,
\end{equation*}
where $\epsilon_p$ is the orientation of $p$ given as $+1$ or $-1$.
To avoid problems of integrability, 
one can assume that $\omega$ is compactly supported (it is
nonzero on at most finitely many points of $M$) or that $M$ is compact (it
is a finite set).

The correct definition of a $1$-form is that it is a ``smooth section'' of
the dual of the vector bundle $T \R^n$.  That is, it is something that eats
a vector field, and spits out a function.  The $1$-form
$dx_k$ is supposed to be the object that does
\begin{equation*}
dx_k\left(
\frac{\partial}{\partial x_k}
\right)
=
1,
\qquad
dx_k\left(
\frac{\partial}{\partial x_j}
\right)
=
0 \quad \text{if $j\not=k$}.
\end{equation*}
For our purposes here, just suppose that a $1$-form
in $\R^n$ is an object of the form
\begin{equation*}
\omega = g_1 dx_1 + g_2 dx_2 + \cdots + g_n dx_n ,
\end{equation*}
where $g_1, g_2, \ldots, g_n$ are smooth functions.  That is, a $1$-form is
at each point a linear combination of $dx_1, dx_2, \ldots, dx_n$ that
varies smoothly from point to point.
Suppose $M$ is a one-dimensional submanifold
(possibly with boundary), $\varphi \colon U \to M$
is a parametrization compatible with the orientation of $M$,
and $g_j$ is supported in $\varphi(U)$.
Define
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{j=1}^n
\int_U g_j\bigl( \varphi(t) \bigr) \varphi_j'(t) \, dt ,
\end{equation*}
where the integral $\int_U \cdots \, dt$
is evaluated with the usual positive orientation
(left to right) as $U \subset \R$, and $\varphi_j$ is the $j$th component
of $\varphi$.

Generally, a $1$-form has support bigger than just $\varphi(U)$.  In this
case, one needs to use a so-called partition of unity to write $\omega$
as a locally finite sum
\begin{equation*}
\omega = \sum_{\ell} \omega_\ell ,
\end{equation*}
where each $\omega_\ell$ has support in the image of a single
parametrization.  By locally finite, we mean that on each compact
neighborhood only finitely many $\omega_\ell$ are nonzero.  
Define
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{\ell}
\int_M \omega_\ell .
\end{equation*}
The definition makes sense only
if this sum actually exists.  For example, if $\omega$ is
compactly supported, then this sum is only finite, and so it exists.

Higher degree forms are constructed out of $1$-forms and $0$-forms
by the so-called wedge product.  Given a $k$-form $\omega$
and an $\ell$-form $\eta$,
\glsadd{not:wedge}%
\begin{equation*}
\omega \wedge \eta
\end{equation*}
is a $(k+\ell)$-form.  We require the wedge product to be bilinear at each point:
If $f$ and $g$ are smooth functions, then
\begin{equation*}
(f \omega + g \eta) \wedge \xi =
f (\omega \wedge \xi) + g (\eta \wedge \xi)
, \qquad
\text{and}
\qquad
\omega \wedge (f \eta + g \xi) = 
f (\omega \wedge \eta ) +
g ( \omega \wedge \xi) . 
\end{equation*}
The wedge product is not commutative; we require it to be 
anticommutative on $1$-forms. If $\omega$ and $\eta$ are $1$-forms,
then
\begin{equation*}
\omega \wedge \eta = - \eta \wedge \omega .
\end{equation*}
The negative keeps track of orientation.
When $\omega$ is a $k$-form and $\eta$ is an $\ell$-form, 
\begin{equation*}
\omega \wedge \eta = {(-1)}^{k\ell} \eta \wedge \omega .
\end{equation*}

We wedge together the basis $1$-forms to
get all $k$-forms.
A $k$-form is then an expression
\begin{equation*}
\omega =
\sum_{j_1=1}^n
\sum_{j_2=1}^n
\cdots
\sum_{j_k=1}^n
g_{j_1,\ldots,j_k}
\,
dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k}  ,
\end{equation*}
where $g_{j_1,\ldots,j_k}$ are smooth functions.
We can simplify even more.  Since the wedge
is anticommutative on $1$-forms,
\begin{equation*}
dx_j \wedge dx_m = 
-dx_m \wedge dx_j
,
\qquad
\text{and}
\qquad
dx_j \wedge dx_j = 0 .
\end{equation*}
In other words,
every form 
$dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k}$ 
is either zero, if any two indices from $j_1,\ldots,j_k$ are equal, or
can be put into the form
$\pm dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k}$, where $j_1 < j_2 < \cdots < j_k$.
Thus, a $k$-form can always be written as
\begin{equation*}
\omega =
\sum_{1 \leq j_1 < j_2 < \cdots < j_k \leq n}
g_{j_1,\ldots,j_k}
\,
dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k}  .
\end{equation*}

Consider
an oriented $k$-dimensional submanifold $M$
(possibly with boundary), a parametrization $\varphi \colon U \to M$
from the orientation,
and a $k$-form $\omega$
supported in $\varphi(U)$ (that is each $g_{j_1,\ldots,j_k}$ is supported in
$\varphi(U)$).
Denote by $t \in U \subset \R^k$
the coordinates on $U$.  Define
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{1 \leq j_1 < j_2 < \cdots < j_k \leq n}
\int_U
g_{j_1,\ldots,j_k}(t)
\det D (\varphi_{j_1},\varphi_{j_2},\ldots,\varphi_{j_k})
\,
dt
\end{equation*}
where the integral $\int_U \cdots\, dt$ is evaluated in the
usual orientation on $\R^k$ with $dt$ the standard
measure on $\R^k$ (think $dt = dt_1 dt_2 \cdots dt_n$), and
$D (\varphi_{j_1},\varphi_{j_2},\ldots,\varphi_{j_k})$
denotes the derivative of the mapping whose $\ell$th component
is $\varphi_{j_\ell}$.

Similarly as before, if $\omega$ is not supported in the
image of a single parametrization, write
\begin{equation*}
\omega = \sum_{\ell} \omega_\ell 
\end{equation*}
as a locally finite sum,
where each $\omega_\ell$ has support in the image of a single
parametrization of the orientation.
Then
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{\ell}
\int_M \omega_\ell .
\end{equation*}
Again, the sum has to exist, such as when $\omega$ is compactly supported
and the sum is finite.

The only nontrivial differential forms on $\R^n$
are $0,1,2,\ldots,n$ forms.  The only $n$-forms are
object of the form
\begin{equation*}
f(x) \,
dx_1 \wedge dx_2 \wedge \cdots \wedge dx_n .
\end{equation*}
The form $dx_1 \wedge dx_2 \wedge \cdots \wedge dx_n$ is called the
volume form.  Integrating it over a domain (an $n$-dimensional submanifold)
gives the standard volume integral.

More generally one defines integration of $k$-forms over $k$-chains,
which are just linear combinations of smooth submanifolds, but we do not
need that level of generality.

\medskip

In computations, we can avoid sets of zero measure ($k$-dimensional),
so we can ignore the boundary of the submanifold.  Similarly, if we parametrize
several subsets we can leave out a measure zero subset.
Let us give a couple of examples of computations.

\begin{example} \label{example:diffformscircleint}
Consider the circle $S^1 \subset
\R^2$.  We use a parametrization $\varphi \colon (-\pi,\pi) \to S^1$
where $\varphi(t) = \bigl(\cos(t),\sin(t)\bigr)$, so the circle is oriented
counter-clockwise.
Let $\omega(x_1,x_2) = P(x_1,x_2) \, dx_1 + Q(x_1,x_2) \, dx_2$, then
\begin{equation*}
\int_{S^1} \omega =
\int_{-\pi}^{\pi} 
\Bigl(
P\bigl(\cos(t),\sin(t)\bigr) \bigl(-\sin(t)\bigr)  +
Q\bigl(\cos(t),\sin(t)\bigr) \cos(t) \Bigr) \, dt .
\end{equation*}
We can ignore the point $(-1,0)$ as a single point is of $1$-dimensional
measure zero.
\end{example}

\begin{example}
Consider a domain $U \subset \R^n$, then $U$ is an oriented submanifold.
We use the parametrization $\varphi \colon U \to U$, where $\varphi(x) =
x$.  Then
\begin{equation*}
\int_U f(x) \, dx_1 \wedge dx_2 \wedge \cdots \wedge dx_n
=
\int_U f(x) \, dx_1 \, dx_2 \,  \cdots \, dx_n
=
\int_U f(x) \, dV ,
\end{equation*}
where $dV$ is the standard volume measure.
\end{example}

\begin{example}
Finally, consider $M$
the upper hemisphere of the unit sphere $S^2 \subset \R^3$ as a submanifold
with boundary.  That is consider
\begin{equation*}
M = \bigl\{ x \in \R^3 : x_1^2+x_2^2+x_3^2=1, x_3 \geq 0 \bigr\} .
\end{equation*}
The boundary is the circle in the $(x_1,x_2)$-plane:
\begin{equation*}
\partial M = \bigl\{ x \in \R^3 : x_1^2+x_2^2=1, x_3 = 0 \bigr\} .
\end{equation*}
Consider the parametrization of $M$ using the spherical coordinates
\begin{equation*}
\varphi(\theta,\psi) =
\bigl(\cos(\theta) \sin(\psi),
 \sin(\theta) \sin(\psi),
 \cos(\psi)\bigr)
\end{equation*}
for $U$ given by $-\pi < \theta < \pi$, $0 < \psi \leq \nicefrac{\pi}{2}$.  After
a rotation this is a subset of a half-plane with the points corresponding to
$\psi = \nicefrac{\pi}{2}$ corresponding to boundary points.  We miss the
points where $\theta = \pi$, including the point $(0,0,1)$, but the set of
those points is a $1$-dimensional curve, and so a set of $2$-dimensional
measure zero. For the purposes of integration we can ignore it.
Let
\begin{equation*}
\omega(x_1,x_2,x_3) =
P(x_1,x_2,x_3) \, dx_1 \wedge dx_2
+ Q(x_1,x_2,x_3) \, dx_1 \wedge dx_3
+ R(x_1,x_2,x_3) \, dx_2 \wedge dx_3 .
\end{equation*}
Then
\begin{equation*}
\begin{split}
\int_M \omega & = 
\int_{-\pi}^\pi
\int_{0}^{\pi/2}
\biggl[
P\bigl(\varphi(\theta,\psi)\bigr) \biggl(
\frac{\partial \varphi_1}{\partial \theta}
\frac{\partial \varphi_2}{\partial \psi}
-
\frac{\partial \varphi_2}{\partial \theta}
\frac{\partial \varphi_1}{\partial \psi}
\biggr)
\\
& \hspace{1.8cm}
+
Q\bigl(\varphi(\theta,\psi)\bigr) \biggl(
\frac{\partial \varphi_1}{\partial \theta}
\frac{\partial \varphi_3}{\partial \psi}
-
\frac{\partial \varphi_3}{\partial \theta}
\frac{\partial \varphi_1}{\partial \psi}
\biggr)
\\
& \hspace{1.8cm}
+
R\bigl(\varphi(\theta,\psi)\bigr) \biggl(
\frac{\partial \varphi_2}{\partial \theta}
\frac{\partial \varphi_3}{\partial \psi}
-
\frac{\partial \varphi_3}{\partial \theta}
\frac{\partial \varphi_2}{\partial \psi}
\biggr)
\biggr]\, d\theta \, d\psi
\\
& =
\int_{-\pi}^\pi
\int_{0}^{\pi/2}
\Bigl[
P\bigl(\cos(\theta)\sin(\psi),\sin(\theta)\sin(\psi),\cos(\psi)\bigr)
\bigl(-\cos(\psi)\sin(\psi) \bigr)
\\
& \hspace{1.8cm}
+
Q\bigl(\cos(\theta)\sin(\psi),\sin(\theta)\sin(\psi),\cos(\psi)\bigr)
\sin(\theta)\sin^2(\psi)
\\
& \hspace{1.8cm}
+
R\bigl(\cos(\theta)\sin(\psi),\sin(\theta)\sin(\psi),\cos(\psi)\bigr) 
\bigl(-\cos(\theta)\sin^2(\psi)\bigr)
\Bigr]
\,
d\theta
\,
d\psi .
\end{split}
\end{equation*}

The induced orientation on the boundary $\partial M$
is the counter-clockwise orientation used in
\exampleref{example:diffformscircleint},
because that is the parametrization we get when we restrict
to the boundary, $\varphi(\theta,\nicefrac{\pi}{2}) =
\bigl(\cos(\theta),\sin(\theta),0\bigr)$.
\end{example}

\medskip

The derivative on $k$-forms is the
\emph{\myindex{exterior derivative}},
which is a linear operator that eats $k$-forms and spits out
$(k+1)$-forms.  For a $k$-form
\begin{equation*}
\omega =
g_{j_1,\ldots,j_k}
\,
dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k}  ,
\end{equation*}
define the exterior derivative $d\omega$ as
\glsadd{not:dpsi}%
\begin{equation*}
d\omega
\overset{\text{def}}{=} 
dg_{j_1,\ldots,j_k}
\wedge
dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k}  =
\sum_{\ell=1}^n
\frac{\partial g_{j_1,\ldots,j_k}}{\partial x_\ell} \,
dx_\ell \wedge
dx_{j_1} \wedge
dx_{j_2} \wedge
\cdots \wedge
dx_{j_k} .
\end{equation*}
Then define $d$ on every $k$-form by extending it linearly.

For example,
\begin{multline*}
d \left(
P \, dx_2 \wedge dx_3
+
Q \, dx_3 \wedge dx_1
+
R \, dx_1 \wedge dx_2
\right)
\\
=
\frac{\partial P}{\partial x_1} \, dx_1 \wedge dx_2 \wedge dx_3
+
\frac{\partial Q}{\partial x_2} \, dx_2 \wedge dx_3 \wedge dx_1
+
\frac{\partial R}{\partial x_3} \, dx_3 \wedge dx_1 \wedge dx_2
\\
=
\left(
\frac{\partial P}{\partial x_1}
+
\frac{\partial Q}{\partial x_2} 
+
\frac{\partial R}{\partial x_3}
\right) \, dx_1 \wedge dx_2 \wedge dx_3 .
\end{multline*}
You should recognize the divergence of the vector field $(P,Q,R)$ from vector
calculus.
All the various derivative operations in $\R^3$ from vector calculus make an
appearance.
If $\omega$ is a $0$-form in $\R^3$, then $d\omega$ is like the gradient.
If $\omega$ is a $1$-form in $\R^3$, then $d\omega$ is like the curl.
If $\omega$ is a $2$-form in $\R^3$, then $d\omega$ is like the divergence.

\medskip

Something to notice is that
\begin{equation*}
d(d\omega) = 0 
\end{equation*}
for every $\omega$,
which follows because partial derivatives commute.  In particular, we get a
so-called complex:  If
$\Lambda^k(M)$ denotes the $k$-forms on an $n$-dimensional submanifold $M$,
then we get the complex
\begin{equation*}
\Lambda^0(M)
\overset{d}{\to}
\Lambda^1(M)
\overset{d}{\to}
\Lambda^2(M)
\overset{d}{\to}
\cdots
\overset{d}{\to}
\Lambda^n(M)
\overset{d}{\to}
0 .
\end{equation*}
We remark that one can study the topology of $M$
by computing from this complex the cohomology groups,
$\frac{\operatorname{ker}(d \colon \Lambda^k
\to \Lambda^{k+1})}{\operatorname{im}(d \colon \Lambda^{k-1} \to \Lambda^k)}$,
which 
is really about global solvability of the differential
equation $d\omega = \eta$ for an unknown $\omega$.  There are variations
on this idea and one appears in \chapterref{ch:dbar}, but we digress.

\medskip

Let us now state \emph{\myindex{Stokes' theorem}}, sometimes
called the \emph{\myindex{generalized Stokes' theorem}} to distinguish it
from the classical Stokes' theorem you know from vector calculus, which is a special case.

\begin{thm}[Stokes]
Suppose $M \subset \R^n$ is an embedded
compact smooth oriented $(k+1)$-dimensional submanifold-with-boundary,
$\partial M$ has the induced orientation,
and $\omega$ is a smooth $k$-form defined on
$M$.  Then
\begin{equation*}
\int_{\partial M} \omega = \int_{M} d\omega .
\end{equation*}
\end{thm}

One can get away with less regularity, both on $\omega$ and $M$ (and
$\partial M$) including ``corners.''
In $\R^2$, it is easy to state
in more generality, see \thmref{thm:greens}.

A final note is that the classical Stokes' theorem is just the generalized
Stokes' theorem with $n=3$, $k=2$.  Classically instead of 
using differential forms, the line integral is an integral of a vector field
instead of a $1$-form
$\omega$, and its derivative $d\omega$ is the curl operator.

As to at least get a flavor of the theorem, let us prove it in a simpler
setting, which however is often almost good enough, and it is the key idea
in the proof.
Suppose $U \subset \R^n$ is a domain such that for any $k=1,\ldots,n$
there exist two smooth functions $\alpha_k$ and $\beta_k$ and $U$
as a set is given by
\begin{align*}
& (x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n) \in \pi_k(U) ,
\\
& \alpha_k(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)
\leq x_k \leq
\beta_k(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n) ,
\end{align*}
where $\pi_k(U)$ is the projection of $U$ onto the
$(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)$ components.
Orient $\partial U$ as usual.

Write $x' = (x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)$, and let $dV_{n-1}$ be the
volume form for $\R^{n-1}$.
Consider the $(n-1)$-form
\begin{equation*}
\omega = f
dx_1 \wedge \cdots \wedge dx_{k-1} \wedge dx_{k+1} \wedge \cdots \wedge dx_n
.
\end{equation*}
Then $d\omega = \frac{\partial f}{\partial x_k} dx_1 \wedge \cdots \wedge
dx_n$.
By the fundamental theorem of
calculus,
\begin{equation*}
\begin{split}
\int_U d\omega &=
\int_U \frac{\partial f}{\partial x_k} \, dV_n
\\
& =
\int_{\pi_k(U)}
\int_{\alpha_k(x')}^{\beta_k(x')}
\frac{\partial f}{\partial x_k} \, dx_k \, dV_{n-1}
\\
& =
\int_{\pi_k(U)}
f(x_1,\ldots,x_{k-1}, \beta_k(x'), x_{k+1}, \ldots, x_n)
\,dV_{n-1}
\\
& \phantom{=xxx}
-
\int_{\pi_k(U)}
f(x_1,\ldots,x_{k-1}, \alpha_k(x'), x_{k+1}, \ldots, x_n)
\,dV_{n-1}
\\
& = 
\int_{\partial U} \omega .
\end{split}
\end{equation*}
Any $(n-1)$-form can be written as a sum of forms like $\omega$ for various
$k$.  Integrating each one of them in the correct direction provides the
result.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Basic terminology and results from algebra} \label{ap:algebra}

Let us quickly review some basic definitions
and a result or two
from commutative
that we need in
\chapterref{ch:analyticvarieties}.  See a book such as
Zariski--Samuel~\cite{ZariskiSamuel} for a full reference.

\begin{defn}
A set $G$ is called a \emph{\myindex{group}} if it has an operation
$x * y$ defined on it and it satisfies the following axioms:
\begin{enumerate}[({G}1)]
\item If $x \in G$ and $y \in G$, then $x * y \in G$.
\item \emph{(associativity)}
$(x*y)*z = x*(y*z)$ for all $x,y,z \in G$.
\item \emph{(identity)}
There exists an element $1 \in G$ such that
$1*x = x$ for all $x \in G$.
\item \emph{(inverse)}
For every element $x\in G$ there exists an element $x^{-1} \in G$
such that $x * x^{-1} = 0$.
\end{enumerate}
A group $G$ is called \emph{\myindex{abelian}} if it also satisfies:
\begin{enumerate}[resume*]
\item \emph{(commutativity)}
$x*y = y*x$ for all $x,y \in G$.
\end{enumerate}

A subset $K \subset G$ is called a \emph{\myindex{subgroup}}
if $K$ is a group with the same operation as the group $G$.
If $G$ and $H$ are groups, a function $f \colon G \to H$ is a
\emph{\myindex{group homomorphism}}\index{homomorphism}
if it respects the group law, that is, $f(a * b) = f(a) * f(b)$.  If $f$ is
bijective, then it is a
\emph{\myindex{group isomorphism}}\index{isomorphism}.
\end{defn}

An example of a group is a group of automorphisms.  For example, let
$U \subset \C$ be open and suppose $G$ is the set of
biholomorphisms $f \colon U \to U$.  Then $G$ is a group
under composition, but $G$ is not necessarily abelian:  If
$U=\C$, then $f(z)= z+1$ and $g(z)=-z$ are members of $G$, but
$f\circ g (z) = -z+1$ and $g \circ f(z) = -z-1$.



\begin{defn}
A set $R$ is called a \emph{\myindex{commutative ring}}\index{ring} if it has two operations
defined on it, addition $x+y$ and multiplication $xy$, and if it satisfies
the following axioms:
\begin{enumerate}[({A}1)]
\item If $x \in R$ and $y \in R$, then $x+y \in R$.
\item \emph{(commutativity of addition)}
$x+y = y+x$ for all $x,y \in R$.
\item \emph{(associativity of addition)}
$(x+y)+z = x+(y+z)$ for all $x,y,z \in R$.
\item There exists an element $0 \in R$ such that
$0+x = x$ for all $x \in R$.
\item For every element $x\in R$ there exists an element $-x \in R$
such that $x + (-x) = 0$.
\end{enumerate}
\begin{enumerate}[({M}1)]
\item If $x \in R$ and $y \in R$, then $xy \in R$.
\item \emph{(commutativity of multiplication)}
$xy = yx$ for all $x,y \in R$.
\item \emph{(associativity of multiplication)}
$(xy)z = x(yz)$ for all $x,y,z \in R$.
\item There exists an element $1 \in R$ (and $1 \not= 0$) such that
$1x = x$ for all $x \in R$.
\item[(D)] \emph{(distributive law)} $x(y+z) = xy+xz$
for all $x,y,z \in R$.
\end{enumerate}

The ring $R$ is called a \emph{\myindex{field}} if furthermore:
\begin{enumerate}%[({F}1)]
\item[(F)] For every $x\in R$ such that $x \not= 0$ there exists an element
$\nicefrac{1}{x} \in R$
such that $x(\nicefrac{1}{x}) = 1$.
\end{enumerate}

In a commutative ring $R$, the elements $u \in R$ for which
there exists an inverse $\nicefrac{1}{u}$ as above are called
\emph{units}\index{unit}.

If $R$ and $S$ are rings, a function $f \colon R \to S$ is a
\emph{\myindex{ring homomorphism}}\index{homomorphism}
if it respects the ring operations, that is,
$f(a + b) = f(a) + f(b)$ and
$f(ab) = f(a)f(b)$, and such that $f(1) = 1$.
If $f$ is
bijective, then it is called a
\emph{\myindex{ring isomorphism}}\index{isomorphism}.
\end{defn}

Namely, a commutative ring is an abelian additive group (by
additive group we just mean we use $+$ for the operation and
$0$ for the respective identity), with multiplication thrown in.
If the multiplication
also defines a group on the set of nonzero elements, then the ring is a
field.  A ring that is not commutative is one that does not satisfy
commutativity of multiplication.  Some authors define ring
without asking for the existence of $1$.

A ring that often comes up in this book is the ring of holomorphic
functions.  Let $\sO(U)$ be the set of holomorphic functions defined
on an open set $U$.  Pointwise addition and multiplication give
a ring structure on $\sO(U)$.  The set of units is the set of
functions that never vanish in $U$.  The set of units
is a multiplicative group.

\medskip

Given a commutative ring $R$, let $R[x]$ be the set of polynomials
\begin{equation*}
P(x) = c_k x^k + c_{k-1} x^{k-1} + \cdots + c_1 x + c_0 ,
\end{equation*}
where $c_0,\ldots,c_k \in R$.  The integer $k$ is the
\emph{\myindex{degree}} of the polynomial and $c_k$ is the
\emph{\myindex{leading coefficient}} of $P(x)$.  If the
leading coefficient is 1, then $P$ is \emph{\myindex{monic}}.
If $R$ is a commutative ring, then so is $R[x]$.
Similarly, we define the commutative ring $R[x_1,\ldots,x_n]$ of
polynomials in $n$ indeterminates.

The most basic result about polynomials,
\thmref{thm:fundamentalthmalg}
the fundamental theorem of algebra, which states that every
nonconstant polynomial over $R=\C$ has a root, is really a theorem in
one complex variable.

\begin{defn}
Let $R$ be a commutative ring.
A subset $I \subset R$ is
an \emph{\myindex{ideal}} 
if $f \in R$ and $g,h \in I$ implies
that $fg \in I$ and $g+h \in I$.
In short, $I \subset R$ is an additive subgroup such that $RI = I$.

Given a set of elements 
$S \subset R$,
the \emph{\myindex{ideal generated by $S$}}
is the intersection $I$ of all ideals containing $S$.
If $S = \{ f_1,\ldots,f_k \}$ is a finite set, we say $I$ is
\emph{finitely generated}\index{finitely generated ideal},
and we write
\glsadd{not:ideal}%
$I = (f_1,\ldots,f_k)$.

A \emph{\myindex{principal ideal}} is an ideal generated by a single
element.
A commutative ring where every ideal is a principal ideal is called
a \emph{\myindex{principal ideal domain}}\index{PID} or a PID\@.

A commutative ring $R$ is \emph{\myindex{Noetherian}}
if every ideal in $R$ is finitely generated.
\end{defn}

It is not difficult to prove that ``an ideal generated by $S$'' really is
an ideal, that is, the intersection of ideals is an ideal.
If an ideal $I$ is generated by $f_1,\ldots,f_k$, then every
$g \in I$ can be written as
\begin{equation*}
g = c_1 f_1 + \cdots + c_k f_k,
\end{equation*}
for some $c_1,\ldots,c_k \in R$.  Clearly the set of such elements is
the smallest ideal containing $f_1,\ldots,f_k$.

\begin{thm}[Hilbert basis theorem] \label{thm:hilbertbasis}
\index{Hilbert basis theorem}%
If $R$ is a Noetherian commutative ring, then $R[x]$ is Noetherian.
\end{thm}

As the proof is rather short, we include it here.

\begin{proof}
Suppose $R$ is Noetherian, and
$I \subset R[x]$ is an ideal.
Starting with the polynomial $f_1$ of minimal degree in $I$,
construct a (possibly finite) sequence of polynomials $f_1,f_2,\ldots$
such that $f_k$ is the polynomial of minimal degree
from the set $I \setminus (f_1,\ldots,f_{k-1})$.
The sequence of degrees $\deg(f_1),\deg(f_2),\ldots$ is by construction
nondecreasing.
Let $c_k$ be the leading coefficient of $f_k$.

As $R$ is Noetherian, then there exists a finite $k$ such
that $(c_1,c_2,\ldots,c_m) \subset (c_1,c_2,\ldots,c_k)$ for all $m$.
Suppose for contradiction there exists a $f_{k+1}$, that is, the sequence
of polynomials
did not end at $k$.  In particular,
$(c_1,\ldots,c_{k+1}) \subset (c_1,\ldots,c_k)$ or
\begin{equation*}
c_{k+1} = a_1 c_1 + \cdots a_k c_k .
\end{equation*}
As degree of $f_{k+1}$ is at least the degree of $f_1$, through $f_k$,
we can define the polynomial
\begin{equation*}
g =
a_1 x^{\deg(f_{k+1})-\deg(f_1)} f_1
+
a_2 x^{\deg(f_{k+1})-\deg(f_2)} f_2
+
\cdots
+
a_k x^{\deg(f_{k+1})-\deg(f_k)} f_k .
\end{equation*}
The polynomial $g$ has the same degree as $f_{k+1}$,
and in fact it also has the same leading term, 
$c_{k+1}$.  On the other hand $g \in (f_1,\ldots,f_{k})$ while
$f_{k+1} \notin (f_1,\ldots,f_k)$ by construction.  The polynomial
$g-f_{k+1}$ is also not in 
$(f_1,\ldots,f_k)$, but as the leading terms canceled,
$\deg(g-f_{k+1}) < \deg(f_{k+1})$, but that is a contradiction, so $f_{k+1}$
does not exist and
$I = (f_1,\ldots,f_k)$.
\end{proof}

\begin{defn}
An element $f \in R$ is \emph{\myindex{irreducible}}
if whenever $f = gh$ for two elements $g,h \in R$, then either $g$ or
$h$ is a unit.
A commutative ring $R$ is a
\emph{\myindex{unique factorization domain}}\index{UFD}
(UFD) if up to 
multiplication by a unit, every element has a unique factorization into
irreducible elements of $R$.
\end{defn}

One version of a result called the Gauss lemma says that just like
the property of being Noetherian, the property of being a UFD is retained
when we take polynomials.

\begin{thm}[Gauss lemma]\label{thm:gausslemma}
\index{Gauss lemma}%
If $R$ is a commutative ring that is a UFD, then $R[x]$ is a UFD\@.
\end{thm}

The proof is not difficult, but it is perhaps beyond the scope of this book.

%\begin{proof}
%FIXME
%\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%FIXME: else I don't get links, weird
%\def\MR#1{\relax\ifhmode\unskip\spacefactor3000 \space\fi%
  %\href{http://www.ams.org/mathscinet-getitem?mr=#1}{MR#1}}
\def\myDOI#1{\href{http://dx.doi.org/#1}{#1}}



%FIXME
%\cleardoublepage  
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Further reading}
\markboth{FURTHER READING}{FURTHER READING}
\begin{bibchapter}[Further Reading] \label{ch:furtherreading}

%Here we list useful books for extra reading.

\begin{biblist}[\normalsize]

\bib{BER:book}{book}{
   author={Baouendi, M. Salah},
   author={Ebenfelt, Peter},
   author={Rothschild, Linda Preiss},
   title={Real submanifolds in complex space and their mappings},
   series={Princeton Mathematical Series},
   volume={47},
   publisher={Princeton University Press},
   place={Princeton, NJ},
   date={1999},
   pages={xii+404},
   isbn={0-691-00498-6},
   review={\MR{1668103}},
   %review={\MR{1668103 (2000b:32066)}},
}


\bib{Boggess}{book}{
   author={Boggess, Albert},
   title={CR manifolds and the tangential Cauchy-Riemann complex},
   series={Studies in Advanced Mathematics},
   publisher={CRC Press},
   place={Boca Raton, FL},
   date={1991},
   pages={xviii+364},
   isbn={0-8493-7152-X},
   review={\MR{1211412}},
   %review={\MR{1211412 (94e:32035)}},
}

\bib{Chirka}{book}{
   author={Chirka, E. M.},
   title={Complex analytic sets},
   series={Mathematics and its Applications (Soviet Series)},
   volume={46},
   %note={Translated from the Russian by R. A. M. Hoksbergen},
   publisher={Kluwer Academic Publishers Group},
   place={Dordrecht},
   date={1989},
   pages={xx+372},
   isbn={0-7923-0234-6},
   review={\MR{1111477}},
   %review={\MR{1111477 (92b:32016)}},
}





\bib{DAngelo}{book}{
   author={D'Angelo, John P.},
   title={Several complex variables and the geometry of real hypersurfaces},
   series={Studies in Advanced Mathematics},
   publisher={CRC Press},
   place={Boca Raton, FL},
   date={1993},
   pages={xiv+272},
   isbn={0-8493-8272-6},
   review={\MR{1224231}},
   %review={\MR{1224231 (94i:32022)}},
}

\bib{GunningRossi}{book}{
   author={Gunning, Robert C.},
   author={Rossi, Hugo},
   title={Analytic functions of several complex variables},
   publisher={Prentice-Hall Inc.},
   place={Englewood Cliffs, N.J.},
   date={1965},
   pages={xiv+317},
   %review={\MR{0180696 (31 \#4927)}},
   review={\MR{0180696}},
}

\bib{Hormander}{book}{
   author={H{\"o}rmander, Lars},
   title={An introduction to complex analysis in several variables},
   series={North-Holland Mathematical Library},
   volume={7},
   edition={3},
   publisher={North-Holland Publishing Co.},
   place={Amsterdam},
   date={1990},
   pages={xii+254},
   isbn={0-444-88446-7},
   review={\MR{1045639}},
   %review={\MR{1045639 (91a:32001)}},
}

%\bib{HornJohnson}{book}{
%   author={Horn, Roger A.},
%   author={Johnson, Charles R.},
%   title={Matrix analysis},
%   publisher={Cambridge University Press},
%   place={Cambridge},
%   date={1985},
%   pages={xiii+561},
%   isbn={0-521-30586-1},
%   review={\MR{832183}},
%   %review={\MR{832183 (87e:15001)}},
%}

\bib{Krantz}{book}{
   author={Krantz, Steven G.},
   title={Function theory of several complex variables},
   series={The Wadsworth \& Brooks/Cole Mathematics Series},
   edition={2},
   publisher={Wadsworth \& Brooks/Cole Advanced Books \& Software},
   place={Pacific Grove, CA},
   date={1992},
   pages={xvi+557},
   isbn={0-534-17088-9},
   review={\MR{1162310}},
   %review={\MR{1162310 (93c:32001)}},
}


%\bib{Rudin:fanal}{book}{
%   author={Rudin, Walter},
%   title={Functional analysis},
%   series={International Series in Pure and Applied Mathematics},
%   edition={2},
%   publisher={McGraw-Hill Inc.},
%   place={New York},
%   date={1991},
%   pages={xviii+424},
%   isbn={0-07-054236-8},
%   review={\MR{1157815}},
%   %review={\MR{1157815 (92k:46001)}},
%}

\bib{Rudin:ball}{book}{
   author={Rudin, Walter},
   title={Function theory in the unit ball of ${\bf C}^{n}$},
   series={Grundlehren der Mathematischen Wissenschaften [Fundamental
   Principles of Mathematical Science]},
   volume={241},
   publisher={Springer-Verlag},
   place={New York},
   date={1980},
   pages={xiii+436},
   isbn={0-387-90514-6},
   review={\MR{601594}},
   %review={\MR{601594 (82i:32002)}},
}

\bib{Rudin:principles}{book}{
   author={Rudin, Walter},
   title={Principles of mathematical analysis},
   edition={3},
   note={International Series in Pure and Applied Mathematics},
   publisher={McGraw-Hill Book Co., New York-Auckland-D\"usseldorf},
   date={1976},
   pages={x+342},
   review={\MR{0385023}},
}



\bib{Whitney}{book}{
   author={Whitney, Hassler},
   title={Complex analytic varieties},
   publisher={Addison-Wesley Publishing Co., Reading, Mass.-London-Don
   Mills, Ont.},
   date={1972},
   pages={xii+399},
   review={\MR{0387634}},
   %review={\MR{0387634 (52 \#8473)}},
}

\bib{ZariskiSamuel}{book}{
   author={Zariski, Oscar},
   author={Samuel, Pierre},
   title={Commutative algebra, Volume I},
   series={The University Series in Higher Mathematics},
   note={With the cooperation of I.\ S.\ Cohen},
   publisher={D.\ Van Nostrand Company, Inc., Princeton, New Jersey},
   date={1958},
   pages={xi+329},
   review={\MR{0090581}},
}

\end{biblist}
\end{bibchapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\cleardoublepage  
\clearpage  
\phantomsection
\addcontentsline{toc}{chapter}{\indexname}  
\microtypesetup{protrusion=false}
\printindex
\microtypesetup{protrusion=true}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% automake on glossaries doesn't work if the index is before the glossary.
% That's why the List of Notation is last, no other reason.  Problem is
% that printindex does a clearpage which screws up the delayed write18
% that glossaries sets up
%

\begingroup
\renewcommand{\pagelistname}{Page}
\setglossarystyle{long3colheader}
% correctly set up with cellspace
\renewenvironment{theglossary}%
  {\setlength\cellspacetoplimit{4pt}
   \setlength\cellspacebottomlimit{4pt}
   \setlength\LTleft{0pt}
   \setlength\LTright{0pt}
   \markboth{LIST OF NOTATION}{LIST OF NOTATION}
   \begin{longtable}{Sl @{\extracolsep{\fill}} Sl @{\extracolsep{\fill}} Sl}}%
  {\end{longtable}}%
\cleardoublepage
\microtypesetup{protrusion=false}
\printglossary[type=notation] 
\microtypesetup{protrusion=true}
\endgroup

\end{document}
