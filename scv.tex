\documentclass[12pt,openany]{book}

%FIXME: for PRINT run for lulu or kdp, search for %PRINT

%\usepackage{pdf14}

\usepackage[shortlabels,inline]{enumitem}
\usepackage{ifpdf}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{graphbox}
\usepackage{xcolor}
\usepackage{chngcntr}
\usepackage{titlesec}
\usepackage{import}
\usepackage{vogtwidebar}
\usepackage{nicefrac}
\usepackage{mathdots}
\usepackage{microtype}
\usepackage{cancel}
\usepackage{framed}
\usepackage{import}
\usepackage{varioref}
\usepackage{faktor}
\usepackage{tabto}

\usepackage{tikz}
\usetikzlibrary{cd}
\usepackage{rotating}

\usepackage{cellspace}
\usepackage[toc,nopostdot,sort=use,automake]{glossaries}

%Palatino
\usepackage[theoremfont]{newpxtext}
\usepackage[vvarbb]{newpxmath}
\linespread{1.05}
\usepackage[scr=boondoxo]{mathalfa} % but we want the nice fancy script fonts

\usepackage[T1]{fontenc}

%symmetric for web
\usepackage[inner=1.2in,outer=1.2in,top=1in,bottom=1in]{geometry}
%PRINT asymetric for book
%\usepackage[inner=1.4in,outer=1.0in,top=1in,bottom=1in]{geometry}

\usepackage[margin=10pt,font=small,labelfont=bf,labelsep=colon,singlelinecheck=false]{caption}

%PRINT
% (not for the coil version or any version other than crown quatro using the
%  ghostscript conversion)
%Now cut page size a bit.  I'll run it through ghostcript anyway
%to convert to the right size, but this is good for crown quatro
%conversion, don't use for the full letter size versions
%\addtolength{\paperwidth}{-0.25in}
%\addtolength{\paperheight}{-0.5in}
%\addtolength{\topmargin}{-0.13in}
%\addtolength{\oddsidemargin}{-0.125in}
%\addtolength{\evensidemargin}{-0.125in}


\usepackage{url}
\usepackage{imakeidx}
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref} % do NOT set [ocgcolorlinks] here!

%If you have an older tex installation you might need
%to comment out the next line:
%PRINT (COMMENT OUT FOR PRINT)
\usepackage[ocgcolorlinks]{ocgx2} %perhaps run without for lulu/kdp

\usepackage[shortalphabetic,msc-links]{amsrefs}

%\usepackage[all]{hypcap} %done by caption

%\usepackage{draftwatermark}
%\SetWatermarkText{Draft of v4.0 as of \today. May change substantially!}
%\SetWatermarkAngle{90}
%\SetWatermarkHorCenter{0.5in}
%\SetWatermarkColor[gray]{0.7}
%\SetWatermarkScale{0.18}

\definecolor{gray75}{gray}{0.75}

\titleformat{\chapter}[hang]{\Huge\bfseries\filleft}%
{\thechapter\hspace*{10pt}{\textcolor{gray75}{$\backslash\!\!\backslash$}}}%
{10pt}%
{}
[\vspace{-1ex}\rule{0.5\textwidth}{0.5pt}\rule{0.5\textwidth}{1pt}]

\titleformat{\section}[hang]{\Large\bfseries}%
{\thesection\hspace*{10pt}{\textcolor{gray75}{$\backslash$}}}%
{10pt}%
{}%

\titleformat{\subsection}[hang]{\large\bfseries}%
{\thesubsection\hspace*{10pt}{\textcolor{gray75}{$\cdot$}}}%
{10pt}%
{}%

\assignpagestyle{\chapter}{empty}


% Footnotes should use symbols, not numbers.  Numbered footnotes are
% evil, (footmisc no longer conflicts with hyperref)
\usepackage[perpage,symbol*]{footmisc}

\usepackage{footnote}

% Enumitem extra penalties
\setlist[enumerate]{beginpenalty=100,midpenalty=-5}

% discourage pagebreak at end of display, put before \end{equation}
\newcommand{\avoidbreak}{\postdisplaypenalty=100}

\clubpenalty=500
\widowpenalty=500

%\overfullrule=10mm

% useful
\newcommand{\ignore}[1]{}

% analysis/geometry stuff
\newcommand{\ann}{\operatorname{ann}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\Orb}{\operatorname{Orb}}
\newcommand{\hol}{\operatorname{hol}}
\newcommand{\aut}{\operatorname{aut}}
\newcommand{\Aut}{\operatorname{Aut}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\sing}{\operatorname{sing}}
\newcommand{\ord}{\operatorname{ord}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\Arg}{\operatorname{Arg}}
\newcommand{\Log}{\operatorname{Log}}

% reals
\newcommand{\esssup}{\operatorname{ess~sup}}
\newcommand{\essran}{\operatorname{essran}}
\newcommand{\innprod}[2]{\langle #1 | #2 \rangle}
\newcommand{\linnprod}[2]{\langle #1 , #2 \rangle}
\newcommand{\blinnprod}[2]{\bigl\langle #1 , #2 \bigr\rangle}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\Nul}{\operatorname{Nul}}
\newcommand{\Ran}{\operatorname{Ran}}
\newcommand{\sabs}[1]{\lvert {#1} \rvert}
\newcommand{\snorm}[1]{\lVert {#1} \rVert}
\newcommand{\babs}[1]{\bigl\lvert {#1} \bigr\rvert}
\newcommand{\bnorm}[1]{\bigl\lVert {#1} \bigr\rVert}
\newcommand{\Babs}[1]{\Bigl\lvert {#1} \Bigr\rvert}
\newcommand{\Bnorm}[1]{\Bigl\lVert {#1} \Bigr\rVert}
\newcommand{\bbabs}[1]{\biggl\lvert {#1} \biggr\rvert}
\newcommand{\bbnorm}[1]{\biggl\lVert {#1} \biggr\rVert}
\newcommand{\BBabs}[1]{\Biggl\lvert {#1} \Biggr\rvert}
\newcommand{\BBnorm}[1]{\Biggl\lVert {#1} \Biggr\rVert}
\newcommand{\abs}[1]{\left\lvert {#1} \right\rvert}
\newcommand{\norm}[1]{\left\lVert {#1} \right\rVert}

% sets (some)
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\D}{{\mathbb{D}}}
\newcommand{\F}{{\mathbb{F}}}

% consistent
\newcommand{\bB}{{\mathbb{B}}}
\newcommand{\bC}{{\mathbb{C}}}
\newcommand{\bR}{{\mathbb{R}}}
\newcommand{\bZ}{{\mathbb{Z}}}
\newcommand{\bN}{{\mathbb{N}}}
\newcommand{\bQ}{{\mathbb{Q}}}
\newcommand{\bD}{{\mathbb{D}}}
\newcommand{\bF}{{\mathbb{F}}}
\newcommand{\bH}{{\mathbb{H}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bP}{{\mathbb{P}}}
\newcommand{\bK}{{\mathbb{K}}}
\newcommand{\bV}{{\mathbb{V}}}
\newcommand{\CP}{{\mathbb{CP}}}
\newcommand{\RP}{{\mathbb{RP}}}
\newcommand{\HP}{{\mathbb{HP}}}
\newcommand{\OP}{{\mathbb{OP}}}
\newcommand{\sA}{{\mathscr{A}}}
\newcommand{\sB}{{\mathscr{B}}}
\newcommand{\sC}{{\mathscr{C}}}
\newcommand{\sF}{{\mathscr{F}}}
\newcommand{\sG}{{\mathscr{G}}}
\newcommand{\sH}{{\mathscr{H}}}
\newcommand{\sM}{{\mathscr{M}}}
\newcommand{\sN}{{\mathscr{N}}}
\newcommand{\sO}{{\mathscr{O}}}
\newcommand{\sP}{{\mathscr{P}}}
\newcommand{\sQ}{{\mathscr{Q}}}
\newcommand{\sR}{{\mathscr{R}}}
\newcommand{\sS}{{\mathscr{S}}}
\newcommand{\sI}{{\mathscr{I}}}
\newcommand{\sL}{{\mathscr{L}}}
\newcommand{\sK}{{\mathscr{K}}}
\newcommand{\sU}{{\mathscr{U}}}
\newcommand{\sV}{{\mathscr{V}}}
\newcommand{\sX}{{\mathscr{X}}}
\newcommand{\sY}{{\mathscr{Y}}}
\newcommand{\sZ}{{\mathscr{Z}}}
\newcommand{\fS}{{\mathfrak{S}}}

\newcommand{\interior}{\operatorname{int}}

% Topo stuff
\newcommand{\id}{\textit{id}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\Tor}{\operatorname{Tor}}
\newcommand{\Torsion}{\operatorname{Torsion}}
\newcommand{\Ext}{\operatorname{Ext}}
\newcommand{\Hom}{\operatorname{Hom}}

%extra thingies
%\newcommand{\mapsfrom}{\ensuremath{\text{\reflectbox{$\mapsto$}}}}
\newcommand{\from}{\ensuremath{\leftarrow}}
\newcommand{\dhat}[1]{\hat{\hat{#1}}}

\definecolor{mypersianblue}{rgb}{0.11, 0.22, 0.73}

\hypersetup{
    pdfborderstyle={/S/U/W 0.5}, %this just in case ocg isn't there
    %PRINT (for print use the below and comment out the above):
    %pdfborder={0 0 0},
    citecolor=mypersianblue,
    filecolor=mypersianblue,
    linkcolor=mypersianblue,
    urlcolor=mypersianblue,
    pdftitle={Tasty Bits of Several Complex Variables},
    pdfsubject={Several Complex Variables},
    pdfkeywords={several complex variables, complex analysis},
    pdfauthor={Jiri Lebl}
}

% Set up our index
\makeindex

% Very simple indexing
\newcommand{\myindex}[1]{#1\index{#1}}

\author{Ji\v{r}\'i Lebl}

\title{Tasty Bits of Several Complex Variables}

% Don't include subsections
\setcounter{tocdepth}{1}

% Better "outline"
%\setcounter{tocdepth}{2}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{claim}[thm]{Claim}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}

\newtheoremstyle{exercise}% name
  {}% Space above
  {}% Space below
  {\itshape}% Body font
  {}% Indent amount 1
  {\bfseries \itshape}% Theorem head font
  {:}% Punctuation after theorem head
  {.5em}% Space after theorem head 2
  {}% Theorem head spec (can be left empty, meaning "normal")

\newenvironment{exbox}{%
    \def\FrameCommand{\vrule width 1pt \relax\hspace{10pt}}%
    \MakeFramed{\advance\hsize-\width\FrameRestore}%
}{%
    \endMakeFramed
}

\newenvironment{exparts}{%
    \leavevmode\begin{enumerate}[a),noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
}{%
    \end{enumerate}
}
\newenvironment{exnumparts}{%
    \leavevmode\begin{enumerate}[1),noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
}{%
    \end{enumerate}
}
\newenvironment{expartshor}[1]{%
    \begingroup%
    \NumTabs{#1}%
    \leavevmode%
    \par%
    \begin{enumerate*}[a),itemjoin={\tab}]
}{%
    \end{enumerate*}\endgroup\par
}

\newenvironment{myfig}{%
\begin{figure}[h!t]
\noindent\rule{\textwidth}{0.5pt}\vspace{12pt}\par\centering}%
{\par\noindent\rule{\textwidth}{0.5pt}
\end{figure}}

\theoremstyle{exercise}
\newtheorem{exercise}{Exercise}[section]

\newtheoremstyle{example}% name
  {}% Space above
  {}% Space below
  {}% Body font
  {}% Indent amount 1
  {\bfseries}% Theorem head font
  {:}% Punctuation after theorem head
  {.5em}% Space after theorem head 2
  {}% Theorem head spec (can be left empty, meaning "normal")

\theoremstyle{example}
\newtheorem{example}[thm]{Example}

% referencing
\newcommand{\figureref}[1]{\hyperref[#1]{Figure~\ref*{#1}}}
\newcommand{\tableref}[1]{\hyperref[#1]{Table~\ref*{#1}}}
\newcommand{\chapterref}[1]{\hyperref[#1]{chapter~\ref*{#1}}}
\newcommand{\Chapterref}[1]{\hyperref[#1]{Chapter~\ref*{#1}}}
\newcommand{\Chdotref}[1]{\hyperref[#1]{Ch.~\ref*{#1}}}
\newcommand{\appendixref}[1]{\hyperref[#1]{appendix~\ref*{#1}}}
\newcommand{\Appendixref}[1]{\hyperref[#1]{Appendix~\ref*{#1}}}
\newcommand{\sectionref}[1]{\hyperref[#1]{section~\ref*{#1}}}
\newcommand{\subsectionref}[1]{\hyperref[#1]{\S~\ref*{#1}}}
\newcommand{\exerciseref}[1]{\hyperref[#1]{Exercise~\ref*{#1}}}
\newcommand{\exampleref}[1]{\hyperref[#1]{Example~\ref*{#1}}}
\newcommand{\thmref}[1]{\hyperref[#1]{Theorem~\ref*{#1}}}
\newcommand{\propref}[1]{\hyperref[#1]{Proposition~\ref*{#1}}}
\newcommand{\lemmaref}[1]{\hyperref[#1]{Lemma~\ref*{#1}}}
\newcommand{\corref}[1]{\hyperref[#1]{Corollary~\ref*{#1}}}
\newcommand{\defnref}[1]{\hyperref[#1]{Definition~\ref*{#1}}}
\newcommand{\remarkref}[1]{\hyperref[#1]{Remark~\ref*{#1}}}

% List of Symbols/Notation
% rubber: depend notations.tex
\loadglsentries{notations}
\makeglossaries

\begin{document}

\ifpdf
  \pdfbookmark{Title Page}{title}
\fi
\newlength{\centeroffset}
\setlength{\centeroffset}{-0.5\oddsidemargin}
\addtolength{\centeroffset}{0.5\evensidemargin}
%\addtolength{\textwidth}{-\centeroffset}
\thispagestyle{empty}
\vspace*{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\Huge\bfseries \sffamily Tasty Bits of Several Complex Variables }
\noindent\rule[-1ex]{\textwidth}{5pt}\\[2.5ex]
\hfill\emph{\Large \sffamily A Whirlwind Tour of the Subject }
\end{minipage}}

\vspace{\stretch{1}}
\noindent\hspace*{\centeroffset}\makebox[0pt][l]{\begin{minipage}{\textwidth}
\flushright
{\bfseries
%by
Ji{\v r}\'i Lebl\\[3ex]}
\today
\\
(version 4.2)
\end{minipage}}

%\addtolength{\textwidth}{\centeroffset}
\vspace{\stretch{2}}


\pagebreak

\vspace*{\fill}

%\begin{small}
\noindent
Typeset in \LaTeX.

\bigskip

\noindent
Copyright \copyright 2014--2025 Ji{\v r}\'i Lebl

%PRINT
% not for the coil version
%\noindent
%ISBN 979-8866906680

%PRINT only for the kdp version
%\medskip
%\noindent
%Cover image: Hike to Emigrant Peak, Montana,
%\copyright 2022 Ji{\v r}\'i Lebl, all rights reserved.
% not needed as the kdp version is now non-free
%Cover image cannot be reused in derivative works.

%PRINT
% note that I am the copyright holder, I can put out versions
% that are under any license, so for kdp, they are non-free
% because kdp is weird.
%\ignore{

\bigskip

%\begin{floatingfigure}{1.4in}
%\vspace{-0.05in}
\noindent
\includegraphics[width=1.38in]{figures/license}
\quad
\includegraphics[width=1.38in]{figures/license2}
%\end{floatingfigure}

\bigskip

\noindent
\textbf{License:}
\\
This work
%PRINT for the kdp version (not needed anymore)
%(except the cover art)
is dual licensed under
the Creative Commons
Attribution-Non\-commercial-Share Alike 4.0 International License and
the Creative Commons
Attribution-Share Alike 4.0 International License.
To view a
copy of these licenses, visit
\url{https://creativecommons.org/licenses/by-nc-sa/4.0/}
or
\url{https://creativecommons.org/licenses/by-sa/4.0/}
or send a letter to
Creative Commons
PO Box 1866, Mountain View, CA 94042, USA\@.
%Creative Commons, 171 Second Street, Suite 300, San Francisco, California,
%94105, USA.

\bigskip

\noindent
You can use, print, duplicate, and share this book as much as you want.  You can
base your own notes on it and reuse parts if you keep the license the
same.  You can assume the license is either CC-BY-NC-SA or CC-BY-SA\@,
whichever is compatible with what you wish to do.
Your derivative work must use at least one of the licenses.
Derivative works must be prominently marked as such.

%PRINT
%}

\bigskip

\noindent
\textbf{Acknowledgments:}
\\
I would like to thank Debraj Chakrabarti, Anirban Dawn, Alekzander Malcom,
John Treuer, Jianou Zhang, Liz Vivas, Trevor Fancher,
Nicholas Lawson McLean, Alan Sola, Achinta Nandi,
Sivaguru Ravisankar,
Tomas Rodriguez,
Mina Farag,
Frank Wikstr\"om,
Isak Ellmer,
George Roman,
and students in my classes for pointing out typos/errors
and helpful suggestions.
Some of the new material in version 4.0
was inspired by the comments and lecture notes from
Richard L\"ark\"ang and Elizabeth Wulcan.

\bigskip

\noindent
During some of the writing of this book,
the author was in part supported by NSF grant DMS-1362337
and Simons Foundation collaboration grant 710294. 

\bigskip

\noindent
\textbf{More information:}
\\
See \url{https://www.jirka.org/scv/} for more information
(including contacts).

%PRINT
% for kdp because kdp is weird.
%\ignore{

\medskip

\noindent
The \LaTeX\ source for the book is available
for possible modification and customization
at github: \url{https://github.com/jirilebl/scv}

%PRINT
%}


% For large print do this
%\large

\microtypesetup{protrusion=false}
\tableofcontents
\microtypesetup{protrusion=true}

%\addtocontents{toc}{\protect\vspace{-2\baselineskip}}
%\addtocontents{toc}{\protect\vspace{-\baselineskip}}
%\addtocontents{toc}{\protect\enlargethispage{\baselineskip}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction} \label{ch:intro}
\addcontentsline{toc}{chapter}{Introduction}
\markboth{INTRODUCTION}{INTRODUCTION}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This book is a polished version of my course notes for Math 6283, Several
Complex Variables, given in
Spring 2014, Spring 2016, Spring 2019, and Fall 2023 semesters
at the Oklahoma State University.
There is more material than can fit in a one semester class
to allow for several different versions of the course.
In fact, I did a different selection each semester I taught it.
Quite a few exercises of various difficulty are
sprinkled throughout the text, and I hope a reader is
at least attempting or thinking about most of them.
Many are required later in the text.
The reader should attempt exercises in sequence; earlier exercises
can help or even be required to solve later ones.

The prerequisites are a decent knowledge of vector calculus, basic
real analysis, and a working knowledge of complex analysis in one variable.
Measure theory (Lebesgue integral and its convergence theorems) is useful,
but it is not essential except in a couple of places later in the book.
The first two chapters and most of the third
are accessible to beginning graduate students after one semester
of a standard single-variable complex
analysis graduate course.
From time to time (e.g.\ proof of Baouendi--Tr\`eves in
\chapterref{ch:crfunctions},
and most of
\chapterref{ch:dbar}, and \chapterref{ch:integralkernels}),
basic knowledge of differential forms is useful, and
in \chapterref{ch:analyticvarieties}
we use some basic ring theory from algebra.
By design, it can replace the second semester of complex analysis,
with the first semester
perhaps taught with my one-variable book~\cite{Lebl:ca}.

This book is not intended as an exhaustive reference.
It is simply a whirlwind tour of several complex variables.
See the end of the book
for a \hyperref[ch:furtherreading]{list of books} for
reference and further reading.  There are also appendices for
a list of one-variable results, an overview of differential forms,
some basic algebra, measure theory, and other bits and pieces of analysis.
See \appendixref{ap:onevarresults},
\appendixref{ap:diffforms},
\appendixref{ap:algebra}, and
\appendixref{ap:analysis}.

\textbf{Changes in edition 4:}
The major addition of this edition is the greatly
expanded chapter on the
$\bar{\partial}$-problem, \chapterref{ch:dbar}.
Many minor changes and additions throughout, especially in chapters
\ref{ch:holfunc}, \ref{ch:convexity}, and \ref{ch:analyticvarieties},
resulted in some renumberings, including some renumbering of exercises.
Finally, I've added a short
appendix listing some useful results from analysis, including the very
basics of measure theory.
See the detailed listing of changes on the book website:
\url{https://www.jirka.org/scv/}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation, single variable, and Cauchy's formula} \label{sec:motivation}


We start with some standard notation.
We use \glsadd{not:C}$\C$ for complex numbers, \glsadd{not:R}$\R$
for real numbers,
\glsadd{not:Z}$\Z$ for integers,
\glsadd{not:N}$\N = \{ 1,2,3,\ldots \}$ for natural
numbers,
\glsadd{not:i}$i = \sqrt{-1}$.  Throughout this book,
the standard terminology of \emph{\myindex{domain}} means a connected open
set.  We try to avoid using it if connectedness is not needed, but
sometimes we use it just for simplicity.

As complex analysis deals with complex numbers, perhaps we should begin
with $\sqrt{-1}$.  Start with the real numbers, $\R$, and add
$\sqrt{-1}$ into our field.  Call this square root $i$, and write the
complex numbers, $\C$, by identifying $\C$ with $\R^2$ using
\begin{equation*}
z = x+iy,
\end{equation*}
where $z \in \C$ and $(x,y) \in \R^2$.
A subtle philosophical issue is that there are two square roots of $-1$.
Two chickens\index{chicken!imaginary} are running around in our yard, and because we like to
know which is which, we catch one and write ``$i$'' on it.  If we happened
to have caught the other chicken, we would have got an exactly equivalent
theory, which we could not tell apart from the original.

Given a complex number $z$, its ``opposite'' is
the \emph{\myindex{complex conjugate}} of $z$ and is defined as
\glsadd{not:conj}%
\begin{equation*}
\bar{z} \overset{\text{def}}{=} x-iy.
\end{equation*}
The size of $z$ is measured by the so-called \emph{\myindex{modulus}},
which is just the \emph{\myindex{Euclidean distance}}:
\glsadd{not:mod}%
\begin{equation*}
\abs{z} \overset{\text{def}}{=} \sqrt{z \bar{z}} = \sqrt{x^2+y^2} .
\end{equation*}

If $z = x+iy \in \C$ for $x,y \in \R$, then $x$ is called the
\emph{\myindex{real part}} and $y$ is called the
\emph{\myindex{imaginary part}}.  We write
\glsadd{not:real}%
\glsadd{not:imag}%
\begin{equation*}
\Re z =
\Re (x+iy) =
\frac{z+\bar{z}}{2}
= x, \qquad
\Im z =
\Im (x+iy) =
\frac{z-\bar{z}}{2i}
=
y .
\end{equation*}


A function $f \colon U \subset \R^n \to \C$ for an open set $U$
is said to be continuously differentiable, or $C^1$ if the first (real)
partial derivatives exist and are continuous.
\glsadd{not:Ck}%
Similarly, it is $C^k$ or \emph{$C^k$-smooth}
\index{Ck-smooth function@$C^k$-smooth function}
if the first $k$ partial derivatives all exist and are continuous.
\glsadd{not:Cinfty}%
Finally, a function is said to be $C^\infty$ or simply
\emph{smooth}\index{smooth function}\footnote{%
While $C^\infty$ is a common definition of \emph{smooth}, not everyone
always means
the same thing by the word \emph{smooth}.  I have seen it mean
differentiable, $C^1$, piecewise-$C^1$, $C^\infty$, holomorphic, \ldots}
if it is \emph{\myindex{infinitely differentiable}},
or in other words, if it is $C^k$ for all $k \in \N$.

\medskip

Complex analysis is the study of holomorphic (or complex-analytic)
functions.
Holomorphic functions are a generalization of polynomials,
and to get there one leaves the land of algebra to arrive in the realm of
analysis.
One can do an awful lot with polynomials, but sometimes they are
just not enough.  For example, there is no nonzero polynomial function that solves
the simplest of differential equations, $f' = f$.  We need the exponential
function, which is holomorphic.

We start with polynomials.  A polynomial in $z$ is
an expression of the form
\begin{equation*}
P(z) = \sum_{k=0}^d c_k \, z^k ,
\end{equation*}
where $c_k \in \C$ and $c_d \not= 0$.  The number $d$ is called the
\emph{degree}\index{degree of a polynomial}
of the
polynomial $P$.  We can plug in some number $z$ and compute
$P(z)$, to obtain a function $P \colon \C \to \C$.

We try to write
\begin{equation*}
f(z) = \sum_{k=0}^\infty c_k \, z^k
\end{equation*}
and all is very fine until we wish to know what $f(z)$ is for some number
$z \in \C$.
We usually mean
\begin{equation*}
\sum_{k=0}^\infty c_k \, z^k
=
\lim_{d\to\infty}
\sum_{k=0}^d c_k \, z^k .
\end{equation*}
As long as the limit exists, we have a function.  You know all
this; it is your one-variable complex analysis.  We typically
start with the functions and prove that we can expand into series.

Let $U \subset \C$ be open.  A function $f \colon U \to \C$
is \emph{\myindex{holomorphic}}
(or \emph{\myindex{complex-analytic}}) if it is
\emph{\myindex{complex-differentiable}} at every point,
that is, if
\begin{equation*}
f'(z)
=
\lim_{\xi \in \C \to 0} \frac{f(z+\xi)-f(z)}{\xi}
\qquad \text{exists for all $z \in U$.}
\end{equation*}
Importantly, the limit is taken with respect to complex $\xi$.
Another vantage point is to start with a continuously
differentiable\footnote{Holomorphic functions end up being infinitely
differentiable anyway, so this hypothesis is not overly restrictive.} $f$,
and say $f = u + i\, v$ is holomorphic if it satisfies
the \emph{\myindex{Cauchy--Riemann equations}}:
\begin{equation*}
\frac{\partial u}{\partial x} =
\frac{\partial v}{\partial y} ,
\qquad
\frac{\partial u}{\partial y} =
-
\frac{\partial v}{\partial x} .
\end{equation*}
The so-called \emph{\myindex{Wirtinger operators}},
\begin{equation*}
\frac{\partial}{\partial z}
\overset{\text{def}}{=}
\frac{1}{2}
\left(
\frac{\partial}{\partial x} - i
\frac{\partial}{\partial y}
\right),
~ ~ ~ ~ ~
\frac{\partial}{\partial \bar{z}}
\overset{\text{def}}{=}
\frac{1}{2}
\left(
\frac{\partial}{\partial x} + i
\frac{\partial}{\partial y}
\right)
,
\end{equation*}
provide an easier way to understand the
Cauchy--Riemann equations.
These operators are determined by insisting on
\glsadd{not:wirt}%
\begin{equation*}
\frac{\partial}{\partial z} z = 1, \quad
\frac{\partial}{\partial z} \bar{z} = 0, \quad
\frac{\partial}{\partial \bar{z}} z = 0, \quad
\frac{\partial}{\partial \bar{z}} \bar{z} = 1.
\end{equation*}

The function $f$ is holomorphic if and only if
\begin{equation*}
\frac{\partial f}{\partial \bar{z}} = 0 .
\end{equation*}
That seems a far nicer statement of the Cauchy--Riemann equations; it is
just one complex equation.  It says
a function is holomorphic if and only if it depends on $z$ but not on
$\bar{z}$ (perhaps that does not make a whole lot of sense at first
glance).
We check:
\begin{equation*}
\frac{\partial f}{\partial \bar{z}}
=
\frac{1}{2}
\left(
\frac{\partial f}{\partial x} + i
\frac{\partial f}{\partial y}
\right)
=
%\frac{1}{2}
%\left(
%\frac{\partial }{\partial x} (u + iv) + i
%\frac{\partial }{\partial y} (u + iv)
%\right)
%=
\frac{1}{2}
\left(
\frac{\partial u}{\partial x}
+ i \frac{\partial v}{\partial x}
+ i \frac{\partial u}{\partial y}
- \frac{\partial v}{\partial y}
\right)
=
\frac{1}{2}
\left(
\frac{\partial u}{\partial x}
- \frac{\partial v}{\partial y}
\right)
+
\frac{i}{2}
\left(
\frac{\partial v}{\partial x}
+ \frac{\partial u}{\partial y}
\right) .
\end{equation*}
This expression is zero if and only if the real parts and the imaginary
parts are zero.  In other words, %if and only if
\begin{equation*}
\frac{\partial u}{\partial x}
- \frac{\partial v}{\partial y}
= 0,
\qquad
\text{and}
\qquad
\frac{\partial v}{\partial x}
+ \frac{\partial u}{\partial y} = 0
.
\end{equation*}
That is, the Cauchy--Riemann equations are satisfied.

%Another common way to define a holomorphic function is to say that
%the complex derivative at each point exists.  If $f'$ exists
%at every point, it equals the derivative in $z$.
If $f$ is holomorphic, the derivative in $z$ is the standard complex derivative you know and love:
\begin{equation*}
\frac{\partial f}{\partial z} (z_0)
=
f'(z_0)
=
\lim_{\xi \to 0} \frac{f(z_0+\xi)-f(z_0)}{\xi} .
\end{equation*}
That is because
\begin{equation*}
\begin{split}
\frac{\partial f}{\partial z}
=
\frac{1}{2}
\left(
\frac{\partial u}{\partial x}
+ \frac{\partial v}{\partial y}
\right)
+
\frac{i}{2}
\left( \frac{\partial v}{\partial x} - \frac{\partial u}{\partial y}
\right)
& =
\frac{\partial u}{\partial x}
+ i \frac{\partial v}{\partial x}
 =
\frac{\partial f}{\partial x}
\\
& =
\frac{1}{i} \left(
\frac{\partial u}{\partial y}
+ i
\frac{\partial v}{\partial y}
\right)
 =
\frac{\partial f}{\partial (iy)}
.
\end{split}
\end{equation*}

A function on $\C$ is a function defined on
$\R^2$ as identified above, and so it is a function of $x$ and $y$.
Writing
$x = \frac{z+\bar{z}}{2}$ and
$y = \frac{z-\bar{z}}{2i}$, think of it as a function of two
complex variables, $z$ and $\bar{z}$.  Pretend for a moment as if $\bar{z}$ did not
depend on $z$.
The Wirtinger operators
work as if $z$ and $\bar{z}$ really were independent variables.  For
instance:
\begin{equation*}
\frac{\partial}{\partial z}
\left[ z^2 \bar{z}^3 + z^{10} \right]
=
2z \bar{z}^3 + 10 z^{9}
\qquad
\text{and}
\qquad
\frac{\partial}{\partial \bar{z}}
\left[ z^2 \bar{z}^3 + z^{10} \right]
=
z^2 ( 3 \bar{z}^2 ) + 0 .
\end{equation*}
A holomorphic function is a function ``not depending on $\bar{z}$.''

The most important theorem in one variable is
the \emph{\myindex{Cauchy integral formula}}\index{Cauchy formula}.

\begin{thm}[Cauchy integral formula]
Let $U \subset \C$ be a bounded domain where the boundary $\partial U$
is a piecewise smooth
simple closed path (a Jordan curve).  Let $f \colon \widebar{U} \to \C$ be a continuous function,
holomorphic in $U$.
Orient $\partial U$ positively (going around counter-clockwise).
Then
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
\qquad \text{for all $z \in U$.}
\end{equation*}
\end{thm}

%The path integral for a smooth path $\gamma \colon [a,b] \to \C$ is defined as
%\begin{equation*}
%\int_\gamma f(z) \, dz
%=
%\int_a^b f\bigl(\gamma(t)\bigr) \gamma'(t) \, dt .
%\end{equation*}

The Cauchy formula is the essential ingredient we need from
one complex variable.  It follows
from Green's theorem\footnote{If you wish to feel inadequate,
note that this theorem, on which all of complex analysis (and all of physics)
rests, was proved by
George Green, who was the son of a miller and had one year of formal
schooling.}  (Stokes' theorem in two
dimensions).  You can look forward to
\thmref{thm:generalizedcauchy} for a proof of a more general formula,
the Cauchy--Pompeiu integral formula.

As a differential form, \glsadd{not:dz}$dz = dx + i \, dy$.  If you are uneasy
about differential forms, you possibly defined the path integral above
directly using
the Riemann--Stieltjes integral in your one-complex-variable class.
Let us write down the formula in terms of the standard Riemann integral
in a special case.  Take the \emph{\myindex{unit disc}}
\glsadd{not:D}%
\begin{equation*}
\D
\overset{\text{def}}{=}
\bigl\{ z \in \C : \sabs{z} < 1 \bigr\} .
\end{equation*}
The boundary is the unit circle
$\partial \D = \bigl\{ z \in \C : \sabs{z} = 1 \bigr\}$ oriented positively,
that is, counter-clockwise.   Parametrize $\partial \D$
by $e^{it}$, where $t$ goes from $0$ to $2\pi$.  If $\zeta = e^{it}$,
then $d\zeta = ie^{it}dt$, and
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial \D}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
=
\frac{1}{2\pi}
\int_0^{2\pi}
\frac{f(e^{it}) e^{it} }{e^{it}-z}
\,
dt .
\end{equation*}
If you are not completely comfortable with path integrals,
try to think about how you would parametrize the path, and
write the integral as an integral any calculus student would recognize.

I venture a guess that 90\% of what you learned in a one-variable complex analysis
course (depending on who taught it)
is more or less a straightforward consequence of the Cauchy
integral formula.
An important theorem from one variable that follows from
the Cauchy formula is the
\emph{\myindex{maximum modulus principle}} (or just
the \emph{\myindex{maximum principle})}.
Let us give its simplest version.

\begin{thm}[Maximum modulus principle]
Suppose $U \subset \C$ is a domain and $f \colon U \to \C$
is holomorphic.
If for some $z_0 \in U$
\begin{equation*}
\sup_{z \in U} \, \sabs{f(z)} = \sabs{f(z_0)} ,
\end{equation*}
\glsadd{not:identeq}%
then $f$ is constant, that is, $f \equiv f(z_0)$.
\end{thm}

That is, if the supremum is attained in the interior of the domain,
then the function must be constant.  Another way to state the maximum
principle is to say: If $f$ extends continuously to the boundary of a
bounded
domain, then the supremum of $\sabs{f(z)}$ is attained on the boundary.
In
one variable you learned that the maximum principle is really a
property of harmonic functions.

\begin{thm}[Maximum principle]
\pagebreak[2]
Let $U \subset \C$ be a domain and $h \colon U \to \R$
harmonic, that is,
\glsadd{not:laplacian}%
\begin{equation*}
\nabla^2 h = \frac{\partial^2 h}{\partial x^2} + \frac{\partial^2 h}{\partial
y^2} = 0 .
\end{equation*}
If for some $z_0 \in U$
\begin{equation*}
\sup_{z \in U} \, h(z) = h(z_0)
\qquad \text{or} \qquad
\inf_{z \in U} \, h(z) = h(z_0) ,
\end{equation*}
then $h$ is constant, that is, $h \equiv h(z_0)$.
\end{thm}

In one variable, if $f = u+iv$ is holomorphic for real-valued $u$ and $v$,
then $u$ and $v$ are harmonic.
Similarly, $\log \sabs{f}$ is harmonic.
Locally, a harmonic function is
the real (or imaginary) part of a holomorphic function, so in
one complex variable, studying
harmonic functions is almost equivalent to studying holomorphic
functions.  Things are decidedly different
in two or more variables.

\medskip

Holomorphic functions admit a power series representation in $z$
at each point $a$:
\begin{equation*}
f(z) = \sum_{k=0}^\infty c_k {(z-a)}^k .
\end{equation*}
No $\bar{z}$ is necessary,
since $\frac{\partial f}{\partial \bar{z}} = 0$.

Let us see the proof using the Cauchy integral formula, as we will
require this computation in several variables as well.
Given $a \in \C$ and $\rho > 0$, define the disc of radius $\rho$ around $a$
\glsadd{not:disc}%
\begin{equation*}
\Delta_\rho(a)
\overset{\text{def}}{=}
\bigl\{ z \in \C : \sabs{z-a} < \rho \bigr\} .
\end{equation*}
Suppose $U \subset \C$ is open, $f \colon U \to \C$ is holomorphic,
$a \in U$, and $\overline{\Delta_\rho(a)} \subset U$ (that is, the closure
of the disc is in $U$, and so its boundary $\partial \Delta_\rho(a)$ is also in $U$).

For $z \in \Delta_\rho(a)$ and $\zeta \in \partial \Delta_\rho(a)$,
\begin{equation*}
\abs{\frac{z-a}{\zeta-a}} =
\frac{\sabs{z-a}}{\rho} < 1 .
\end{equation*}
In fact, if $\sabs{z-a} \leq \rho' < \rho$, then
$\abs{\frac{z-a}{\zeta-a}} \leq \frac{\rho'}{\rho} < 1$.  Therefore,
the geometric series
\begin{equation*}
\sum_{k=0}^\infty
{\left(\frac{z-a}{\zeta-a}\right)}^k
=
\frac{1}{1-
\frac{z-a}{\zeta-a}}
=
\frac{\zeta-a}{\zeta-z}
\end{equation*}
converges uniformly absolutely for $(z,\zeta) \in \overline{\Delta_{\rho'}(a)}
\times \partial \Delta_\rho(a)$ (that is, $\sum_k {\bigl\lvert
\frac{z-a}{\zeta-a} \bigr\rvert}^k$
converges uniformly).
%In particular, the
%series converges uniformly absolutely in $z$ on compact subsets of $\Delta_{\rho}(a)$.

Let $\gamma$
be the path going around
$\partial \Delta_\rho(a)$ once in the positive direction.  Compute
\begin{equation*}
\begin{split}
f(z)
& =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
\\
& =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-a}
\frac{\zeta-a}{\zeta-z}
\,
d \zeta
\\
& =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-a}
\sum_{k=0}^\infty
{\left(\frac{z-a}{\zeta-a}\right)}^k
\,
d \zeta
\\
& =
\sum_{k=0}^\infty
\left(
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{k+1}}
\,
d \zeta
\right)
{(z-a)}^k .
\end{split}
\end{equation*}
In the last equality, we may
interchange the limit on the sum with the integral either
via Fubini's theorem or via uniform convergence:
$z$ is fixed and if $M$ is the supremum of $\abs{\frac{f(\zeta)}{\zeta-a}} =
\frac{\sabs{f(\zeta)}}{\rho}$ on $\partial \Delta_\rho(a)$,
then
\begin{equation*}
\abs{
\frac{f(\zeta)}{\zeta-a}
{\left(\frac{z-a}{\zeta-a}\right)}^k
}
\leq
M
{\left(\frac{\abs{z-a}}{\rho}\right)}^k,
\qquad \text{and} \qquad
\frac{\abs{z-a}}{\rho} < 1 .
\end{equation*}

The key point is writing the \emph{\myindex{Cauchy kernel}}
$\frac{1}{\zeta-z}$ as
\begin{equation*}
\frac{1}{\zeta-z}
=
\frac{1}{\zeta-a}
\frac{\zeta-a}{\zeta-z} ,
\end{equation*}
and then using the geometric series.

Not only have we proved that $f$ has a power series, but we computed
that the radius of convergence is at least $R$, where $R$ is the maximum $R$
such that $\Delta_R(a) \subset U$.  We also obtained a formula for the
coefficients
\begin{equation*}
c_k =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{k+1}}
\,
d \zeta  .
\end{equation*}

For a set $K$, denote the \emph{\myindex{supremum norm}}:
\glsadd{not:supnorm}%
\begin{equation*}
\snorm{f}_K
\overset{\text{def}}{=}
\sup_{z \in K} \sabs{f(z)} .
\end{equation*}
By a brute force estimation, we obtain the very useful
\emph{\myindex{Cauchy estimates}}:
\begin{equation*}
\sabs{c_k} =
\abs{
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{k+1}}
\,
d \zeta
}
\leq
\frac{1}{2\pi}
\int_{\gamma}
\frac{\snorm{f}_{\gamma}}{\rho^{k+1}}
\,
\sabs{d \zeta}
=
\frac{\snorm{f}_{\gamma}}{\rho^{k}} .
\end{equation*}

We differentiate Cauchy's formula $k$ times (using the Wirtinger
$\frac{\partial}{\partial z}$ operator),
\begin{equation*}
f^{(k)}(z)
=
\frac{\partial^k f}{\partial z^k} (z)
=
\frac{1}{2\pi i}
\int_{\gamma}
\frac{k! f(\zeta)}{{(\zeta-z)}^{k+1}}
\,
d \zeta  ,
\end{equation*}
and therefore
\begin{equation*}
k! \, c_k =
f^{(k)}(a)
=
\frac{\partial^k f}{\partial z^k}(a) .
\end{equation*}
Hence, we can control derivatives of $f$
by the size of the function:
\begin{equation*}
\babs{f^{(k)}(a)}
=
\abs{\frac{\partial^k f}{\partial z^k}(a)}
\leq
\frac{k! \snorm{f}_{\gamma}}{\rho^{k}} .
\end{equation*}
This estimate is one of the key properties of
holomorphic functions, and the reason why the correct topology for
the set of holomorphic functions is the same as the topology for continuous functions.
Consequently,
obstructions to solving problems in complex analysis are often topological
in character.

For a further review of one-variable results,
see \appendixref{ap:onevarresults}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Holomorphic Functions in Several Variables} \label{ch:holfunc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Onto several variables} \label{sec:ontosevvar}

Let $\C^n = \overbrace{\C \times \C \times \cdots \times \C}^{\text{$n$
times}}$ denote the $n$-dimensional
\emph{\myindex{complex Euclidean space}}\index{Euclidean space}.  Denote
by $z = (z_1,z_2,\ldots,z_n)$ the coordinates of $\C^n$.
Let $x =
(x_1,x_2,\ldots,x_n)$ and $y = (y_1,y_2,\ldots,y_n)$ denote the coordinates in
$\R^n$.
Identify $\C^n$ with $\R^{2n}$ by letting
$z = x+iy$, that is, $z_k = x_k + i y_k$ for every $k$.
As in one complex variable, write $\bar{z} = x-iy$.
We call $z$ the \emph{\myindex{holomorphic coordinates}}
and $\bar{z}$ the \emph{\myindex{antiholomorphic coordinates}}.

\begin{defn}
For $\rho = (\rho_1,\rho_2,\ldots,\rho_n)$ where $\rho_k > 0$ and $a \in \C^n$,
define
a \emph{\myindex{polydisc}}
\glsadd{not:disc}%
\begin{equation*}
\Delta_\rho(a)  \overset{\text{def}}{=}
\bigl\{ z \in \C^n : \sabs{z_k - a_k} < \rho_k ~\text{for $k=1,2,\ldots,n$} \bigr\} .
\end{equation*}
Call $a$ the
\emph{center}\index{center of a polydisc}
and $\rho$ the
\emph{polyradius}\index{polyradius of a polydisc}
or simply the
\emph{radius}\index{radius of a polydisc}
of the polydisc $\Delta_\rho(a)$.
If $\rho > 0$ is a number, then
\begin{equation*}
\Delta_\rho(a)  \overset{\text{def}}{=}
\bigl\{ z \in \C^n : \sabs{z_k - a_k} < \rho ~\text{for $k=1,2,\ldots,n$} \bigr\} .
\end{equation*}
In two variables, a polydisc is sometimes called a \emph{\myindex{bidisc}}.
As there is the unit disc $\D$ in one variable, so is there
the \emph{\myindex{unit polydisc}} in several variables:
\begin{equation*}
\D^n = \D \times \D \times \cdots \times \D = \Delta_1(0) =
\bigl\{ z \in \C^n : \sabs{z_k} < 1 ~\text{for $k=1,2,\ldots,n$} \bigr\} .
\end{equation*}
\end{defn}

In more than one complex dimension, it is difficult to draw exact pictures
for lack of real dimensions on our paper.
We visualize the unit polydisc in two variables (bidisc)
as in
\figureref{fig:polydisc}
by plotting against the modulus of the variables.
\begin{myfig}
\subimport*{figures/}{polydisc.pdf_t}
\caption{The bidisc.\label{fig:polydisc}}
\end{myfig}

Recall the
\emph{\myindex{Euclidean inner product}} on $\C^n$:
\glsadd{not:hermprod}%
\begin{equation*}
\linnprod{z}{w} \overset{\text{def}}{=} %z \cdot \bar{w}
z_1 \bar{w}_1 +
z_2 \bar{w}_2 + \cdots +
z_n \bar{w}_n .
\end{equation*}
The inner product gives us the standard
\emph{\myindex{Euclidean norm}} on $\C^n$:
\glsadd{not:eucnorm}%
\begin{equation*}
\snorm{z}  \overset{\text{def}}{=} \sqrt{\linnprod{z}{z}}
=
\sqrt{\sabs{z_1}^2 +
\sabs{z_2}^2 + \cdots +
\sabs{z_n}^2} .
\end{equation*}
This norm agrees with the standard Euclidean norm on $\R^{2n}$.
Define \emph{balls}\index{ball} as in $\R^{2n}$:
\glsadd{not:ball}%
\begin{equation*}
B_\rho(a)  \overset{\text{def}}{=}
\bigl\{ z \in \C^n : \snorm{z - a} < \rho \bigr\} ,
\end{equation*}
And the \emph{\myindex{unit ball}},
\glsadd{not:unitball}%
\begin{equation*}
\bB_n \overset{\text{def}}{=} B_1(0) =
\bigl\{ z \in \C^n : \snorm{z} < 1 \bigr\} .
\end{equation*}

A ball centered at the origin
can also be pictured by plotting against the modulus of the
variables, since the inequality defining the ball only
depends on the moduli of the variables.
Not every domain can be drawn like this, but if it can,
it is called a \emph{Reinhardt domain}, more on this later.
A picture of $\bB_2$ is in \figureref{fig:ball}.
\begin{myfig}
\subimport*{figures/}{ball.pdf_t}
\caption{The ball $\bB_2$ as a Reinhardt domain.\label{fig:ball}}
\end{myfig}

\begin{defn}
Let $U \subset
\C^n$ be open.  A
function
$f \colon U \to \C$ is \emph{\myindex{holomorphic}} if
it is \emph{locally bounded}%
\index{locally bounded function}\footnote{For
every $p \in U$, there is a neighborhood $N$ of $p$
such that $f|_N$ is bounded.  Equivalently, $f$ is bounded on compact
subsets of $U$.  It is a deep result of Hartogs that we might
in fact just drop ``locally bounded'' from the definition and obtain the same
set of functions.}
%A deep result of Hartogs, which we skip, says that
%we do not need to assume $f$ to be locally bounded.}
and holomorphic in each variable separately.
That is, $f$ is holomorphic if it is locally bounded
and complex-differentiable in each variable separately:
\begin{equation*}
\lim_{\xi \in \C \to 0} \frac{f(z_1,\ldots,z_k+\xi,\ldots,z_n) - f(z)}{\xi}
\qquad \text{exists for all $z \in U$ and all $k=1,2,\ldots,n$}.
\end{equation*}
In this book, the words ``differentiable'' and ``derivative''
(without the ``complex-'') refer
to plain-vanilla real differentiability.
\end{defn}

As in one variable, we define the \emph{\myindex{Wirtinger operators}}
\glsadd{not:wirt}%
\begin{equation*}
\frac{\partial}{\partial z_k}  \overset{\text{def}}{=}
\frac{1}{2} \left(
\frac{\partial}{\partial x_k} - i \frac{\partial}{\partial y_k}
\right) ,
\qquad
\frac{\partial}{\partial \bar{z}_k}  \overset{\text{def}}{=}
\frac{1}{2} \left(
\frac{\partial}{\partial x_k} + i \frac{\partial}{\partial y_k}
\right) .
\end{equation*}
An alternative definition is to say that a continuously
differentiable function $f \colon U \to \C$ is
\emph{holomorphic} if
it satisfies the
\emph{\myindex{Cauchy--Riemann equations}}
\begin{equation*}
\frac{\partial f}{\partial \bar{z}_k}  = 0 \qquad \text{for $k=1,2,\ldots,n$}.
\end{equation*}
For holomorphic functions, using the natural definition for partial
derivatives obtains the Wirtinger $\frac{\partial}{\partial z_k}$.
Namely, if $f$ is holomorphic, then
\begin{equation*}
\frac{\partial f}{\partial z_k}(z)
=
\lim_{\xi \in \C \to 0} \frac{f(z_1,\ldots,z_k+\xi,\ldots,z_n) - f(z)}{\xi}
.
\end{equation*}

Due to the following proposition, the alternative definition using the
Cauchy--Riemann equations is just as good as the definition we gave.
%In the proof, if you wish to stay with just the Riemann integral instead of
%the Lebesgue integral, feel free to assume that $f$ is continuously
%differentiable.

\begin{prop}
Let $U \subset \C^n$ be an open set and
suppose $f \colon U \to \C$ is holomorphic.  Then $f$ is infinitely
differentiable.
\end{prop}

\begin{proof}
Suppose $\Delta = \Delta_{\rho}(a) = \Delta_1 \times \cdots \times \Delta_n$
is a polydisc centered at $a$, where each $\Delta_k$ is a disc,
and suppose $\overline{\Delta} \subset U$, that is, $f$ is holomorphic
on a neighborhood of the closure of $\Delta$.
Let $z$ be in $\Delta$.
Orient $\partial \Delta_1$ positively and
apply the Cauchy formula (after all $f$ is holomorphic in $z_1$):
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial \Delta_1}
\frac{f(\zeta_1,z_2,\ldots,z_n)}{\zeta_1-z_1}
\,
d \zeta_1 .
\end{equation*}
Apply it again on the second variable, again orienting
$\partial \Delta_2$ positively:
\begin{equation*}
f(z) =
\frac{1}{{(2\pi i)}^2}
\int_{\partial \Delta_1}
\int_{\partial \Delta_2}
\frac{f(\zeta_1,\zeta_2,z_3,\ldots,z_n)}{(\zeta_1-z_1)(\zeta_2-z_2)}
\,
d \zeta_2
\,
d \zeta_1 .
\end{equation*}

Applying the formula $n$ times, we obtain
\begin{equation} \label{iteratedcauchy:eq}
f(z) =
\frac{1}{{(2\pi i)}^n}
\int_{\partial \Delta_1}
\int_{\partial \Delta_2}
\cdots
\int_{\partial \Delta_n}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{(\zeta_1-z_1)(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_n
\cdots
d \zeta_2
\,
d \zeta_1 .
\end{equation}
As $f$ is bounded on the compact set
$\partial \Delta_1 \times \cdots \times \partial \Delta_n$,
we find that $f$ is continuous in $\Delta$, and hence on $U$.
We may differentiate underneath the
integral via the standard Leibniz rule,
because the integrand and its partial derivatives
with respect to $x_k$ and $y_k$, where $z_k=x_k+i y_k$, are
all continuous, as long as $z$
is a positive distance away from
$\partial \Delta_1 \times \cdots \times \partial \Delta_n$.
We may differentiate as many times as we wish.
\end{proof}

In \eqref{iteratedcauchy:eq} above,
we derived the Cauchy integral formula in several variables.  To
write the formula more concisely, we apply Fubini's theorem to write it as
a single integral.  We will write it down using differential forms.  If you
are unfamiliar with differential forms, think of the integral
as the iterated integral above, and you can read the next few paragraphs
a little lightly.
It is enough to understand real differential forms; we simply allow
complex coefficients here.
See \appendixref{ap:diffforms} for an overview of differential forms,
or
Rudin~\cite{Rudin:principles} for an introduction with all the details.

Given real coordinates $x = (x_1,\ldots,x_n)$, a one-form $d x_k$ is a linear functional on tangent vectors
such that $\bigl\langle d x_k , \frac{\partial}{\partial x_k} \bigr\rangle = 1$ and
$\bigl\langle d x_k , \frac{\partial}{\partial x_{\ell}} \bigr\rangle = 0$ if $k \not=
{\ell}$,
where we use the pairing notation
\glsadd{not:pairing}%
$\langle \omega , v \rangle$ instead of the functional notation
$\omega(v)$ as is traditional to indicate multilinearity.
As $z_k = x_k + i y_k$ and
$\bar{z}_k = x_k - i y_k$,
\glsadd{not:dz}\glsadd{not:dzbar}%
\begin{equation*}
d z_k = d x_k + i \, d y_k ,
\qquad
d \bar{z}_k = d x_k - i \, d y_k .
\end{equation*}
Let
\glsadd{not:kronecker}%
$\delta_{k}^{\ell}$ be the Kronecker delta, that is, $\delta_k^k = 1$,
and $\delta_k^{\ell} = 0$ if $k \not= {\ell}$.  Then, as expected,
\begin{equation*}
\left\langle
d z_k , \frac{\partial}{\partial z_{\ell}} \right\rangle =
\delta_k^{\ell} ,
\qquad
\left\langle
d z_k , \frac{\partial}{\partial \bar{z}_{\ell}} \right\rangle = 0 ,
\qquad
\left\langle
d \bar{z}_k , \frac{\partial}{\partial z_{\ell}} \right\rangle = 0 ,
\qquad
\left\langle
d \bar{z}_k , \frac{\partial}{\partial \bar{z}_{\ell}} \right\rangle =
\delta_k^{\ell}
.
\end{equation*}
One-forms are the objects
\begin{equation*}
\sum_{k=1}^n \alpha_k \, d z_k +
\beta_k \, d \bar{z}_k ,
\end{equation*}
where $\alpha_k$ and $\beta_k$ are functions (of $z$).
Two-forms are combinations of wedge products,
$\omega \wedge \eta$, of one-forms.  A wedge of a two-form and
a one-form is a three-form, etc.
An $m$-form is an object that
can be integrated on a so-called $m$-chain, for example, a
$m$-dimensional surface.  The wedge product takes care of the orientation
as it is anticommutative on one-forms:
For one-forms $\omega$ and $\eta$, we have
$\omega \wedge \eta = - \eta \wedge \omega$.

At this point, we need to talk about orientation in $\C^n$, that is,
the ordering of the real coordinates.  There are two
natural real-linear isomorphisms of $\C^n$ and $\R^{2n}$.  We
identify $z = x+iy$ as either
\begin{equation*}
(x,y) = (x_1,\ldots,x_n,y_1,\ldots,y_n) \qquad
\text{or} \qquad
(x_1,y_1,x_2,y_2,\ldots,x_n,y_n) .
\end{equation*}
If we take the natural orientation of $\R^{2n}$,
it is possible (if $n$ is even) that we obtain
two opposite orientations on $\C^n$ (if $n$ is even, the real linear map
that takes one ordering to the other has determinant $-1$).
The orientation we take as the natural orientation of $\C^n$ (in this book)
corresponds to
the second ordering above, that
is, $(x_1,y_1,\ldots,x_n,y_n)$.  Either isomorphism may be used
in computation as long as it is used consistently, and the underlying
orientation is kept in mind.

\begin{thm}[Cauchy integral formula]
\index{Cauchy integral formula in several variables}\index{Cauchy formula}
Let $\Delta \subset \C^n$ be a polydisc.
% centered at $a \in \C^n$.
Suppose
$f \colon \overline{\Delta} \to \C$ is a continuous function
holomorphic in $\Delta$.
Write $\Gamma = \partial \Delta_1 \times \cdots \times \partial \Delta_n$
oriented appropriately (each $\partial \Delta_k$ oriented positively).
Then for $z \in \Delta$
\begin{equation*}
f(z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{(\zeta_1-z_1)(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n .
\end{equation*}
\end{thm}

We stated a more general result where $f$ is only continuous
on $\overline{\Delta}$ and holomorphic in $\Delta$.  The proof of this
slight generalization is contained within the next two exercises.

\begin{exbox}
\begin{exercise}
Suppose $f \colon \overline{\D^2} \to \C$ is continuous and holomorphic
on $\D^2$.  For every $\theta \in \R$, prove
\begin{equation*}
g_1(\xi) = f(\xi,e^{i\theta}) \qquad \text{and} \qquad
g_2(\xi) = f(e^{i\theta},\xi)
\avoidbreak
\end{equation*}
are holomorphic in $\D$.
\end{exercise}

\begin{exercise}
Prove the theorem above, that is, the slightly more general Cauchy integral
formula where $f$ is only continuous on $\overline{\Delta}$ and
holomorphic in $\Delta$.
\end{exercise}
\end{exbox}


The Cauchy integral formula shows
an important and subtle point about holomorphic functions in several
variables:
The value of
the function $f$ on $\Delta$ is completely determined by the values of $f$ on
the set $\Gamma$, which is much smaller than the boundary of the polydisc
$\partial \Delta$.  In fact, $\Gamma$ is of real dimension $n$, while
the boundary of the polydisc is of real dimension $2n-1$.
The set $\Gamma = \partial \Delta_1 \times \cdots \times \partial \Delta_n$
is called
the \emph{\myindex{distinguished boundary}}.
See \figureref{fig:polydisc-dist} for the distinguished boundary of the
bidisc.
\begin{myfig}
\subimport*{figures/}{polydisc-dist.pdf_t}
\caption{The distinguished boundary of $\D^2$.\label{fig:polydisc-dist}}
\end{myfig}

The set $\Gamma$ is a $2$-dimensional torus, like the surface of a
donut.  Whereas the set $\partial \D^2 =
(\partial \D \times \overline{\D}) \cup
(\overline{\D} \times \partial \D)$ is the union of two filled donuts, or more
precisely, it is both the inside and the outside of the donut put together,
and these two things meet on the surface of the donut.  So the
set $\Gamma$ is quite small in comparison to the entire boundary
$\partial \D^2$.

\begin{exbox}
\begin{exercise}
%Prove the following stronger version of the maximum principle.
Suppose $\Delta$ is a polydisc, $\Gamma$ its distinguished boundary,
and $f \colon \overline{\Delta} \to \C$ is continuous on $\overline{\Delta}$
and holomorphic on $\Delta$.
Prove
$\sabs{f(z)}$ achieves its maximum on $\Gamma$.
\end{exercise}

\begin{exercise}
A ball is different from a polydisc.  Prove that for every $p \in \partial \bB_n$
there exists a continuous $f \colon \overline{\bB_n} \to \C$, holomorphic
on $\bB_n$, such that $\sabs{f(z)}$ achieves a strict maximum at~$p$.
\end{exercise}

\begin{exercise}
Show that in the real setting, differentiable
in each variable separately does not imply differentiable even if
the function is locally bounded.
Let $f(x,y) = \frac{xy}{x^2+y^2}$ outside the origin
and $f(0,0) = 0$.  Prove that $f$ is a
locally bounded function in $\R^2$, which is differentiable
in each variable separately (all partial derivatives exist at every point),
but $f$ is not even continuous.  There is something very
special about the holomorphic category.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is open.
Prove that $f \colon U \to \C$ is holomorphic if and only if
$f$ is locally bounded and
for every $a,b \in \C^n$, the
function
$\zeta \mapsto f(\zeta a + b)$ is holomorphic on
the open set $\{ \zeta \in \C : \zeta a + b \in U \}$.
\end{exercise}

\begin{exercise}
Prove a several complex variables version of Morera's theorem (see
\thmref{thm:onevarmorera}).
A triangle $T \subset \C^n$ is the closed convex hull of three points, so
including the inside.  Orient $T$ in some way %(will not matter which way)
and orient $\partial T$ accordingly.
A triangle $T$ \emph{lies in a complex line} if
its vertices $a,b,c$ satisfy
$\zeta (b-a) = c-a$ for some $\zeta \in \C$.
Suppose $U \subset \C^n$ is open and $f \colon U \to \C$ is continuous.
Prove that $f$ is holomorphic if and only if
\begin{equation*}
\int_{\partial T} f(z) \, dz_k = 0
\end{equation*}
for every triangle $T \subset U$ that lies in a complex line,
and every $k=1,2,\ldots,n$.
Hint: The previous exercise may be useful.
\end{exercise}

\begin{exercise} \label{exercise:isolatedhartogs}
Let $f \colon \overline{\D^2} \setminus \{ 0 \} \to \C$ be continuous
and holomorphic on $\D^2 \setminus \{ 0 \}$.
\begin{exparts}
\item
Prove that $f$ is bounded.  Hint: Consider the functions
$\xi \mapsto f(\xi,a)$ and $\xi \mapsto f(a,\xi)$ for
different $a$.
\item
Using the Riemann extension in one variable, prove that
there exists a continuous $F \colon \overline{\D^2} \to \C$,
holomorphic on $\D^2$, such that $f=F$ on
$\overline{\D^2} \setminus \{ 0 \}$.
\end{exparts}
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Power series representation}

As you noticed, writing out all the components can be a pain.
%It would become even more painful later on.
Just as we write
vectors as $z$ instead of $(z_1,z_2,\ldots,z_n)$, we similarly
define the so-called
\emph{\myindex{multi-index notation}}
to deal with more complicated formulas such as the ones above.

Let $\alpha \in \N_0^n$
be a vector of nonnegative integers
\glsadd{not:N0}%
(where $\N_0 = \N \cup \{ 0\}$).
We write
\glsadd{not:zalpha}%
\glsadd{not:dzmulti}%
\glsadd{not:absalpha}%
\glsadd{not:alphabang}%
\glsadd{not:alphader}%
\begin{align*}
z^\alpha & \overset{\text{def}}{=} z_1^{\alpha_1}z_2^{\alpha_2} \cdots
z_n^{\alpha_n} ,
&
\sabs{z}^\alpha & \overset{\text{def}}{=} \sabs{z_1}^{\alpha_1}\sabs{z_2}^{\alpha_2} \cdots
\sabs{z_n}^{\alpha_n} ,
\displaybreak[0]\\
\frac{1}{z} & \overset{\text{def}}{=} \frac{1}{z_1z_2 \cdots z_n} ,
&
\frac{z}{w} & \overset{\text{def}}{=}
\left(\frac{z_1}{w_1}, \frac{z_2}{w_2}, \ldots, \frac{z_n}{w_n} \right) ,
\displaybreak[0]\\
\frac{\partial^{\sabs{\alpha}}}{\partial z^\alpha} & \overset{\text{def}}{=}
\frac{\partial^{\alpha_1}}{\partial z_1^{\alpha_1}}
\frac{\partial^{\alpha_2}}{\partial z_2^{\alpha_2}}
\cdots
\frac{\partial^{\alpha_n}}{\partial z_n^{\alpha_n}} ,
&
dz & \overset{\text{def}}{=} dz_1 \wedge dz_2 \wedge \cdots \wedge dz_n ,
\displaybreak[0]\\
\sabs{\alpha} & \overset{\text{def}}{=} \alpha_1 + \alpha_2 + \cdots + \alpha_n ,
&
\alpha! & \overset{\text{def}}{=} \alpha_1!\alpha_2! \cdots \alpha_n! .
\end{align*}
We can also make sense of this notation, especially the notation $z^\alpha$,
if $\alpha \in \Z^n$, that is, if it includes negative integers.
Although usually, $\alpha$ is assumed to be in $\N_0^n$.
Furthermore, when we use $1$ as a vector, it means $(1,1,\ldots,1)$.
If $z \in \C^n$, then
\begin{equation*}
1-z = (1-z_1,1-z_2,\ldots,1-z_n) ,
\qquad \text{or} \qquad
z^{\alpha+1} = z_1^{\alpha_1+1}z_2^{\alpha_2+1} \cdots z_n^{\alpha_n+1} .
\end{equation*}
It goes without saying that when using this notation it is
important to be careful to always realize which symbol lives where,
and most of all, to not get carried away.  For
instance, we can interpret $\frac{1}{z}$ in different ways depending
on whether we interpret $1$ as a vector or not, and whether we
expect a vector or a number.  Best to just keep to the limited set of
cases as given above, and only use it when it is clear what is meant.
In this notation, the Cauchy formula becomes the perhaps deceptively simple
\begin{equation*}
f(z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta .
\end{equation*}

Let us move on to power series.  For simplicity, we start with
power series at the origin.  Using the multi-index notation, we write
such a series as
\begin{equation*}
\sum_{\alpha \in \N_0^n} c_\alpha {z}^\alpha .
\end{equation*}
You must admit that the
above is far nicer to write than writing, for example, in $\C^3$,
\begin{equation} \label{eq:iteratedsum}
\sum_{k=0}^\infty
\sum_{\ell=0}^\infty
\sum_{m=0}^\infty
c_{k\ell m} z_1^kz_2^\ell z_3^m ,
\end{equation}
which is not even exactly the definition of the series sum (see
below).
When it is clear
from context that we are talking about a power series
and all the powers are nonnegative,
we write simply
\glsadd{not:zalphasum}%
\begin{equation*}
\sum_{\alpha} c_\alpha {z}^\alpha .
\end{equation*}

It is important to note what this means.  The sum does not
have a natural ordering.  We are summing over $\alpha \in \N_0^n$, and there
is no natural ordering of $\N_0^n$.  It makes no sense to
talk about conditional convergence.  When we say the series
\emph{converges}%
\index{power series convergence}%
\index{convergence of power series}, we mean absolutely.
Fortunately, power series do converge
absolutely, so the ordering does not matter.  If you want to write
the limit in terms of partial sums, you pick some ordering of the
multi-indices, $\alpha(1), \alpha(2), \ldots$, and then
\begin{equation*}
\sum_{\alpha}
c_\alpha {z}^\alpha
=
\lim_{m \to \infty}
\sum_{k=1}^m
c_{\alpha(k)} {z}^{\alpha(k)} .
\end{equation*}
By the Fubini theorem (for sums), this limit is equal to the iterated
sum such as \eqref{eq:iteratedsum}.

A power series $\sum_\alpha c_\alpha z^\alpha$
\emph{\myindex{converges uniformly absolutely}}
\index{uniform absolute convergence}
for $z \in X$ when $\sum_\alpha \sabs{c_\alpha z^\alpha}$
converges uniformly for $z \in X$.
The \emph{\myindex{geometric series in several variables}} is the
series $\sum_{\alpha} z^\alpha$.
For
$z \in \D^n$ (unit polydisc),
\begin{equation*}
\begin{split}
\frac{1}{1-z} & =
\frac{1}{(1-z_1)(1-z_2)\cdots(1-z_n)} =
\left(
\sum_{k=0}^\infty {z_1}^k
\right)
\left(
\sum_{k=0}^\infty {z_2}^k
\right)
\cdots
\left(
\sum_{k=0}^\infty {z_n}^k
\right)
\\
& =
\sum_{k_1=0}^\infty
\sum_{k_2=0}^\infty
\cdots
\sum_{k_n=0}^\infty
\left(
{z_1}^{k_1}
{z_n}^{k_2}
\cdots
{z_n}^{k_n}
\right)
=
\sum_{\alpha} z^\alpha .
\end{split}
\end{equation*}
The series converges uniformly absolutely
on all compact subsets of the unit
polydisc:  Any compact set in the unit
polydisc is contained in a closed polydisc $\overline{\Delta}$
centered at $0$ of radius $1-\epsilon$
for some $\epsilon > 0$.  The convergence is uniformly absolute on
$\overline{\Delta}$.  This claim follows by simply noting the
same fact for each factor is true in one dimension.

Holomorphic functions are precisely those that allow
a power series expansion:

\begin{thm}
Let $\Delta = \Delta_\rho(a) \subset \C^n$ be a polydisc.
Suppose
$f \colon \overline{\Delta} \to \C$ is a continuous function
holomorphic in $\Delta$.
Then on $\Delta$, $f$ is equal to a power series
converging uniformly absolutely on compact subsets of $\Delta$:
\begin{equation} \label{holfunc:ps}
f(z) = \sum_{\alpha} c_\alpha {(z-a)}^\alpha .
\end{equation}

Conversely, if $f \colon \Delta \to \C$ is defined by \eqref{holfunc:ps} converging
uniformly absolutely on compact subsets of $\Delta$, then $f$ is holomorphic on
$\Delta$.
\end{thm}

The hypothesis that $f$ is continuous on $\overline{\Delta}$ is not
necessary.  We will prove in a moment that the power series is unique
and hence we could have used an arbitrary smaller polydisc centered at $a$
for the development.

\begin{proof}
Suppose a continuous $f \colon \overline{\Delta} \to \C$ is holomorphic on $\Delta$.
Let
$\Gamma = \partial \Delta_1 \times \cdots \times \partial \Delta_n$
be oriented positively.
Take $z \in \Delta$ and $\zeta \in \Gamma$.
As in one variable, write the Cauchy kernel as
\begin{equation*}
\frac{1}{\zeta-z} =
\frac{1}{\zeta-a}
\left(
\frac{1}{1-\frac{z-a}{\zeta-a}}
\right)
=
\frac{1}{\zeta-a}
\sum_{\alpha}
{\left(\frac{z-a}{\zeta-a}\right)}^\alpha .
\end{equation*}
Interpret the formulas as
$\frac{1}{\zeta-z} = \frac{1}{(\zeta_1-z_1) \cdots (\zeta_n-z_n)}$,
$\frac{1}{\zeta-a} = \frac{1}{(\zeta_1-a_1) \cdots (\zeta_n-a_n)}$
and
$\frac{z-a}{\zeta-a} =
\left(
\frac{z_1-a_1}{\zeta_1-a_1}, \ldots,
\frac{z_n-a_n}{\zeta_n-a_n}
\right)$.
The multivariable geometric series is a product of the geometric series
in one variable, and the geometric series in one variable
is uniformly absolutely convergent on compact subsets of the unit disc.
So the series
above converges uniformly absolutely for $(z,\zeta) \in K \times \Gamma$
for every compact subset $K$ of $\Delta$.

For $z \in \Delta$,
\begin{equation*}
\begin{split}
f(z)
& =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{\zeta-z}
d \zeta
%\\
% (this step doesn't work without extra notation in several variables.)
%& =
%\frac{1}{{(2\pi i)}^n}
%\int_{\Gamma}
%\frac{f(\zeta)}{\zeta-a}
%\frac{\zeta-a}{\zeta-z}
%d \zeta
\\
& =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{\zeta-a}
\sum_{\alpha}
{\left(\frac{z-a}{\zeta-a}\right)}^{\alpha}
d \zeta
\\
& =
\sum_{\alpha}
\left(
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{\alpha+1}}
\,
d \zeta
\right)
{(z-a)}^{\alpha} .
\end{split}
\end{equation*}
The last equality follows by Fubini or uniform convergence just as it does in one variable.
%because the convergence of the sum is uniform in
%$\zeta \in \Gamma$ for a fixed $z$.
Uniform absolute convergence (as $z$ moves) on compact subsets of the final
series follows from the
uniform absolute convergence of the geometric series.  It is also a direct
consequence of the Cauchy estimates below.
We have shown that
\begin{equation*}
f(z) =
\sum_{\alpha}
c_{\alpha}
{(z-a)}^{\alpha} ,
\quad \text{where} \quad
c_\alpha
=
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta)}{{(\zeta-a)}^{\alpha+1}}
\,
d \zeta .
\end{equation*}
Notice how strikingly similar the computation is to one variable.

Let us prove the converse statement.
The limit of the series
is continuous, as it is a uniform-on-compact-sets limit of continuous
functions, and hence it is locally bounded in $\Delta$.  Next,
we restrict to each variable in turn (fixing the others),
\begin{equation*}
z_k \mapsto \sum_{\alpha} c_\alpha {(z-a)}^\alpha .
\end{equation*}
This one-variable function is holomorphic as it is a uniform limit on compact subsets of
holomorphic functions.  Thus $f$ is holomorphic by definition.
\end{proof}

The converse statement also follows by applying the Cauchy--Riemann
equations to the series termwise.
We leave that as an exercise.
First, one must show that the term-by-term derivative
series also converges uniformly absolutely on compact subsets.
Then one applies the theorem from real analysis about derivatives
of limits: If a sequence of functions and the sequences of its derivatives converge
uniformly, then the derivatives converge to the derivative of the limit.

\begin{exbox}
\begin{exercise}
Prove the claim above that if a power series converges uniformly absolutely
on compact subsets of a polydisc $\Delta$, then the term-by-term derivative
converges.
Do the proof without using the analogous result for single-variable series.
\end{exercise}
\end{exbox}

\pagebreak[1]
A third way to prove the converse statement of the theorem
is to note that partial sums are
holomorphic and write them using the Cauchy formula.  Uniform
convergence shows that the limit also satisfies the Cauchy formula, and
differentiating under the integral obtains the result.

\begin{exbox}
\begin{exercise}
Follow the logic above to prove the converse of the
theorem
without using the analogous result for single-variable series.
Hint:
Let
$\Delta'' \subset \Delta' \subset \Delta$
be polydiscs with the same center $a$
such that $\overline{\Delta''} \subset \Delta'$
and $\overline{\Delta'} \subset \Delta$.
Apply Cauchy formula on $\Delta'$
for $z \in \overline{\Delta''}$.
\end{exercise}

\begin{exercise}
Suppose that $\Delta \subset \C^n$ is a possibly unbounded polydisc centered at $a \in
\C^n$, where by possibly unbounded we mean that some of the factors can be
all of $\C$ (that is, some components of the polyradius are allowed to be
$\infty$).
Prove that if $f \colon \Delta \to \C$ is holomorphic,
then there is a power series representation
$\sum_\alpha c_\alpha (z-a)^\alpha$ converging uniformly on compact subsets
to $f$ on $\Delta$.
\end{exercise}

\begin{exercise} \label{exercise:Laurentser}
One can also do a \emph{\myindex{Laurent series}} expansion.  Suppose
$a \in \C^n$ and
$U = \Delta_1 \times \cdots \times \Delta_k \times \Delta_{k+1}^* \times
\cdots \times \Delta_n^* \subset \C^n$, where each $\Delta_\ell$ is a disc
centered at $a_\ell$ or $\C$, and $\Delta_\ell^* = \Delta_\ell \setminus \{
a_\ell
\}$.
Prove that if $f \colon U \to \C$ is holomorphic, then there is a series representation
$\sum_\alpha c_\alpha (z-a)^\alpha$, where $\alpha_{k+1},\ldots,\alpha_{n}$
now range over all integers, converging uniformly on compact subsets
to $f$ on $U$.
\end{exercise}
\end{exbox}

\begin{prop}
\pagebreak[2]
Let $\Delta = \Delta_\rho(a) \subset \C^n$ be a polydisc,
and $\Gamma$ its distinguished boundary.
Suppose
$f \colon \overline{\Delta} \to \C$ is a continuous function
holomorphic in $\Delta$.
Then, for $z \in \Delta$,
\begin{equation*}
\frac{\partial^{\sabs{\alpha}}f}{\partial z^\alpha} (z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{\alpha! f(\zeta)}{{(\zeta-z)}^{\alpha+1}}
\,
d \zeta .
\end{equation*}
In particular, if $f$ is given by \eqref{holfunc:ps}, then
\begin{equation*}
c_\alpha = \frac{1}{\alpha!} \frac{\partial^{\sabs{\alpha}}f}{\partial
z^\alpha} (a),
\avoidbreak
\end{equation*}
and we have the \emph{\myindex{Cauchy estimates}}:
\begin{equation*}
\abs{c_\alpha} \leq \frac{\snorm{f}_\Gamma}{\rho^\alpha} .
\end{equation*}
\end{prop}

Consequently, the coefficients of the power series depend only on
the derivatives of $f$ at $a$
(and so on the values of $f$ in an arbitrarily small
neighborhood of $a$) and not the specific polydisc used in the theorem.

\begin{proof}
By the Leibniz rule,
if $z \in \Delta$ (not on the boundary),
we can differentiate under the integral in the Cauchy formula.
We are talking regular real
partial differentiation, and we use it to apply the Wirtinger operator.
The point is that
\begin{equation*}
\frac{\partial}{\partial z_\ell} \left[
\frac{1}{{(\zeta_\ell-z_\ell)}^k} \right]
=
\frac{k}{{(\zeta_\ell-z_\ell)}^{k+1}} .
\end{equation*}
Let us do a single derivative to
get the idea:
\begin{equation*}
\begin{split}
\frac{\partial f}{\partial z_1}(z) &=
\frac{\partial}{\partial z_1} \left[
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{(\zeta_1-z_1)(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n
\right]
\\
& =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{{(\zeta_1-z_1)}^2(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n .
\end{split}
\end{equation*}
How about we do it a second time:
\begin{equation*}
\frac{\partial^2 f}{\partial z_1^2}(z)
=
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{2 f(\zeta_1,\zeta_2,\ldots,\zeta_n)}{{(\zeta_1-z_1)}^3(\zeta_2-z_2)\cdots(\zeta_n-z_n)}
\,
d \zeta_1
\wedge
d \zeta_2
\wedge
\cdots
\wedge
d \zeta_n .
\end{equation*}
Notice the $2$ before the $f$.  Next derivative, a $3$ is coming out.
After $m$ derivatives in $z_1$, you get the constant $m!$.
It is exactly the same thing that happens in one variable.  A moment's
thought will convince you that the following formula is correct for
$\alpha \in \N_0^n$:
\begin{equation*}
\frac{\partial^{\sabs{\alpha}}f}{\partial z^\alpha} (z) =
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{\alpha! \, f(\zeta)}{{(\zeta-z)}^{\alpha+1}}
\,
d \zeta .
\end{equation*}

Therefore,
\begin{equation*}
\alpha! \, c_\alpha =
\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha} (a) .
\end{equation*}
We obtain the Cauchy estimates as before:
\begin{equation*}
\abs{\frac{\partial^{\sabs{\alpha}}f}{\partial z^\alpha}(a)}
=
\abs{
\frac{1}{{(2\pi i)}^n}
\int_{\Gamma}
\frac{\alpha! \, f(\zeta)}{{(\zeta-a)}^{\alpha+1}}
\,
d \zeta }
\leq
\frac{1}{{(2\pi)}^n}
\int_{\Gamma}
\frac{\alpha! \, \sabs{f(\zeta)}}{\rho^{\alpha+1}}
\,
\sabs{d \zeta}
\leq
\frac{\alpha!}{\rho^\alpha}
\snorm{f}_\Gamma . \qedhere
\end{equation*}
%Or
%\begin{equation*}
%\sabs{c_\alpha} \leq
%\frac{\snorm{f}_\Gamma}{\rho^\alpha} .
%\end{equation*}
\end{proof}

As in one-variable theory, the Cauchy estimates prove the following
proposition.

\begin{prop}
Let $U \subset \C^n$ be an open set.
Suppose the sequence $f_\ell \colon U \to \C$ converges uniformly on compact subsets
to $f \colon U \to \C$.  If every $f_\ell$ is holomorphic, then $f$ is
holomorphic, and for every $\alpha$, the sequence
$\left\{ \frac{\partial^{\sabs{\alpha}} f_\ell}{\partial z^\alpha}\right\}$
converges to
$\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha}$ uniformly on compact
subsets.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove the proposition above.
\end{exercise}
\end{exbox}

Given a power series,
let $W \subset \C^n$ be the set of all points where the series
converges absolutely.
The interior of $W$ is called the
\emph{\myindex{domain of convergence}} of the series.
In one variable, every domain of convergence is a disc and hence is
described with a single number (the radius).
In several variables, the domain of convergence
is not as easy to describe.
For the multivariable geometric series,
the domain of convergence is the unit polydisc,
but in general, the domain of convergence is more complicated.

\begin{example}
In $\C^2$, the series
\begin{equation*}
\sum_{k=0}^\infty z_1 z_2^k
\end{equation*}
converges absolutely exactly on the set
\begin{equation*}
\bigl\{ z \in \C^2 : \sabs{z_2} < 1 \bigr\}
\cup
\bigl\{ z \in \C^2 : z_1 = 0 \bigr\} .
\end{equation*}
This set is not quite a polydisc.  It is neither an open set nor a closed set,
and its closure is not the closure of the domain of convergence,
which is the set $\bigl\{ z \in \C^2 : \sabs{z_2} < 1 \bigr\}$.
\end{example}

\begin{example}
The series
\begin{equation*}
\sum_{k=0}^\infty z_1^k z_2^k
\end{equation*}
converges absolutely exactly on the set
\begin{equation*}
\bigl\{ z \in \C^2 : \sabs{z_1 z_2} < 1 \bigr\} .
\end{equation*}
The picture is definitely more complicated than a polydisc.
See \figureref{fig:convergence-example-2}.
\begin{myfig}
\subimport*{figures/}{convergence-example-2.pdf_t}
\caption{Domain of convergence of $\sum_k z_1^kz_2^k$.\label{fig:convergence-example-2}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Find the domain of convergence of $\sum_{k,\ell} \frac{1}{\ell !} z_1^k z_2^\ell$
and draw the corresponding picture.
\end{exercise}

\begin{exercise}
Find the domain of convergence of $\sum_{k,\ell} c_{k \ell} z_1^k z_2^\ell$
and draw the corresponding picture if $c_{\ell \ell} = 2^\ell$, $c_{0 \ell} = c_{k 0} =
1$ and $c_{k \ell} = 0$ otherwise.
\end{exercise}

\begin{exercise}
Suppose a power series in two variables can be written
as a sum of a power series in $z_1$ and a power series in $z_2$.
Show that the domain of convergence is a polydisc.
\end{exercise}
\end{exbox}

A domain
$U \subset \C^n$ is a
\emph{\myindex{Reinhardt domain}} if whenever
$z \in U$ and $\sabs{z_k} = \sabs{w_k}$ for all $k$, then $w \in U$.
The domains we were drawing so far are
Reinhardt domains.
They are exactly the domains that you can
draw by plotting what happens for the moduli of the variables.
A domain is a
\emph{\myindex{complete Reinhardt domain}}\index{Reinhardt domain!complete}
if $z \in U$, then $\overline{\Delta_r(0)} \subset U$ where
$r = (r_1,\ldots,r_n)$ and $r_k = \sabs{z_k}$ for all $k$.
So a complete Reinhardt domain is a union (possibly infinite) of polydiscs
centered at the origin.

\begin{prop} \label{prop:domainofconvergence}
Let $\sum_{\alpha} c_{\alpha} z^\alpha$ be a convergent power series.
Prove that its domain of convergence is a complete Reinhardt domain.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove \propref{prop:domainofconvergence}.
\end{exercise}
\end{exbox}

\begin{thm}[Identity theorem\index{identity theorem}]\label{thm:identity}
\pagebreak[2]
Let $U \subset \C^n$ be a domain (connected open set) and let
$f \colon U \to \C$ be holomorphic.
If $f|_N \equiv 0$ for a nonempty open subset $N \subset U$,
then $f \equiv 0$.
\end{thm}

\begin{proof}
Let $Z$ be the set where all derivatives of all orders of $f$ are zero; then
$N \subset Z$, so $Z$ is nonempty.  The set $Z$ is closed in $U$
as all derivatives are continuous.
Take an arbitrary $a \in Z$.
Expand $f$
in a power series around $a$ converging to $f$ in a polydisc
$\Delta_\rho(a) \subset U$.
As the coefficients are given by derivatives
of $f$, the power series is the zero series.  Hence, $f$ is
identically zero in $\Delta_\rho(a)$.  Therefore, $Z$ is open.
As $Z$ is also closed and nonempty, and $U$ is connected,
we have $Z = U$.
\end{proof}

The theorem is often used to show that
if two holomorphic functions $f$ and $g$
are equal on a small open set,
then $f \equiv g$.  In one variable (see \thmref{thm:onevaridentity}),
the hypothesis that $N$ has a limit point in $U$ (rather than being open)
is sufficient.  In several
variables, things are not so simple: $f(z_1,z_2) = z_1$ is zero on the set
$\{ z \in \C^2 : z_1=0 \}$, all of whose points are its limit points.
When $n \geq 2$, zeros are never isolated,
see \exerciseref{exercise:noisolatedzeros}.
For now, let us move on.

\begin{thm}[Maximum principle\index{maximum principle}]
Let $U \subset \C^n$ be a domain.
Let $f \colon U \to \C$ be holomorphic and suppose $\sabs{f(z)}$
attains a local maximum at some $a \in U$.  Then $f \equiv f(a)$.
\end{thm}

\begin{proof}
Suppose $\sabs{f(z)}$ attains a local maximum at $a \in U$.  Consider a polydisc
$\Delta = \Delta_1 \times \cdots \times \Delta_n \subset U$
centered at $a$.  The function
\begin{equation*}
z_1 \mapsto f(z_1,a_2,\ldots,a_n)
\end{equation*}
is holomorphic
on the disc $\Delta_1$ and its modulus attains the maximum
at the center.  Therefore, it is constant by the maximum principle in one variable,
that is, $f(z_1,a_2,\ldots,a_n)  = f(a)$ for all $z_1 \in \Delta_1$.  For
any fixed $z_1
\in \Delta_1$, consider the function
\begin{equation*}
z_2 \mapsto f(z_1,z_2,a_3,\ldots,a_n)  .
\end{equation*}
This function, holomorphic on the disc $\Delta_2$, again attains its maximum modulus at the center of $\Delta_2$
and hence is constant on $\Delta_2$.  Iterating this procedure, we obtain
that $f(z) = f(a)$ for all $z \in \Delta$.  The identity theorem says
that $f(z) = f(a)$ for all $z \in U$.
\end{proof}

\begin{exbox}
\begin{exercise} \label{exercise:averageDelta}
Let $V$ be the volume measure on $\R^{2n}$ and hence on $\C^n$.
Suppose $\Delta$ centered at $a \in \C^n$, and $f$ is a function holomorphic on
a neighborhood of $\overline{\Delta}$.  Prove
\begin{equation*}
f(a) =
\frac{1}{V(\Delta)}
\int_{\Delta} f(\zeta) \, dV(\zeta) ,
\end{equation*}
where $V(\Delta)$ is the volume of $\Delta$ and $dV$ is the volume measure.
That is, $f(a)$ is an average of the values on a polydisc centered at $a$.
\end{exercise}

\begin{exercise}
Prove the maximum principle by using the Cauchy formula instead.  Hint:
Use the previous exercise.
\end{exercise}

\begin{exercise}
Prove a several variables analogue of the \emph{\myindex{Schwarz's lemma}}:
Suppose $f$ is holomorphic in a neighborhood of $\overline{\bB_n}$,
$f(0) = 0$, and for some $k \in \N$ we have
$\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha} (0) =
0$ whenever $\sabs{\alpha} < k$.  Further suppose
for all $z \in \bB_n$,
$\sabs{f(z)} \leq M$ for some $M$.  Show that
\begin{equation*}
\sabs{f(z)} \leq M \snorm{z}^k
\qquad
\text{for all $z \in \overline{\bB_n}$}.
\end{equation*}
\end{exercise}

\begin{exercise}
Apply the one-variable Liouville's theorem to prove it for several variables.
That is, suppose $f \colon \C^n \to \C$ is holomorphic and bounded.
Prove $f$ is constant.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Improve Liouville's theorem slightly in $\C^2$.
A complex line though the origin is
the image of a nonzero linear map $L \colon \C \to \C^n$.
\begin{exparts}
\item
Prove that
for every collection of finitely many complex lines through the origin,
there exists an entire nonconstant holomorphic function ($n \geq 2$)
bounded (hence constant) on these complex lines.
\item
Prove that if an entire holomorphic function in $\C^2$ is bounded on
countably many distinct
complex lines through the origin, then it is constant.
\item
Find a nonconstant entire holomorphic function in $\C^3$ that is
bounded on
countably many distinct
complex lines through the origin.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]
Prove the several variables version of \emph{\myindex{Montel's theorem}}:
Suppose $\{ f_k \}$ is a uniformly bounded sequence of holomorphic functions on an open set $U
\subset \C^n$.
Show that there exists a subsequence $\{ f_{k_j} \}$ that converges
uniformly on compact subsets to some holomorphic function $f$.
Hint: Mimic the one-variable proof.
\end{exercise}

\begin{exercise}
Prove a several variables version of \emph{\myindex{Hurwitz's theorem}}:
Suppose $\{ f_k \}$ is a sequence of nowhere zero
holomorphic functions on a domain $U
\subset \C^n$ converging uniformly on compact subsets to a function $f$.
Show that either $f$ is identically zero or that $f$ is nowhere zero.
Hint: Feel free to use the \hyperref[thm:onevarhurwitz]{one-variable result}.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Suppose $p \in \C^n$ is a point and $D \subset \C^n$ is a ball centered at
$p \in D$.
A holomorphic function $f \colon D \to \C$ can be
\emph{analytically continued}\index{analytic continuation}
along a path
$\gamma \colon [0,1] \to \C^n$, $\gamma(0) = p$, if for every $t \in [0,1]$ there exists
a ball $D_t$ centered at $\gamma(t)$, where $D_0 = D$, and a holomorphic function
$f_t \colon D_t \to \C$, where $f_0 = f$, and for each $t_0 \in [0,1]$ there is an
$\epsilon > 0$ such that if $\sabs{t-t_0} < \epsilon$, then $f_t = f_{t_0}$
in $D_t \cap D_{t_0}$.
Prove a several variables version of the \emph{\myindex{Monodromy theorem}}:
If $U \subset \C^n$ is a simply connected domain, $D \subset U$ a ball, and
$f \colon D \to \C$ a holomorphic function that can be analytically
continued from $p \in D$ to every $q \in U$, then there exists
a unique holomorphic function $F \colon U \to \C$ such that $F|_D = f$.
\end{exercise}
\end{exbox}

\begin{defn}
Let $U \subset \C^n$ be an open set.
\glsadd{not:O}%
Define $\sO(U)$ to be the \emph{\myindex{ring of holomorphic functions}}
$f \colon U \to \C$.
The letter $\sO$ is used to recognize the fundamental contribution to
several complex variables by
Kiyoshi Oka\footnote{%
See \url{https://en.wikipedia.org/wiki/Kiyoshi_Oka}.}.
\end{defn}

\pagebreak[1]
The set $\sO(U)$ really is a commutative ring
under pointwise addition and multiplication (exercise below).
For us, $\sO(U)$ will always mean the set of $\C$-valued functions,
however, in the literature the notation is sometimes used
to simply denote holomorphicity no matter the codomain.


\begin{exbox}
\begin{exercise}
Prove that $\sO(U)$ is actually a commutative ring with the operations
\begin{equation*}
(f+g)(z) = f(z)+g(z), \qquad (fg)(z) = f(z)g(z) .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $\sO(U)$ is an \myindex{integral domain} (has no zero divisors) if and only
if $U$ is connected.
That is, show that $U$ being connected is equivalent to the following
property of $U$:
If $h(z) = f(z)g(z)$ is identically zero for $f,g \in \sO(U)$,
then either $f$ or $g$ is
identically zero.
\end{exercise}
\end{exbox}

A function $F$ defined on a dense open subset of $U$ is
\emph{\myindex{meromorphic}} if locally near every $p \in U$,
$F = \nicefrac{f}{g}$ for $f$ and $g$ holomorphic in some neighborhood of
$p$.
It is from a deep result
of Oka that,
for domains $U \subset \C^n$, every meromorphic function can be represented
as $\nicefrac{f}{g}$ globally.  That is, the ring of meromorphic functions
is the field of fractions of $\sO(U)$.
This problem is the so-called
\emph{\myindex{Poincar\'e problem}}, and its solution is no longer positive
once one generalizes $U$ to complex manifolds.
The points of $U$ through which $F$ does not extend holomorphically are called the
\emph{\myindex{poles}} of $F$.
Namely, poles are the points where
$g=0$ for every possible representation $\nicefrac{f}{g}$.
Unlike in one variable, in several variables, poles are never isolated
points.
There is also a new type of singular
point for meromorphic functions in more than one
variable:

\begin{exbox}
\begin{exercise}
In two variables, one can no longer think of a meromorphic function $F$
having the value $\infty$ when the denominator vanishes.
Show that $F(z,w) = \nicefrac{z}{w}$ achieves all values of $\C$
in every neighborhood of the origin.  We call the origin
a \emph{\myindex{point of indeterminacy}}.
\end{exercise}

\begin{exercise} \label{exercise:noisolatedzeros}
Prove that zeros
are never isolated in $\C^n$ for $n \geq 2$.
Hint: Consider $z_1 \mapsto f(z_1,z_2,\ldots,z_n)$ as you move
$z_2,\ldots,z_n$ around, and use, perhaps,
\hyperref[thm:onevarhurwitz]{Hurwitz}.
\end{exercise}
\end{exbox}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Derivatives} \label{sec:derivatives}

Given a function $f = u+iv$, the complex conjugate is $\bar{f} = u-iv$,
defined simply by $z \mapsto \overline{f(z)}$.
When $f$ is holomorphic, then $\bar{f}$ is
called an \emph{\myindex{antiholomorphic function}}.  An antiholomorphic function is a
function that depends on $\bar{z}$ but not on $z$.
So if we write the variable, we write $\bar{f}$ as
$\bar{f}(\bar{z})$.  Let us see why this makes sense.  Using the definitions
of the Wirtinger operators,
\begin{equation*}
\frac{\partial \bar{f}}{\partial z_\ell} =
\overline{\frac{\partial f}{\partial \bar{z}_\ell}} = 0,
\qquad
\frac{\partial \bar{f}}{\partial \bar{z}_\ell} = \overline{
\left(\frac{\partial f}{\partial z_\ell} \right) } ,
\qquad
\text{for all $\ell=1,\ldots,n$.}
\end{equation*}

For functions that are neither holomorphic or antiholomorphic, we
pretend they depend on both $z$ and $\bar{z}$.
Since we want to write functions in terms of $z$ and $\bar{z}$,
let us figure out how the chain rule works for Wirtinger derivatives,
rather than writing derivatives in terms of $x$ and $y$.

\begin{prop}[Complex chain rule]\index{complex chain rule}
\pagebreak[2]
Suppose
$U \subset \C^n$ and $V \subset \C^m$ are open, and suppose
$f \colon U \to V$ and $g \colon V \to \C$ are (real) differentiable
mappings.  Write the variables as
$z = (z_1,\ldots,z_n) \in U \subset \C^n$ and $w = (w_1,\ldots,w_m) \in V
\subset \C^m$.  Then for $\ell=1,\ldots,n$,
\begin{equation} \label{eq:chainrule}
\begin{aligned}
&
\frac{\partial}{\partial z_\ell} \left[ g \circ f \right]
=
\sum_{k=1}^m \left(
\frac{\partial g}{\partial w_k}
\frac{\partial f_k}{\partial z_\ell}
+
\frac{\partial g}{\partial \bar{w}_k}
\frac{\partial \bar{f}_k}{\partial z_\ell}
\right),
\\
&
\frac{\partial}{\partial \bar{z}_\ell} \left[ g \circ f \right]
=
\sum_{k=1}^m \left(
\frac{\partial g}{\partial w_k}
\frac{\partial f_k}{\partial \bar{z}_\ell}
+
\frac{\partial g}{\partial \bar{w}_k}
\frac{\partial \bar{f}_k}{\partial \bar{z}_\ell}
\right) .
\end{aligned}
\end{equation}
\end{prop}

\begin{proof}
Write $f = u+iv$, $z = x+iy$, $w=s+it$, and let
$f$ be a function of $z$, and $g$ be a function of $w$.
The composition plugs in $f$ for $w$, and so it plugs in $u$ for $s$, and
$v$ for $t$.
Using the standard chain rule,
\begin{align*}
\frac{\partial}{\partial z_\ell} \left[ g \circ f \right]
& =
\frac{1}{2}
\left(
\frac{\partial}{\partial x_\ell} - i
\frac{\partial}{\partial y_\ell}
\right)
 \left[ g \circ f \right]
\displaybreak[0]\\
& =
\frac{1}{2}
\sum_{k=1}^m \left(
\frac{\partial g}{\partial s_k} \frac{\partial u_k}{\partial x_\ell}
+
\frac{\partial g}{\partial t_k} \frac{\partial v_k}{\partial x_\ell}
-
i
\left(
\frac{\partial g}{\partial s_k} \frac{\partial u_k}{\partial y_\ell}
+
\frac{\partial g}{\partial t_k} \frac{\partial v_k}{\partial y_\ell}
\right)
\right)
\displaybreak[0]\\
& =
\sum_{k=1}^m \left(
\frac{\partial g}{\partial s_k}
\,
\frac{1}{2}
\left(
\frac{\partial u_k}{\partial x_\ell}
-
i
\frac{\partial u_k}{\partial y_\ell}
\right)
+
\frac{\partial g}{\partial t_k}
\,
\frac{1}{2}
\left(
\frac{\partial v_k}{\partial x_\ell}
-
i
\frac{\partial v_k}{\partial y_\ell}
\right)
\right)
\displaybreak[0]\\
& =
\sum_{k=1}^m \left(
\frac{\partial g}{\partial s_k}
\frac{\partial u_k}{\partial z_\ell}
+
\frac{\partial g}{\partial t_k}
\frac{\partial v_k}{\partial z_\ell}
\right) .
\end{align*}
For $k = 1, \ldots, m$,
\begin{equation*}
\frac{\partial}{\partial s_k}
=
\frac{\partial}{\partial w_k}
+
\frac{\partial}{\partial \bar{w}_k} ,
\qquad
\frac{\partial}{\partial t_k}
=
i \left(
\frac{\partial}{\partial w_k}
-
\frac{\partial}{\partial \bar{w}_k}
\right) .
\end{equation*}
Continuing:
\begin{align*}
\frac{\partial}{\partial z_\ell} \left[ g \circ f \right]
& =
\sum_{k=1}^m \left(
\frac{\partial g}{\partial s_k}
\frac{\partial u_k}{\partial z_\ell}
+
\frac{\partial g}{\partial t_k}
\frac{\partial v_k}{\partial z_\ell}
\right)
\displaybreak[0]\\
& =
\sum_{k=1}^m \left(
\left(
\frac{\partial g}{\partial w_k}
\frac{\partial u_k}{\partial z_\ell}
+
\frac{\partial g}{\partial \bar{w}_k}
\frac{\partial u_k}{\partial z_\ell}
\right)
+
i
\left(
\frac{\partial g}{\partial w_k}
\frac{\partial v_k}{\partial z_\ell}
-
\frac{\partial g}{\partial \bar{w}_k}
\frac{\partial v_k}{\partial z_\ell}
\right)
\right)
\displaybreak[0]\\
& =
\sum_{k=1}^m \left(
\frac{\partial g}{\partial w_k}
\left(
\frac{\partial u_k}{\partial z_\ell}
+
i
\frac{\partial v_k}{\partial z_\ell}
\right)
+
\frac{\partial g}{\partial \bar{w}_k}
\left(
\frac{\partial u_k}{\partial z_\ell}
-i
\frac{\partial v_k}{\partial z_\ell}
\right)
\right)
\displaybreak[0]\\
& =
\sum_{k=1}^m \left(
\frac{\partial g}{\partial w_k}
\frac{\partial f_k}{\partial z_\ell}
+
\frac{\partial g}{\partial \bar{w}_k}
\frac{\partial \bar{f}_k}{\partial z_\ell}
\right) .
\end{align*}

The $\bar{z}$ derivative works similarly.
\end{proof}

Because of the proposition,
when we deal with a possibly
nonholomorphic function $f$, we often write $f(z,\bar{z})$ and treat $f$ as
a function of $z$ and $\bar{z}$.

\begin{remark}
It is good to notice the subtlety of what we just said.  Formally it seems
as if $z$ and $\bar{z}$ are independent variables when taking
derivatives, but in reality, they are not independent if we actually wish to
evaluate the function.  Under the hood, a smooth function that is not
necessarily holomorphic is really a function of the real variables
$x$ and $y$, where $z = x+iy$.
\end{remark}

\begin{remark}
We could have swapped $z$ and $\bar{z}$, by
flipping the bars everywhere.  There is no difference between the two,
they are twins in effect.  We just need to know which one is which.
After all, it all starts with taking the two square roots of $-1$ and
deciding which one is $i$ (remember the chickens?).
There is no ``natural choice'' for that, but once
we make that choice we must be consistent.  And once we picked which
root
is $i$, we also picked what is holomorphic and what is
antiholomorphic.  This is a subtle philosophical as much as a mathematical point.
\end{remark}

\begin{defn}
Let $U \subset \C^n$ be open.  A mapping $f \colon U \to \C^m$
is said to be \emph{holomorphic}\index{holomorphic mapping}
if each component is holomorphic.  That
is, if $f = (f_1,\ldots,f_m)$, then each $f_k$ is a holomorphic function.
\end{defn}

As in one variable, the composition of holomorphic functions (mappings) is
holomorphic.

\begin{thm}
Let $U \subset \C^n$ and $V \subset \C^m$ be open sets, and suppose
$f \colon U \to V$ and $g \colon V \to \C^q$ are both holomorphic.
\glsadd{not:composition}%
Then the composition $g \circ f$ is holomorphic.
\end{thm}

\begin{proof}
The proof is almost trivial by chain rule.
Again let $g$ be a function of $w \in V$ and $f$ be a function of $z \in U$.
For $\ell = 1,\ldots,n$ and $\nu=1,\ldots,q$, compute
\begin{equation*}
\frac{\partial}{\partial \bar{z}_\ell} \left[ g_\nu \circ f \right]
=
\sum_{k=1}^m
\left(
\frac{\partial g_\nu}{\partial w_k}
\cancelto{0}{\frac{\partial f_k}{\partial \bar{z}_\ell}}
+
\cancelto{0}{\frac{\partial g_\nu}{\partial \bar{w}_k}}
\frac{\partial \bar{f}_k}{\partial \bar{z}_\ell}
\right)
=
0 . \qedhere
\end{equation*}
\end{proof}

For holomorphic mappings the chain rule simplifies, and it formally looks
like the familiar vector calculus rule.
Suppose again
$U \subset \C^n$ and $V \subset \C^m$ are open, and
$f \colon U \to V$ and $g \colon V \to \C$ are holomorphic.
Name the variables
$z = (z_1,\ldots,z_n) \in U \subset \C^n$ and $w = (w_1,\ldots,w_m) \in V
\subset \C^m$.  In formula \eqref{eq:chainrule} for the $z_\ell$ derivative,
the $\bar{w}_\ell$ derivative of $g$ is zero and the $z_\ell$ derivative of
$\bar{f}_k$ is also zero because $f$ and $g$ are holomorphic.
Therefore, for $\ell=1,\ldots,n$,
\begin{equation*}
\frac{\partial}{\partial z_\ell} \left[ g \circ f \right]
=
\sum_{k=1}^m
\frac{\partial g}{\partial w_k}
\frac{\partial f_k}{\partial z_\ell} .
\end{equation*}

\begin{exbox}
\begin{exercise}
Using only the Wirtinger derivatives, prove that a holomorphic function
that is real-valued must be constant.
\end{exercise}

\begin{exercise}
Let $f$ be a holomorphic function on $\C^n$.
When we write $\bar{f}$ we mean the function $z \mapsto \overline{f(z)}$,
and we usually write $\bar{f}(\bar{z})$ as the function is antiholomorphic.
However,
if we write $\bar{f}(z)$ we really mean $z \mapsto \overline{f(\bar{z})}$,
that is, composing both the function and the argument with conjugation.
Prove $z \mapsto \bar{f}(z)$ is holomorphic, and prove $f$ is
real-valued on $\R^n$ (when $y=0$) if and only if $f(z) =
\bar{f}(z)$ for all $z \in \C^n$.
\end{exercise}
\end{exbox}

For a $U \subset \C^n$, a holomorphic mapping $f \colon U \to \C^m$,
and a point $p \in U$,
define the holomorphic derivative, sometimes called the
\emph{(holomorphic) \myindex{Jacobian matrix}}\index{holomorphic Jacobian matrix},
\glsadd{not:Df}%
\begin{equation*}
Df(p)
\overset{\text{def}}{=}
\left[
\frac{\partial f_k}{\partial z_\ell} (p)
\right]_{k \ell} .
\end{equation*}
The notation $f'(p) = Df(p)$ is also used.
Unless otherwise stated, if the mapping is holomorphic,
\emph{Jacobian} will refer to the holomorphic Jacobian.

\begin{exbox}
\begin{exercise}
Suppose $U \subset \C^n$ is open, $\R^n$ is naturally embedded in $\C^n$.
Consider a holomorphic mapping $f \colon U \to \C^m$ and suppose that
$f|_{U \cap \R^n}$ maps into $\R^m \subset \C^m$.  Prove that given
$p \in U \cap \R^n$, the real
Jacobian matrix at $p$ of the map
$f|_{U \cap \R^n} \colon U \cap \R^n \to \R^m$ is equal to the holomorphic
Jacobian matrix of the map $f$ at $p$.  In particular, $Df(p)$ is a matrix
with real entries.
\end{exercise}
\end{exbox}

By the holomorphic chain rule above, as in the theory of real functions,
the derivative of the composition is the composition of derivatives
(multiplied as matrices).

\begin{prop}[Chain rule for holomorphic mappings\index{chain rule for holomorphic mappings}]
Let $U \subset \C^n$ and $V \subset \C^m$ be open sets.  Suppose
$f \colon U \to V$ and $g \colon V \to \C^k$ are both holomorphic,
and $p \in U$.  Then
\begin{equation*}
D(g \circ f)(p) = Dg\bigl(f(p)\bigr) \, Df(p) .
\end{equation*}
\end{prop}

In shorthand, we often simply write $D(g \circ f) = Dg Df$.

\begin{exbox}
\begin{exercise}
Prove the proposition.
\end{exercise}
\end{exbox}

Suppose $U \subset \C^n$, $p \in U$, and $f \colon U \to \C^m$
is differentiable at $p$.
Since $\C^n$ is identified with $\R^{2n}$, the mapping $f$
takes $U \subset \R^{2n}$ to $\R^{2m}$.  The normal vector-calculus Jacobian at $p$
of this mapping (a $2m \times 2n$ real matrix) is called the
\emph{\myindex{real Jacobian}}, and we write it as
\glsadd{not:DRf}%
$D_\R f (p)$.

\begin{prop}
Let $U \subset \C^n$ be open, $p \in U$, and
$f \colon U \to \C^n$ be holomorphic.  Then
\begin{equation*}
\abs{\det D f(p) }^2 =
\det D_\R f(p) .
\end{equation*}
\end{prop}

The expression $\det D f(p)$ is called the
\emph{(holomorphic)
\myindex{Jacobian determinant}}\index{holomorphic Jacobian determinant}
and clearly it is important to know if we are talking about
the holomorphic Jacobian determinant or the standard real Jacobian
determinant $\det D_\R f(p)$.  Recall from vector calculus that
if the real Jacobian determinant $\det D_\R
f(p)$ of a smooth mapping is positive, then the mapping preserves
orientation.  In particular, the proposition
says that holomorphic mappings preserve orientation.

\begin{proof}
Write $f$ as
$(\Re f_1,\Im f_1, \ldots, \Re f_n, \Im f_n)$
as a function of $(x_1,y_1,\ldots,x_n,y_n)$,
using our identification of $\C^n$ and $\R^{2n}$.
The statement is about the two Jacobians at $p$, that is, the derivatives
at $p$.  Hence, we can assume that
$p=0$ and $f$ is complex linear, $f(z) = Az$ for some $n \times n$
matrix $A$.  It is just a statement about matrices.
The matrix $A$ is the (holomorphic) Jacobian matrix of $f$.
Let $B$ be the real Jacobian matrix of $f$.

We change the basis of $B$ to be $(z,\bar{z})$
using $z = x+iy$ and $\bar{z}=x-iy$
on both the target and the source.
The change of basis is some invertible
complex matrix $M$ such that
$M^{-1} B M$ (the real Jacobian matrix $B$ in this new basis)
is a matrix of the derivatives of
$(f_1,\ldots,f_n,\bar{f}_1,\ldots,\bar{f}_n)$
in terms of
$(z_1,\ldots,z_n,\bar{z}_1,\ldots,\bar{z}_n)$.
That is,
\begin{equation*}
M^{-1} B M =
\begin{bmatrix}
A & 0 \\
0 & \widebar{A}
\end{bmatrix} .
\end{equation*}
Thus
\begin{multline*}
\det (B) =
\det(M^{-1}M B)
=
\det(M^{-1} B M)
\\
=
\det(A) \det(\widebar{A})
=
\det(A) \, \overline{\det(A)}
=
\abs{\det(A)}^2 .  \qedhere
\end{multline*}
\end{proof}

The regular (real) implicit function theorem and the chain rule
give that the implicit function theorem holds in the holomorphic setting.
The main thing to check is to verify that the solution given by the
standard implicit function theorem is holomorphic, which follows by the
chain rule.

\begin{thm}[Implicit function theorem\index{implicit function
theorem}\index{holomorphic implicit function theorem}]\label{thm:ift}%
\pagebreak[2]
Let $U \subset \C^{n} \times \C^{m}$ be an open set, let $(z,w) \in \C^n \times
\C^m$ be our coordinates, and let $f \colon U \to \C^m$
be a holomorphic mapping.  Let $(z^0,w^0) \in U$ be a point such that
$f(z^0,w^0) = 0$ and such that the $m \times m$ matrix
\begin{equation*}
\left[
\frac{\partial f_k}{\partial w_\ell} (z^0,w^0)
\right]_{k\ell}
\end{equation*}
is invertible.
Then there exists an
open set $V \subset \C^n$ with $z^0 \in V$,
open set $W \subset \C^m$ with $w^0 \in W$,
$V \times W \subset U$,
and
a holomorphic
mapping $g \colon V \to W$, with $g(z^0) = w^0$
such that
for every $z \in V$, the point $g(z)$ is the unique point in $W$
such that
\begin{equation*}
f\bigl(z,g(z)\bigr) = 0 .
\end{equation*}
\end{thm}

\begin{exbox}
\begin{exercise}
Prove the holomorphic implicit function theorem above.
Hint: Check that the normal implicit function theorem for $C^1$
functions applies, and then show that the $g$ you obtain is holomorphic.
\end{exercise}

\begin{exercise}
State and prove a holomorphic version of the inverse function theorem.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is a domain and $f \colon U \to \C^m$
a holomorphic mapping.
\begin{exparts}
\item
Prove the vector-valued version of the maximum principle:
If $\snorm{f(z)}$ achieves a (local) maximum at $p \in U$,
then $f$ is constant.
\item
Find a counterexample to a vector-valued mimimum principle:
Find an $f$ such that $\snorm{f(z)}$ achieves a nonzero minimum,
but where $f$ is not constant.
\end{exparts}
\end{exercise}
\end{exbox}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak[2]
\section{Inequivalence of ball and polydisc}

\begin{defn}
Two domains $U \subset \C^n$ and $V \subset \C^n$ are said to be
\emph{\myindex{biholomorphic}} or
\emph{\myindex{biholomorphically equivalent}}
if there exists a one-to-one and onto holomorphic map $f
\colon U \to V$ such that the inverse
\glsadd{not:inverse}%
$f^{-1} \colon V \to U$ is holomorphic.
The mapping $f$ is said to be a
\emph{biholomorphic map}\index{map!biholomorphic} or a
\emph{\myindex{biholomorphism}}.
\end{defn}

As function theory on two biholomorphic domains is the same,
one of the main questions in complex analysis is to classify domains up
to biholomorphic transformations.  In one variable, there is the rather
striking theorem due to Riemann:

\begin{thm}[Riemann mapping theorem\index{Riemann mapping theorem}]
If $U \subset \C$ is a nonempty simply connected domain such that $U \neq \C$,
then $U$ is biholomorphic to $\D$.
\end{thm}

In one variable, a topological property on $U$ is enough to classify a whole
class of domains.  It is one of the reasons why studying the disc is so
important in one variable, and why many theorems are stated for
the disc only.
There is no such theorem in several variables.
We will show momentarily that the unit ball and the polydisc,
\begin{equation*}
\bB_n = \bigl\{ z \in \C^n : \snorm{z} < 1 \bigr\}
\qquad \text{and} \qquad
\D^n = \bigl\{ z \in \C^n : \sabs{z_k} < 1 ~\text{for $k=1,\ldots,n$} \bigr\} ,
\end{equation*}
are \emph{not} biholomorphically equivalent.  Both are simply
connected (have no holes), and they are the two most obvious generalizations
of the disc to several variables.  They are homeomorphic, that is, topology
does not see any difference.

\begin{exbox}
\begin{exercise}
Prove that there exists a \emph{\myindex{homeomorphism}} $f \colon \bB_n \to
\D^n$,
that is, $f$ is a bijection, and both $f$ and $f^{-1}$ are continuous.
\end{exercise}
\end{exbox}


Let us stick with $n=2$.
Instead of proving that $\bB_2$ and
$\D^2$ are biholomorphically
inequivalent we will prove a stronger theorem.  First a
definition.

\begin{defn}
Suppose $f \colon X \to Y$ is a continuous map between two topological
spaces.  Then $f$ is a \emph{\myindex{proper map}}\index{map!proper} if for every compact
\glsadd{not:compact}%
\glsadd{not:pullback}%
$K \subset \subset Y$, the set $f^{-1}(K)$ is compact.
\end{defn}

The notation ``$\subset \subset$'' is a common notation for a relatively
compact subset, that is, the closure is compact in the relative (subspace)
topology.  Often the distinction between compact and relatively
compact is not important.  For instance, in the definition above we can replace
compact with relatively compact.
So the notation is sometimes used if ``compact'' is meant.

Vaguely, ``proper'' means that ``boundary goes to the boundary.''
As a continuous map, $f$ pushes compacts to compacts; a proper map is
one where the inverse does so too.  If the inverse is a continuous
function, then clearly $f$ is proper,
but not every proper map is invertible.
For example, the map $f \colon \D \to \D$ given by $f(z) = z^2$ is proper,
but not invertible.  The codomain of $f$ is important.
If we replace $f$ by $g \colon \D \to \C$, still given by $g(z)=z^2$,
then the map is
no longer proper.  Let us state the main result of this section.

\begin{thm}[Rothstein 1935] \label{thm:Rothstein}
\index{Rothstein's theorem}
There exists no proper holomorphic mapping of the unit bidisc $\D^2 = \D \times \D
\subset \C^2$ to the unit ball $\bB_2 \subset \C^2$.
\end{thm}

As a biholomorphic mapping is proper,
the unit bidisc is not biholomorphically
equivalent to the unit ball in $\C^2$.
The inequivalence of the ball and the polydisc was first proved by
Poincar\'e by computing the automorphism groups of $\D^2$ and $\bB_2$,
although his proof
assumed the maps extended past the boundary.
The first complete proof was by Henri Cartan in 1931, though the theorem is
popularly attributed to Poincar\'e.  It seems standard practice that any general audience talk
about several complex variables contains a mention of Poincar\'e,
and often the reference is to this exact theorem.

We need some lemmas before we get to the proof of the result.  First,
a certain one-dimensional object plays an important role in the geometry
of several complex variables.  It allows us to apply one-variable
results in several variables.  It is especially important in
understanding the boundary behavior of holomorphic functions.  It also
prominently appears in complex geometry.

\begin{defn}
A nonconstant holomorphic mapping
$\varphi \colon \D \to \C^n$ is called an \emph{\myindex{analytic disc}}.
If the mapping $\varphi$ extends continuously to the closed unit disc
$\overline{\D}$, then the mapping
$\varphi \colon \overline{\D} \to \C^n$ is called
a \emph{\myindex{closed analytic disc}}.

Often we call the image $\Delta = \varphi(\D)$ the analytic disc
rather than the mapping.  For a closed analytic disc we write
$\partial \Delta = \varphi( \partial \D)$ and call it the boundary
of the analytic disc.
\end{defn}

In some sense, analytic discs play the role of line segments in $\C^n$.  It
is important to always keep in mind that there is a mapping defining the
disc, even if we are more interested in the set.  Obviously for a given
image, the mapping $\varphi$ is not unique.

Consider the boundaries of
the unit bidisc $\D \times \D \subset \C^2$
and the unit ball $\bB_2 \subset \C^2$.
Notice the boundary of the unit bidisc contains analytic discs
$\{p\} \times \D$
and $\D \times \{p\}$ for $p \in \partial \D$.
That is, through every point in the boundary, except for the distinguished
boundary $\partial \D \times \partial \D$, there exists an analytic disc
lying entirely inside the boundary.  On the other hand, the ball
contains no analytic discs in its boundary.

\begin{prop}\label{prop:noanaldiscinsphere}
\glsadd{not:unitsphere}%
The unit sphere $S^{2n-1} = \partial \bB_n \subset \C^n$
contains no analytic discs.
\end{prop}

\begin{proof}
Suppose there is a holomorphic function $g \colon \D \to \C^n$
such that the image $g(\D)$ is inside the unit sphere.  In other words,
for all $z \in \D$,
\begin{equation*}
\snorm{g(z)}^2
= \sabs{g_1(z)}^2 + \sabs{g_2(z)}^2 + \cdots + \sabs{g_n(z)}^2 = 1 .
\end{equation*}
Without loss of generality (after composing with a unitary matrix),
assume that $g(0) = (1,0,0,\ldots,0)$.
Consider the first component and notice that $g_1(0) = 1$.
If a sum of positive numbers is less than or equal to $1$,
then they all are, and hence $\sabs{g_1(z)} \leq 1$.
The maximum principle says
that $g_1(z) = 1$ for all $z \in \D$.  But then $g_k(z) = 0$
for all $k=2,\ldots,n$ and all $z \in \D$.  Therefore, $g$ is constant and
thus not an analytic disc.
\end{proof}

The fact that the sphere contains no analytic discs
is the most important geometric distinction between the boundary of
the polydisc and the sphere.

\begin{exbox}
\begin{exercise}
Modify the proof to show some stronger results.
\begin{exparts}
\item
Let $\Delta$ be an analytic disc
and $\Delta \cap \partial \bB_n \not= \emptyset$.
Prove $\Delta$ contains points not in
$\overline{\bB_n}$.
\item
Let $\Delta$ be an analytic disc.
Prove that $\Delta \cap \partial \bB_n$ is nowhere dense in $\Delta$.
\item
Find an analytic disc in $\C^2$, such that $(1,0) \in \Delta$, $\Delta \cap \bB_2 =
\emptyset$, and locally near
$(1,0) \in \partial \bB_2$, the set
$\Delta \cap \partial \bB_2$ is the
curve defined by $\Im z_1=0$, $\Im z_2=0$,
${(\Re z_1)}^2+ {(\Re z_2)}^2 = 1$.
\end{exparts}
\end{exercise}
\end{exbox}

Before we prove the theorem, let us make the statement about
proper maps taking boundary to boundary precise.

\begin{lemma} \label{lemma:bndrytobndry}
Let $U \subset \R^n$ and $V \subset \R^m$ be bounded domains and
let $f \colon U \to V$ be continuous.
Then $f$ is proper if and only if
for every sequence $\{ p_k \}$ in $U$ such that $p_k \to p \in \partial U$,
the set of limit points of $\bigl\{ f(p_k) \bigr\}$ lies in $\partial V$.
\end{lemma}

\begin{proof}
Suppose $f$ is proper.  Let
$\{ p_k \}$ be a sequence in $U$ such that $p_k \to p \in \partial U$.
Take any convergent subsequence $\bigl\{ f(p_{k_\ell}) \bigr\}$ of
$\bigl\{ f(p_k) \bigr\}$
converging to some $q \in \widebar{V}$.  Consider
$E = \bigl\{ f(p_{k_\ell}) \bigr\}$ as a set.  Let $\widebar{E}$ be the closure of $E$
in $V$ (subspace topology).  If $q \in V$, then $\widebar{E} = E \cup \{ q
\}$ and $\widebar{E}$ is compact.
Otherwise, if $q \notin V$, then
$\widebar{E} = E$ and $\widebar{E}$ is not compact.
The inverse image $f^{-1}(\widebar{E})$
is not compact (it contains a sequence going to $p \in \partial U$)
and hence $\widebar{E}$ is not
compact either as $f$ is proper.  Thus $q \notin V$, and hence $q \in
\partial V$.
As we took an arbitrary convergent subsequence of $\bigl\{ f(p_k) \bigr\}$,
$q$ was an arbitrary limit point.
Therefore, all limit points are in $\partial V$.

Let us prove the converse.
Suppose that for every sequence
$\{ p_k \}$ in $U$ such that $p_k \to p \in \partial U$,
the set of limit points of $\bigl\{ f(p_k) \bigr\}$ lies in $\partial V$.
Take a closed set $E \subset V$ (subspace topology) and suppose $f^{-1}(E)$
is not compact.  Then there exists a sequence $\{ p_k \}$ in $f^{-1}(E)$
such that $p_k \to p \in \partial U$, because $f^{-1}(E)$ is closed
(in $U$), bounded, but not compact.
The hypothesis then says that the limit points of
$\bigl\{ f(p_k) \bigr\}$ are in $\partial V$.  Hence $E$ has limit points in
$\partial V$ and is not compact.
\end{proof}

\begin{exbox}
\begin{exercise}
Let $U \subset \R^n$ and $V \subset \R^m$ be bounded domains and
let $f \colon \widebar{U} \to \widebar{V}$ be continuous.
Suppose $f(U) \subset V$, and $g \colon U \to V$ is defined by
$g(x) = f(x)$ for all $x \in U$.
Prove that $g$ is proper if and only if $f(\partial U) \subset \partial V$.
\end{exercise}

\begin{exercise}
Let $f \colon X \to Y$ be a continuous function of locally compact Hausdorff topological spaces.
Let $X_\infty$ and $Y_\infty$ be the
one-point compactifications of $X$ and $Y$.
Then $f$ is a proper map if and only if it extends as a continuous map
$f_\infty \colon X_\infty \to Y_\infty$ by letting
$f_\infty |_X = f$ and
 $f_\infty(\infty) = \infty$.
\end{exercise}
\end{exbox}

We now have all the lemmas needed to prove the theorem of Rothstein.

\begin{proof}[Proof of \thmref{thm:Rothstein}]
Suppose there is a proper holomorphic map $f \colon \D^2
\to \bB_2$.
Fix some $e^{i\theta}$ in the boundary of the disc $\D$.  Take a sequence
$w_k \in \D$ such that $w_k \to e^{i\theta}$.   The functions
$g_k(\zeta) =  f(\zeta,w_k)$ map the unit disc into $\bB_2$.
By \hyperref[thm:onevarmontel]{Montel's theorem} and
by passing to a subsequence, assume that
the sequence of functions converges (uniformly on compact subsets) to
a limit $g \colon \D \to \overline{\bB}_2$.  As $(\zeta,w_k) \to
(\zeta,e^{i\theta}) \in \partial \D^2$, then by
\lemmaref{lemma:bndrytobndry}, $g(\D) \subset \partial \bB_2$,
and hence $g$ is constant by \propref{prop:noanaldiscinsphere}.

Let $g_k'$ denote the derivative (we differentiate each component).
The functions $g_k'$ converge to $g' = 0$.
So for an arbitrary fixed $\zeta \in \D$,
$\frac{\partial f}{\partial z_1} (\zeta, w_k) \to 0$.
This limit holds for all $e^{i\theta}$ and some subsequence of
an arbitrary sequence $\{ w_k \}$ where $w_k \to e^{i\theta}$.  The
holomorphic mapping $w \mapsto \frac{\partial f}{\partial z_1} (\zeta, w)$,
therefore, extends continuously
to the closure $\overline{\D}$ and is zero on $\partial \D$.
We apply the maximum
principle or the Cauchy formula and the fact that $\zeta$ was arbitrary to find
$\frac{\partial f}{\partial z_1} \equiv 0$.  By symmetry
$\frac{\partial f}{\partial z_2} \equiv 0$.  Therefore, $f$ is constant,
which is a contradiction as $f$ was proper.

The proof is illustrated in \figureref{fig:rothstein}.
In the picture, on the left-hand side is the bidisc, and we
restrict $f$ to the horizontal gray lines (where the second component is
fixed to be $w_k$) and take a limit to produce an analytic disc
in the boundary of $\bB_2$.  We then show that $\frac{\partial f}{\partial
z_1} = 0$ on the vertical gray line (where the first component is fixed to
be $\zeta$).  The right-hand side shows the disc where $z_1 = \zeta$ is
fixed, which corresponds to the vertical gray line on the left.
\begin{myfig}
\subimport*{figures/}{rothstein.pdf_t}
\caption{The proof of Rothstein's theorem.\label{fig:rothstein}}
\end{myfig}
\end{proof}

The proof says that the reason why there is not even a proper mapping is the fact
that the boundary of the polydisc contains analytic discs, while
the sphere does not.
The proof extends easily to higher dimensions as well, and the proof
of the generalization is left as an exercise.

\begin{thm} \label{thm:nopropmapprodandnodisc}
Let $U = U' \times U'' \subset \C^n \times \C^k$, $n,k \geq 1$, and $V
\subset \C^m$, $m \geq 1$, be bounded
domains such that $\partial V$ contains no analytic discs.
Then there exist no proper
holomorphic mapping $f \colon U \to V$.
\end{thm}

\begin{exbox}
\begin{exercise}
Prove \thmref{thm:nopropmapprodandnodisc}.
\end{exercise}
\end{exbox}

The key takeaway from this section is that
in several variables, to see if two domains are equivalent,
the geometry of the boundaries makes a difference,
not just the topology of the domains.

\medskip

The following is a fun exercise in one dimension about proper maps of discs:

\begin{exbox}
\begin{exercise}
Let $f \colon \D \to \D$ be a proper holomorphic  map.  Then
\begin{equation*}
f(z) =
e^{i\theta} \prod_{k=1}^m \frac{z-a_k}{1-\bar{a}_k z} ,
\end{equation*}
where $\theta \in \R$ and $a_k \in \D$ (that is, $f$ is a finite
Blaschke product).  Hint: Consider $f^{-1}(0)$.
\end{exercise}
\end{exbox}

In several variables, when $\D$ is replaced by a ball,
this question (what are the proper maps)
becomes far more involved, and if the dimensions of the balls are
different, it is not solved in general.

\begin{exbox}
\begin{exercise}
Suppose $f \colon U \to \D$ be a proper holomorphic map where $U \subset
\C^n$ is a nonempty domain.  Prove that $n=1$.  Hint: Consider the same idea as in
\exerciseref{exercise:noisolatedzeros}.
\end{exercise}

\begin{exercise}
Suppose $f \colon \overline{\bB_n} \to \C^m$ is a nonconstant continuous
map such that $f|_{\bB_n}$ is holomorphic and $\snorm{f(z)} = 1$ whenever
$\snorm{z}=1$.  Prove that
$f|_{\bB_n}$ maps into $\bB_m$ and furthermore that this map is proper.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Cartan's uniqueness theorem}

The following theorem is another analogue of Schwarz's lemma to
several variables.  It says that for a bounded domain, it is enough to know
that a self mapping is the identity at a single point to show that it is the
identity everywhere.  As there are quite a few theorems named for Cartan,
this one is often referred to as the
\emph{\myindex{Cartan's uniqueness theorem}}.  It is useful in
computing the automorphism groups of certain domains.
An \emph{\myindex{automorphism}} of $U$ is a biholomorphic map from
$U$ onto $U$.
Automorphisms form a group under composition,
called the \emph{\myindex{automorphism group}}.
As exercises, you will use the theorem to compute the automorphism groups
of $\bB_n$ and $\D^n$.

\begin{thm}[Cartan]
Suppose $U \subset \C^n$ is a bounded domain, $a \in U$,
$f \colon U \to U$ is a holomorphic mapping, $f(a) = a$,
and $Df(a)$ is the identity.  Then $f(z) = z\,$ for all $z \in U$.
\end{thm}

\begin{exbox}
\begin{exercise}
Find a counterexample to the theorem if $U$ is unbounded.
Hint: For simplicity take $a=0$ and $U=\C^n$.
\end{exercise}
\end{exbox}


Before we get into the proof, we write the Taylor
series of a function in a nicer way, splitting it up into parts of different
degree.
A polynomial $P \colon \C^n \to \C$ is \emph{\myindex{homogeneous}}
of degree $d$ if
\begin{equation*}
P(s z) = s^d P(z)
\end{equation*}
for all $s \in \C$ and $z \in \C^n$.
A homogeneous polynomial of degree $d$ is a polynomial whose
every monomial
is of total degree $d$.  For instance, $z^2w-iz^3+9zw^2$ is homogeneous of
degree $3$ in the variables $(z,w) \in \C^2$.
A polynomial vector-valued mapping is homogeneous of degree $d$
if each component is.
If $f$ is holomorphic near $a \in \C^n$, then
write the power series of $f$ at $a$ as
\begin{equation*}
\sum_{m=0}^{\infty} f_m(z-a) ,
\end{equation*}
where $f_m$ is a homogeneous polynomial of degree $m$.
The $f_m$ is called the
\emph{\myindex{degree $m$ homogeneous part}}\index{homogeneous part} of $f$
at $a$.  The $f_m$ would be vector-valued if $f$ is vector-valued,
such as in the statement of the theorem.
In the proof, we will require the vector-valued
Cauchy estimates (exercise below)\footnote{The normal Cauchy estimates
could also be used in the proof of Cartan by applying them
componentwise.}.

\begin{exbox}
\begin{exercise}
Prove a vector-valued version of the Cauchy estimates.  Suppose $f
\colon \overline{\Delta_r(a)} \to \C^m$ is continuous function holomorphic
on a polydisc $\Delta_r(a) \subset \C^n$.  Let $\Gamma$ denote the distinguished
boundary of $\Delta$.  Show that for every multi-index $\alpha$,
\begin{equation*}
\norm{\frac{\partial^{\abs{\alpha}}f}{\partial z^\alpha} (a)}
\leq
\frac{\alpha!}{r^\alpha} \sup_{z\in \Gamma} \norm{f(z)} .
\end{equation*}
\end{exercise}
\end{exbox}



\begin{proof}[Proof of Cartan's uniqueness theorem]
Without loss of generality, assume $a=0$.
Write $f$ as a power series at the origin, written in homogeneous parts:
\begin{equation*}
f(z) = z + f_k(z) + \sum_{m=k+1}^\infty f_m(z) = z + f_k(z) + \text{higher
order terms},
\end{equation*}
where $k \geq 2$ is an integer such that $f_2(z),f_3(z),\ldots,f_{k-1}(z)$ is zero.
The degree-one homogeneous part is simply the vector $z$,
because
the derivative of $f$ at the origin is the identity.
Compose $f$ with itself $\ell$ times:
\begin{equation*}
f^\ell(z) = \underbrace{f \circ f \circ \cdots \circ f}_{\ell\text{ times}}
(z) .
\end{equation*}
As $f(U) \subset U$, then $f^\ell$ is a holomorphic map
of $U$ to $U$.  As $U$ is bounded, there is an $M$ such that $\snorm{z} \leq
M$ for all $z \in U$.  Therefore, $\snorm{f(z)} \leq M$ for all $z \in U$, and
$\snorm{f^\ell(z)} \leq M$ for all $z \in U$.

Note that
\begin{equation*}
f_k\bigl(f(z)\bigr)
=
f_k\bigl(z + \text{higher order terms}\bigr)
=
f_k(z) + \text{higher order terms}.
\end{equation*}
Therefore,
\begin{multline*}
f^2(z) =
f\bigl(f(z)\bigr) =
f(z) + f_k\bigl(f(z)\bigr) + \text{higher order terms}
\\
=
z + 2 f_k(z) + \text{higher order terms}.
\end{multline*}
Continuing this procedure,
\begin{equation*}
f^\ell(z) = z + \ell f_k(z) + \text{higher order terms}.
\end{equation*}
Suppose $\Delta_r(0)$ is a polydisc whose
closure is in $U$.
Via Cauchy estimates,
for every multi-index $\alpha$ with $\sabs{\alpha}=k$,
\begin{equation*}
\frac{\alpha!}{r^\alpha} M
\geq
\norm{\frac{\partial^{\sabs{\alpha}} f^\ell}{\partial z^\alpha}(0)}
=
\ell
\norm{\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha}(0)} .
\end{equation*}
The inequality holds for all $\ell \in \N$, and so
$\frac{\partial^{\sabs{\alpha}} f}{\partial z^\alpha}(0) = 0$.  Therefore,
$f_k \equiv 0$.  On the domain of convergence of the expansion,
we get $f(z) = z$, as there is no other
nonzero homogeneous part in the expansion of $f$.  As $U$ is connected,
then the identity theorem says $f(z) = z$ for all $z \in U$.
\end{proof}

As an application, let us classify all biholomorphisms of all bounded
circular domains that fix a point.
A \emph{\myindex{circular domain}} is a domain
$U \subset \C^n$ such that if $z \in U$, then $e^{i\theta} z \in U$ for
all $\theta \in \R$.

\begin{cor}
Suppose $U, V \subset \C^n$ are bounded circular domains with $0 \in U$, $0 \in
V$, and $f \colon U \to V$ is
a biholomorphic map such that $f(0) = 0$.  Then $f$ is linear.
\end{cor}

For example, $\bB_n$ is circular and bounded.  So a biholomorphism of $\bB_n$
(an automorphism)
that fixes the origin is linear.  Similarly, a polydisc centered at zero is
also circular and bounded.  In fact, every Reinhardt domain is circular.

\begin{proof}
The map $g(z) = f^{-1}\bigl(e^{-i\theta}f(e^{i\theta} z)\bigr)$ is an
automorphism of $U$ and via the
chain rule, $g'(0) = I$.  Therefore,
Cartan says that
$f^{-1}\bigl(e^{-i\theta}f(e^{i\theta} z)\bigr) = z$, or in other words,
\begin{equation*}
f(e^{i\theta} z) = e^{i\theta}f(z) .
\end{equation*}
Write $f$ near zero as $f(z) = \sum_{m=1}^\infty f_m(z)$ where $f_m$ are
homogeneous polynomials of degree $m$ (notice $f_0 = 0$).  Then
\begin{equation*}
\sum_{m=1}^\infty e^{i\theta} f_m(z)
=
e^{i\theta} \sum_{m=1}^\infty f_m(z)
=
\sum_{m=1}^\infty f_m(e^{i\theta} z)
=
\sum_{m=1}^\infty e^{im\theta}f_m(z) .
\end{equation*}
By the uniqueness of the Taylor expansion,
$e^{i\theta} f_m(z)  = e^{im\theta} f_m(z)$, or
$f_m(z)  = e^{i(m-1)\theta} f_m(z)$,
for all $m$, all $z$, and all $\theta$.
If $m\not=1$, we obtain that $f_m \equiv
0$, which proves the claim.
\end{proof}

\begin{exbox}
\begin{exercise} \label{exercise:autofpolydisc}
Show that every automorphism $f$ of $\D^n$ (that is, a biholomorphism $f \colon \D^n \to \D^n$)
is given as
\begin{equation*}
f(z) = P \left(
e^{i\theta_1} \frac{z_1-a_1}{1-\bar{a}_1z_1} ,
e^{i\theta_2} \frac{z_2-a_2}{1-\bar{a}_2z_2} , \ldots,
e^{i\theta_n} \frac{z_n-a_n}{1-\bar{a}_nz_n} \right)
\end{equation*}
for $\theta \in \R^n$, $a \in \D^n$, and
a permutation matrix $P$.
\end{exercise}

\begin{exercise}
Given $a \in \bB_n$, define the linear map $P_a z =
\frac{\linnprod{z}{a}}{\linnprod{a}{a}}a$ if $a \not= 0$ and $P_0z = 0$.
Let $s_a = \sqrt{1-\snorm{a}^2}$.  Show that every automorphism $f$ of
$\bB_n$ (that is, a biholomorphism $f \colon \bB_n \to \bB_n$)
can be written as
\begin{equation*}
f(z) = U \frac{a-P_az-s_a(I-P_a)z}{1-\linnprod{z}{a}}
\end{equation*}
for a unitary matrix $U$ and some $a \in \bB_n$.
\end{exercise}

\begin{exercise}
Using the previous two exercises, show that $\D^n$ and $\bB_n$, $n \geq 2$,
are not biholomorphic via a method more in the spirit of what Poincar\'e
used: Show that the groups of automorphisms of the two domains are
different groups when $n \geq 2$.
\end{exercise}

\begin{exercise} \label{exercise:boundedeigen}
Suppose $U \subset \C^n$ is a bounded open set, $a \in U$, and $f \colon U \to U$ is a
holomorphic mapping such that $f(a) = a$.  Show that every eigenvalue
$\lambda$ of the matrix $Df(a)$ satisfies $\sabs{\lambda} \leq 1$.
\end{exercise}

\begin{exercise}[Tricky]
For any $n$, find a domain $U \subset \C^n$ such that the only biholomorphism $f \colon U
\to U$ is the identity $f(z) = z$.  Hint: Take the polydisc (or the ball)
and remove some number of points (be careful in how you choose them).  Then
show that $f$ extends to a biholomorphism of the polydisc.  Then see what
happens to those points you took out.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Show that Cartan's uniqueness theorem is not true in the real case,
even for rational
functions.  That is, find a rational function $R(t)$ of
a real variable $t$, such that $R$ that takes $(-1,1)$ to
$(-1,1)$, $R'(0) = 1$, and $R(t)$ is not the identity.  You can even make
$R$ bijective.
\item
Show that \exerciseref{exercise:boundedeigen} is not true in the real
case.
For every $\alpha \in \R$, find a rational function $R(t)$ of
a real variable $t$, such that $R$ takes $(-1,1)$ to $(-1,1)$ and
$R'(0) = \alpha$.
\end{exparts}
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set, $a \in U$,
$f \colon U \to U$ is a holomorphic mapping,
$f(a) = a$, and suppose that $\sabs{\lambda} < 1$
for every eigenvalue $\lambda$ of
$D f(a)$.  Prove that there exists a neighborhood $W$ of $a$, such that
$\lim_{\ell \to \infty} f^{\ell}(z) = a$ for all $z \in W$.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a bounded open set and $a \in U$.
Show that the mapping $\varphi \mapsto \bigl(\varphi(a),D\varphi(a)\bigr)$
from the set $\Aut(U)$ of automorphisms of $U$ to $\C^n \times \C^{n^2}$
is injective.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Riemann extension, zero sets, and injective maps}
\label{sec:riemannextzerosetsinjmaps}

In one dimension, if a function is holomorphic in
$U \setminus \{ p \}$ and
locally bounded\index{locally bounded in $U$}\footnote{%
$f \colon U \setminus X \to \C$ is locally bounded in $U$
if for every $p \in U$, there is a neighborhood $W$ of
$p$ such that $f$ is bounded on $W \cap (U \setminus X)$.}
in $U$, in particular bounded near
$p$, then the function extends holomorphically to $U$ (see
\propref{prop:onevarclassifysing} \ref{prop:onevarclassifysing:i}).
In several variables,
the same theorem holds, and the analogue of a single point
is the zero set of a holomorphic function.

\begin{thm}[\myindex{Riemann extension theorem}]
Let $U \subset \C^n$ be a domain,  $g \in \sO(U)$, and $g$ is not
identically zero.  Let
$N = g^{-1}(0)$ be the zero set of $g$.
If $f \in \sO(U \setminus N)$ is locally bounded in $U$,
then there exists a unique $F \in \sO(U)$ such that $F|_{U \setminus N} = f$.
\end{thm}

The proof is an application of the Riemann extension theorem from one dimension.
And just as in one dimension, the boundedness condition is necessary to
expect an extension.  For instance, $\frac{1}{g(z)}$ is not bounded near $N$
and indeed does not extend through $N$.

\begin{proof}
Take any $p \in N$, and let $L$ be a complex line through $p$.
That is,
$L$ is an image of an affine mapping
$\varphi \colon \C \to \C^n$ defined by
$\varphi(\xi) = a\xi + p$, for a vector $a \in \C^n$.
The composition $g \circ \varphi$
is a holomorphic function of one variable, and it
is either identically zero, or
the zero at $\xi=0$ is isolated.
The function $g$ is not identically zero in any neighborhood of $p$
by \hyperref[thm:identity]{the identity theorem}.
So there is some line $L$ such that $g \circ \varphi$
is not identically zero, or in other words, $p$
is an isolated point of $L \cap N$.

Write $z' =
(z_1,\ldots,z_{n-1})$ and $z=(z',z_n)$.
Without loss of generality, $p = 0$ and $L$ is the line
obtained by $z' = 0$.
So $g \circ \varphi$ is $\xi \mapsto g(0,\xi)$.
There is a small
$r > 0$ such that $g$ is nonzero on the set
given by $\sabs{z_n} = r$ and $z' = 0$.
By continuity,
$g$ is nonzero on the set given by
$\sabs{z_n} = r$ and $\snorm{z'} <\epsilon$ for some $\epsilon >0$.
In particular, for any fixed $s \in \C^{n-1}$, with $\snorm{s} < \epsilon$,
setting $z' = s$,
the zeros of $\xi \mapsto g(s,\xi)$ are isolated.  See
\figureref{fig:riemann-ext-zeros}.

\begin{myfig}
\subimport*{figures/}{riemann-ext-zeros.pdf_t}
\caption{Good neighborhood of the origin with respect to
the zero set of $g$.\label{fig:riemann-ext-zeros}}
\end{myfig}

For $\snorm{z'} <
\epsilon$ and $\abs{z_n} < r$, write
\begin{equation*}
F(z',z_n) =
\frac{1}{2\pi i}
\int_{\sabs{\xi}=r} \frac{f(z',\xi)}{\xi-z_n} \,d\xi .
\end{equation*}
The function $\xi \to f(z',\xi)$ is bounded and thus extends holomorphically to the entire
disc of radius $r$ by the Riemann extension from one dimension.  By Cauchy
integral formula,
$F$ is equal to $f$ at the points where they are both defined.
By differentiating under the integral, the function $F$ is holomorphic
in all variables.

In a neighborhood of each point of $N$,
$F$ is continuous
(holomorphic in fact).
A continuous extension of $f$ must be unique
on the closure of
$U \setminus N$ in the subspace topology,
$\overline{(U \setminus N)} \cap U$.
Due to the identity theorem, the set $N$ has empty interior,
so $\overline{(U \setminus N)} \cap U = U$.  Hence, $F$ is the unique
continuous extension of $f$ to $U$.
\end{proof}

\begin{exbox}
\begin{exercise}
Let $F$ be a meromorphic function on an open set $U \subset \C^n$.  Show
that if $p \in U$ is a pole (near $p$, $F=\nicefrac{f}{g}$, and $F$ does not
extend through $p$), then there exists a sequence $\{ p_k \}$
converging to $p$ such that $F(p_k) \to \infty$.  Namely, $F$ is
unbounded near $p$.
\end{exercise}

\begin{exercise}
Suppose that $U \subset \C^n$ is open and $N \subset U$ is a closed set
such that for every $\zeta \in \C$, the set
$\{ z \in U : z_n = \zeta \} \cap N$ is countable.
Suppose that $f \colon U \setminus N \to \C$ is holomorphic and
locally bounded in $U$.  Then $f$ uniquely extends to a holomorphic function
of $U$.  Hint: Every countable closed subset of $\C$ has isolated points.
\end{exercise}

\begin{exercise}
Suppose $U = \{ z \in \D^2 : z_1 \not=0 \text{ and } z_2 \not= 0 \}$.
Compute the group of automorphisms $\Aut(U)$.
Hint: See \exerciseref{exercise:autofpolydisc}.
\end{exercise}
\end{exbox}

The set of zeros of a holomorphic function has a nice structure at most
points.

\begin{thm} \label{thm:regptsdense}
Suppose $U \subset \C^n$ is a domain,
$f \in \sO(U)$, and $f$ is not identically zero.
Let $N = f^{-1}(0)$.
Then there exists an open and dense
(subspace topology) subset
$N_{\mathit{reg}} \subset N$
such that at each $p \in N_{\mathit{reg}}$, after possibly reordering variables,
$N$ can be locally (that is, in some neighborhood) written as
\begin{equation*}
z_n = g(z_1,\ldots,z_{n-1})
\avoidbreak
\end{equation*}
for a holomorphic function $g$.
\end{thm}

\begin{proof}
If $N$ is locally a graph at $p$, then it is a graph
for every point of $N$ near $p$.  So $N_{\mathit{reg}}$
is open.
If for every point $p_0 \in N$
and every neighborhood $W$ of $p_0$, we show that
$N \cap W$ has a regular point, then $N_{\mathit{reg}}$ is dense.
Replacing $N$ with $N \cap W$, it thus suffices to show
$N_{\mathit{reg}}$ is nonempty.

Since $f$ is not identically zero, then not all derivatives (of arbitrary
order) of $f$
vanish identically on $N$.
If some first order derivative of $f$ does not vanish identically on $N$,
let $h=f$.
Otherwise, suppose $k$ is such that a derivative of $f$ of order $k$
does not vanish identically on $N$, and
all derivatives of $f$ order less than $k$ vanish identically on
$N$.  Let $h$ be one of the derivatives of order $k-1$.
We obtain a function $h \colon U \to \C$, holomorphic, vanishing on $N$,
and such that
without loss of generality the $z_n$ derivative does not vanish identically
on $N$.  Then there is some point $p \in N$ such that $\frac{\partial
h}{\partial z_n}(p) \not= 0$.
We apply the implicit function theorem at $p$ to find $g$ such that
\begin{equation*}
h\bigr(z_1,\ldots,z_{n-1},g(z_1,\ldots,z_{n-1})\bigr) = 0 ,
\end{equation*}
and $z_n = g(z_1,\ldots,z_{n-1})$ is the unique solution to
$h=0$ near $p$.

The zero set of $h$ contains $N$, the zero set of $f$.
We must show equality near $p$.  That is, we need to show that
near $p$, every zero of $h$ is also a zero of $f$.
Write $p = (p',p_n)$.  Then the function
\begin{equation*}
\xi \mapsto f(p',\xi)
\end{equation*}
has an isolated zero in a small disc $\Delta$ around $p_n$ and is
nonzero on the circle $\partial \Delta$.  By
\hyperref[thm:onevarrouche]{Rouch\'e's theorem},
$\xi \mapsto f(z',\xi)$ must have a zero for all $z'$ sufficiently close to $p'$
(close enough to make $\sabs{f(p',\xi)-f(z',\xi)} < \sabs{f(p',\xi)}$ for all $\xi \in
\partial \Delta$).
Since $g(z')$ is the unique solution $z_n$ to $h(z',z_n) = 0$
near $p$ and the
zero set of $f$ is contained in the zero set of $h$, we are done.
\end{proof}

The zero set $N$ of a holomorphic function is an example of a so-called
\emph{\myindex{subvariety}}
or an \emph{\myindex{analytic set}},
although the general definition of a
subvariety is more complicated.
See \chapterref{ch:analyticvarieties}.
Points where $N$ is a graph of a holomorphic mapping are called
\emph{regular points}\index{regular point}, and we write them as
$N_{\mathit{reg}}$ as above.  In particular,
since $N$ is a graph of a single holomorphic function, they are called
regular points of (complex) dimension $n-1$, or (complex) codimension $1$.
The set of regular points is what is called an
$(n-1)$-dimensional \emph{\myindex{complex submanifold}}\index{submanifold!complex}.
It is also a real
submanifold of real dimension $2n-2$.
The points on a subvariety that are not regular are called
\emph{singular points}\index{singular point}.

To wit, one of important consequences of the theorem is that the zero set of
a holomorphic function is always quite large when $n \geq 2$.

\begin{example}
For $U = \C^2$,
let $f(z) = z_1^2-z_2^2$ and consider $X = f^{-1}(0)$. As $\nabla f =
(2z_1,2z_2) \not= 0$
outside of the origin, we can solve for $z_1$ or $z_2$ and so
all points of $X \setminus \{ 0 \}$ are regular.  In fact,
$z_1 = z_2$ and $z_1 = -z_2$ are the two possibilities.
In no neighborhood of the origin, however, is there a way to uniquely solve for either
$z_1$ or $z_2$, since you always get two possible solutions:  If you could
solve $z_1 = g(z_2)$, then both $z_2 = g(z_2)$ and $-z_2 = g(z_2)$ must be
true, a contradiction for any nonzero $z_2$.  Similarly,
we cannot solve for $z_2$.
So the origin is a singular point.

To see that you may have needed to use derivatives of the function in the
proof of the theorem,
notice
that the function $\varphi(z) = {(z_1^2-z_2^2)}^2$ has the same zero set $X$,
but both
$\frac{\partial \varphi}{\partial z_1}$ and
$\frac{\partial \varphi}{\partial z_2}$ vanish on $X$.  Using
$h= \frac{\partial \varphi}{\partial z_1}$ or
$h= \frac{\partial \varphi}{\partial z_2}$ in the proof will work.

Similarly, $\psi(z) = {(z_1-z_2)}^2 (z_1+z_2)$ has the same
zero set $X$, and $h=\psi$ will work at regular points where $z_1 = -z_2$,
but $h = \frac{\partial \psi}{\partial z_1}$ or
$h = \frac{\partial \psi}{\partial z_2}$ must be used where $z_1 = z_2$.
\end{example}

\begin{example}
\pagebreak[2]
The theorem is not true in the nonholomorphic setting.  The set
where $x_1^2 +  x_2^2 = 0$ in $\R^2$ is only the origin, clearly not a graph
of any function of one variable.  The first part of the theorem works, but
the $h$ you find is either $2x_1$ or $2x_2$, and its zero set is too big.
\end{example}

\begin{exbox}
\begin{exercise}
Find all the regular points of the subvariety
$X = \bigl\{ z \in \C^2 : z_1^2 = z_2^3 \bigr\}$.
Hint: The trick is showing that you've found all of them.
\end{exercise}

\begin{exercise} \label{exercise:connectedcomplement}
Suppose $U \subset \C^n$ is a domain and $f \in \sO(U)$.
Show that the complement of the zero set, $U \setminus f^{-1}(0)$, is
connected.
\end{exercise}

\begin{exercise} \label{exercise:zerosetnotcompact}
Suppose $U \subset \C^n$ is a domain and $f \in \sO(U)$.
Show that the zero set $f^{-1}(0)$ is not compact if it is nonempty.
Hint: A compact set has a point farthest from the origin.
\end{exercise}
\end{exbox}

\begin{remark}
It is rather surprising that by a famous theorem of Whitney,
any closed set whatsoever in $\R^n$
is the zero set of a $C^\infty$-smooth function.
\end{remark}

Let us now prove that a one-to-one holomorphic
mapping is biholomorphic, a result definitely not true in the
smooth setting: $x \mapsto x^3$ is smooth, one-to-one, onto map
of $\R$ to $\R$, but the inverse is not differentiable.

\begin{thm} \label{thm:injective}
Suppose $U \subset \C^n$ is an open set and $f \colon U \to \C^n$ is
holomorphic and one-to-one.
Then the Jacobian determinant is never equal to zero on $U$.

In particular, if a holomorphic map $f \colon U \to V$ is
one-to-one and onto for two open sets $U,V \subset \C^n$, then $f$ is
biholomorphic.
\end{thm}

The function $f$ is locally biholomorphic, in particular
$f^{-1}$ is holomorphic,
on the set where the Jacobian determinant
\begin{equation*}
J_f(z) = \det Df(z) = \det \left[ \frac{\partial f_k}{\partial z_\ell}(z)
\right]_{k\ell}
\end{equation*}
is not zero.  This follows from the inverse function theorem, which is just
a special case of \hyperref[thm:ift]{the implicit function theorem}.
The trick to prove the theorem above is to prove that $J_f$ is nowhere zero.

In one complex dimension, every holomorphic function $f$ can, in
the proper local holomorphic coordinates (and up to adding a constant),
be written as $z^d$ for $d=0,1,2,\ldots$:
Near a $z_0 \in \C$,
there exists a constant $c$ and a local biholomorphic $g$
with $g(z_0) = 0$ such that
$f(z) = c + {\bigl( g(z) \bigr)}^d$.
So $f$ is one-to-one precisely if $d=1$.
Such a simple result
does not hold in several variables in general, but if the mapping is
locally one-to-one, then the present theorem says that such a mapping can be
locally written as the identity.

\begin{proof}[Proof of the theorem]
We proceed by induction.  We know the theorem for $n=1$.
Suppose $n > 1$ and suppose we know the theorem is true for dimension $n-1$.

Suppose for contradiction that $J_f = 0$ somewhere.
First suppose that $J_f$ is not identically zero.
Find a regular point $q$ of the zero set of $J_f$.
Write the zero set of $J_f$ near $q$ as
\begin{equation*}
z_n = g(z_1,\ldots,z_{n-1})
\end{equation*}
for some holomorphic $g$.
If we prove the theorem near $q$, we are done.  Without loss of generality
assume $q=0$.  The biholomorphic (near the origin) map
\begin{equation*}
\Psi(z_1,\ldots,z_n) = \bigl(z_1,z_2,\ldots,z_{n-1},z_n-g(z_1,\ldots,z_{n-1}) \bigr)
\end{equation*}
takes the zero set of $J_f$ to the set given by $z_n=0$.  By considering
$f \circ \Psi^{-1}$ instead of $f$, we may assume
that $J_f = 0$ on the set given by $z_n=0$.  We may also
assume that $f(0) = 0$.

If $J_f$ vanishes identically, then there is no need to do anything other
than a translation.  In either case,
we may assume that $0 \in U$, $f(0)=0$, and
$J_f = 0$ when $z_n=0$.
Really, all we need is for the set where $J_f=0$ to be a sufficiently
large set.

We wish to show that all the derivatives of $f$ in the $z_1,\ldots,z_{n-1}$
variables vanish whenever $z_n = 0$.  This
would clearly contradict $f$ being one-to-one,
as $f(z_1,\ldots,z_{n-1},0)$ would be constant.
So for any point on $z_n=0$,
consider one of the components
of $f$ and one of the derivatives of that component.
Without loss of generality, suppose the point is $0$, and
for contradiction suppose
$\frac{\partial f_1}{\partial z_1}(0) \not= 0$.
The map
\begin{equation*}
G(z_1,\ldots,z_n) = \bigl(f_1(z),z_2,\ldots,z_n\bigr)
\end{equation*}
is biholomorphic on a small neighborhood of the origin.
The function $f \circ G^{-1}$ is holomorphic and one-to-one on a small
neighborhood.  By the definition of $G$,
\begin{equation*}
f \circ G^{-1} (w_1,\ldots,w_n) = \bigl(w_1,h(w)\bigr) ,
\end{equation*}
where $h$ is a holomorphic mapping taking a neighborhood of the
origin in $\C^n$ to $\C^{n-1}$.
The mapping
\begin{equation*}
\varphi(w_2,\ldots,w_n) = h(0,w_2,\ldots,w_n)
\end{equation*}
is a one-to-one holomorphic mapping of a neighborhood of the origin in
$\C^{n-1}$ to $\C^{n-1}$.  By the induction hypothesis, the Jacobian determinant of
$\varphi$ is nowhere zero.

If we differentiate $f \circ G^{-1}$, we notice
$D(f \circ G^{-1}) = Df \circ D(G^{-1})$.
So at the origin
\begin{equation*}
\det D(f \circ G^{-1}) = \bigl(\det Df\bigr) \bigl(\det D(G^{-1})\bigr) = 0.
\end{equation*}
We obtain a contradiction, as at the origin
\begin{equation*}
\det
D(f \circ G^{-1})
= \det D\varphi \not= 0 . \qedhere
\end{equation*}
\end{proof}

The theorem is no longer true if the dimensions of the domain and range of the
mapping are not equal.

\begin{exbox}
\begin{exercise}
Take the subvariety
$X = \bigl\{ z \in \C^2 : z_1^2 = z_2^3 \bigr\}$.
Find a one-to-one holomorphic mapping $f \colon \C \to X$.
Note that the derivative of $f$ vanishes at a certain point.
So \thmref{thm:injective} has no analogue when the domain and range have
different dimension.
\end{exercise}

\begin{exercise}
Find a continuous function $f \colon \R \to \R^2$ that is one-to-one but
such that the inverse $f^{-1} \colon f(\R) \to \R$ is not continuous.
\end{exercise}
\end{exbox}

\pagebreak[1]
This is an appropriate place to state a well-known and as yet unsolved conjecture (and most
likely ridiculously hard to solve):
the \emph{\myindex{Jacobian conjecture}}.
This conjecture is a converse to the theorem above in a special case:
\emph{Suppose $F \colon \C^n \to \C^n$ is a polynomial map (each component is a
polynomial) and the Jacobian derivative $J_F$ is never zero, then $F$ is
invertible with a polynomial inverse.}
Clearly $F$ would be locally one-to-one, but proving (or
disproving)
the existence of a global polynomial inverse is the content of the conjecture.

\begin{exbox}
\begin{exercise}
Prove the Jacobian conjecture for $n=1$.  That is, prove that if
$F \colon \C \to \C$ is a polynomial such that $F'$ is never zero,
then $F$ has an inverse, which is a polynomial.
\end{exercise}

\begin{exercise}
Let $F \colon \C^n \to \C^n$ be an injective polynomial map.
Prove $J_F$ is a nonzero constant.
\end{exercise}

\begin{exercise}
Prove that the Jacobian conjecture is false if
``polynomial'' is replaced with ``entire holomorphic,'' even for $n=1$.
\end{exercise}

\begin{exercise}
Prove that if a holomorphic $f \colon \C \to \C$ is injective, then it is
onto, and therefore $f(z) = az + b$ for $a \not= 0$.
\end{exercise}
\end{exbox}

We remark that while every injective holomorphic
map of $f \colon \C \to \C$ is onto, the same is not true in higher
dimensions.
In $\C^n$, $n \geq 2$, there exist so-called
\emph{Fatou--Bieberbach domains}\index{Fatou--Bieberbach domain},
that is, proper subsets of $\C^n$ that are biholomorphic to $\C^n$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Convexity and Pseudoconvexity} \label{ch:convexity}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Domains of holomorphy \& holomorphic extension}

It turns out that not every domain in $\C^n$ is a natural domain for
holomorphic functions.

\begin{defn} \label{defn:domainofhol}
Let $U \subset \C^n$ be a domain\footnote{\emph{Domain of
holomorphy} can make sense for disconnected sets (not domains), and some authors
do define it so.}
(connected open set).  The set $U$ is
a \emph{\myindex{domain of holomorphy}} if there do not exist
nonempty open sets
$V$ and $W$, with $V \subset U \cap W$, $W \not\subset U$, and $W$
connected, such that for every $f \in \sO(U)$ there exists an $F \in
\sO(W)$ with $f(z) = F(z)$ for all $z \in V$.  See
\figureref{fig:domain-of-hol-def}.
\end{defn}

\begin{myfig}
\subimport*{figures/}{domain-of-hol-def.pdf_t}
\caption{Definition of domain of holomorphy.\label{fig:domain-of-hol-def}}
\end{myfig}

The idea is that if a domain $U$
is not a domain of holomorphy and $V$, $W$ exist as in the
definition, then $f$ ``extends across the boundary'' somewhere.

\begin{example}
The unit ball $\bB_n \subset \C^n$ is a domain of holomorphy.  Proof:
Consider $U=\bB_n$, and suppose $V$, $W$ as in the definition exist.
As $W$ is connected and open, it is path connected.  There are
points in $W$ that are not in $\bB_n$, so there
is a path $\gamma$ in $W$ going
from a point $q \in V$ to some $p \in \partial \bB_n \cap W$,
and assume $\gamma \setminus \{ p \} \subset \bB_n$.
Without loss of generality (after composing with
rotations, that is, unitary matrices), assume $p =
(1,0,0,\ldots,0)$.  Consider $f(z) = \frac{1}{1-z_1}$.
The function $F$ equals $f$ on the component of
$\bB_n \cap W$ that contains $q$.  But that component contains $p$ and
so $F$ blows up at $p$ (so it cannot be holomorphic).
The contradiction shows that no $V$ and $W$ exist.
\end{example}

In one dimension, this notion has no real content:
Every domain in $\C$ is a domain
of holomorphy (exercise below).

\begin{exbox}
\begin{exercise}[Easy]
In $\C$, every domain is a domain of holomorphy.
\end{exercise}

\begin{exercise}
If $U_k \subset \C^n$ are domains of holomorphy (possibly an infinite set of
domains), then the interior of
$\bigcap_{k} U_k$
is either empty or every connected component is a domain of holomorphy.
\end{exercise}

\begin{exercise}[Easy]
Show that a polydisc in $\C^n$ is a domain of holomorphy.
\end{exercise}

\begin{exercise} \label{exercise:cartesianproddomofhol}
Suppose $U_k \subset \C^{n_k}$, $k=1,\ldots,\ell$ are domains of holomorphy,
show that
$U_1 \times \cdots \times U_\ell$ is a domain of holomorphy.
In particular every cartesian product of domains in $\C$ is a domain of
holomorphy.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is a domain of holomorphy and $f \in \sO(U)$
is a function.  Show that $U \setminus f^{-1}(0)$ is a domain of holomorphy.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Given $p \in \partial \bB_n$, find a function $f$ holomorphic on
$\bB_n$,
$C^\infty$-smooth on $\overline{\bB_n}$ (all real partial derivatives of
all orders extend
continuously to $\overline{\bB_n}$), that does not extend past $p$
as a holomorphic function.
Hint: For the principal branch of $\sqrt{\cdot}$ the function $\xi \mapsto
e^{-1/\sqrt{\xi}}$ is holomorphic for $\Re \xi > 0$ and extends to
be continuous (even smooth) on all of $\Re \xi \geq 0$.
\item
Find a function $f$ holomorphic on $\bB_n$
that does not extend past any point of
$\partial \bB_n$.
\end{exparts}
\end{exercise}
\end{exbox}

Various notions of convexity will play a big role later on.
A set $S$ is \emph{\myindex{geometrically convex}}\index{convex!geometrically}\index{convex}
if $t x + (1-t)y \in S$
for all $x,y \in S$ and $t \in [0,1]$.
The exercise below
says that every geometrically convex domain is a domain of holomorphy.
Domains of holomorphy are often not geometrically convex
(e.g.\ every domain in $\C$ is a domain of holomorphy),
so classical convexity is not the correct notion, but it is
in the right direction.

\begin{exbox}
\begin{exercise}
Show that a geometrically convex domain in $\C^n$ is a domain of holomorphy.
\end{exercise}
\end{exbox}

In the following, when we say $f \in \sO(U)$ extends holomorphically to $V$ where
$U \subset V$, we mean that there
exists a function $F \in \sO(V)$ such that $f = F$ on
$U$.

\begin{remark} \label{remark:notsimpledefofdoh}
The subtlety of the definition of a domain of holomorphy is that it does not
necessarily talk about functions extending to a larger set, since we must
take into account single-valuedness.  For instance, let $f$ be the principal branch
of the logarithm defined on the slit plane
$U = \C \setminus \{ z \in \C : \Im z = 0, \Re z \leq 0 \}$.
We can locally define an extension from one side through the boundary
of the domain, but we cannot define an extension on a open set that
contains $U$.  This one-dimensional example should be motivation for why we let $V$
be a proper subset of $U \cap W$, and why $W$ need not contain all of $U$.
This one dimensional intuition can be extended to an actual example in
$\C^n$, see \exerciseref{exercise:notsimpledefofdoh}.
\end{remark}

In dimension two or higher, not every domain is a domain of holomorphy.  We have
the following theorem.  The domain $H$ in the theorem is called the
\emph{\myindex{Hartogs figure}}.

\begin{thm} \label{thm:extensionhartogsfigure}
Let $(z,w) = (z_1,\ldots,z_m,w_{1},\ldots,w_{k}) \in \C^m \times \C^k$ be the coordinates.  For two numbers
$0 < a,b < 1$, define the set $H \subset \D^{m+k}$
by
\begin{multline*}
H = \bigl\{ (z,w) \in \D^{m+k} : \sabs{z_\ell} > a ~\text{for $\ell=1,\ldots,m$}
\bigr\}
\\
\cup
\bigl\{ (z,w) \in \D^{m+k} : \sabs{w_\ell} < b ~\text{for $\ell=1,\ldots,k$}
\bigr\} .
\end{multline*}
If $f \in \sO(H)$, then $f$ extends holomorphically to $\D^{m+k}$.
\end{thm}

In $\C^2$ if $m=1$ and $k=1$, see
\figureref{fig:hartogs-figure} (the $c$ will come up in
the proof).

\begin{myfig}
\newcommand{\hartogstext}{\parbox[t]{2.3in}{In diagrams, the Hartogs figure is
often drawn as:}}
\subimport*{figures/}{hartogs-figure.pdf_t}
\caption{Hartogs figure.\label{fig:hartogs-figure}}
\end{myfig}

\begin{proof}
Pick a $c \in (a,1)$.  Let
\begin{equation*}
\Gamma =
\bigl\{ z \in \D^{m} : \sabs{z_\ell} = c ~\text{for $\ell=1,\ldots,m$ } \bigr\}.
\end{equation*}
The set $\Gamma$ is the distinguished boundary of $c \D^m$,
a polydisc centered at $0$ of radius $c$ in $\C^m$.
Define
\begin{equation*}
F(z,w)
=
\frac{1}{{(2\pi i)}^m}
\int_\Gamma \frac{f(\xi,w)}{\xi-z} \, d\xi .
\end{equation*}
Clearly, $F$ is well-defined on
\begin{equation*}
c\D^m \times \D^k
\end{equation*}
as $\xi$ only
ranges through $\Gamma$ and so as long as $w \in \D^k$ then $(\xi,w) \in H$.

The function $F$ is holomorphic in $w$ as we can differentiate
underneath the integral and $f$ is holomorphic in $w$ on $H$.  Furthermore,
$F$ is holomorphic in $z$ as the kernel $\frac{1}{\xi-z}$ is holomorphic in
$z$ as long as $z \in c\D^m$.

For any fixed $w$ with $\sabs{w_\ell} < b$ for all $\ell$,
the Cauchy integral formula says
$F(z,w) = f(z,w)$ for all $z \in c \D^m$.
Hence, $F=f$ on the open set
$c\D^m \times b\D^k$,
and so they are equal on
$(c\D^m \times \D^k) \cap H$.
Combining $F$ and $f$, we obtain a holomorphic
function on $\D^{m+k}$ that extends $f$.
\end{proof}

The theorem is used in many situations to extend holomorphic functions.
We usually need to translate, scale, rotate (apply a unitary matrix),
and even take more general biholomorphic mappings
of $H$, to place it wherever we need it.
The corresponding polydisc---or the image of
the polydisc under the appropriate biholomorphic mapping if one was
used---to which all holomorphic functions on $H$ extend is denoted
by $\widehat{H}$ and is called the \emph{hull} of $H$.%
\index{hull of a Hartogs figure}

Let us state a simple but useful case of the so-called
\emph{\myindex{Hartogs phenomenon}}.  You have already proved a version of
this result in \exerciseref{exercise:isolatedhartogs}, but let us prove it
with the Hartogs figure.

\begin{cor}
Let $U \subset \C^n$, $n \geq 2$, be an open set and $p \in U$.
Then every $f \in \sO\bigl(U \setminus \{ p \} \bigr)$
extends holomorphically to $U$.
\end{cor}

\begin{proof}
Without loss of generality,
by translating and scaling (those operations are after all holomorphic),
we assume that $p = \bigl(0,\ldots,0,\frac{3}{4}\bigr)$
and the unit polydisc $\D^n$ is contained in $U$.  We fit a Hartogs figure $H$
in $U$
by letting $m=n-1$ and $k=1$, writing $\C^n = \C^{n-1} \times \C^{1}$,
and taking $a = b = \frac{1}{2}$.
Then $H \subset U$, and $p \in \D^n \setminus H$.
\thmref{thm:extensionhartogsfigure} says that
$f$ extends to be holomorphic through $p$.
\end{proof}

This result provides (yet) another reason why holomorphic functions in several
variables have no isolated zeros (or poles).  If a zero of $f$ was isolated, then
consider $\nicefrac{1}{f}$ to obtain a contradiction.
But the extension works in an even more surprising fashion.  We could
take out a very large set, for example, any geometrically
convex compact subset:

\begin{exbox}
\begin{exercise} \label{exercise:convexhartogs}
Suppose $U \subset \C^n$, $n \geq 2$, be an open set and $K \subset \subset U$
is a compact geometrically
convex subset.
If $f \in \sO(U \setminus K)$,
then $f$ extends to be holomorphic in $U$.
Hint: Find a nice point on $\partial K$ and try extending a little bit.
Then make sure your extension is single-valued.
\end{exercise}
\end{exbox}

Convexity of $K$ is not needed; we only need that $U\setminus K$
is connected, but the proof is harder and we will get to it in
\sectionref{sec:hartogsphenom}.
The single-valuedness of the extension is the key point that makes the
general proof harder.

Notice the surprising consequence of the exercise:
Every holomorphic function on the shell
\begin{equation*}
\bB_n \setminus \overline{B_{1-\epsilon}(0)} =
\bigl\{ z \in \C^n : 1-\epsilon < \snorm{z} < 1 \bigr\}
\end{equation*}
for any $\epsilon > 0$ automatically
extends to a holomorphic function of $\bB_n$.
In fact, we will show later that one can take this to the limit:
A function only defined on a sphere that satisfies the Cauchy--Riemann
equations on the sphere will also extend holomorphically to the interior.
We need $n > 1$.
The extension result decisively does not work in one dimension; consider $\nicefrac{1}{z}$.
You have already shown in an exercise that when $n \geq 2$, the zero
sets of holomorphic functions is never compact, here is another reason why.
If $n \geq 2$ and $f \in \sO(\bB_n)$ has a nonempty zero set,
then the zero set must contain points arbitrarily close to the boundary.
If the set of zeros were compact in $\bB_n$, then we could try to
extend the function $\nicefrac{1}{f}$.

\begin{exbox}
\begin{exercise}[\myindex{Hartogs triangle}]\label{exercise:hartogstriangle}
Let
\begin{equation*}
T = \bigl\{ (z_1,z_2) \in \D^2 : \sabs{z_2} < \sabs{z_1} \bigr\} .
\end{equation*}
Show that $T$ is a domain of holomorphy.  Then show that if
\begin{equation*}
\widetilde{T} = T \cup B_{\epsilon}(0)
\end{equation*}
for an arbitrarily small $\epsilon > 0$, then $\widetilde{T}$ is not a domain
of holomorphy.  In fact, every function holomorphic on $\widetilde{T}$
extends to a holomorphic function of $\D^2$.
\end{exercise}

\begin{exercise} \label{exercise:C2minusR2}
Take the natural embedding of $\R^2 \subset \C^2$.  Suppose
$f \in \sO(\C^2 \setminus \R^2)$.  Show that $f$ extends holomorphically
to all of $\C^2$.  Hint: Change coordinates before using Hartogs.
\end{exercise}

\begin{exercise} \label{exercise:fatcylinder1}
Suppose
\begin{equation*}
U = \bigl\{ (z,w) \in \D^2 : \nicefrac{1}{2} < \sabs{z} \bigr\} .
\end{equation*}
Draw $U$.
Let $\gamma = \bigl\{ z \in \C : \sabs{z} = \nicefrac{3}{4} \bigr\}$ oriented positively.
If $f \in \sO(U)$, then show that the function
\begin{equation*}
F(z,w)
=
\frac{1}{2\pi i}
\int_\gamma \frac{f(\xi,w)}{\xi-z} \, d\xi
\end{equation*}
is well-defined in
$\bigl( (\nicefrac{3}{4}) \D \bigr) \times \D$, holomorphic where defined, yet
it is not necessarily true that $F = f$ on the intersections of their
domains.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set such that for every
$z \in \C^n \setminus \{ 0 \}$, there is a $\lambda \in \C$ such that
$\lambda z \in U$.  Let $f \colon U \to \C$ be holomorphic with
$f(\lambda z) = f(z)$ whenever $z \in U$, $\lambda \in \C$ and $\lambda z
\in U$.
\begin{exparts}
\item
(easy) Prove that $f$ is constant.
\item
(hard) Relax the requirement on
$f$ to being meromorphic: $f = \nicefrac{g}{h}$
for holomorphic $g$ and $h$.
Find a nonconstant example, and prove that such an $f$ must be rational (that
is, $g$ and $h$ must be polynomials).
\end{exparts}
\end{exercise}

\begin{exercise} \label{exercise:fatcylinder2}
Suppose
\begin{equation*}
U = \bigl\{ z \in \D^3 :
\nicefrac{1}{2} < \sabs{z_1} \quad\text{or}\quad
\nicefrac{1}{2} < \sabs{z_2} \bigr\} .
\avoidbreak
\end{equation*}
Prove that every function $f \in \sO(U)$ extends to $\D^3$.
Compare to \exerciseref{exercise:fatcylinder1}.
\end{exercise}

\begin{exercise} \label{exercise:codim2extends}
Suppose $U = \C^n \setminus \{ z \in \C^n : z_1 = z_2 = 0 \}$, $n
\geq 2$.  Show that every $f \in \sO(U)$ extends holomorphically to
$\C^n$.
\end{exercise}


\begin{exercise} \label{exercise:notsimpledefofdoh}
Construct an example domain $U \subset \C^2$ that is not a domain of
holomorphy, but such that there is no domain $W \subset \C^2$ with $U
\subset W$ such that every $f \in \sO(U)$ extends to $W$.
Hint: Extending the example from \remarkref{remark:notsimpledefofdoh} will
almost give you a $U$, but it will be a domain of holomorphy, you need to modify it
a little bit.
\end{exercise}
\end{exbox}

\begin{example}
By
\exerciseref{exercise:C2minusR2},
$U_1 = \C^2 \setminus \R^2$
is not a domain of holomorphy.  On the other hand,
$U_2 = \C^2 \setminus \{ z \in \C^2 : z_2 = 0 \}$ is a domain of holomorphy;
the function $f(z) = \frac{1}{z_2}$ cannot extend.
Therefore, $U_1$ and $U_2$ are rather different as far as complex variables are
concerned, yet they are the same set if we ignore the complex structure.
They are both a $4$-dimensional real vector space minus a $2$-dimensional
real vector subspace.  That is, $U_1$ is the set
where either $\Im z_1 \not= 0$ or $\Im z_2 \not= 0$,
while $U_2$ is the set
where either $\Re z_2 \not= 0$ or $\Im z_2 \not= 0$.

The condition of being a domain of holomorphy,
requires something more than just some real geometric condition on the
set.  Namely, we have shown that the image of a domain of holomorphy
via an orthonormal real-linear mapping
(so preserving distances, angles, straight lines,
etc.)\ need not be a domain of holomorphy.  In particular, when we want to
``rotate'' in complex analysis we use a complex linear mapping,
a unitary matrix.
\end{example}

In fact, one does not need a whole Hartogs figure to extend a holomorphic
function, a sequence of discs suffices.  We will see another version of
this theorem later, \thmref{thm:contprinciple2}.

\begin{thm}[Kontinuit\"atssatz---Continuity
principle\index{Kontinuit\"atssatz!first version}\index{continuity
principle!first version}, first version\footnote{%
Sometimes this (or similar) theorem is called Behnke--Sommer, although the
first version of it (where the discs are complex lines) were proved by Hartogs.}]
\label{thm:contprinciple1}
Suppose $U \subset \C^n$ is open and there exists a sequence of
closed analytic discs $\varphi_k \colon \widebar{\D} \to \C^n$ converging
(pointwise) to a closed analytic disc $\varphi$, such that
$\varphi_k(\widebar{\D}) \subset U$ and $\varphi(\partial \D) \subset U$.
Then there exists an $s$ such that for every $f \in \sO(U)$
and for every $p \in \varphi(\D)$,
there is an $F \in \sO\bigl(\Delta_s(p)\bigr)$ where $F=f$ on some
open subset of $U \cap \Delta_s(p)$.
\end{thm}

In particular, a $U$ that possesses such discs where $\varphi(\D)$ does not
lie entirely in $U$ is not a domain of holomorphy.
The continuity principle is illustrated in \figureref{fig:contprinc1}, where
analytic discs are drawn as lines and the boundaries as black dots.
Note that the conclusion is that $F$ continues analytically past any point
in $\varphi(\D)$.
However, we do not necessarily get single-valued extension to a whole
neighborhood of $\varphi(\D)$ without a further hypothesis,
see exercises below.%
\footnote{A counterexample can be found in S.\ Ivashkovich, \emph{Discrete
and Continuous Versions of the Continuity Principle}, The Journal of
Gemetric Analysis, \textbf{32} (2022), Paper No. 226.}

\begin{myfig}
\subimport*{figures/}{contprinc1.pdf_t}
\caption{Continuity principle for extension of
functions.\label{fig:contprinc1}}
\end{myfig}

\begin{proof}
Via Montel and considering slightly smaller discs
(restrictions to discs of radii $1-\epsilon$), we may
assume that $\varphi_k$ converge uniformly to $\varphi$ on $\widebar{\D}$.
Fix some $f \in \sO(U)$.
%The functions
%$f \circ \varphi_k$ are holomorphic for all $n$, and as $\varphi_k$ converges
%uniformly, $f \circ \varphi_k$ converges uniformly on $\partial \D$.
%By the maximum principle this sequence
%converges uniformly in $\widebar{\D}$ to some continuous $g \colon
%\widebar{\D} \to \C$ holomorphic on $\D$.

As $\varphi(\partial \D)$ is compact, there exists an $r > 0$ such that
for each $z \in \varphi(\partial \D)$, $\Delta_r(z) \subset U$, meaning that
the power series of $f$ converges in $\Delta_r(z)$.
Pick a positive $s < r$.
As $\varphi_k$ converge uniformly, for sufficiently high $k$,
\begin{equation*}
\bigcup_{q \in \varphi_k(\partial \D)} \overline{\Delta_s(q)}
\end{equation*}
is a compact subset of $U$ and hence $f$ is bounded by some $M$ on this set.
Cauchy estimates give that
\begin{equation} \label{eq:kontinuitatonecauchy}
\abs{\frac{\partial^\alpha f}{\partial z^\alpha}(q)}
\leq
\frac{M \alpha!}{s^\alpha}
\end{equation}
for all $q \in \varphi_k(\partial \D)$.  By the maximum principle,
\eqref{eq:kontinuitatonecauchy} holds for all $q \in \varphi_k(\widebar{\D})$,
and hence the power series for $f$ at all $q \in \varphi_k(\widebar{\D})$
converges in $\Delta_s(q)$.
Thus we get an $F$ defined by this power series on $\Delta_s(q)$ which
agrees with $f$ in a neighborhood of $q$.
By considering a large enough $k$, and a slightly smaller $s'$,
then for every $p \in \varphi(\widebar{\D})$ we can fit a $\Delta_{s'}(p)
\subset \Delta_s(q)$ for some $q \in \varphi_k(\widebar{\D})$ and where
$q \in \Delta_{s'}(p)$.
Since $p$ must be in the closure of $U$ by necessity, then $\Delta_{s'}(p)$
intersects $U$, and as it contains $q$, $F$ agrees with $f$ on some open
subset of $\Delta_{s'}(p)$.

The $s$ (and $s'$) only depends on the distance between the boundary of $U$
and $\varphi(\partial \D)$, so it does not depend on $f$ and moreover,
the assumption to restrict to smaller discs in the beginning of the proof
is valid.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that given an analytic disc $\varphi \colon \D \to \C^n$ and a point
$p \in \varphi(\D)$, then for small enough $\epsilon > 0$, the set
$\Delta_\epsilon(p) \cap \varphi(\D)$ is connected.  Hint: Pull back the
coordinate functions from $\C^n$ to $\D$.
\end{exercise}

\begin{exercise}
Prove that if furthermore $\varphi$ is injective in the proof,
then there exists an entire neighborhood $W$ of $\varphi(\widebar{\D})$
and for every $f \in \sO(U)$, there is an $F \in \sO(W)$ such that
$f=F$ on some open neighborhood of $\varphi(\partial \D)$.
\end{exercise}

\begin{exercise}
Suppose that given an open $U \subset \C^n$
and there exists a collection of
closed analytic discs $\Delta_\alpha \subset U$
such that $\bigcup_\alpha \partial \Delta_\alpha \subset \subset U$.
Show that for every $p$ in the closure of $\bigcup_\alpha \Delta_\alpha$
(closure in $\C^n$) there exists an analytic disc through $p$ and a sequence
of discs converging to it that satisfy the hypotheses of the theorem.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tangent vectors, the Hessian, and convexity}

An exercise in the previous section showed that every convex domain is a
domain of holomorphy.  However, classical convexity is too strong.
By \exerciseref{exercise:cartesianproddomofhol},
for any domains $U \subset \C$ and $V \subset \C$, the set
$U \times V$ is a domain of holomorphy in $\C^2$.  The domains
$U$ and $V$, and hence $U \times V$, can be spectacularly nonconvex.
But we should not discard convexity completely.
There is a notion of
\emph{pseudoconvexity}, which vaguely means ``convexity in the
complex directions'' and is the correct notion to distinguish
domains of holomorphy.
Let us figure out what classical convexity means locally for a smooth boundary.

\begin{defn} \label{def:hypersurface}
A set $M \subset \R^n$ is a
\glsadd{not:Ck}%
$C^k$-smooth \emph{\myindex{hypersurface}}%
\index{Ck-smooth hypersurface@$C^k$-smooth hypersurface}%
\index{hypersurface!Ck-smooth@$C^k$-smooth}
if at each point
$p \in M$, there exists a $k$-times continuously
differentiable function $r \colon V \to \R$
with nonvanishing derivative, defined in a neighborhood $V$ of $p$
such that $M \cap V = \bigl\{ x \in V : r(x) = 0 \bigr\}$.  The function $r$ is
called the \emph{\myindex{defining function}} of $M$ (at $p$).

An open set (or domain) $U \subset \R^n$ with
\emph{$C^k$-smooth boundary}%
\index{Ck-smooth boundary@$C^k$-smooth boundary}
is a set where
$\partial U$ is a $C^k$-smooth hypersurface,
and at every $p \in \partial U$ there is a defining
function $r$ such that
$r < 0$ for points in $U$ and $r > 0$
for points not in $U$.
See \figureref{fig:deffun}.

By simply
\index{domain with smooth boundary}%
\index{open set with smooth boundary}%
\emph{smooth}\index{smooth boundary},
\glsadd{not:Cinfty}%
we mean $C^\infty$-smooth,\index{Cinfinity-smooth@$C^\infty$-smooth}
that is, the $r$ is infinitely differentiable.
\end{defn}

\begin{myfig}
\subimport*{figures/}{deffun.pdf_t}
\caption{Local defining function for a domain.\label{fig:deffun}}
\end{myfig}

What we really defined is an \emph{\myindex{embedded hypersurface}}%
\index{hypersurface!embedded}.  In
particular, in this book the topology on the set $M$ will be the subset
topology.  Furthermore, in this book we generally deal with smooth
(that is, $C^\infty$) functions and hypersurfaces.  Dealing with
$C^k$-smooth functions for finite $k$ introduces technicalities that make
certain theorems and arguments unnecessarily difficult.

As the derivative of $r$ is nonvanishing, a
hypersurface $M$ is locally the graph of one variable over the rest
using the implicit function theorem.  That is, $M$ is a smooth
hypersurface if it is locally a set defined by
$x_k = \varphi(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)$ for some $k$ and some
smooth function $\varphi$.

The definition of an open set with smooth boundary is not simply that the
boundary is a smooth hypersurface, that is not enough.  It says that
one side of that hypersurface is in $U$ and one side is not in $U$:
As the derivative of $r$ never vanishes, $r$ has
different signs on different sides of $\bigl\{ x \in V : r(x) = 0 \bigr\}$.  The
verification of this fact is left to the reader.
(Hint: Look at where the gradient points.)
We can, in fact, find a
single global defining function for every open set with smooth boundary,
but we have no need of this.

Same definition works for $\C^n$, where we treat $\C^n$ as $\R^{2n}$.
For example, the ball $\bB_n$ is a domain with smooth boundary with defining
function $r(z,\bar{z}) = \snorm{z}^2-1$.
In $\C^n$
a hypersurface defined as above is a \emph{\myindex{real hypersurface}}%
\index{hypersurface!real},
to distinguish it from a complex hypersurface that would be the zero set of
a holomorphic function, although we may leave out the word ``real''
if it is clear from context.

\begin{defn}
For a point $p \in \R^n$, the set of \emph{tangent vectors}\index{vector} $T_p \R^n$ is given by
\glsadd{not:realtangentspace}%
\begin{equation*}
T_p \R^n = \operatorname{span}_{\R} \left\{
\frac{\partial}{\partial x_1}\Big|_p,
\ldots,
\frac{\partial}{\partial x_n}\Big|_p \right\} .
\end{equation*}
\end{defn}

That is, a vector $X_p \in T_p \R^n$ is an object of the form
\begin{equation*}
X_p = \sum_{k=1}^n a_k
\frac{\partial}{\partial x_k}\Big|_p ,
\end{equation*}
for real numbers $a_k$.  For computations, $X_p$ could be represented
by an $n$-vector $a = (a_1,\ldots,a_n)$.  However, if $p \not= q$, then
$T_p \R^n$ and
$T_q \R^n$ are distinct spaces.
\glsadd{not:evalpartial}%
An object
$\frac{\partial}{\partial x_k}\big|_p$
is a real linear functional\footnote{Linear real-valued function.}
on the space of smooth functions:
When applied to a smooth function $g$, it gives
$\frac{\partial g}{\partial x_k} \big|_p$.  Therefore, $X_p$ is also such a
functional.  It is the directional derivative from calculus;
it is computed as $X_p f = \nabla f|_p \cdot (a_1,\ldots,a_n)$.

\begin{savenotes}
\begin{exbox}
\begin{exercise}
Let $X$ be a real linear functional on the set of real
polynomials\footnote{This result works for smooth functions too
by applying Taylor's theorem.} in $n$ variables
such that $X (fg) = (Xf) g(0) + f(0) (Xg)$.  Show that $X$
can be identified with an element of $T_0 \R^n$.
\end{exercise}
\end{exbox}
\end{savenotes}


\begin{defn}
Let $M \subset \R^n$ be a smooth hypersurface,
$p \in M$, and $r$ is a defining function at $p$,
then a vector $X_p \in T_p \R^n$ is \emph{tangent}\index{tangent vector}
to $M$ at $p$ if
\begin{equation*}
X_p r = 0, \qquad \text{or in other words} \qquad
\sum_{k=1}^n a_k \frac{\partial r}{\partial x_k} \Big|_p = 0 .
\end{equation*}
\glsadd{not:realtangentspace}%
The set of such tangent vectors is denoted by $T_p M$,
called the \emph{\myindex{tangent space}} of $M$ at $p$.
\end{defn}

The space $T_pM$ is an $(n-1)$-dimensional real vector space---it is a subspace
of an $n$-dimensional $T_p\R^n$ given by a single linear equation.
Recall from calculus that the gradient $\nabla r|_p$ is
``normal'' to $M$ at $p$, and
the tangent space is given by all the $n$-vectors $a$
that are orthogonal to the normal, that is, $\nabla r|_p \cdot a = 0$.

\pagebreak[1]
We cheated in the terminology, and assumed without justification that $T_pM$
depends only on $M$, not on $r$.
Fortunately, the definition of $T_pM$ is independent of the choice of $r$ by the next two
exercises.

\begin{exbox}
\begin{exercise} \label{exercise:smoothdivision}
Suppose $M \subset \R^n$ is a smooth hypersurface and
$r$ is a smooth defining function for $M$ at $p$.
\begin{exparts}
\item
Suppose $\varphi$ is another
smooth defining function of $M$ on a neighborhood of $p$.
Show that there exists a smooth nonvanishing function $g$ such that
$\varphi = g r$ (in a neighborhood of $p$).
\item
Now suppose $\varphi$ is an arbitrary smooth function that vanishes on $M$ (not
necessarily a defining function).
Again show that $\varphi = g r$, but now $g$ may possibly vanish.
\end{exparts}
\nopagebreak
Hint: First suppose $r=x_n$ and
find a $g$ such that $\varphi = x_n g$.  Then find
a local change of variables to make $M$ into the set given by $x_n = 0$.
A useful calculus fact:
If $f(0) = 0$ and $f$ is smooth, then
$s \int_0^1 f'(ts) \,dt = f(s)$,
and $\int_0^1 f'(ts) \,dt$ is a smooth function of $s$.
\end{exercise}

\begin{exercise}
Show that $T_pM$ is independent of the defining function:
Prove that if $r$ and $\tilde{r}$ are defining functions for $M$ at $p$, then
$\sum_k a_k \frac{\partial r}{\partial x_k} \big|_p = 0$
if and only if
$\sum_k a_k \frac{\partial \tilde{r}}{\partial x_k} \big|_p = 0$.
\end{exercise}
\end{exbox}

The tangent space $T_p M$ is the set of derivatives
\emph{along} $M$ at $p$.  If $r$ is a defining function of $M$, and $f$ and $h$
are two smooth functions such that $f=h$ on $M$, then
\exerciseref{exercise:smoothdivision}
says that
\begin{equation*}
f-h = g r, \qquad \text{or} \qquad
f = h + g r,
\end{equation*}
for some smooth $g$.  Applying $X_p$ we find
\begin{equation*}
X_p f =
X_p h + X_p (gr) =
X_p h + (X_p g)r + g(X_p r) =
X_p h + (X_p g)r.
\end{equation*}
So $X_p f = X_p h$ on $M$ (where $r=0$).  In other words, $X_p f$ only
depends on the values of $f$ on $M$.

This brings up a natural question about what is a smooth function on $M$.
By definition, a function $f$ defined on $M$ (or any other subset of $\R^n$
that is not open) is \emph{smooth} if it is
locally the restriction to $M$ of a smooth function in an open neighborhood.
This extension of $f$ is not unique, so
the above calculation shows that differentiating $f$
via $T_p M$ is indepdendent on how $f$ is extended to a neighborhood.

\begin{example}
If $M \subset \R^n$ is given by $x_n = 0$, then $T_p M$ is given by
derivatives of the form
\begin{equation*}
X_p =  \sum_{k=1}^{n-1} a_k \frac{\partial}{\partial x_k} \Big|_p .
\end{equation*}
That is, derivatives along the first $n-1$ variables only.
\end{example}

\begin{defn}
The disjoint union
\glsadd{not:realtangentbundle}%
\begin{equation*}
T\R^n = \bigcup_{p \in \R^n} T_p \R^n
\end{equation*}
is called the \emph{\myindex{tangent bundle}}.  There is a
natural identification $\R^n \times \R^n \cong T\R^n$:
\begin{equation*}
(p,a) \in \R^n \times \R^n
\quad
\mapsto
\quad
\sum_{k=1}^n a_k \frac{\partial}{\partial x_k} \Big|_p \in T\R^n .
\end{equation*}
The topology and smooth structure on $T\R^n$ comes from this identification.
The wording ``bundle'' (a bundle of fibers)
comes from the natural projection $\pi \colon T\R^n
\to \R^n$, where fibers are $\pi^{-1}(p) = T_p\R^n$.

A smooth
\emph{\myindex{vector field}} in $T\R^n$ is an object of the form
\begin{equation*}
X = \sum_{k=1}^n a_k
\frac{\partial}{\partial x_k} ,
\end{equation*}
where $a_k$ are smooth
functions.
That is, $X$ is a smooth function
$X \colon V \subset \R^n \to T\R^n$ such that $X(p) \in T_p \R^n$.
Usually, we write $X_p$ rather than $X(p)$.
To be more fancy,
say $X$ is a \emph{\myindex{section}} of $T \R^n$.

Similarly, the tangent bundle of $M$ is
\glsadd{not:realtangentbundle}%
\begin{equation*}
TM = \bigcup_{p \in M} T_p M .
\end{equation*}
A vector field $X$ in $TM$ is a vector field
such that $X_p \in T_p M$ for all $p \in M$.
\end{defn}

Before we move on, we note how smooth maps transform tangent spaces.
Given a smooth
$f \colon U \subset \R^n \to \R^m$,
the derivative at $p$ is a linear mapping of
the tangent spaces: $Df(p) \colon T_p \R^n \to T_{f(p)} \R^m$.
If $X_p \in T_p \R^n$, then
$Df(p) X_p$ should be in $T_{f(p)} \R^m$.
The vector
$Df(p) X_p$ is defined by how it acts on
smooth functions $\varphi$ of a neighborhood
of $f(p)$ in $\R^m$:
%then
%$Df(p) X_p$ acts on $\varphi$ as
\glsadd{not:Df}%
\begin{equation*}
\bigl( Df(p) X_p \bigr) \varphi = X_p (\varphi \circ f) .
\end{equation*}
It is the only reasonable way to put those three objects together.
When the spaces are $\C^n$ and $\C^m$, we denote this
derivative as
\glsadd{not:DRf}%
$D_\R f$
to distinguish it from the holomorphic derivative.
As far as calculus computations are concerned,
the linear mapping $Df(p)$ is
the Jacobian matrix acting on vectors in the standard basis of the tangent space
as given above.
This is why we use the same notation for the Jacobian
matrix and the derivative acting on tangent spaces.
To verify this claim, it is enough to see where the basis element
$\frac{\partial}{\partial x_k}\big|_p$ goes, and the form of $Df(p)$
as a matrix
follows by the chain rule.
For instance, the derivative of the mapping $f(x_1,x_2) =
(x_1+2x_2+x_1^2,3x_1+4x_2+x_1x_2)$ at the origin is given by the matrix
$\left[ \begin{smallmatrix} 1 & 2 \\ 3 & 4 \end{smallmatrix} \right]$,
and so the vector
$X_p = a\frac{\partial}{\partial x_1}\big|_0
+
b\frac{\partial}{\partial x_2}\big|_0$
gets taken to
$Df(0) X_0 = (a+2b)\frac{\partial}{\partial y_1}\big|_0
+
(3a+4b)\frac{\partial}{\partial y_2}\big|_0$, where $(y_1,y_2)$ are the
coordinates on the target.  You should check on some test
function, such as
$\varphi(y_1,y_2) = \alpha y_1 + \beta y_2$, that the definition above is
satisfied.

Suppose that for a smooth map $f$ and smooth hypersurfaces $M$ and $M'$
you have $f(M) \subset M'$.  Then you get the same containment for the
tangent spaces.  Indeed, suppose that $r$ is a defining function for $M$
near $p$ and $r'$ is a defining function for $M'$ near $f(p)$, and
suppose that $X_p \in T_p M$.  Then $r' \circ f$ is zero on $M$,
and hence
\begin{equation*}
\bigl( Df(p) X_p \bigr) r' = X_p (r' \circ f) = X_p (0) = 0 .
\end{equation*}
If the map is a diffeomorphism (has an inverse), then $f(M)$ is a smooth
hypersurface with defining function $r \circ f^{-1}$, the derivative is
an invertible linear map, and we get that $Df(p)$ restricts to an
isomorphism of $T_pM$ and $T_{f(p)} f(M)$.  That is, we proved the following
proposition.

\begin{prop} \label{prop:DMtoM}
Suppose $U \subset \R^n$ is open, $M \subset U$ is a smooth hypersurface,
$f \colon U \to \R^m$ is a smooth function, $M' \subset \R^m$ is a smooth
hypersurface such that $f(M) \subset M'$, and $p \in M$.  Then
\begin{equation*}
Df(p)(T_pM) \subset T_{f(p)} M' .
\end{equation*}
Moreover, if $m=n$ and $f$ is a diffeomorphism (bijective onto some open set
$U'$ such that $f^{-1}$ is smooth), then $f(M)$ is a smooth hypersurface and
$Df(p)(T_pM) = T_{f(p)} f(M)$.
\end{prop}

Now that we know what tangent vectors are and how they transform,
let us define convexity
for domains with smooth boundary.

\begin{defn}
Suppose $U \subset \R^n$ is an open set with
smooth boundary, %$C^k$-smooth boundary, $k \geq 2$,
and $r$ is a defining function for $\partial U$ at $p \in \partial U$
such that $r < 0$ on $U$.
If
\begin{equation*}
\sum_{k=1,\ell=1}^n
a_k a_\ell \frac{\partial^2 r}{\partial x_k \partial x_\ell} \Big|_p \geq 0 ,
\qquad \text{for all} \qquad
X_p = \sum_{k=1}^n a_k
\frac{\partial}{\partial x_k}\Big|_p \quad \in \quad T_p \partial U,
\end{equation*}
then $U$ is said to be \emph{\myindex{convex}} at $p$.  If the inequality
above is strict for all nonzero $X_p \in T_p \partial U$, then $U$ is said to be
\emph{\myindex{strongly convex}} at $p$.

A domain $U$ is \emph{convex} if it is convex at all $p \in \partial U$.
If $U$ is bounded\footnote{Matters are a little more complicated
with the ``strong'' terminology if $U$ is unbounded, so sometimes
\emph{\myindex{strictly convex}} is used instead.},
we say $U$ is \emph{strongly convex} if it is strongly convex at all
$p \in \partial U$.
\end{defn}

The matrix
\begin{equation*}
\left[ \frac{\partial^2 r}{\partial x_k \partial x_\ell} \Big|_p
\right]_{k\ell}
\end{equation*}
is the
\emph{\myindex{Hessian}} of $r$ at $p$.
So, $U$ is convex at $p \in \partial U$ if
the Hessian
of $r$ at $p$ as a bilinear form is positive semidefinite
when restricted to $T_p \partial U$.
More concretely, let $H$ be the Hessian of $r$ at $p$, and treat
$a \in \R^n$ as a column vector.
Then
$\partial U$ is convex at $p$ whenever
\begin{equation*}
a^t H a \geq 0 , \qquad \text{for all $a \in \R^n$ such that} \quad \nabla r|_p
\cdot a = 0 .
\end{equation*}
This bilinear form given by the Hessian is the second fundamental form from Riemannian
geometry in mild disguise (or perhaps it is the other way around).

We cheated a little bit, since we have not proved that
the notion of convexity is well-defined.  In particular, there are many possible
defining functions.

\begin{exbox}
\begin{exercise}
Show that the definition of convexity is independent of the defining
function.  Hint: If $\tilde{r}$ is another defining function near $p$,
then there is a smooth function $g > 0$ such that $\tilde{r} = g r$.
\end{exercise}
\end{exbox}

\begin{example}
The unit disc in $\R^2$ is strongly convex.
Proof:
Let $(x,y)$ be the coordinates and
let $r(x,y) = x^2+y^2-1$ be the
defining function.
The tangent space of the circle is one-dimensional, so we simply need to
find a single nonzero tangent vector at each point.
Consider the gradient
\glsadd{not:gradient}%
$\nabla r = (2x,2y)$ to check that
\begin{equation*}
X = y \frac{\partial}{\partial x} - x \frac{\partial}{\partial y}
\end{equation*}
is tangent to the circle, that is,
$Xr = X(x^2+y^2-1) = (2x,2y) \cdot (y,-x) = 0$ on the circle---by chance, $Xr=0$ everywhere.
The vector field $X$ is nonzero on the circle, so at each point it gives a
basis of the tangent space.  See \figureref{fig:circle-tangent}.
\begin{myfig}
\subimport*{figures/}{circle-tangent.pdf_t}
\caption{Tangent vector to a circle.\label{fig:circle-tangent}}
\end{myfig}

The Hessian matrix of $r$ is
\begin{equation*}
\begin{bmatrix}
\frac{\partial^2 r}{\partial x^2} &
\frac{\partial^2 r}{\partial x \partial y} \\
\frac{\partial^2 r}{\partial y \partial x} &
\frac{\partial^2 r}{\partial y^2}
\end{bmatrix}
=
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix} .
\end{equation*}
Applying the vector $(y,-x)$ gets us
\begin{equation*}
\begin{bmatrix}
y & -x
\end{bmatrix}
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix}
\begin{bmatrix}
y \\ -x
\end{bmatrix}
=
2y^2+2x^2 = 2 > 0 .
\end{equation*}
So the domain given by $r < 0$ is strongly convex at all points.
\end{example}

In general, to construct a tangent vector field for
a curve in $\R^2$,
consider
$r_y \frac{\partial}{\partial x} - r_x \frac{\partial}{\partial y}$.  In
higher dimensions, running through enough pairs of variables gets
a basis of $TM$.

\begin{exbox}
\begin{exercise}
Show that if an open set with smooth boundary is strongly convex at a point $p$,
then it is strongly convex for all points in some neighborhood of $p$.
Then find an example of
an open set with smooth boundary that is convex at one point $p$,
but not convex at points arbitrarily near $p$.
\end{exercise}

\begin{exercise}
Show that the domain in $\R^2$ defined by $x^4+y^4 < 1$ is convex, but not strongly convex.
Find all the points where the domain is not strongly convex.
\end{exercise}

\begin{exercise}
Show that the domain in $\R^3$ defined by ${(x_1^2+x_2^2)}^2 < x_3$ is
strongly convex at all points except the origin, where it is just convex
(but not strongly).
\end{exercise}
\end{exbox}

The right sort of changes of coordinates that preserve convexity are
invertible real affine linear mappings.
It is rather clear for geometric convexity, as these are precisely the maps
that take lines to lines,
but it takes a little bit of computation for convexity at a point of
a smooth boundary (exercise below).
A useful analogy to keep in mind (but not to go overboard with) is that
holomorphic functions are sort of like affine functions.  And so it will be
with convexity being replaced with pseudoconvexity in just a little bit, and
affine linear maps with holomorphic maps.

\begin{exbox}
\begin{exercise}
Prove that translations and invertible linear maps (matrices) preserve
convexity and strong convexity at a point for a domain with smooth boundary.
\end{exercise}
\end{exbox}

In the following, \glsadd{not:Ol}%
we use the \emph{\myindex{big-oh notation}},
although we use a perhaps less standard shorthand\footnote{%
The standard notation for $O(\ell)$ is $O(\snorm{x}^{\ell})$ and
it means that
$\abs{\frac{f(x)}{\snorm{x}^\ell}}$ is bounded as $x \to p$.}.
A smooth function is $O(\ell)$ at a point $p$ (usually the origin),
if all its derivatives of order $0, 1, \ldots,  \ell-1$ vanish at $p$.
For example, if $f$ is $O(3)$ at the origin,
then $f(0)=0$, and its first and second derivatives vanish at the origin.

For computations it is often useful to use a more convenient
defining function, that is, it is convenient to write $M$ as a graph.

\begin{prop} \label{prop:realgraphcoords}
Suppose $M \subset \R^n$ is a smooth hypersurface,
and $p \in M$.  Then after a rotation (orthogonal matrix) and translation,
$p$ is the origin, and near the origin, $M$ is given by
\begin{equation*}
y = \varphi(x) ,
\end{equation*}
where $(x,y) \in \R^{n-1} \times \R$ are our coordinates and
$\varphi$ is a smooth
function that is $O(2)$ at the origin,
namely, $\varphi(0) = 0$ and $d\varphi(0) = 0$.
Consequently,
\begin{equation*}
T_0M = \operatorname{span}_{\R} \left\{
\frac{\partial}{\partial x_1}\Big|_p,
\ldots,
\frac{\partial}{\partial x_{n-1}}\Big|_p \right\} .
\end{equation*}

\nopagebreak
If $M$ is the boundary of an open set $U$ with smooth boundary and
$r < 0$ on $U$,
then the rotation can be chosen
such that $y > \varphi(x)$ for points in $U$.
See \figureref{fig:deffun-graph}.
\end{prop}

\begin{myfig}
\subimport*{figures/}{deffun-graph.pdf_t}
\caption{Defining a domain as a graph.\label{fig:deffun-graph}}
\end{myfig}

\begin{proof}
Let $r$ be a defining function at $p$.  Take $v = \nabla r|_p$.
By translating $p$ to the origin, and applying a rotation (an orthogonal matrix),
we assume $v = (0,0,\ldots,0,v_n)$, where $v_n < 0$.  Denote our
coordinates by $(x,y) \in \R^{n-1} \times \R$.  As $\nabla r|_0 =
v$, then $\frac{\partial r}{\partial y}(0) \not= 0$.
The implicit function theorem gives a
smooth function $\varphi$ such that
$r\bigl(x,\varphi(x)\bigr) = 0$ for all $x$ in a neighborhood of the
origin, and $\bigl\{ (x,y) : y=\varphi(x) \bigr\}$ are all the
solutions to $r = 0$ near the origin.

We need to show that the derivative of $\varphi$ at $0$ vanishes.
As
$r\bigl(x,\varphi(x)\bigr) = 0$ for all $x$ in a neighborhood of the
origin, we differentiate.
For every $k=1,\ldots,n-1$,
\begin{equation*}
0 =
\frac{\partial}{\partial x_k} \Bigl[
r\bigl(x,\varphi(x)\bigr)
\Bigr]
=
\left(
\sum_{\ell=1}^{n-1}
\frac{\partial r}{\partial x_\ell}
\frac{\partial x_\ell}{\partial x_k}
\right)
+
\frac{\partial r}{\partial y}
\frac{\partial \varphi}{\partial x_k}
=
\frac{\partial r}{\partial x_k}
+
\frac{\partial r}{\partial y}
\frac{\partial \varphi}{\partial x_k} .
\avoidbreak
\end{equation*}
At the origin,
$\frac{\partial r}{\partial x_k}(0,0) = 0$ and
$\frac{\partial r}{\partial y}(0,0) = v_n \not= 0$, and therefore
$\frac{\partial \varphi}{\partial x_k}(0) = 0$.
That $T_0 M$ is the span of the $x_k$ derivatives
follows at once from the fact that $\nabla r|_0 = (0,\ldots,0,v_n)$.

To prove the final statement, note that
$r < 0$ on $U$.  It is enough to check that $r$ is
negative for $(0,y)$ if $y > 0$ is small, which follows as $\frac{\partial
r}{\partial y}(0,0) = v_n < 0$.
\end{proof}

The advantage of this representation is that the tangent space at $p$
can be identified with the $x$ coordinates for the purposes of computation.
Considering $x$ as a column vector, the Taylor expansion of a smooth function $\varphi$ at the origin is
\begin{equation*}
\varphi(x) = \varphi(0) + \nabla \varphi|_0 \cdot x + \frac{1}{2}\, x^t H x + E(x) ,
\end{equation*}
where $H = \left[
\frac{\partial^2 \varphi}{\partial x_k \partial x_\ell} \big|_{0} \right]_{k\ell}$
is the Hessian matrix of $\varphi$ at the origin,
and $E$ is $O(3)$, namely, $E(0) = 0$, and
all first and second order derivatives of $E$ vanish at $0$.
In the context of the lemma above,
the $\varphi$ is $O(2)$ at the origin, i.e.\ $\varphi(0) = 0$
and $\nabla \varphi|_0 = 0$.  So we write the hypersurface
$M$ as
\begin{equation*}
y = \frac{1}{2}\, x^t H x + E(x) .
\end{equation*}
If $M$ is the boundary
$\partial U$ of an open set $U$, then
we pick the rotation so that $y > \frac{1}{2}\,x^t H x + E(x)$ on $U$.
It is an easy exercise to show that $U$ is convex at $p$ if
$H$ positive semidefinite, and $U$ is strongly convex at $p$ if $H$ is positive definite.

\begin{exbox}
\begin{exercise}
Prove the statement above about $H$ and convexity at $p$.
\end{exercise}

\begin{exercise}
Let $r$ be a defining function at $p$ for a smooth hypersurface
$M \subset \R^n$.
We say $M$ is convex from both sides at $p$ if both the set given by
$r > 0$ and the set given by $r < 0$ are convex at $p$.
Prove that if a hypersurface $M \subset \R^n$ is convex from both sides at all
points, then it is locally just a hyperplane (the zero set of a real affine
function).
\end{exercise}

\begin{exercise}
Suppose $U$ is a domain with smooth boundary that is strongly convex
at $p \in \partial U$.
Then there exists a real affine change of variables (translation and an
invertible linear map), such that after the change of variables, $p=0$ and near $0$, $\partial U$ is given by
$y = x^t x + E(x)$ where $E(x)$ is $O(3)$ and
$y > x^tx + E(x)$ on $U$.
\end{exercise}
\end{exbox}

Recall that $U$ is
\emph{\myindex{geometrically convex}}\index{convex!geometrically}
if for every $p,q \in U$ the
line between $p$ and $q$ is in $U$, that is,
$tp +(1-t)q \in U$ for all $t \in [0,1]$.
Geometric convexity is a \emph{global} condition; it considers
the entire $U$.  The notion of convexity for a smooth boundary is
\emph{local} in that you only need to know $\partial U$ in a small
neighborhood.  For domains with
smooth boundaries the two notions are equivalent.  Proving
one direction is easy.

\begin{exbox}
\begin{exercise}
Suppose a domain $U \subset \R^n$ with smooth
boundary is geometrically
convex.  Show that $U$ is convex.
\end{exercise}
\end{exbox}

The other direction is considerably more complicated, and we will not worry
about it here.  Proving a global condition from a local one is often
trickier, but also often more interesting.
Similar difficulties will be present once we move back to
several complex variables and try to relate pseudoconvexity with domains of
holomorphy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Holomorphic vectors, Levi form, pseudoconvexity}

As $\C^n$ is identified with $\R^{2n}$ using $z=x+iy$, we have
$T_p\C^n = T_p\R^{2n}$.  If we take the complex span instead of the real
span, we get the \emph{\myindex{complexified tangent space}}\footnote{%
Abstractly, a real vector space $X$
can be complexified as
$X \otimes_{\R} \C$.}%
\glsadd{not:complexifiedtangentspace}%
\begin{equation*}
\C T_p\C^n
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial x_1}\Big|_p,
\frac{\partial}{\partial y_1}\Big|_p,
\ldots,
\frac{\partial}{\partial x_n}\Big|_p ,
\frac{\partial}{\partial y_n}\Big|_p \right\} .
\end{equation*}
We simply replace all the real coefficients with complex ones.
The space $\C T_p\C^n$ is a $2n$-dimensional complex vector space.
Both
$\frac{\partial}{\partial z_k}\big|_p$ and
$\frac{\partial}{\partial \bar{z}_k}\big|_p$ are in $\C T_p\C^n$, and
\begin{equation*}
\C T_p\C^n
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial z_1}\Big|_p,
\frac{\partial}{\partial \bar{z}_1}\Big|_p,
\ldots,
\frac{\partial}{\partial z_n}\Big|_p ,
\frac{\partial}{\partial \bar{z}_n}\Big|_p \right\} .
\end{equation*}
Define
\glsadd{not:holtangentspace}%
\glsadd{not:aholtangentspace}%
\begin{equation*}
T_p^{(1,0)} \C^n \overset{\text{def}}{=}
\operatorname{span}_{\C} \left\{
\frac{\partial}{\partial z_1}\Big|_p,
\ldots,
\frac{\partial}{\partial z_n}\Big|_p  \right\}
\quad \text{and} \quad
T_p^{(0,1)} \C^n \overset{\text{def}}{=}
\operatorname{span}_{\C} \left\{
\frac{\partial}{\partial \bar{z}_1}\Big|_p,
\ldots,
\frac{\partial}{\partial \bar{z}_n}\Big|_p \right\} .
\end{equation*}
The vectors in $T_p^{(1,0)} \C^n$ are the
\emph{\myindex{holomorphic vectors}} and vectors in
$T_p^{(0,1)} \C^n$ are the
\emph{\myindex{antiholomorphic vectors}}.
We decompose the full tangent space as the direct sum
\begin{equation*}
\C T_p\C^n
=
T_p^{(1,0)} \C^n \oplus
T_p^{(0,1)} \C^n .
\end{equation*}
A holomorphic function is one that vanishes on $T_p^{(0,1)} \C^n$.

Let us see what holomorphic mappings do to these spaces when we treat
holomorphic mappings as smooth mappings.
Given a smooth
mapping $f$ from (an open subset of) $\C^n$ to $\C^m$, its derivative at $p \in \C^n$
is a real-linear mapping $D_\R f(p) \colon T_p\C^n \to T_{f(p)} \C^m$.
Given the basis above, this mapping is represented by
the standard real Jacobian matrix, that is, a real $2m \times 2n$ matrix
that we wrote before as $D_\R f(p)$.
As a basis for $T_p\C^n$ is a basis for $\C T_p \C^n$, the mapping
$D_\R f(p) \colon T_p\C^n \to T_{f(p)} \C^m$ naturally uniquely
extends to
\glsadd{not:DCf}%
\begin{equation*}
D_\C f(p) \colon \C T_p\C^n \to \C T_{f(p)} \C^m .
\end{equation*}

\begin{prop} \label{prop:holvectmap}
Let $f \colon U \subset \C^n \to \C^m$ be a holomorphic mapping with
$p \in U$.
%Suppose
%$D_\R f(p) \colon T_p\C^n \to T_{f(p)} \C^m$
%is the real derivative of $f$ at $p$.
%\glsadd{not:DCf}%
%We naturally
%extend the derivative to $D_\C f(p) \colon \C T_p\C^n \to \C T_{f(p)}
%\C^m$.
Then
\begin{equation*}
D_\C f(p)\Bigl(T_p^{(1,0)} \C^n\Bigr) \subset T_{f(p)}^{(1,0)} \C^m
\qquad \text{and} \qquad
D_\C f(p)\Bigl(T_p^{(0,1)} \C^n\Bigr) \subset T_{f(p)}^{(0,1)} \C^m .
\end{equation*}
If $f$ is a biholomorphism, then $D_\C f(p)$ restricted to $T_p^{(1,0)} \C^n$
is a vector space isomorphism.  Similarly for $T_p^{(0,1)} \C^n$.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove the proposition.
Hint: Start with $D_\R f(p)$ as a real $2m \times 2n$ matrix to show it
extends (it is the same matrix if you think of it as a matrix
and use the same basis vectors).
Think of $\C^n$ and $\C^m$ in terms of the $z$s and the
$\bar{z}$s and think of $f$ as a mapping
\begin{equation*}
(z,\bar{z}) \mapsto \bigl( f(z) , \bar{f}(\bar{z}) \bigr) .
\end{equation*}
Write the derivative as a matrix in terms of the $z$s and the $\bar{z}$s
and $f$s and $\bar{f}$s and the result will follow.  That is just changing
the basis.
\end{exercise}

\begin{exercise}
Prove a converse to the proposition.  If $f \colon U \subset \C^n \to \C^m$
is a smooth mapping such that
$D_\C f(p)\Bigl(T_p^{(1,0)} \C^n\Bigr) \subset T_{f(p)}^{(1,0)} \C^m$
at every $p \in U$, then $f$ is holomorphic.
\end{exercise}
\end{exbox}

For holomorphic mappings and holomorphic vectors,
when we say ``derivative of $f$,'' we mean the holomorphic part of the
derivative, which we write as
\glsadd{not:Df}%
\begin{equation*}
D f(p) \colon T_p^{(1,0)} \C^n \to T_{f(p)}^{(1,0)} \C^m ,
\qquad
D f(p) = D_{\C} f(p) \big|_{T_p^{(1,0)} \C^n} .
\end{equation*}
That is, we restrict $D_\C f(p)$ to $T_p^{(1,0)} \C^n$.
Let $z$ be the coordinates on $\C^n$ and
$w$ the coordinates on $\C^m$.
In the bases
$\left\{ \frac{\partial}{\partial z_1} \big|_p,\ldots,
\frac{\partial}{\partial z_n} \big|_p \right\}$
and
$\left\{ \frac{\partial}{\partial w_1} \big|_{f(p)},\ldots,
\frac{\partial}{\partial w_m} \big|_{f(p)} \right\}$,
the holomorphic derivative $Df(p)$
is represented by the $m \times n$ Jacobian matrix
\begin{equation*}
\left[
\frac{\partial f_k}{\partial z_\ell} \Big|_p
\right]_{k\ell} ,
\end{equation*}
which we have seen in \sectionref{sec:derivatives}
and for which we also used the notation $Df(p)$.

As before, define the tangent bundles
\begin{equation*}
\C T\C^n,
\quad
T^{(1,0)} \C^n,
\quad \text{and} \quad
T^{(0,1)} \C^n ,
\end{equation*}
by taking the disjoint unions.
One can also define vector fields in these bundles.

Let us describe $\C T_pM$
for a smooth real hypersurface $M \subset \C^n$.
Let $r$ be a real-valued defining function of
$M$ at $p$.  A vector
$X_p \in \C T_p\C^n$ is in
$\C T_pM$ whenever $X_p r = 0$.  That is,
\begin{equation*}
X_p = \sum_{k=1}^n
\left(
a_k
\frac{\partial}{\partial z_k} \Big|_p
+
b_k
\frac{\partial}{\partial \bar{z}_k} \Big|_p
\right) \in \C T_p M
\quad
\text{whenever}
\quad
 \sum_{k=1}^n
\left(
a_k
\frac{\partial r}{\partial z_k} \Big|_p
+
b_k
\frac{\partial r}{\partial \bar{z}_k} \Big|_p
\right)
= 0 .
\end{equation*}
Therefore, $\C T_p M$ is a $(2n-1)$-dimensional complex vector space.
We decompose $\C T_p M$ as
\begin{equation*}
\C T_pM =
T_p^{(1,0)} M \oplus T_p^{(0,1)} M \oplus B_p ,
\end{equation*}
where
\begin{equation*}
T_p^{(1,0)} M \overset{\text{def}}{=} \bigl( \C T_pM \bigr) \cap
\bigl( T_p^{(1,0)} \C^n \bigr),  \quad \text{and}
\quad
T_p^{(0,1)} M \overset{\text{def}}{=} \bigl( \C T_pM \bigr) \cap
\bigl( T_p^{(0,1)} \C^n \bigr) .
\end{equation*}
The $B_p$ is the ``leftover'' and must
be included for the dimensions to work out.\footnote{%
The $B_p$ is sometimes colloquially called the ``bad direction.''}

\begin{exbox}
\begin{exercise}
Prove that there is another way of getting at these spaces.  Consider a smooth
hypersurface $M$ and $p \in M$.  Let $J$ be the linear map of $T_p \C^n$ to
itself that corresponds to multiplication by $i$
(the derivative of the actual multiplication by $i$).
Write $T^c_p M = J(T_p M) \cap T_pM$ (the subspace fixed by $J$, sometimes
called the \emph{\myindex{complex tangent space}} despite being a real
vector space).
The map $J$ restricts to an endomorphism of $T^c_pM$ and thus it naturally induces
an endomorphism of $\C T^c_pM$.
Then $T^{(1,0)}_pM$ and
$T^{(0,1)}_pM$ are the eigenspaces of $J$, which has eigenvalues $\pm i$.
\end{exercise}
\end{exbox}

Make sure to notice what sort of vector spaces these are.
The space $T_pM$ is a real vector space;
$\C T_pM$, $T_p^{(1,0)}M$, $T_p^{(0,1)} M$, and $B_p$
are complex vector spaces.
To see that these give vector bundles,
we must first show that their dimensions do not vary
from point to point.  The easiest way to see this fact is to write down
convenient local coordinates.  First, let us note that
a biholomorphic map preserves the tangent holomorphic and antiholomorphic
vectors.  That is, we get the following
analogue of \propref{prop:DMtoM}.
Note that a biholomorphic map is a diffeomorphism.

\begin{prop} \label{prop:whereTgoeshol}
Suppose $M \subset \C^n$ is a smooth real hypersurface, $p \in M$,
and $U \subset \C^n$ is open with $M \subset U$, and supose $M' \subset
\C^m$ a smooth real hypersurface.
Let $f \colon U \to \C^m$ be holomorphic such $f(M) \subset M'$.
Let $D_\C f(p)$ be
the complexified real derivative as before.  Then
\begin{equation*}
D_\C f(p)\Bigl(T_p^{(1,0)}M\Bigr) \subset T_{f(p)}^{(1,0)}M', \qquad
D_\C f(p)\Bigl(T_p^{(0,1)}M\Bigr) \subset T_{f(p)}^{(0,1)}M'.
\avoidbreak
\end{equation*}
Moreover, if $m=n$ and $f$ is a biholomorphism,
then $f(M)$ is a smooth real hypersurface,
$D_\C f(p)$ is invertible,
$D_\C f(p)\Bigl(T_p^{(1,0)}M\Bigr) = T_{f(p)}^{(1,0)} f(M)$
and
$D_\C f(p)\Bigl(T_p^{(0,1)}M\Bigr) = T_{f(p)}^{(0,1)} f(M)$.
That is, the spaces are isomorphic as complex vector spaces.
\end{prop}

The proposition is local, if $U$ is only a neighborhood of $p$,
replace $M$ with $M \cap U$.

\begin{proof}
Apply \propref{prop:holvectmap} and
\propref{prop:DMtoM}.
That is,
\begin{multline*}
D_\C f(p)\Bigl(T_p^{(1,0)}\C^n\Bigr) \subset T_{f(p)}^{(1,0)}\C^m, \quad
D_\C f(p)\Bigl(T_p^{(0,1)}\C^n\Bigr) \subset T_{f(p)}^{(0,1)}\C^m, \quad
\text{and} \\
D_\C f(p)\bigl(\C T_pM\bigr) \subset \C T_{f(p)}M' .
\avoidbreak
\end{multline*}
Then $D_\C f(p)$ must take
$T_p^{(1,0)}M$ to $T_{f(p)}^{(1,0)}M'$ and
$T_p^{(0,1)}M$ to $T_{f(p)}^{(0,1)}M'$.
The ``Moreover'' follows
from the ``Moreover'' of \propref{prop:DMtoM}.
\end{proof}

We again wish to write a hypersurface as a graph.  In this context, the
right sort of transformations are biholomorphic transformations.
Translations are biholomorphic, and
the rotation we will want to use is applying a unitary matrix to $\C^n$.

\begin{prop} \label{prop:graphcoordinatesCn}
\pagebreak[2]
Let $M \subset \C^n$ be a smooth real hypersurface, $p \in M$.
After a translation and a rotation by a unitary
matrix, $p$ is the origin, and near the origin,
$M$ is written in variables $(z,w) \in \C^{n-1}
\times \C$ as
\begin{equation*}
\Im w = \varphi(z,\bar{z},\Re w) ,
\end{equation*}
with the $\varphi(0)$  and $d\varphi(0) = 0$.  Consequently,
\begin{gather*}
T_0^{(1,0)} M
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial z_1}\Big|_0,
\ldots,
\frac{\partial}{\partial z_{n-1}}\Big|_0 \right\} ,
\qquad
T_0^{(0,1)} M
= \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial \bar{z}_1}\Big|_0,
\ldots,
\frac{\partial}{\partial \bar{z}_{n-1}}\Big|_0 \right\} ,
\\
B_0 = \operatorname{span}_{\C} \left\{
\frac{\partial}{\partial (\Re w)}\Big|_0 \right\} .
\avoidbreak
\end{gather*}
In particular,
$\dim_\C T_p^{(1,0)} M = \dim_\C T_p^{(0,1)} M = n-1$ and
$\dim_\C B_p = 1$.

\nopagebreak
If $M$ is the boundary of a open set $U$ with smooth boundary,
the rotation can be chosen so that
$\Im w > \varphi(z,\bar{z},\Re w)$ on $U$.
\end{prop}

Remark the notation $\varphi(z,\bar{z},\Re w)$, where we are using the
$z,\bar{z}$ notation for the $z$ directions, but since $\varphi$ does not
depend on $\Im w$, we cannot do the same with the $w$.

\begin{proof}
\pagebreak[2]
Apply a translation to put $p=0$ and in the
same manner as in \propref{prop:realgraphcoords}
apply a unitary matrix to make sure that $\nabla r$ is
in the direction
$-\frac{\partial}{\partial (\Im w)}\big|_0$.  That $\varphi(0) = 0$
and $d\varphi(0) = 0$ follows as before.
A translation and a unitary matrix are holomorphic
and, in fact, biholomorphic, so
via \propref{prop:whereTgoeshol} the tangent spaces are all
transformed correctly.
The rest of the proposition follows at once as
$\frac{\partial}{\partial (\Im w)}\big|_0$ is the normal vector to $M$
at the origin.
\end{proof}

\begin{remark}
When $M$ is of dimension less than $2n-1$ (not a hypersurface anymore),
the conclusion of the
proposition on the dimensions does not hold.
That is, we still have $\dim_\C T_p^{(1,0)} M = \dim_\C T_p^{(0,1)}
M$, but this number need not be constant from point to point.  Fortunately,
the boundaries of domains with smooth boundaries are by definition
hypersurfaces and this complication does not arise.
\end{remark}

\begin{defn}
Suppose $U \subset \C^n$ is an open set with
smooth boundary,
and $r$ is a defining function for $\partial U$ at $p \in \partial
U$ such that $r < 0$ on $U$.
If
\begin{equation*}
\sum_{k=1,\ell=1}^n
\bar{a}_k a_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_p \geq 0
\qquad
\text{for all}
\qquad
X_p = \sum_{k=1}^n a_k
\frac{\partial}{\partial z_k}\Big|_p  \quad \in \quad
T^{(1,0)}_p \partial U,
\end{equation*}
then $U$ is \emph{\myindex{pseudoconvex}} at $p$
(or \emph{\myindex{Levi pseudoconvex}}).
If the inequality
above is strict for all nonzero $X_p \in T^{(1,0)}_p \partial U$, then $U$ is
\emph{\myindex{strongly pseudoconvex}} at $p$.
If $U$ is pseudoconvex, but not strongly pseudoconvex, at $p$, then
$U$ is \emph{\myindex{weakly pseudoconvex}}.

A domain $U$ is \emph{pseudoconvex} if it is pseudoconvex at all $p \in \partial U$.
For a bounded\footnote{The definition for unbounded domains is not
consistent in the literature.
Sometimes \emph{\myindex{strictly pseudoconvex}} is used.} $U$, we say $U$
is \emph{strongly pseudoconvex} if it is strongly pseudoconvex
at all $p \in \partial U$.

For $X_p \in T^{(1,0)}_p\partial U$, the Hermitian quadratic form
\glsadd{not:Leviform}%
\begin{equation*}
\sL(X_p,X_p)
=
\sum_{k=1,\ell=1}^n
\bar{a}_k a_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_p
\end{equation*}
is called the \emph{\myindex{Levi form}} at $p$.  So $U$ is pseudoconvex
(resp.\ strongly pseudoconvex) at
$p \in \partial U$ if the Levi form is positive semidefinite
(resp.\ positive definite) at $p$.
The Levi form can be defined for any real hypersurface $M$,
although one has to decide which side of $M$ is ``the inside.''
\end{defn}

The matrix
\begin{equation*}
\left[ \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_p
\right]_{k \ell}
\end{equation*}
is called the
\emph{\myindex{complex Hessian}}\index{Hessian!complex} of $r$ at
$p$.\footnote{%
People sometimes call the complex Hessian the ``Levi form of $r$,''
which is incorrect.  The Levi form is something defined for a boundary
or a hypersurface acting only on its tangent vectors.}
So, $U$ is pseudoconvex at $p \in \partial U$ if
the complex Hessian
of $r$ at $p$ as a Hermitian form is positive (semi)definite
when restricted to tangent vectors in $T^{(1,0)}_p \partial U$.
For example, the unit ball $\bB_n$ is
strongly pseudoconvex as can be seen by
computing the
Levi form directly from $r(z,\bar{z}) = \snorm{z}^2-1$, that is,
the complex Hessian of $r$ is the identity matrix.

We remark that the complex Hessian is not the full (real) Hessian.
Let us write down the full Hessian, using the
basis of $\frac{\partial}{\partial z}$s and
$\frac{\partial}{\partial \bar{z}}$s.  It is
the Hermitian matrix
\begin{equation*}
\begin{bmatrix}
\frac{\partial^2 r}{\partial \bar{z}_1 \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_1 \partial z_n}
&
\frac{\partial^2 r}{\partial \bar{z}_1 \partial \bar{z}_1}
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_1 \partial \bar{z}_n}
\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots
\\
\frac{\partial^2 r}{\partial \bar{z}_n \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_n \partial z_n}
&
\frac{\partial^2 r}{\partial \bar{z}_n \partial \bar{z}_1}
& \cdots &
\frac{\partial^2 r}{\partial \bar{z}_n \partial \bar{z}_n}
\\
%%%%%
\frac{\partial^2 r}{\partial z_1 \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial z_1 \partial z_n}
&
\frac{\partial^2 r}{\partial z_1 \partial \bar{z}_1}
& \cdots &
\frac{\partial^2 r}{\partial z_1 \partial \bar{z}_n}
\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots
\\
\frac{\partial^2 r}{\partial z_n \partial z_1}
& \cdots &
\frac{\partial^2 r}{\partial z_n \partial z_n}
&
\frac{\partial^2 r}{\partial z_n \partial \bar{z}_1}
& \cdots &
\frac{\partial^2 r}{\partial z_n \partial \bar{z}_n}
\end{bmatrix}
.
\end{equation*}
To make it a Hermitian form, note that when multiplying on the left by $X_p$
we are also taking the conjugate so the rows for the $z$s and the $\bar{z}$s
are flipped.\footnote{It is common to also write it not flipped, in which case it
will be a symmetric martrix.}
Note that it is Hermitian only for a real-valued $r$ (see an exercise below).
The complex Hessian is the upper left,
or the transpose of the lower right, block---if you write the full Hessian as
$\Bigl[ \begin{smallmatrix} L & \widebar{Z} \\ Z & L^t \end{smallmatrix}
\Bigr]$, then $L$ is the complex Hessian.
Note that $L$ is a smaller matrix and we apply it only to a subspace
of the complexified tangent space.

We illustrate the change of basis in one dimension, and leave higher
dimensions to the student.  Let $z =
x+iy$ be in $\C$, and denote by $T$ the change of basis matrix:
\begin{equation*}
T =
\begin{bmatrix}
\nicefrac{1}{2} & \nicefrac{1}{2} \\
\nicefrac{-i}{2} & \nicefrac{i}{2}
\end{bmatrix}
,
\qquad
T^*
\begin{bmatrix}
\frac{\partial^2 r}{\partial x \partial x} &&
\frac{\partial^2 r}{\partial x \partial y}
\\
\frac{\partial^2 r}{\partial y \partial x} &&
\frac{\partial^2 r}{\partial y \partial y}
\end{bmatrix}
T
=
\begin{bmatrix}
\frac{\partial^2 r}{\partial \bar{z} \partial z} &&
\frac{\partial^2 r}{\partial \bar{z} \partial \bar{z}}
\\
\frac{\partial^2 r}{\partial z \partial z} &&
\frac{\partial^2 r}{\partial z \partial \bar{z}}
\end{bmatrix}
,
\end{equation*}
where
\glsadd{not:star}%
$T^* = \widebar{T}^t$ is the conjugate transpose.
By Sylvester's law of inertia from linear algebra, star-congruence
preserves the inertia (the number of positive, negative, and zero
eigenvalues).  So the
inertia of the full Hessian in terms of $x$s and $y$s is the same
as for the full Hessian in terms of $z$s and $\bar{z}$s.
The relationship between the eigenvalues of the full Hessian and the complex
Hessian is not as straightforward as may at first seem, but there is
a relationship there nonetheless.

\begin{exbox}
\begin{exercise}[Easy]
If $r$ is real-valued, then both the complex Hessian of $r$
and the full Hessian in terms of $z$s and $\bar{z}$
are Hermitian matrices.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Consider one dimension, $z = x+iy$, and the real Hessian
in terms of $x$ and $y$:
\begin{equation*}
H=\begin{bmatrix}
\frac{\partial^2 r}{\partial x\partial x} &&
\frac{\partial^2 r}{\partial x \partial y}
\\
\frac{\partial^2 r}{\partial y \partial x} &&
\frac{\partial^2 r}{\partial y\partial y}
\end{bmatrix} .
\end{equation*}
Prove that the complex Hessian $L$ (a number now) is $\nicefrac{1}{4}$
of the trace of $H$.
Thus, if $H$ is positive definite, then $L > 0$, and if $H$ is negative
definite, then $L < 0$.  Then show by example that
if $H$ has mixed eigenvalues (positive
and negative), then $L$ can be positive, negative, or zero.
\end{exercise}

\begin{exercise}
\pagebreak[2]
For every dimension,
find the change of variables $T^* H T$ to go from the real Hessian in terms
of $x$ and $y$ to the Hessian in terms of $z$ and $\bar{z}$.
Hint: If you figure it out for $n=2$, it
will be easy to do in general.
\end{exercise}

\begin{exercise}
Prove in every dimension that if the real Hessian (in terms of $x$ and $y$) is
positive (semi)definite, then the complex Hessian is positive (semi)definite.
Hint: A Hermitian matrix $L$ is positive definite if $v^*Lv > 0$ for all
nonzero vectors $v$ and semidefinite if $v^*Lv \geq 0$ for all $v$.
\end{exercise}
\end{exbox}

Let us also see how a complex linear change of variables
acts on the Hessian.  A complex linear mapping $A$ as
an $n \times n$ complex matrix
transforms the tangent space
in the basis of $\frac{\partial}{\partial z}$s and
$\frac{\partial}{\partial \bar{z}}$s
via the derivative $D_{\C} A$ written as a $2n \times 2n$ matrix.
A direct computation shows $D_{\C} A = A \oplus \widebar{A} =
\left[ \begin{smallmatrix} A & 0 \\ 0 & \widebar{A} \end{smallmatrix}
\right]$.
Write the full Hessian as
$\left[ \begin{smallmatrix} L & \widebar{Z} \\ Z & L^t \end{smallmatrix}
\right]$, where $L$ is the complex Hessian.  The complex linear change
of variables $A$ transforms
the full Hessian as
\begin{equation*}
{\begin{bmatrix} A & 0 \\ 0 & \widebar{A} \end{bmatrix}}^*
\begin{bmatrix} L & \widebar{Z} \\ Z & L^t \end{bmatrix}
\begin{bmatrix} A & 0 \\ 0 & \widebar{A} \end{bmatrix}
=
\begin{bmatrix} A^*LA & \overline{A^tZA} \\ A^tZA & {(A^*LA)}^t \end{bmatrix} ,
\end{equation*}
Again by Sylvester's law of inertia,
$L$ and $A^*LA$ have the same inertia, that is,
the number of positive, negative, and zero eigenvalues.

The Levi form itself does depend on the defining function, but the signs of
the eigenvalues do not.  It is common to say ``the
Levi form'' without mentioning a specific defining function
even though that is not completely correct.
The proof of the following proposition is left as an exercise.

\begin{prop} \label{prop:inertiainvariant}
If $U \subset \C^n$ is an open set with smooth boundary and $p \in \partial
U$, then the inertia of the Levi form at $p$
does not depend on the defining function.
Consequently, the notion of pseudoconvexity and strong pseudoconvexity is
independent of the defining function.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove \propref{prop:inertiainvariant}.
\end{exercise}

\begin{exercise}
Show that a convex domain with smooth boundary
is pseudoconvex, and show that (a bounded) strongly convex
domain with smooth boundary is strongly pseudoconvex.
\end{exercise}

\begin{exercise}
Show that if an open set with smooth boundary is strongly pseudoconvex at a point, it is strongly
pseudoconvex at all nearby points.
\end{exercise}
\end{exbox}

We are generally interested what happens under a holomorphic change of
coordinates, that is, a biholomorphic mapping.  And as far as pseudoconvexity
is concerned we are interested in local changes of coordinates as
pseudoconvexity is a local property.  Before proving
that pseudoconvexity is a biholomorphic invariant, let us note where the
Levi form appears in the graph coordinates from
\propref{prop:graphcoordinatesCn}, that is, when our boundary (the
hypersurface) is given near the origin by
\begin{equation*}
\Im w = \varphi(z,\bar{z},\Re w) ,
\end{equation*}
where $\varphi$ is $O(2)$.
Let $r(z,\bar{z},w,\bar{w}) = \varphi(z,\bar{z},\Re w) - \Im w$ be our
defining function.  The complex Hessian of $r$ (an $n \times n$ matrix) has the form
\begin{equation*}
\begin{bmatrix}
L & * \\
* & *
\end{bmatrix}
\qquad \text{where} \quad
L = \left[
\frac{\partial^2 \varphi}{\partial \bar{z}_k \partial z_{\ell}}\Big|_0
\right]_{k \ell} .
\end{equation*}
Note that $L$ is an $(n-1) \times (n-1)$ matrix.
The vectors in $T_0^{(1,0)} \partial U$ are the span
of $\left\{
\frac{\partial}{\partial z_1}\big|_0,
\ldots,
\frac{\partial}{\partial z_{n-1}}\big|_0 \right\}$.
That is, as an $n$-vector,
a vector in $T_0^{(1,0)} \partial U$ is represented by $(a,0) \in \C^n$ for
some
$a \in \C^{n-1}$.  The Levi form at the origin is then $a^* L a$,
in other words, it is given by the $(n-1) \times (n-1)$ matrix $L$.
If this matrix $L$ is positive
semidefinite, then $\partial U$ is pseudoconvex at $0$.

\begin{example}
Let us change variables to write the ball $\bB_n$ in different
local holomorphic coordinates where the Levi form is displayed nicely.
The sphere $\partial \bB_n$ is defined in the variables $Z = (Z_1,\ldots,Z_n) \in \C^n$
by $\snorm{Z} = 1$.
We change variables to $(z_1,\ldots,z_{n-1},w)$ where
\begin{equation*}
z_k = \frac{Z_k}{1-Z_n} \quad \text{ for all $k=1,\ldots,n-1$}, \qquad
w = i\frac{1+Z_n}{1-Z_n} .
\end{equation*}
This change of variables is a biholomorphic map from the set where $Z_n \not= 1$
to the set where $w\not= -i$ (exercise).  For us, it suffices
that the map is invertible near
$(0,\ldots,0,-1)$, which follows by computing the derivative.
Notice
that the last component is the inverse of the Cayley transform (which takes
the disc to the upper half-plane).

We claim that the mapping takes the unit sphere given by $\snorm{Z} = 1$
(without the point $(0,\ldots,0,1)$), to
the set defined by
\begin{equation*}
\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 ,
\end{equation*}
and that it takes $(0,\ldots,0,-1)$ to the origin (this part is trivial).
Let us check:
\begin{equation*}
\begin{split}
\sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 - \Im w
& =
\abs{\frac{Z_1}{1-Z_n}}^2
+ \cdots +
\abs{\frac{Z_{n-1}}{1-Z_n}}^2
-
\frac{
i\frac{1+Z_n}{1-Z_n} -
\overline{
i\frac{1+Z_n}{1-Z_n}}
}{2i}
\\
& =
\frac{\sabs{Z_1}^2}{\sabs{1-Z_n}^2}
+ \cdots +
\frac{\sabs{Z_{n-1}}^2}{\sabs{1-Z_n}^2}
-
\frac{1+Z_n}{2(1-Z_n)} -
\frac{1+\bar{Z}_n}{2(1-\bar{Z}_n)}
\\
& =
\frac{\sabs{Z_1}^2
+ \cdots +
\sabs{Z_{n-1}}^2
+
\sabs{Z_n}^2-1}{\sabs{1-Z_n}^2} .
\end{split}
\end{equation*}
Therefore, $\sabs{Z_1}^2 + \cdots + \sabs{Z_n}^2 = 1$ if and only if $\Im w =
\sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2$.
As the map takes the point $(0,\ldots,0,-1)$ to the origin, we can
think of the set given by
\begin{equation*}
\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2
\end{equation*}
as the sphere in local holomorphic coordinates at $(0,\ldots,0,-1)$ (by symmetry
of the sphere we could have done this at any point by rotation).
In the coordinates $(z,w)$, the ball (the inside of the sphere) is the set given by
\begin{equation*}
\Im w > \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2 .
\end{equation*}
In these coordinates, the Levi form is just the identity matrix at the
origin, and so the domain is strongly pseudoconvex at the origin.
We will prove that (strong) pseudoconvexity is a biholomorphic invariant,
and so the ball is strongly pseudoconvex.

Not the entire sphere gets transformed, the points where $Z_n=1$
get ``sent to infinity.''
The hypersurface $\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2$
is sometimes called the \emph{\myindex{Lewy hypersurface}}, and in the
literature some even say it \emph{is} the sphere\footnote{That is not, in
fact, completely incorrect.
If we think of the sphere in the complex projective space,
we are simply looking at the sphere in a different coordinate patch.}.
Pretending $z$ is just one real direction, see \figureref{fig:lewy-hyper}.
As an aside,
the hypersurface
$\Im w = \sabs{z_1}^2 + \cdots + \sabs{z_{n-1}}^2$ is also called the
\emph{\myindex{Heisenberg group}}.  The group in this case
is defined on the parameters $(z,\Re w)$ of this hypersurface with the
group law $(z,\Re w)(z',\Re w') =
(z+z',\Re w + \Re w' + 2 \Im z \cdot z')$.
\begin{myfig}
\subimport*{figures/}{lewy-hyper.eepic}
\caption{Lewy hypersurface.\label{fig:lewy-hyper}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Prove the assertion in the example about the mapping being biholomorphic
on the sets described above.
\end{exercise}
\end{exbox}

Let us see how the Hessian of $r$ changes under a biholomorphic change
of coordinates.  Let $f \colon V \to V'$ be a biholomorphic map
between two domains in $\C^n$, and let $r \colon V' \to \R$ be a smooth
function with nonvanishing derivative.  Let us compute the Hessian of
$r \circ f \colon V \to \R$.
We first compute what happens to the nonmixed derivatives.
As we have to apply chain rule twice, to keep better track of things,
we write where the derivatives are being evaluated inside the computation,
as they are, after all, functions.
For clarity, let $z$ be the coordinates in $V$
and $\zeta$ the coordinates in $V'$.
That is, $r$ is a function of $\zeta$ and $\bar{\zeta}$,
$f$ is a function of $z$, and $\bar{f}$ is a function of $\bar{z}$.
So $r \circ f$ is a function of $z$ and $\bar{z}$.
\begin{align*}
\frac{\partial^2 (r \circ f)}{\partial z_k \partial z_\ell}
%\bigg|_{(z,\bar{z})}
& =
\frac{\partial}{\partial z_k }
\sum_{m=1}^n
\biggl(
\frac{\partial r}{\partial \zeta_m}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial f_m}{\partial z_\ell} \bigg|_{z}
+
\frac{\partial r}{\partial \bar{\zeta}_m}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\cancelto{0}{\frac{\partial \bar{f}_m}{\partial z_\ell} \bigg|_{\bar{z}}}
\biggr)
\displaybreak[0]\\
& =
\sum_{m,\nu=1}^n
\biggl(
\frac{\partial^2 r}{\partial \zeta_\nu \partial \zeta_m}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial f_\nu}{\partial z_k} \bigg|_z
\frac{\partial f_m}{\partial z_\ell} \bigg|_z
+
\frac{\partial^2 r}{\partial \bar{\zeta}_\nu \partial \zeta_m}
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\smash{\cancelto{0}{\frac{\partial \bar{f}_\nu}{\partial z_k} \bigg|_{\bar{z}} }}
\frac{\partial f_m}{\partial z_\ell} \bigg|_z
\biggr)
\\
& \qquad +
\sum_{m=1}^n
\frac{\partial r}{\partial \zeta_m} \bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial^2 f_m}{\partial z_k \partial z_\ell} \bigg|_z
\displaybreak[0]\\
&=
\sum_{m,\nu=1}^n
\frac{\partial^2 r}{\partial \zeta_\nu \partial \zeta_m}
\frac{\partial f_\nu}{\partial z_k}
\frac{\partial f_m}{\partial z_\ell}
+
\sum_{m=1}^n
\frac{\partial r}{\partial \zeta_m}
\frac{\partial^2 f_m}{\partial z_k \partial z_\ell} .
\end{align*}
The matrix
$\left[ \frac{\partial^2 (r \circ f)}{\partial z_k \partial z_\ell} \right]$
can have different eigenvalues than the matrix
$\left[ \frac{\partial^2 r}{\partial \zeta_k \partial \zeta_\ell} \right]$.
If $r$ has nonvanishing gradient, then
using the second term, we can (locally) choose $f$ in such a way as to make
the matrix
$\left[ \frac{\partial^2 (r \circ f)}{\partial z_k \partial z_\ell} \right]$
be the zero matrix (or anything else) at a certain point as we can choose
the second derivatives of $f$ arbitrarily at that point.  See the exercise below.  Nothing about the matrix
$\left[ \frac{\partial^2 r}{\partial \zeta_k \partial \zeta_\ell} \right]$ is
preserved under a biholomorphic map.  And that is precisely why it does not
appear in the definition of pseudoconvexity.
The story for
$\left[ \frac{\partial^2 (r \circ f)}{\partial \bar{z}_k \partial
\bar{z}_\ell} \right]$
and
$\left[ \frac{\partial^2 r}{\partial \bar{\zeta}_k \partial \bar{\zeta}_\ell} \right]$ is
exactly the same.

\begin{exbox}
\begin{exercise}
Given a real function $r$ with nonvanishing gradient at $p \in \C^n$.  Find
a local change of coordinates $f$ at $p$ (so $f$ ought to be a holomorphic
mapping with an invertible derivative at $p$) such that
$\Bigl[ \frac{\partial^2 (r \circ f)}{\partial z_k \partial z_\ell} \Big|_p
\Bigr]$
and
$\Bigl[ \frac{\partial^2 (r \circ f)}{\partial \bar{z}_k \partial \bar{z}_\ell}
\Big|_p \Bigr]$
are just the zero matrices.
\end{exercise}
\end{exbox}

Let us look at the mixed derivatives:
\begin{equation*}
\begin{split}
\frac{\partial^2 (r \circ f)}{\partial \bar{z}_k \partial z_\ell}
%\bigg|_{(z,\bar{z})}
& =
\frac{\partial}{\partial \bar{z}_k }
\sum_{m=1}^n
\left(
\frac{\partial r}{\partial \zeta_m} \bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial f_m}{\partial z_\ell} \bigg|_z
\right)
\\
& =
\sum_{m,\nu=1}^n
\frac{\partial^2 r}{\partial \bar{\zeta}_\nu \partial \zeta_m }
\bigg|_{(f(z),\bar{f}(\bar{z}))}
\frac{\partial \bar{f}_\nu}{\partial \bar{z}_k} \bigg|_{\bar{z}}
\frac{\partial f_m}{\partial z_\ell} \bigg|_z
+
\sum_{m=1}^n
\frac{\partial r}{\partial \zeta_m} \bigg|_{(f(z),\bar{f}(\bar{z}))}
\smash{\cancelto{0}{\frac{\partial^2 f_m}{\partial \bar{z}_k \partial
z_\ell} \bigg|_z}}
\\
&=
\sum_{m,\nu=1}^n
\frac{\partial^2 r}{\partial \bar{\zeta}_\nu \partial \zeta_m}
\frac{\partial \bar{f}_\nu}{\partial \bar{z}_k}
\frac{\partial f_m}{\partial z_\ell} .
\end{split}
\end{equation*}
The complex Hessian of $r \circ f$ is the complex Hessian $L$ of $r$
conjugated as $D^*LD$, where $D$ is the holomorphic
derivative matrix of $f$ at $z$ and
$D^*$ is its conjugate transpose.  Sylvester's law of inertia
says that the number of positive, negative, and zero
eigenvalues of $D^*LD$ is the same as that for $L$.  The
eigenvalues may change, but their signs do not.
We are only considering $L$ and $D^*LD$ on a subspace.  In linear algebra
language, consider an invertible $D$, a subspace $T$, and its image $DT$.
Then the inertia of $L$ restricted to $DT$ is the same
as the inertia of $D^*LD$ restricted to $T$.

Let $M$ be a smooth real hypersurface given by $r=0$, then $f^{-1}(M)$ is
a smooth real hypersurface given by $r \circ f = 0$.
The holomorphic derivative $D = Df(p)$
takes
$T_{p}^{(1,0)}f^{-1}(M)$ isomorphically to $T_{f(p)}^{(1,0)}M$.
So $L$ is positive (semi)definite
on $T_{f(p)}^{(1,0)}M$ if and only if $D^*LD$ is positive (semi)definite
on $T_{p}^{(1,0)} f^{-1}(M)$.
We have almost proved the following theorem.  In short, pseudoconvexity is a
biholomorphic invariant.

\begin{thm}
\pagebreak[2]
Suppose $U, U' \subset \C^n$ are open sets with smooth boundary,
$p \in \partial U$, $V \subset \C^n$ a neighborhood of $p$,
$q \in \partial U'$, $V' \subset \C^n$ a neighborhood of $q$,
and $f \colon V \to V'$ a biholomorphic map with $f(p) = q$, such that
$f(U \cap V) = U' \cap V'$.
See \figureref{fig:bndry-bihol}.

Then the inertia of the Levi form of $U$ at $p$ is the same as the inertia of
the Levi form of $U'$ at $q$.
In particular, $U$ is pseudoconvex at $p$ if and only if $U'$ is pseudoconvex at $q$.
Similarly,
$U$ is strongly pseudoconvex at $p$ if and only if $U'$ is strongly pseudoconvex at $q$.
\end{thm}

\begin{myfig}
\subimport*{figures/}{bndry-bihol.pdf_t}
\caption{Local boundary biholomorphism.\label{fig:bndry-bihol}}
\end{myfig}

To finish proving the theorem, the only thing left is to observe that if
$f(U \cap V) = U' \cap V'$, then $f(\partial U \cap V) = \partial U' \cap
V'$, and to note that if $r$ is a defining function for $U'$ at $q$,
then $r \circ f$ is a defining function for $U$ at $p$.

\begin{exbox}
\begin{exercise}
Find an example of a bounded domain in $\C^n$, $n \geq 2$, with smooth boundary that is not convex,
but that is pseudoconvex.
\end{exercise}
\end{exbox}

So while the Levi form is not invariant under holomorphic changes of coordinates,
its inertia is.
Putting this together with the other observations we made,
we find the normal form for the
quadratic part of the defining equation for a smooth real hypersurface
under biholomorphic transformations.
It is possible to do better than the following lemma, but it is not always possible
to get rid of the dependence on $\Re w$ in the higher order terms.

\begin{lemma} \label{lemma:normformquad}
Let $M \subset \C^n$ be a smooth real hypersurface and $p \in M$.  Then there
exists a local biholomorphic change of coordinates taking $p$ to $0$
and $M$ to the hypersurface given by
\begin{equation*}
\Im w = \sum_{k=1}^\alpha \sabs{z_k}^2 - \sum_{k=\alpha+1}^{\alpha+\beta}
\sabs{z_k}^2 +
E(z,\bar{z},\Re w) ,
\avoidbreak
\end{equation*}
where $E$ is $O(3)$ at the origin.
If $M$ is a boundary, then the coordinates are chosen so that
the domain is given by replacing $=$ with $>$ as usual and where
$\alpha$ is the number of positive eigenvalues of the Levi form at $p$,
$\beta$ is the number of negative eigenvalues, and $\alpha+\beta \leq n-1$.
\end{lemma}

Recall that $O(\ell)$ at the origin means
a function that together with its derivatives up to and including
order $\ell-1$ vanish at the origin.

\begin{proof}
Change coordinates so that $M$ is given by
$\Im w = \varphi(z,\bar{z},\Re w)$,  where $\varphi$ is $O(2)$.
Apply Taylor's theorem to $\varphi$ up to the second order:
\begin{equation*}
\varphi(z,\bar{z},\Re w) = q(z,\bar{z}) + (\Re w) (Lz + \overline{Lz}) +
a {(\Re w)}^2 +
O(3) ,
\end{equation*}
where $q$ is quadratic, $L \colon \C^{n-1} \to \C$ is linear, and $a \in \R$.
If $L \not= 0$,
do a linear change of coordinates in the $z$ only to make $Lz = z_1$.
So assume $Lz = \epsilon z_1$ where $\epsilon = 0$ or $\epsilon = 1$.

Change coordinates
by leaving $z$ unchanged and letting $w = w'+bw'^2+cw'z_1$.  Ignore
$q(z,\bar{z})$ for a moment, as
this change of coordinates does not affect it.  Also, only work up to
second order.
\begin{equation*}
\begin{split}
-\Im w +
& \epsilon (\Re w) (z_1+\bar{z}_1)
+
a {(\Re w)}^2
\\
& =
-\frac{w-\bar{w}}{2i} +
\epsilon \frac{w+\bar{w}}{2}(z_1+\bar{z}_1)
+
a{\left(\frac{w+\bar{w}}{2}\right)}^2
\\
& =
-\frac{w'+bw'^2+cw'z_1-\bar{w}'-\bar{b}\bar{w}'^2-\bar{c}\bar{w}'\bar{z}_1}{2i}
\\
& \phantom{=}~
+\epsilon \frac{w'+bw'^2+cw'z_1+\bar{w}'+\bar{b}\bar{w}'^2+\bar{c}\bar{w}'\bar{z}_1}{2}(z_1+\bar{z}_1)
\\
& \phantom{=}~
+ a \frac{{(w'+bw'^2+cw'z_1+\bar{w}'+\bar{b}\bar{w}'^2+\bar{c}\bar{w}'\bar{z}_1)}^2}{4}
%\\
%& =
%-\frac{w'-\bar{w}'}{2i}
%+\frac{(i-c)w'z_1+(i+\bar{c})\bar{w}'\bar{z}_1
%+iw'\bar{z}_1+i\bar{w}'z_1}{2i}
%+O(3)
\\
& =
-\frac{w'-\bar{w}'}{2i}
+\frac{
\bigl((\epsilon i-c)w'
+\epsilon i\bar{w}'\bigr)z_1
+\bigl((\epsilon i+\bar{c})\bar{w}'
+\epsilon iw'\bigr)\bar{z}_1
}{2i}
\\
& \phantom{=}~
+ \frac{(ia-2b)w'^2+(ia+2\bar{b})\bar{w}'^2+2iaw'\bar{w}'}{4i}
+O(3) .
\end{split}
\end{equation*}
We cannot quite get rid of all the quadratic terms in $\varphi$, but we
choose $b$ and $c$ to make the second order terms not depend on $\Re w'$.
Set $b=ia$ and $c=2i\epsilon$, and add $q(z,\bar{z}) + O(3)$ into the mix
to get
\begin{equation*}
\begin{split}
-\Im w + \varphi(z,\bar{z}, & \Re w)  =
-\Im w +
q(z,\bar{z}) +
\epsilon (\Re w) (z_1+\bar{z}_1)
+ a{(\Re w)}^2
+O(3)
\\
%& =
%-\frac{w'-\bar{w}'}{2i}
%+
%q(z,\bar{z})
%+\frac{
%\bigl(-iw'
%+i\bar{w}'\bigr)z_1
%+\bigl(-i\bar{w}'
%+iw'\bigr)\bar{z}_1
%}{2i}
%+O(3)
%\\
& =
-\frac{w'-\bar{w}'}{2i}
+
q(z,\bar{z})
%\\
%& \phantom{=}~
- \epsilon i
\frac{w' -\bar{w}'}{2i}
( z_1 -\bar{z}_1)
+ a {\left(\frac{w'-\bar{w}'}{2i}\right)}^2
+O(3)
\\
& =
-\Im w'
+ q(z,\bar{z})
-
\epsilon i
(\Im w')
( z_1 -\bar{z}_1)
+
a {(\Im w')}^2
+O(3) .
\end{split}
\end{equation*}
The right-hand side is the defining function in the $(z,w')$ coordinates.
As $M$ is no longer written as a graph of $\Im w'$ over the
rest, apply the implicit
function theorem to solve for $\Im w'$
and write the hypersurface as a graph again.
The expression for $\Im w'$
is $O(2)$, and so $-i\epsilon (\Im w')(z_1-\bar{z}_1)+a{(\Im w')}^2$
is $O(3)$.
If we write $M$ as a graph,
\begin{equation*}
\Im w' = q(z,\bar{z}) + E(z,\bar{z},\Re w'),
\end{equation*}
then $E$ is $O(3)$.

Write the quadratic polynomial $q$ as
\begin{equation} \label{eq:generalquadraticq}
q(z,\bar{z}) =
\sum_{k,\ell=1}^{n-1}
a_{k\ell} z_kz_\ell
+
b_{k\ell} \bar{z}_k\bar{z}_\ell
+
c_{k\ell} \bar{z}_kz_\ell .
\end{equation}
The $a_{k\ell}$ and $b_{k\ell}$ are not uniquely determined, but
we can pick the matrices $[a_{k\ell}]$ and $[b_{k\ell}]$ to be symmetric
to make them uniquely determined.
As $q$ is real-valued, it is left as an exercise to show that
$a_{k\ell} = \overline{b_{k\ell}}$ and
$c_{k\ell} = \overline{c_{\ell k}}$.  That is, the
matrix $[b_{k\ell}]$ is the complex conjugate of $[a_{k\ell}]$ and
$[c_{k\ell}]$ is Hermitian.

We make another change of coordinates.  Fix the $z$s again, and set
\begin{equation} \label{eq:wprimeexpression}
w' = w'' + i
\sum_{k,\ell=1}^{n-1}
a_{k\ell} z_kz_\ell .
\end{equation}
In particular,
\begin{equation*}
\Im w'
= \Im w''
+ \Im \biggl(
i
\sum_{k,\ell=1}^{n-1}
a_{k\ell} z_kz_\ell
\biggr)
%=
%\Im w''
%-
%\sum_{k,\ell=1}^{n-1}
%\bigl(
%a_{k\ell} z_kz_\ell
%+
%\overline{a_{k\ell}} \bar{z}_k\bar{z}_\ell
%\bigr)
=
\Im w''
+
\sum_{k,\ell=1}^{n-1}
\bigl(
a_{k\ell} z_kz_\ell
+
b_{k\ell} \bar{z}_k\bar{z}_\ell
\bigr) ,
\end{equation*}
as $\overline{a_{k\ell}} = b_{k\ell}$.  Plugging
\eqref{eq:wprimeexpression} into $\Im w' = q(z,\bar{z}) + E(z,\bar{z},\Re w')$
and solving for $\Im w''$ cancels the
holomorphic and antiholomorphic terms in $q$, and leaves $E$ as $O(3)$.
After this change of coordinates we may assume
that $q$ is a Hermitian form,
\begin{equation*}
q(z,\bar{z}) = \sum_{k,\ell=1}^{n-1} c_{k\ell} z_k \bar{z}_\ell .
\end{equation*}
As $q$ is real-valued, the matrix
$C = [ c_{k\ell} ]$ is Hermitian.  In linear algebra notation,
$q(z,\bar{z}) = z^*Cz$,
where we think of $z$ as a column vector.
If $T$ is a linear transformation on the $z$ variables, say $z'=Tz$, we
obtain ${z'}^*Cz' = {(Tz)}^*CTz = z^* ( T^*CT) z$.  Thus, we normalize $C$
up to $*$-congruence.  A Hermitian matrix
is $*$-congruent to a diagonal matrix with only $1$s, $-1$s, and $0$s on the
diagonal, again by Sylvester's law of inertia.
Writing out what that means is precisely the conclusion of the
proposition.
If $M$ is a boundary, we make sure the interior of the domain is
given by $\Im w > \varphi(z,\bar{z},\Re w)$
by possibly replacing $w$ with $-w$, which reverses the signs of the 
eigenvalues.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the assertions in the proof.  First, that if $q$ is a quadratic as in
\eqref{eq:generalquadraticq}, then the matrices
$[a_{k\ell}]$ and
$[b_{k\ell}]$ can be chosen to be symmetric, in which case
all the coefficients are uniquely determined.
Second, that if $q$ is real valued, then
 $a_{k\ell} = \overline{b_{k\ell}}$ and $c_{k\ell} = \overline{c_{\ell k}}$
for all $k$ and $\ell$.
\end{exercise}
\end{exbox}

\begin{lemma}[Narasimhan's lemma\index{Narasimhan's lemma}%
\footnote{A statement essentially of Narasimhan's lemma was already used by Helmut
Knesser in 1936.}]
Let $U \subset \C^n$ be an open set with smooth boundary
that is strongly pseudoconvex at $p \in \partial U$.  Then there exists a local biholomorphic change of
coordinates fixing $p$ such that in these new coordinates, $U$ is strongly
convex at $p$ and hence strongly convex at all points near $p$.
\end{lemma}

\begin{exbox}
\begin{exercise}
Prove Narasimhan's lemma.  Hint: See the proof of \lemmaref{lemma:normformquad}.
\end{exercise}

\begin{exercise}
Prove that an open $U \subset \C^n$ with smooth boundary is pseudoconvex at
$p$ if and only if there exist local holomorphic coordinates at $p$ such that
$U$ is convex at $p$.
\end{exercise}
\end{exbox}

To make use of convexity, the domain needs to be convex at all points
(all points near $p$), so
Narasimhan's lemma only works at points of strong
pseudoconvexity.
For weakly pseudoconvex points the situation is far more complicated.
While it is possible to use
weak pseudoconvexity at $p$ to make the domain convex at $p$, the same
change of variables does not necessarily make the domain convex at nearby
points.  In particular, it is not always possible for a domain that is
weakly pseudoconvex at all points to be made convex in a neighborhood.
What makes the lemma work is that if
$U$ is strongly (pseudo)convex at $p$, it will also be so
at nearby points.

\medskip


Let us prove the easy direction of the famous
\emph{\myindex{Levi problem}}.  The Levi problem was a long-standing
problem%
\footnote{E.\ E.\ Levi stated the problem in 1911, but it was not fully
solved until the 1950s, by Oka and others.}
in several complex variables to classify domains of holomorphy in
$\C^n$.  The answer is that a domain is a domain of holomorphy if and only
if it is pseudoconvex.  Just as the problem of trying to show that
the classical geometric convexity is the same as convexity as we have
defined it,
the Levi problem has an easier direction and a harder direction.
The easier direction is to show that a domain of holomorphy is pseudoconvex, and
the harder direction is to show that a pseudoconvex domain is a domain of
holomorphy.  See H\"ormander's book~\cite{Hormander} for the proof
of the hard direction.

\begin{thm}[Tomato can principle\index{tomato can principle}] \label{thm:tomatocan}
Suppose
$U \subset \C^n$ is an open set with smooth boundary and
the Levi form has a negative eigenvalue at $p \in
\partial U$.
Then every holomorphic function on $U$
extends to a neighborhood of $p$.
In particular, $U$ is not
a domain of holomorphy.
\end{thm}

Pseudoconvex at $p$ means that all eigenvalues of the Levi form are
nonnegative.
The theorem says that a domain of holomorphy must be pseudoconvex.
The theorem's name comes from the proof, and sometimes other theorems using a
similar proof of a ``tomato can'' of analytic discs are called
tomato can principles.
The general statement of proof of the principle is that ``an
analytic function holomorphic in a neighborhood of the sides and the bottom
of a tomato can extends to the inside.''  And the theorem we gave as
the principle states that ``if the Levi form at $p$ has a negative
eigenvalue, we can fit a tomato can from inside the domain over $p$.''

\begin{proof}
We change variables so that $p = 0$, and
near $p$, $U$ is given by
\begin{equation*}
\Im w > -\sabs{z_1}^2 + \sum_{k=2}^{n-1} \epsilon_k \sabs{z_k}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) ,
\end{equation*}
where $z' = (z_2,\ldots,z_{n-1})$, $\epsilon_k = -1,0,1$, and $E$ is $O(3)$.
We embed an analytic disc via the map
$\xi \in \widebar{\D} \overset{\varphi}{\mapsto} (\lambda \xi, 0, 0, \ldots, 0)$
for some small $\lambda > 0$.
Clearly $\varphi(0) = 0 \in \partial U$.  For $\xi \not= 0$ near the origin
\begin{equation*}
-\lambda^2 \sabs{\xi}^2 + \sum_{k=2}^{n-1} \epsilon_k \sabs{0}^2 + E(\lambda
\xi,0,\lambda \bar{\xi},0,0)
=
-\lambda^2 \sabs{\xi}^2 + E(\lambda
\xi,0,\lambda \bar{\xi},0,0)
%=
%-\sabs{\xi}^2 + E(\xi,0,\bar{\xi},0,0)
< 0 ,
\end{equation*}
because
the function above has a strict maximum at $\xi = 0$
by the second derivative test.
Therefore, for $\xi \not= 0$ near the origin,
$\varphi(\xi) \in U$.  By picking $\lambda$ small enough,
$\varphi(\overline{\D}\setminus\{0\}) \subset U$.

We can ``wiggle the disc a little'' and
find discs entirely in $U$.  In particular, for all small enough $s > 0$,
the closed disc given by
\begin{equation*}
\xi \in \widebar{\D} \overset{\varphi_s}{\mapsto} (\lambda \xi, 0, 0, \ldots, 0, i s)
\end{equation*}
(that is, for slightly positive $\Im w$)
is entirely inside $U$.
Fix such a small $s > 0$.
Suppose $\epsilon > 0$ is small and $\epsilon < s$.
Define the Hartogs figure
\begin{equation*}
\begin{split}
H =
& \bigl\{ (z,w) : \lambda - \epsilon < \sabs{z_1} < \lambda + \epsilon,
\sabs{z_k} < \epsilon \text{ for } k=2,\ldots,n-1, \text{ and }
\sabs{w-is} < s+\epsilon \bigr\}
\\
&
\cup
\bigl\{ (z,w) : \sabs{z_1} < \lambda + \epsilon,
\sabs{z_k} < \epsilon \text{ for } k=2,\ldots,n-1, \text{ and }
\sabs{w-is} < \epsilon \bigr\} .
\end{split}
\end{equation*}
The set where $\sabs{z_1} = \lambda$, $z' = 0$,
and $\sabs{w-is} \leq s$ is inside $U$, so
an $\epsilon$-neighborhood of that is in $U$.
For
$w = is$ the whole disc where $\sabs{z_1} \leq \lambda$ and $z'=0$ is in $U$,
so an $\epsilon$-neighborhood of that is in $U$.
Thus, for small enough $\epsilon >0$, $H \subset U$.
We are really just taking a Hartogs figure in the $z_1,w$ variables, and then
``fattening it up'' to the $z'$ variables.
In \figureref{fig:hartogs-figure-tomato}, we picture the Hartogs figure in the $\sabs{z_1}$ and $\sabs{w-is}$
variables.  The boundary $\partial U$ and $U$ are only pictured diagrammatically.
Also, we make a ``picture'' the analytic discs giving the ``tomato can.''
In the picture, the $U$ is below its boundary $\partial U$, unlike usually.

\begin{myfig}
\subimport*{figures/}{hartogs-figure-tomato.pdf_t}
\caption{Tomato can principle.\label{fig:hartogs-figure-tomato}}
\end{myfig}

The origin is in the hull of $H$, and so
every function holomorphic in $U$, and so in $H$, extends through the origin.
Hence $U$ is not a domain of holomorphy.
\end{proof}

Another, perhaps a little less concrete, way to finish the proof that does not use a Hartogs figure
is to apply the first version of Kontinuit\"atssatz
(\thmref{thm:contprinciple1}) with the sequence of discs $\{ \varphi_{1/k} \}$.

\begin{exbox}
\begin{exercise}
\pagebreak[2]
For the following domains in $U \subset \C^2$,
find all the
points in $\partial U$ where $U$ is weakly pseudoconvex, all the points
where it is strongly pseudoconvex, and all the points where it is
not pseudoconvex.  Is $U$ pseudoconvex?
\begin{exparts}
\item
$\Im w > \sabs{z}^4$
\item
$\Im w > \sabs{z}^2(\Re w)$
\item
$\Im w > (\Re z)(\Re w)$
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $U \subset \C^n$ be an open set with smooth boundary that is
strongly pseudoconvex at $p \in \partial U$.  Show that
$p$ is a so-called \emph{\myindex{peak point}}: There
exists a neighborhood $W$ of $p$ and a holomorphic
$f \colon W \to \C$ such that $f(p)=1$ and $\sabs{f(z)} < 1$ for all
$z \in W \cap \widebar{U} \setminus \{ p \}$.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set with smooth boundary.  Suppose
for $p \in \partial U$, there is a neighborhood $W$ of $p$
and a holomorphic $f \colon W \to
\C$ such that $df(p) \not= 0$, $f(p) = 0$, but
$f$ is never zero on $W \cap U$.  Show that $U$ is pseudoconvex
at $p$.  Hint: You may need the holomorphic implicit function theorem
(\thmref{thm:ift}).
Note: The result does not require the $df$ to not vanish, but it is
harder to prove without that hypothesis.
\end{exercise}
\end{exbox}

A hyperplane is the ``degenerate'' case of normal convexity,
that is, a hyperplane is convex from both sides.
There is also a flat case of pseudoconvexity.  A smooth real hypersurface
$M \subset \C^n$ is \emph{\myindex{Levi-flat}} if the Levi form
vanishes at every point of $M$.  The zero matrix is positive semidefinite
and negative semidefinite, so both sides of $M$ are pseudoconvex.
Conversely, the only hypersurface pseudoconvex from both sides is a
Levi-flat one.

\begin{exbox}
\begin{exercise}
Suppose $U = V \times \C^{n-1} \subset \C^n$, where $V \subset \C$ is an
open set with smooth boundary.  Show that $U$ is has a smooth Levi-flat boundary.
\end{exercise}

\begin{exercise}
Prove that a real hyperplane is Levi-flat.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be open, $f \in \sO(U)$, and
$M = \bigl\{ z \in U : \Im f(z) = 0 \bigr\}$.  Show that
if $df(p) \not=0$ for some $p \in M$, then near $p$,
$M$ is a Levi-flat hypersurface.
\end{exercise}


\begin{exercise}
Suppose $M \subset \C^n$ is a smooth Levi-flat hypersurface,
$p \in M$, and
a complex line $L$ is tangent to $M$ at $p$.
Prove that $p$ is not an isolated point of $L \cap M$.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is an open set with smooth boundary
and $\partial U$ is Levi-flat.
Show that $U$ is unbounded.
Hint: If $U$ were bounded, consider the point on $\partial U$ farthest from
the origin.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{From harmonic to plurisubharmonic functions}
\label{sec:harmonic}

We start with a quick review of harmonic and subharmonic functions in $\C$.
For a more detailed treatment, see a one-variable book such as~\cite{Lebl:ca}.

\begin{defn}
Let $U \subset \R^n$ be open.
A $C^2$-smooth
$f \colon U \to \R$ is
\emph{\myindex{harmonic}} if\footnote{The operator $\nabla^2$,
sometimes also written $\Delta$,
is the \emph{\myindex{Laplacian}}.
It is the trace of the Hessian matrix.}
\begin{equation*}
\nabla^2 f =
\frac{\partial^2 f}{\partial x_1^2} +
\cdots +
\frac{\partial^2 f}{\partial x_n^2} = 0
\quad \text{ on $U$.}
\end{equation*}

A function $f \colon U \to \R \cup \{ -\infty \}$ is
\emph{\myindex{subharmonic}} if it is upper-semicontinuous%
\footnote{Recall $f$ is \emph{\myindex{upper-semicontinuous}}
if $\limsup_{t\to x} f(t) \leq f(x)$ for all $x \in U$.}
and for every
ball $B_r(a)$ with $\overline{B_r(a)} \subset U$,
and every function $g$
continuous on $\overline{B_r(a)}$ and
harmonic on $B_r(a)$
such that $f(x) \leq g(x)$ for $x \in
\partial B_r(a)$, we have
\begin{equation*}
f(x) \leq g(x), \quad \text{ for all } x \in
B_r(a) .
\end{equation*}
\end{defn}

In other words, a subharmonic function is a function that is less than every
harmonic function on every ball whenever that holds on the boundary.
When $n=1$ in the definition of a subharmonic function,
it is the same as the standard definition of a
convex function of one real variable, where affine linear functions play the role of harmonic
functions: A function of one real variable is
\emph{convex}\index{convex function} if
for every interval it is less than the affine linear function with the same
end points.
A function of one real variable is harmonic if the second
derivative vanishes, and it is therefore affine linear.
In one real dimension it is also easier to picture.
The function $f$ is convex if on
every interval $[\alpha,\beta]$, $f \leq g$ for every affine linear $g$
bigger than $f$ at the endpoints $\alpha$ and $\beta$.  In particular, we can
take the $g$ that is equal to $f$ at the endpoints.  See
\figureref{fig:convexfunc}.
The picture is analogous for subharmonic functions for $n > 1$,
but it is harder to draw.

\begin{myfig}
\subimport*{figures/}{convexfunc.pdf_t}
\caption{Convex function.\label{fig:convexfunc}}
\end{myfig}

We will consider harmonic and subharmonic functions in $\C \cong
\R^2$.
Let us go through some basic results on harmonic and subharmonic
functions in $\C$ that you have probably seen in detail in your one-variable class.
Consequently, we leave some of these results as exercises.
In this section (and not just here)
we often write $f(z)$ for a function
even if it is not holomorphic.

\begin{exbox}
\begin{exercise}
An upper-semicontinuous function achieves a maximum on compact sets.
You may assume the function to be extended-real-valued.
\end{exercise}

\begin{exercise} \label{exercise:harmonicrealimag}
Let $U \subset \C$ be open.
Show that for a $C^2$ function $f \colon U \to \R$,
\begin{equation*}
\frac{\partial^2}{\partial \bar{z}\partial z} f = \frac{1}{4} \nabla^2 f .
\end{equation*}
Use this to show that $f$ is harmonic if and only if it is
(locally) the real or imaginary part
of a holomorphic function.
Hint: The key is to find an antiderivative of a holomorphic function.
\end{exercise}

\begin{exercise}
Prove the identity theorem.  Let $U \subset \C$ be a domain
and $f \colon U \to \R$ harmonic such that $f=0$ on a nonempty open subset
of $U$.  Then $f\equiv 0$.
\end{exercise}
\end{exbox}

Via \exerciseref{exercise:harmonicrealimag},
harmonic functions are locally real parts of holomorphic functions,
and hence they are infinitely differentiable.
In fact,
on a simply connected domain in $\C$, any
harmonic function is the real part of a holomorphic function.

It is useful to find a harmonic function given boundary values.
This problem is called the \emph{\myindex{Dirichlet problem}}, and it is
solvable for many (though not all) domains.
The proof of the following special case is contained in the exercises
following the theorem.
The
\emph{\myindex{Poisson kernel}} for the unit disc $\D \subset \C$ is
\glsadd{not:Pr}%
\begin{equation*}
P_r(\theta)
= \frac{1}{2\pi} \frac{1-r^2}{1+r^2-2r \cos \theta}
= \frac{1}{2\pi}
\operatorname{Re} \left( \frac{1+re^{i\theta}}{1-re^{i\theta}}\right) ,
\qquad \text{for $0 \leq r < 1$.}
\end{equation*}

\begin{thm} \label{thm:dirichsol}
Let $u \colon \partial \D \to \R$ be a continuous function.
Define
$Pu \colon \overline{\D} \to \R$ by
\begin{equation*}
Pu(re^{i\theta})
=
\int_{-\pi}^\pi u(e^{it}) P_r(\theta-t) \, dt
\quad \text{if $r < 1$} \qquad \text{and} \qquad
Pu(e^{i\theta}) = u(e^{i\theta}).
\avoidbreak
\end{equation*}
Then $Pu$ is harmonic in $\D$ and continuous on $\overline{\D}$.
\end{thm}

In the proof, it is useful to consider how
the graph of $P_r$ as a function of $\theta$ looks for a fixed $r$.
That is, $P_r$ acts like an approximate identity;
integrating against $P_r(\theta-t)$ gives
a weighted average of $u$ with the values near $e^{i\theta}$ getting
weighted more and more as $r \to 1$.
See \figureref{fig:poisson-kernel}.

\begin{myfig}
\includegraphics[width=0.5\textwidth]{figures/poisson-kernel.pdf}
\caption{Graph of $P_r$ for $r=0.5$,
$r=0.7$, and $r=0.85$ on $[-\pi,\pi]$.\label{fig:poisson-kernel}}
\end{myfig}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Prove $P_r(\theta) > 0$ for all $0 \leq r < 1$ and all $\theta$.
\item
Prove $\int_{-\pi}^{\pi} P_r(\theta) \, d\theta = 1$.
\item
Prove for any given $\delta > 0$,
$\sup \{P_r(\theta) : \delta \leq \abs{\theta} \leq \pi \} \to 0$ as
$r \to 1$.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[1]%
Prove \thmref{thm:dirichsol} using the following guideline:
\begin{exparts}
\item
Poisson kernel is harmonic
as a function of $z=re^{i\theta} \in \D$, and hence
$Pu$ is harmonic.
\item
$P$ acts like an
approximate identity: Prove that
$Pu(re^{i\theta}) \to u(e^{i\theta})$ uniformly as
$r \to 1$.  Hint: Split the integral to $[-\delta,\delta]$ and the rest
and use the previous exercise.
\item
Prove that $Pu(z)$ tends to $u(z_0)$ as
$z \in \D \to z_0 \in \partial \D$.
\end{exparts}
\end{exercise}

\begin{exercise}
\pagebreak[2]%
State and prove a version of \thmref{thm:dirichsol} for an arbitrary disc
$\Delta_r(a)$.
\end{exercise}

\begin{exercise}
Prove that the Dirichlet problem is not solvable in the punctured disc $\D
\setminus \{ 0 \}$.
Hint: Let $u = 0$ on $\partial \D$ and $u(0)=1$.
The solution would be
less than $- \epsilon \log \sabs{z}$ for every $\epsilon > 0$.
\end{exercise}
\end{exbox}

The Poisson kernel is a reproducing kernel for
holomorphic (and antiholomorphic) functions, as (the real and imaginary parts of) holomorphic functions are harmonic.
Poisson kernel exists for higher dimensions as well.
Solving the Dirichlet problem using the Poisson kernel leads to
the following result.

\begin{prop}[Mean-value property and sub-mean-value property]
\pagebreak[2]%
\label{prop:meansubmeanprop}%
\index{mean-value property}%
\index{sub-mean-value property}%
Let $U \subset \C$ be an open set.
\begin{enumerate}[(i)]
\item
\label{prop:meansubmeanprop:i}%
A continuous function
$f \colon U \to \R$
is harmonic if and only if
\begin{equation*}
f(a) = \frac{1}{2\pi} \int_0^{2\pi} f(a+re^{i\theta})\, d\theta
\qquad \text{whenever} \quad
\overline{\Delta_r(a)} \subset U .
\end{equation*}
\item
\label{prop:meansubmeanprop:ii}%
An upper-semicontinuous function $f \colon U \to \R \cup \{ -\infty \}$
is subharmonic if and only if
\begin{equation*}
f(a) \leq \frac{1}{2\pi} \int_0^{2\pi} f(a+re^{i\theta})\, d\theta
\qquad \text{whenever} \quad
\overline{\Delta_r(a)} \subset U .
\end{equation*}
\end{enumerate}
\end{prop}

For the sub-mean-value property you may have to use
the Lebesgue integral to integrate an upper-semicontinuous function,
and to use the version of the Poisson integral above, you need to
approximate by continuous functions on the boundary in the right way.
On first reading, feel free to think of continuous subharmonic
functions and not too much will be lost.

\begin{exbox}
\begin{exercise}
Fill in the details of the proof of \propref{prop:meansubmeanprop}.
Hint 1: \ref{prop:meansubmeanprop:i} follows from
\ref{prop:meansubmeanprop:ii} if you can solve
the Dirichlet problem in a disc.
Hint 2: Prove the reverse direction of \ref{prop:meansubmeanprop:ii}
by contrapositive.
\end{exercise}

\begin{exercise} \label{exercise:limsupsubharmonic}
Suppose $U \subset \C$ is open and
$f \colon U \to \R \cup\{- \infty \}$ is subharmonic.  Prove
\begin{equation*}
\limsup_{w \to z} f(w) = f(z)
\qquad \text{for all $z \in U$.}
\end{equation*}
\end{exercise}

\begin{exercise} \label{exercise:fminusgsubharmonic}
Suppose $U \subset \C$ is open and $g \colon U \to \R$ is harmonic.
Then $f \colon U \to \R \cup \{ -\infty \}$ is subharmonic if and only if $f-g$
is subharmonic.
\end{exercise}
\end{exbox}

\begin{prop}[Maximum principle]
\index{maximum principle!subharmonic functions}
Suppose $U \subset \C$ is a domain and $f \colon U \to \R \cup \{ -\infty \}$
is subharmonic.  If $f$ attains a maximum in $U$, then $f$ is constant.
\end{prop}

\begin{proof}
Suppose $f$ attains a maximum at $a \in U$.
If
$\overline{\Delta_r(a)} \subset U$, then
\begin{equation*}
f(a) \leq \frac{1}{2\pi} \int_0^{2\pi} f(a+re^{i\theta})\, d\theta \leq f(a)
.
\end{equation*}
Hence, $f = f(a)$ almost everywhere on $\partial \Delta_r(a)$.
By upper-semicontinuity, $f = f(a)$ everywhere on $\partial \Delta_r(a)$.
This was true for all $r$
with $\overline{\Delta_r(a)} \subset U$, so $f=f(a)$ on $\Delta_r(a)$,
and so the set where $f=f(a)$ is open.  The set where an upper-semicontinuous
function attains a maximum is closed.  So $f=f(a)$ on $U$ as $U$ is
connected.
\end{proof}

A very useful fact we will use over and over without mentioning much is that subharmonicity
(like harmonicity)
is a local property,
even if it does not seems so from the definition.
The proof is left as an exercise.

\begin{prop} \label{prop:shlocal}
Given an open
$U \subset \C$, an upper-semicontinuous $f \colon U \to \R \cup \{ -\infty \}$
is subharmonic if
for every $p \in U$ there is an $R_p > 0$ where $\Delta_{R_p}(p) \subset U$ 
and such that the estimate in
\propref{prop:meansubmeanprop} part \ref{prop:meansubmeanprop:ii}
holds for all $r$ with $0 < r < R_p$.

In particular,
a function $f \colon U \to \R \cup \{ -\infty \}$ is subharmonic if
and only if for every $p \in U$ there exists a neighborhood $W$ of $p$,
$W \subset U$, such that $f|_{W}$ is subharmonic.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove \propref{prop:shlocal}.
Hint: Analyze your proof of \propref{prop:meansubmeanprop}.
%Hint: Perhaps try to use
%the maximum principle and \exerciseref{exercise:fminusgsubharmonic}.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C$ is a bounded open set, $f \colon \widebar{U} \to \R
\cup \{-\infty\}$ is upper-semicontinuous such that $f|_U$
is subharmonic, $g \colon \widebar{U} \to \R$ is continuous
such that $g|_U$ is harmonic and
$f(z) \leq g(z)$ for all $z \in \partial U$.  Prove that
$f(z) \leq g(z)$ for all $z \in U$.
\end{exercise}

\begin{exercise} \label{exercise:onlyniceuneededforsubharmonic}
Let $g$ be a function
harmonic on a disc $\Delta \subset \C$ and continuous on
$\overline{\Delta}$.  Prove that for every $\epsilon > 0$ there exists
a function $g_\epsilon$, harmonic in a neighborhood of $\overline{\Delta}$,
such that $g(z) \leq g_\epsilon(z) \leq g(z)+\epsilon$ for all $z \in
\overline{\Delta}$.
In particular, to test subharmonicity, we only need to consider those
$g$ that are harmonic a bit past the boundary of the disc.
\end{exercise}
\end{exbox}

\begin{prop}
Suppose $U \subset \C$ is an open set and $f \colon U \to \R$ is a $C^2$ function.
The function $f$ is subharmonic if and only if
$\nabla^2 f \geq 0$.
\end{prop}

In analogy to convex functions, a $C^2$-smooth function $f$ of one
real variable is convex if and only if $f''(x) \geq 0$ for all $x$.

\begin{proof}
Suppose $f$ is a $C^2$-smooth function on a subset of $\C \cong \R^2$
with $\nabla^2 f \geq 0$.  We wish to show that $f$ is subharmonic.
Take a disc $\Delta$ such that $\overline{\Delta} \subset U$.
Consider a function
$g$ continuous on $\overline{\Delta}$,
harmonic on $\Delta$, and such that
$f \leq g$ on the boundary $\partial \Delta$.  Because
$\nabla^2 (f-g) = \nabla^2 f \geq 0$, we assume $g = 0$ and $f \leq 0$
on the boundary $\partial \Delta$.
We need to show that $f \leq 0$ on $\Delta$.

Suppose $\nabla^2 f > 0$ at all points on $\Delta$.
The Laplacian $\nabla^2 f$ is the trace of the Hessian matrix,
that is, the sum of the eigenvalues.  Thus $f$ has no maximum
in $\Delta$, since at a maximum both eigenvalues of the Hessian matrix
would be nonpositive.
Therefore, $f \leq 0$ on all of $\overline{\Delta}$.

Next suppose only that $\nabla^2 f \geq 0$.
Let $M$ be the maximum of $x^2+y^2$ on $\overline{\Delta}$.
Take $f_n(x,y) = f(x,y) + \frac{1}{n}
( x^2+y^2 ) - \frac{1}{n}M$.  Clearly $\nabla^2 f_n > 0$ everywhere on
$\Delta$ and
$f_n \leq 0$ on the boundary, so $f_n \leq 0$
on all of $\overline{\Delta}$.  As $f_n \to f$, we obtain that
$f \leq 0$ on all of $\overline{\Delta}$.

The other direction is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
Finish the proof of the proposition above.
\end{exercise}
\end{exbox}

\begin{prop}
\pagebreak[2]%
Suppose $U \subset \C$ is an open set and $f_\alpha \colon U \to \R \cup \{ -\infty \}$
is a family of subharmonic functions.  Let
\begin{equation*}
\varphi(z) = \sup_\alpha\, f_\alpha(z) .
\avoidbreak
\end{equation*}
If the family is finite, then $\varphi$ is subharmonic.
If the family is infinite, $\varphi(z) \not= \infty$ for
all $z$, and $\varphi$
is upper-semicontinuous, then $\varphi$ is subharmonic.
\end{prop}

\begin{proof}
Suppose $\overline{\Delta_r(a)} \subset U$.  For all $\alpha$,
\begin{equation*}
\frac{1}{2\pi} \int_0^{2\pi} \varphi (a+re^{i\theta})\, d\theta
\geq
\frac{1}{2\pi} \int_0^{2\pi} f_\alpha (a+re^{i\theta})\, d\theta
\geq f_\alpha(a) .
\end{equation*}
Taking the supremum on the right over $\alpha$ obtains the results.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that if $\varphi \colon \R \to \R$ is a monotonically increasing
convex function, $U \subset \C$ is an open set, and $f \colon U \to \R$
is subharmonic, then $\varphi \circ f$ is subharmonic.
\end{exercise}

\begin{exercise}
Let $U \subset \C$ be open, $\{ f_n \}$ a sequence of
subharmonic functions uniformly bounded above on compact subsets, and
$\{ c_n \}$ a sequence of positive real numbers such that
$\sum_{n=1}^\infty c_n < \infty$.
Prove that $f = \sum_{n=1}^\infty c_n f_n$ is subharmonic.  Make sure to prove
the function is upper-semicontinuous.
\end{exercise}

\begin{exercise}
\pagebreak[2]%
Suppose $U \subset \C$ is a bounded open set, and $\{ p_n \}$ a sequence of points in
$U$. For $z \in U$, define
$f(z) = \sum_{n=1}^\infty 2^{-n} \log \sabs{z-p_n}$, possibly taking on the
value $-\infty$.
\begin{exparts}
\item
Show that $f$ is a subharmonic function in $U$.
\item
If $U = \D$ and $p_n = \nicefrac{1}{n}$, show that $f$ is discontinuous at 0
(the natural topology on $\R \cup \{ -\infty \}$).
\item
If $\{ p_n \}$ is dense in $U$, show that $f$
is discontinuous on a dense set.
Hint: Prove that $f^{-1}(-\infty)$ is a small (but dense) set.
Another hint: Integrate the partial sums, and use polar coordinates.
\end{exparts}
\end{exercise}
\end{exbox}

\pagebreak[2]
There are too many harmonic functions in $\C^n \cong \R^{2n}$.
The real and imaginary parts of holomorphic functions in $\C^n$
form a smaller set when $n > 1$.  Notice that when a holomorphic
function is restricted to a complex line, we obtain a holomorphic
function of one variable.  So the real and imaginary parts
of a holomorphic function had better be harmonic on every complex line.
It turns out, this is precisely the right class of functions.

\begin{defn}
Let $U \subset \C^n$ be open.
A $C^2$-smooth $f \colon U \to \R$ is
\emph{\myindex{pluriharmonic}} if for every $a,b \in \C^n$, the function
of one variable
\begin{equation*}
\xi \mapsto f(a+b\xi)
\end{equation*}
is harmonic where defined (on $\{ \xi \in \C : a+b\xi \in U \}$).
That is, $f$ is harmonic on every complex line.

A function $f \colon U \to \R \cup \{ -\infty \}$ is
\emph{\myindex{plurisubharmonic}}, sometimes \emph{\myindex{plush}}
or \emph{\myindex{psh}} for short,
if it is upper-semicontinuous and for every
$a,b \in \C^n$, the function of one variable
\begin{equation*}
\xi \mapsto f(a+b\xi)
\end{equation*}
is subharmonic where defined.
\end{defn}

A harmonic function of one complex variable is in some sense a
generalization of an affine linear function of one real variable.
Similarly, as far as several complex variables are concerned, a
pluriharmonic function is the right generalization to $\C^n$
of an affine linear function on $\R^n$.
In the same way,
plurisubharmonic functions are the correct complex variable generalizations
of convex functions.  A convex function of one real variable is like a
subharmonic function, and a convex function of several real variables is
a function that is convex when restricted to any real line.

Many properties of harmonic and subharmonic
functions in $\C$
have immediate
generalizations to pluriharmonic and plurisubharmonic functions in
$\C^n$.  We emphasize three such immediate generalizations, the maximum
principle, the fact that the property of plurisubharmonicity is local,
and the fact that functions are pluriharmonic if and only if they are
(locally) the real and imaginary parts of holomorphic functions.
We will leave these as exercises.

\begin{exbox}
\begin{exercise}
Let $U \subset \C^n$ be open.
Prove that
a $C^2$-smooth $f \colon U \to \R$ is pluriharmonic if and
only if
\begin{equation*}
\frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell} = 0
\quad \text{ on $U$ for all $k,\ell=1,\ldots,n$.}
\end{equation*}
\end{exercise}

\begin{exercise}
Show that a pluriharmonic function is harmonic.  On the other hand, find an
example of a harmonic function that is not pluriharmonic.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be open.
Show that $f \colon U \to \R$ is pluriharmonic if and
only if it is locally the real or imaginary part of a holomorphic function.
Hint:  Using a previous exercise
$\frac{\partial f}{\partial z_\ell}$ is holomorphic for all $\ell$.
Assume that $U$ is simply connected, $p \in U$, and
$f(p) = 0$.
Consider the line integral from $p$ to a nearby $q \in U$:
\begin{equation*}
F(q) =
\int_{p}^q \sum_{\ell=1}^n
\frac{\partial f}{\partial z_\ell}(\zeta) \, d\zeta_\ell .
\avoidbreak
\end{equation*}
Prove that it is path independent, compute derivatives of $F$, and
find out what is $F+\bar{F}-f$.
\end{exercise}

\begin{exercise}
Prove the maximum
principle\index{maximum principle!plurisubharmonic functions}:
If $U \subset \C^n$ is a domain and $f \colon U \to \R \cup
\{-\infty\}$ is
plurisubharmonic and achieves a maximum at $p \in U$, then $f$ is constant.
\end{exercise}

\begin{exercise}
Show that plurisubharmonicity is a local property, that is,
$f$ is plurisubharmonic if and only if $f$ is plurisubharmonic in
some neighborhood of each point.
\end{exercise}
\end{exbox}

\begin{samepage}
\begin{prop}
Let $U \subset \C^n$ be open.
A $C^2$-smooth $f \colon U \to \R$ is plurisubharmonic
if and only if the complex Hessian matrix
\begin{equation*}
\left[
\frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell}
\right]_{k\ell}
\end{equation*}
is positive semidefinite at every
point.
\end{prop}
\end{samepage}

\begin{proof}
First suppose the complex Hessian has a negative eigenvalue at some $p \in U$.
After a translation assume $p=0$.
As $f$ is real-valued, the complex Hessian
$\left[
\frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell}
\Big|_0
\right]_{k\ell}$ is Hermitian.  A complex linear change of coordinates
acts on the complex Hessian by $*$-congruence, and therefore we can
diagonalize,
using Sylvester's Law of Inertia again.
So assume that
$\left[
\frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell}
\Big|_0
\right]_{k\ell}$ is diagonal.  If the complex Hessian has a negative eigenvalue, then
one of the diagonal entries is negative.
Without loss of generality suppose
$\frac{\partial^2 f}{\partial \bar{z}_1 \partial z_1}\Big|_0 < 0$.
The function $z_1 \mapsto f(z_1,0,\ldots,0)$ has
a negative Laplacian and therefore is not subharmonic, and thus $f$ itself
is not plurisubharmonic.

For the other direction, suppose the complex Hessian is positive
semidefinite at all points.
After an affine change of coordinates assume that an arbitrary
complex line $\xi \mapsto a+b\xi$ is setting all but the first variable to
zero, that is, $a=0$ and $b=(1,0,\ldots,0)$.
As the complex Hessian is positive semidefinite,
$\frac{\partial^2 f}{\partial \bar{z}_1 \partial z_1} \geq 0$ for all
points $(z_1,0,\ldots,0)$.  We proved above that $\nabla^2 g \geq 0$
implies $g$ is subharmonic, and we are done.
\end{proof}

\begin{exbox}
\begin{exercise} \label{exercise:modholplush}
Suppose $U \subset \C^n$ is open and $f \colon U \to \C$ is holomorphic.
\begin{exparts}
\item
Show $\log \sabs{f(z)}$ is pluriharmonic on $U \setminus f^{-1}(0)$ and
plurisubharmonic on $U$.
\item
Show $\sabs{f(z)}^{\eta}$ is plurisubharmonic for all $\eta > 0$.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that the set of plurisubharmonic functions on an open set $U \subset \C^n$
is a cone in the sense that if $a,b > 0$ are constants and
$f, g \colon U \to \R \cup \{ -\infty \}$ are plurisubharmonic, then
$a f + b g$ is plurisubharmonic.
\end{exercise}
\end{exbox}

\begin{thm} \label{thm:subharlim}
\pagebreak[2]%
Suppose $U \subset \C^n$ is an open set and $f \colon U \to \R \cup \{
-\infty \}$ is plurisubharmonic.
Let $U_\epsilon \subset U$
be the set of points further than $\epsilon > 0$ away from $\partial U$.
For every $\epsilon > 0$,
there exists a smooth plurisubharmonic function
$f_\epsilon \colon U_\epsilon \to \R$ such that $f_\epsilon(z) \geq
f(z)$, and
\begin{equation*}
f(z) = \lim_{\epsilon \to 0} f_\epsilon(z) \qquad \text{for all $z \in U$}.
\end{equation*}
\end{thm}

That is, $f$ is a (pointwise) limit of smooth plurisubharmonic functions.
The idea of the proof is important and useful in many other
contexts.

\begin{proof}
We smooth $f$ out by convolving with so-called
\emph{mollifiers}\index{mollifier}, or
\emph{approximate delta functions}\index{approximate delta function}.
Many different mollifiers
work, but we use a specific one for concreteness.
For $\epsilon > 0$, define
\begin{equation*}
g(z) =
\begin{cases}
C e^{-1/(1-\snorm{z}^2)} & \text{ if $\snorm{z} < 1$,}
\\
0 & \text{ if $\snorm{z} \geq 1$,}
\end{cases}
\qquad
\text{and}
\qquad
g_\epsilon(z) = \frac{1}{\epsilon^{2n}} g(z/\epsilon) .
\end{equation*}
It is left as an exercise that $g$, and so $g_\epsilon$, is smooth.
The function $g$ has compact
support as it is only nonzero inside the unit ball.  The support of
$g_\epsilon$ is the $\epsilon$-ball.  Both are nonnegative.  Choose $C$ so that
\begin{equation*}
\int_{\C^n} g\, dV = 1 ,
\qquad \text{ and therefore } \qquad
\int_{\C^n} g_\epsilon\, dV = 1 .
\end{equation*}
\glsadd{not:dV}%
Here $dV$ is the volume measure.
The function $g$ only depends on $\snorm{z}$.
To get an idea of
how these functions work,
see \figureref{fig:graph-of-mollifier}.

\begin{myfig}
\includegraphics[width=0.5\textwidth]{figures/graph-of-mollifier.pdf}
\caption{Graphs of $e^{-1/(1-x^2)}$,
$\frac{1}{0.5}e^{-1/(1-{(x/0.5)}^2)}$, and
$\frac{1}{0.25}e^{-1/(1-{(x/0.25)}^2)}$.\label{fig:graph-of-mollifier}}
\end{myfig}

Compare the graphs to the graphs of the Poisson kernel as a function of
$\theta$, which is also a type of mollifier.  The idea of
integrating against the right approximate delta function with the desired properties
is similar to the solution of the Dirichlet problem using the Poisson kernel.

The function $f$ is bounded above on compact sets as it is upper semicontinuous.
If $f$ is not bounded below, replace $f$ with $\max \bigl\{ f ,
\frac{-1}{\epsilon}
\bigr\}$, which is still plurisubharmonic.  Thus, without loss of
generality,
assume that $f$ is locally bounded.
For $z \in U_\epsilon$, define $f_\epsilon$ as
the convolution with $g_\epsilon$:
\glsadd{not:convolution}%
\begin{equation*}
f_\epsilon(z) = (f * g_\epsilon)(z) =
\int_{\C^n} f(w) g_\epsilon (z-w) \, dV(w) =
\int_{\C^n} f(z-w) g_\epsilon (w) \, dV(w) .
\end{equation*}
The two forms of the integral follow easily via change of variables.
We are perhaps abusing notation a bit as $f$ is only defined on $U$,
but it is not a problem as long as $z \in
U_\epsilon$, because $g_\epsilon$ is then zero when $f$ is undefined.
By differentiating the first form under the integral, we find that
$f_\epsilon$ is smooth.

\pagebreak[1]
Let us show that $f_\epsilon$ is plurisubharmonic.
Restrict to a line $\xi \mapsto a+b\xi$.
We wish to prove the sub-mean-value property using a circle
of radius $r$ around $\xi = 0$:
\begin{equation*}
\begin{split}
\frac{1}{2\pi} \int_0^{2\pi} f_\epsilon(a+bre^{i\theta})\, d\theta & =
\frac{1}{2\pi} \int_0^{2\pi}
\int_{\C^n}
f\bigl(a+bre^{i\theta}-w\bigr) g_\epsilon (w) \, dV(w)
\,d\theta
\\
& =
\int_{\C^n}
\left(
\frac{1}{2\pi} \int_0^{2\pi}
f\bigl(a-w+bre^{i\theta}\bigr) \, d\theta \right) g_\epsilon (w) \, dV(w)
\\
& \geq
\int_{\C^n}
f(a-w) g_\epsilon (w) \, dV(w)  = f_\epsilon(a).
\end{split}
\end{equation*}
For the inequality, we used $g_\epsilon \geq 0$.
So $f_\epsilon$ is plurisubharmonic.

Let us show that $f_\epsilon(z) \geq f(z)$ for all $z \in U_\epsilon$.
The function $g_\epsilon(w)$ only depends on $\sabs{w_1},\ldots,\sabs{w_n}$,
in fact,
$g_\epsilon(w_1,\ldots,w_n) =
g_\epsilon(\sabs{w_1},\ldots,\sabs{w_n})$.
Without loss of generality, we consider $z=0$ and we use polar coordinates
for the integral.
\pagebreak[1]
\begin{equation*}
\begin{split}
f_\epsilon(0)
& =
\int_{\C^n} f(-w) g_\epsilon (\sabs{w_1},\ldots,\sabs{w_n})
\, dV(w)
\\
& =
\int_0^\epsilon \cdots
\int_0^\epsilon
\left(
\int_0^{2\pi}
\cdots
\int_0^{2\pi}
 f(-r_1e^{i\theta_1},\ldots,
-r_ne^{i\theta_n}) \,
d\theta_1 \cdots d\theta_n \right)
\\
%& \phantom{=}\qquad
& \hspace{2.8in}
g_\epsilon (r_1,\ldots,r_n) \,
 r_1 \cdots r_n \,d r_1 \cdots d r_n
\\
& \geq
\int_0^\epsilon \cdots
\int_0^\epsilon
\left(
\int_0^{2\pi}
\cdots
\int_0^{2\pi}
(2\pi)
 f(0,-r_2e^{i\theta_2},\ldots,
-r_ne^{i\theta_n}) \,
d\theta_2 \cdots d\theta_n \right)
\\
%& \phantom{=}\qquad
& \hspace{2.8in}
g_\epsilon (r_1,\ldots,r_n) \,
 r_1 \cdots r_n \,d r_1 \cdots d r_n
\\
& \geq
f(0)
\int_0^\epsilon \cdots
\int_0^\epsilon
{(2\pi)}^n
g_\epsilon (r_1,\ldots,r_n) \,
 r_1 \cdots r_n \,d r_1 \cdots d r_n
\\
& = f(0) \int_{\C^n} g_\epsilon (w) \, dV(w)
%\\
%&
= f(0) .
\end{split}
\end{equation*}
The second equality above
follows as $g_\epsilon$ is zero
outside the polydisc of radius $\epsilon$.
For the inequalities, we again needed that $g_\epsilon \geq 0$.
The penultimate equality follows from the fact that
$2\pi = \int_0^{2\pi}d \theta$.

Finally, for a fixed $z$, we show $\lim_{\epsilon \to 0} f_\epsilon (z) = f(z)$.
For subharmonic, and so for plurisubharmonic,
functions, $\limsup_{\zeta\to z} f(\zeta) = f(z)$,
see \exerciseref{exercise:limsupsubharmonic}.
So given $\delta >0$, find an $\epsilon >0$ such that
$f(\zeta)-f(z) \leq \delta$ for all $\zeta \in B_\epsilon(z)$.
\begin{equation*}
\begin{split}
f_\epsilon(z) - f(z)
& =
\int_{B_\epsilon(0)} f(z-w) g_\epsilon (w)
\, dV(w)
- f(z)
\int_{B_\epsilon(0)} g_\epsilon (w)
\, dV(w)
\\
& =
\int_{B_\epsilon(0)} \bigl(f(z-w)-f(z)\bigr)\, g_\epsilon (w)
\, dV(w)
\\
& \leq
\delta
\int_{B_\epsilon(0)} g_\epsilon (w)
\, dV(w)
= \delta .
\avoidbreak
\end{split}
\end{equation*}
Again we used that $g_\epsilon \geq 0$.
We find $0 \leq f_\epsilon(z) - f(z) \leq \delta$, and so $f_\epsilon(z) \to
f(z)$.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that $g$ in the proof above is smooth on all of $\C^n$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Show that for a subharmonic function $f$, $\int_0^{2\pi} f(a+re^{i\theta}) \,
d\theta$ is a monotone function of $r$.
Hint: Try a $C^2$ function first and use Green's theorem.
\item
Use this
fact to show that the $f_\epsilon(z)$ from \thmref{thm:subharlim} is monotone
decreasing in $\epsilon$.
\end{exparts}
\end{exercise}
\end{exbox}

As smooth plurisubharmonic functions have a
local characterization in terms of derivatives,
we obtain the following useful corollary,
whose proof is an exercise.

\begin{cor} \label{cor:pshcompose}
Let $U \subset \C^n$
and $V \subset \C^m$ be open.
Prove that
if $g \colon U \to V$ is holomorphic and
$f \colon V \to \R \cup \{ - \infty \}$ is plurisubharmonic,
then $f \circ g$ is plurisubharmonic.
\end{cor}

\begin{exbox}
\begin{exercise}
Prove \corref{cor:pshcompose}.
Hint: Prove it first for $C^2$ functions,
then use the approximation.
Monotone convergence is useful.
\end{exercise}

\begin{exercise}
Using the computation from
\thmref{thm:subharlim} show that if $f$ is pluriharmonic, then
$f_\epsilon = f$ (where it makes sense), obtaining another proof that
a pluriharmonic $f$ is $C^\infty$.
\end{exercise}

\begin{exercise}
Let the $f$ in \thmref{thm:subharlim} be continuous and suppose $K \subset
\subset U$.  For small enough $\epsilon >0$, $K \subset U_\epsilon$.
Show that $f_\epsilon$ converges uniformly to $f$ on $K$.
\end{exercise}

\begin{exercise}
Let the $f$ in \thmref{thm:subharlim} be $C^k$-smooth for some $k \geq 0$.
Show that all derivatives of $f_\epsilon$ up to order $k$ converge uniformly
on compact sets to the corresponding derivatives of $f$.  See also previous
exercise.
\end{exercise}
\end{exbox}

Let us prove the theorem of
Rad\'o, which is a complementary result to the Riemann extension theorem.
Here on the one hand the function is
continuous and vanishes on the set you wish to extend across, but on the
other hand you know nothing about this set.
It is sometimes covered in a one-variable course,
and in several variables it follows directly from
the one-variable result.

\begin{thm}[Rad\'o] \index{Rad\'o's theorem}\label{thm:rado}
Let $U \subset \C^n$ be open and $f \colon U \to \C$ a continuous
function that is holomorphic on the set
\begin{equation*}
U' = \bigl\{ z \in U : f(z) \not= 0 \bigr\} .
\avoidbreak
\end{equation*}
Then $f \in \sO(U)$.
\end{thm}

\begin{proof}
First assume $n=1$.  The conclusion is local, so it is
enough to prove it for a small disc $\Delta$ such that $f$ is continuous
on the closure $\overline{\Delta}$.  Let $\Delta' \subset \Delta$ be the set
where $f$ is nonzero.  If $\Delta'$ is empty, we are done as
$f$ is identically zero and hence holomorphic.

Let $u$ be the real part of $f$.  On $\Delta'$, $u$ is a harmonic function.
Let $Pu$ be the Poisson integral of $u$ on $\Delta$.  Hence $Pu$
equals $u$ on $\partial \Delta$, and $Pu$ is harmonic in all of $\Delta$.
Consider the function
$Pu(z) - u(z)$ on $\overline{\Delta}$.  The function is zero
on $\partial \Delta$ and it is harmonic on $\Delta'$.  By rescaling $f$,
we assume $\abs{f(z)} < 1$ for all $z \in \overline{\Delta}$.
The function $z \mapsto \log \sabs{f(z)}$ is harmonic on $\Delta'$, it is
$-\infty$ when $f(z) = 0$, and hence it is upper-semicontinuous on
$\overline{\Delta}$.
Applying
the sub-mean-value property near points where $f$ vanishes
and the fact that subharmonicity is local, we find that
$\log \sabs{f(z)}$ is subharmonic on $\Delta$.
As $\sabs{f(z)} < 1$, we find that
$\log \sabs{f(z)}$ is negative on $\overline{\Delta}$.
So for every $t > 0$,
the function
$z \mapsto t \log \abs{f(z)}$ is subharmonic and negative
and the function
$z \mapsto -t \log \abs{f(z)}$ is superharmonic
(minus a subharmonic function) and positive.
See \figureref{fig:radosthm}.
It is immediate that for all $t > 0$ and
$z \in \partial \Delta$,
we have
\begin{equation} \label{eq:radobound}
t \log \abs{f(z)} \leq Pu(z)-u(z) \leq -t \log \abs{f(z)}  .
\end{equation}
The functions
$z \mapsto t \log \abs{f(z)} - \bigl(Pu(z)-u(z)\bigr)$
and
$z \mapsto t \log \abs{f(z)} - \bigl(u(z)-Pu(z)\bigr)$
are harmonic on $\Delta'$ and $-\infty$ whenever $f(z)=0$.
Thus both are upper-semicontinuous on $\overline{\Delta}$
and subharmonic on $\Delta$.
The maximum principle shows that
\eqref{eq:radobound} holds
for all $z \in \overline{\Delta}$ and all $t > 0$.

\begin{myfig}
\subimport*{figures/}{radosthm.pdf_t}
\caption{Proof of Rad\'o's theorem.\label{fig:radosthm}}
\end{myfig}

Taking the limit
$t \to 0$ shows that $Pu = u$ on $\Delta'$.
Let $W = \Delta \setminus \overline{\Delta'}$.
On $W$, $u=0$ and so $Pu-u$ is harmonic on $W$
and continuous on $\widebar{W}$.  Furthermore,
$Pu-u=0$ on $\overline{\Delta'} \cup \partial \Delta$,
and so $Pu-u=0$ on $\partial W$.  By the maximum principle, $Pu=u$ on $W$
and therefore on all of $\overline{\Delta}$.
Similarly, if $v$ is the imaginary part of $f$, then $Pv = v$ on
$\overline{\Delta}$.
In other words, $u$ and $v$ are harmonic on $\Delta$.
As $\Delta$ is simply connected,
let $\tilde{v}$ be the harmonic conjugate of $u$ that equals $v$ at
some point of $\Delta'$.  As $f$ is holomorphic on $\Delta'$,
the harmonic functions $\tilde{v}$ and $v$
are equal on the nonempty open subset $\Delta'$ of $\Delta$ and so
they are equal everywhere.  Consequently, $f = u +iv$ is holomorphic on
$\Delta$.

The extension of the proof to several variables is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
Use the one-variable result to extend the theorem to several variables.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hartogs pseudoconvexity}

By \corref{cor:pshcompose},
plurisubharmonicity is preserved under holomorphic mappings.
In particular, if $\varphi \colon \widebar{\D} \to
\C^n$ is an analytic disc and $f$ is plurisubharmonic in a neighborhood of
$\varphi(\widebar{\D})$, then $f \circ \varphi$ is subharmonic on $\D$.
As subharmonic functions satisfy the maximum principle, we find
that $f(z) \leq \sup_{w \in \varphi(\partial \D)} f(w)$ for
all $z \in \varphi(\D)$.  Let us give a general definition
for this type of situation.

\begin{defn}
Let $\sF$ be a class of (extended\footnote{%
By extended reals we mean $\R \cup \{ -\infty,\infty\}$.})-real-valued
functions defined on an open $U \subset \R^n$.  If $K
\subset U$, define $\widehat{K}$, the \emph{\myindex{hull}} of $K$ with
respect to $\sF$, as the set
\glsadd{not:Khat}%
\begin{equation*}
\widehat{K} \overset{\text{def}}{=} \Bigl\{ x \in U : f(x) \leq \sup_{y\in K} f(y)
\text{ for all $f \in \sF$ } \Bigr\} .
\end{equation*}
An open set $U$ is \emph{convex with respect to $\sF$}\index{convex!with respect to $\sF$}
if for every $K \subset \subset U$, the hull $\widehat{K} \subset \subset U$.%
\footnote{Recall that $\subset \subset$ means a relatively
compact subset;
the closure in the relative (subspace) topology is compact.}
\end{defn}

Clearly $K \subset \widehat{K}$.  The key is to show that $\widehat{K}$
is not ``too large'' for $U$.
Keep in mind that the functions in $\sF$ are defined on $U$, so $\widehat{K}$
depends on $U$ not just on $K$.  An easy mistake is to consider functions defined
on a larger set, obtaining a smaller $\sF$ and hence a larger
$\widehat{K}$.  Sometimes it is useful to write $\widehat{K}_{\sF}$ to
denote the dependence on $\sF$, especially when talking about several different
hulls.

For example, if $U=\R$ and $\sF$ is the set of real-valued smooth
$f \colon \R \to \R$ with $f''(x) \geq 0$, then
$\widehat{\{ a, b \}} = [a,b]$ for any $a,b \in \R$.
In general, if $\sF$ is the set of convex functions, then
a domain $U \subset \R^n$ is geometrically convex if and only if it is
convex with respect to convex functions\footnote{%
The technicality is, of course, that we must define convex functions on
not-necessarily-convex sets, and that is not completely straightforward.},
although let us not define
what that means except for smooth functions in exercises below.

\begin{exbox}
\begin{exercise} \label{exercise:geomconvexfuncs}
Suppose $U \subset \R^n$ is a domain.
\begin{exparts}
\item
Show that $U$
is geometrically convex if and only if it is
convex with respect to the affine linear functions.
\item
Suppose $U$
has smooth boundary.
Show that $U$ is
convex if and only if it is
convex with respect to the smooth convex functions on $U$,
that is, with respect to smooth functions with positive semidefinite Hessian.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that every open set $U \subset \R^n$ is convex with respect to real
polynomials.
\end{exercise}
\end{exbox}

\begin{thm}[Kontinuit\"atssatz---Continuity
principle\index{Kontinuit\"atssatz!second version}\index{continuity
principle!second version}, second version]
\label{thm:contprinciple2}
Suppose an open set $U \subset \C^n$ is convex with respect to plurisubharmonic
functions,
then given any collection of closed analytic discs $\Delta_\alpha \subset U$
such that $\bigcup_\alpha \partial \Delta_\alpha \subset \subset U$,
we have
$\bigcup_\alpha \Delta_\alpha \subset \subset U$.
\end{thm}

Various similar theorems are named the \emph{continuity principle},
we have now seen two.
What they have in common is a family of analytic discs whose
boundaries stay inside a domain, and where the conclusion has to do
with extension of holomorphic functions, with domains of holomorphy, or
with some sort of convexity as above.

\begin{proof}
Let $f$ be a plurisubharmonic function on $U$.  If $\varphi_\alpha \colon
\overline{\D} \to U$ is the holomorphic (in $\D$) mapping giving the closed
analytic disc, then $f \circ \varphi_\alpha$ is subharmonic.
By the maximum principle,
$f$ on $\Delta_\alpha$ must be less than or equal to the supremum
of $f$ on $\partial \Delta_\alpha$, so $\overline{\Delta_\alpha}$
is in the hull of
$\partial \Delta_\alpha$.
In other words,
$\bigcup_\alpha \Delta_\alpha$ is in the hull of
$\bigcup_\alpha \partial \Delta_\alpha$ and therefore
$\bigcup_\alpha \Delta_\alpha \subset \subset U$ by convexity.
\end{proof}

Let us illustrate the failure of the continuity principle.
If you have discs (denoted by straight line segments)
that approach the boundary as in \figureref{fig:contprinc},
then the domain is not convex with respect to plurisubharmonic functions.
In the diagram, the boundaries of the discs are
denoted by the dark dots at the end of the segments.
In fact, for standard geometric convexity,
we can prove a continuity principle where
we do replace discs with line segments,
see the exercises below.

\begin{myfig}
\subimport*{figures/}{contprinc.pdf_t}
\caption{Failure of the continuity principle.\label{fig:contprinc}}
\end{myfig}

\begin{exbox}
\begin{exercise}
Suppose $U \subset \C^n$ is a domain and $K \subset \subset U$ is a nonempty
compact subset.  Prove that $U \setminus K$ is not convex with respect to
plurisubharmonic functions.
\end{exercise}

\begin{exercise} \label{exercise:affinedisctouchingsmooth}
Suppose $U \subset \C^n$ is a domain with smooth boundary,
$p \in \partial U$,
and $\Delta$ is an affine linear analytic disc with $p \in \Delta$, but
$\Delta \setminus \{ p \} \subset U$.  Prove that $U$ is not convex with
respect to the plurisubharmonic functions.
\end{exercise}

\begin{exercise}
Prove the corresponding
Kontinuit\"atssatz, and its converse, for geometric convexity:
Prove that a domain $U \subset \R^n$ is geometrically convex if and only if
whenever $[x_\alpha,y_\alpha] \subset U$
is a collection of straight line segments such that
$\bigcup_{\alpha} \{ x_\alpha,y_\alpha \} \subset \subset U$
implies
$\bigcup_{\alpha} [ x_\alpha,y_\alpha ] \subset \subset U$.
\end{exercise}
\end{exbox}

We now define another version of pseudoconvexity,
this time only in terms of the interior
of the domain.

\begin{defn}
\pagebreak[2]
Let $U \subset \C^n$ be open.  An $f \colon U \to \R$ is an \emph{\myindex{exhaustion function}} for $U$ if
\begin{equation*}
\bigl\{ z \in U : f(z) < r \bigr\} \subset \subset U
\qquad \text{for every $r \in \R$.}
\end{equation*}
A domain $U \subset \C^n$ is
\emph{\myindex{Hartogs pseudoconvex}}\index{pseudoconvex}
if
there exists a continuous plurisubharmonic exhaustion function.
The set
$\{ z \in U : f(z) < r \}$ is called the \emph{\myindex{sublevel set}} of
$f$,
or the $r$-\emph{sublevel set}.
\end{defn}

\begin{example}
The unit ball $\bB_n$ is Hartogs pseudoconvex.  The continuous
function
\begin{equation*}
z \mapsto - \log \bigl( 1-\snorm{z}^2 \bigr)
\end{equation*}
is an exhaustion function, and it is easy to
check directly that it is plurisubharmonic.
\end{example}

\begin{example}
The entire $\C^n$ is Hartogs pseudoconvex as $\snorm{z}^2$ is
a continuous plurisubharmonic exhaustion function.
Also, because $\snorm{z}^2$ is plurisubharmonic, then given any $K \subset \subset
\C^n$, the hull $\widehat{K}$ with respect to plurisubharmonic functions must
be bounded.  In other words, $\C^n$ is convex with respect to
plurisubharmonic functions.
\end{example}

\begin{samepage}
\begin{thm}
Suppose $U \subsetneq \C^n$ is a domain.  The following are equivalent:
\begin{enumerate}[(i)]
\item \label{thm:pscvx:itemi}
$-\log \rho(z)$ is plurisubharmonic, where $\rho(z)$ is the distance from $z$
to $\partial U$.
\item \label{thm:pscvx:itemii}
%$U$ has a continuous plurisubharmonic exhaustion function,
%that is,
$U$ is Hartogs pseudoconvex.
\item \label{thm:pscvx:itemiii}
$U$ is convex with respect to plurisubharmonic functions defined on $U$.
\item \label{thm:pscvx:itemiv}
The conclusion of
\hyperref[thm:contprinciple2]{Kontinuit\"atssatz (second version)}
holds:
for any collection of closed analytic discs $\Delta_\alpha \subset U$
such that $\bigcup_\alpha \partial \Delta_\alpha \subset \subset U$,
we have
$\bigcup_\alpha \Delta_\alpha \subset \subset U$.
\end{enumerate}
\end{thm}
\end{samepage}

\begin{proof}
\ref{thm:pscvx:itemi}
$\Rightarrow$
\ref{thm:pscvx:itemii}:
If $U$ is bounded,
the function $-\log \rho(z)$ is clearly a continuous exhaustion function.
If $U$ is unbounded, take
$z \mapsto \max \{ -\log \rho(z) , \snorm{z}^2 \}$.

\ref{thm:pscvx:itemii}
$\Rightarrow$
\ref{thm:pscvx:itemiii}:
Suppose $f$ is a continuous plurisubharmonic exhaustion function.
If $K \subset \subset U$, then for some $r$ we have
$K \subset \{ z \in U : f(z) < r \} \subset \subset U$.
But then by definition of the hull $\widehat{K}$ we have
$\widehat{K} \subset \{ z \in U : f(z) < r \} \subset \subset U$.

\ref{thm:pscvx:itemiii}
$\Rightarrow$
\ref{thm:pscvx:itemiv}:
That is simply the statement of
\hyperref[thm:contprinciple2]{Kontinuit\"atssatz (second version)}.

\ref{thm:pscvx:itemiv}
$\Rightarrow$
\ref{thm:pscvx:itemi}:
As long as $U \not= \C^n$,
the function $-\log \rho(z)$ is real-valued and continuous.
For $c \in \C^n$ with $\snorm{c}=1$, let
$\rho_c(z)$ be the supremum of the radii of the affine discs centered at $z$
in the direction $c$ that lie in $U$.  That is,
\begin{equation*}
\rho_c(z) =
\sup \bigl\{ \lambda > 0 :
z+ \zeta c \in U \text{ for all $\zeta \in \lambda\D$} \bigr\} .
\end{equation*}
As $\rho(z) = \inf_c \rho_c(z)$,
\begin{equation*}
- \log \rho(z) = \sup_{\snorm{c}=1} \bigl(-\log \rho_c(z)\bigr) .
\end{equation*}
If we prove that for all $a, b \in \C^n$ and
$c \in \C^n$ with $\snorm{c}=1$, the function $\xi \mapsto -\log \rho_c(a+b\xi)$ is
subharmonic, then $\xi \mapsto - \log \rho(a+b\xi)$ is subharmonic,
and we are done.
See \figureref{fig:distfun-hartogs} for the setup.
\begin{myfig}
\subimport*{figures/}{distfun-hartogs.pdf_t}
\caption{Largest disc in the direction of $c$.
The disc is drawn as a line.\label{fig:distfun-hartogs}}
\end{myfig}

Suppose $\Delta \subset \C$ is a disc such that
$a+b\xi \in U$
for all $\xi \in
\overline{\Delta}$.
If $u$ is a harmonic function on $\Delta$ continuous on $\overline{\Delta}$
such that
$- \log \rho_c(a+b\xi) \leq u(\xi)$ on $\partial \Delta$, we must
show that the inequality holds on $\Delta$.
By \exerciseref{exercise:onlyniceuneededforsubharmonic},
we may assume $u$ is harmonic on a neighborhood of $\overline{\Delta}$
and so let $u = \Re f$ for a holomorphic function $f$.
Suppose $\xi \in \partial \Delta$ for a moment.
We have  $- \log \rho_c(a+b\xi) \leq \Re
f(\xi)$,
or in other words
\begin{equation*}
\rho_c(a+b\xi) \geq e^{-\Re f(\xi)} = \babs{e^{-f(\xi)}}.
\end{equation*}
Using $\zeta = t e^{-f(\xi)}$ in the definition of $\rho_c(a+b\xi)$, the
statement above is equivalent
to
\begin{equation*}
(a+b\xi)+te^{-f(\xi)}c \in U \quad \text{for all $t \in \D$}.
\end{equation*}
This statement holds whenever $\xi \in \partial \Delta$.  We must prove that
it also holds for all $\xi \in \Delta$.

The function $\varphi_t(\xi) =
(a+b\xi)+te^{-f(\xi)}c$ gives a closed analytic disc with boundary inside
$U$.  We have a family of analytic discs, parametrized by $t$, whose boundaries are in
$U$ for all $t$ with $\sabs{t} < 1$.  For $t=0$ the entire disc is
inside $U$.  As $\varphi_t(\xi)$ is continuous in both $t$ and $\xi$ and
$\overline{\Delta}$ is compact,
$\varphi_t(\Delta) \subset U$ for $t$ in some neighborhood of $0$.
Take $0 < t_0 < 1$ such that
$\varphi_t(\Delta) \subset U$ for all $t$ with $\sabs{t} < t_0$.
Then
\begin{equation*}
\bigcup_{\sabs{t} < t_0} \varphi_t(\partial \Delta)
\subset
\bigcup_{\sabs{t} \leq t_0} \varphi_t(\partial \Delta)
\subset \subset U ,
\end{equation*}
because continuous functions take compact sets to compact sets.
The hypothesis \ref{thm:pscvx:itemiv} implies
\begin{equation*}
\bigcup_{\sabs{t} < t_0} \varphi_t(\Delta)
\subset \subset U  .
\end{equation*}
By continuity again,
$\bigcup_{\sabs{t} \leq t_0} \varphi_t(\Delta)
\subset \subset U$, and so
$\bigcup_{\sabs{t} < t_0+\epsilon} \varphi_t(\Delta)
\subset \subset U$
for some $\epsilon > 0$.
Consequently $\varphi_t(\Delta) \subset U$
for all $t$ with $\sabs{t} < 1$.  Thus
$(a+b\xi)+te^{-f(\xi)}c \in U$ for all $\xi \in \Delta$ and all $\sabs{t} <
1$.  This implies $\rho_c(a+b\xi) \geq e^{-\Re f(\xi)}$ for all $\xi \in
\Delta$, which in
turn implies $-\log \rho_c(a+b\xi) \leq \Re f(\xi) = u(\xi)$ for all $\xi
\in \Delta$.
Therefore, $-\log \rho_c(a+b\xi)$ is subharmonic.
\end{proof}

\begin{exbox}
\begin{exercise} \label{exercise:intersectionhpseudo}
Show that if $U_1 \subset \C^n$ and $U_2 \subset \C^n$ are Hartogs
pseudoconvex domains, then so are all the topological components of $U_1 \cap U_2$.
\end{exercise}

\begin{exercise}
Show that if $U \subset \C^n$ and $V \subset \C^m$ are Hartogs
pseudoconvex domains, then so is $U \times V$.
\end{exercise}

\begin{exercise}
Show that every domain $U \subset \C$ is Hartogs pseudoconvex.
\end{exercise}

\begin{exercise} \label{exercise:nestedunions}
Consider the union $U = \bigcup_k U_k$ of a nested sequence of Hartogs pseudoconvex
domains, $U_{k-1} \subset U_k \subset \C^n$.  Show that $U$ is Hartogs pseudoconvex.
\end{exercise}

\begin{exercise}
Let $\R^2 \subset \C^2$ be naturally embedded (that is, it is the
set where $z_1$ and $z_2$ are real).  Show that the set $\C^2 \setminus
\R^2$ is not Hartogs pseudoconvex.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain and $f \in \sO(U)$.  Prove that
$U' = \bigl\{ z \in U : f(z) \not= 0 \bigr\}$ is a Hartogs pseudoconvex
domain.  Hint: See also \exerciseref{exercise:connectedcomplement}.
\end{exercise}

\begin{exercise} \label{exercise:biholHartogs}
Suppose $U,V \subset \C^n$ are biholomorphic domains.
Prove that $U$ is Hartogs pseudoconvex if and only if $V$ is
Hartogs pseudoconvex.
\end{exercise}

\begin{exercise}
Let $U = \bigl\{ z \in \C^2 : \sabs{z_1} > \sabs{z_2} \bigr\}$.
\begin{exparts}
\item
Prove that $U$ is a Hartogs pseudoconvex domain.
\item
Find a closed analytic disc $\Delta$ in $\C^2$ such that $0 \in \Delta$
($0 \notin U$)
and $\Delta \setminus \{ 0 \} \subset U$ (in particular $\partial \Delta
\subset U$).
\item
What do you think would happen if you tried to move $\Delta$ a
little bit to avoid the intersection with the complement?
Think about the \hyperref[thm:contprinciple2]{continuity principle}
(second version).
Compare with \exerciseref{exercise:affinedisctouchingsmooth}.
\end{exparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain and $f \colon \widebar{U} \to \R$ be
continuous, plurisubharmonic, negative on $U$, and $f=0$ on $\partial
U$.  Prove that $U$ is Hartogs pseudoconvex.
\end{exercise}
\end{exbox}

The statement corresponding to \exerciseref{exercise:nestedunions} on nested unions
for domains of holomorphy is
the \emph{\myindex{Behnke--Stein theorem}}, which follows using this exercise and the solution
of the Levi problem.
Behnke--Stein is easier to prove without the solution to the Levi problem,
see \exerciseref{exercise:behnkestein}, and is, in fact, generally used in
the solution of the Levi problem.

\exerciseref{exercise:biholHartogs} says that (Hartogs) pseudoconvexity is a
biholomorphic invariant.  That is a good indication that we are looking at a
correct notion.  It also allows us to change variables to more convenient
ones when proving a specific domain is (Hartogs) pseudoconvex.

It is not immediately clear from the definition,
but Hartogs pseudoconvexity
is also a local property of the boundary.

\begin{lemma}
\pagebreak[2]
A domain $U \subset \C^n$ is Hartogs pseudoconvex if and only if
for every point $p \in \partial U$ there exists a neighborhood $W$ of $p$
such that $W \cap U$ is Hartogs pseudoconvex.
\end{lemma}

\begin{proof}
One direction is trivial, so consider the other.
Suppose $p \in \partial U$, and let
$W$ be such that $U \cap W$
is Hartogs pseudoconvex.  Intersection of
Hartogs pseudoconvex domains is Hartogs pseudoconvex,
see \exerciseref{exercise:intersectionhpseudo}, so
assume $W = B_r(p)$.
Let $B = B_{r/2}(p)$.  If $q \in B \cap U$, the distance from $q$ to the boundary of $W \cap U$ is the same as
the distance to $\partial U$.  The setup is illustrated in
\figureref{fig:hartogs-pseudoconvex-local}.

\begin{myfig}
\subimport*{figures/}{hartogs-pseudoconvex-local.pdf_t}
\caption{Local Hartogs pseudoconvexity.\label{fig:hartogs-pseudoconvex-local}}
\end{myfig}

The part of the boundary $\partial U$ in $W$ is marked by a thick
black line, the part of the boundary of $\partial (W \cap U)$ that is
the boundary of $W$ is marked by a thick gray line.  A point $q \in B$ is
marked and a ball of radius $\nicefrac{r}{2}$ around $q$ is dotted.
No point of distance $\nicefrac{r}{2}$ from $q$ is in $\partial W$, and
the distance of $q$ to $\partial U$ is at most $\nicefrac{r}{2}$ as $p \in \partial U$
and $p$ is the center of $B$.
\glsadd{not:dist}%
Let $\operatorname{dist}(x,y)$ denote the
euclidean distance function\footnote{If $x$ and/or $y$ are sets
of points, we take the infimum of the euclidean distance over all the points.}.
Then for $z \in B \cap U$
\begin{equation*}
- \log \, \operatorname{dist}(z, \partial U) =
- \log \, \operatorname{dist}\bigl(z, \partial (U \cap W)\bigr).
\end{equation*}
The right-hand side is plurisubharmonic as $U \cap W$ is Hartogs
pseudoconvex.  Such a ball $B$ exists around every $p \in \partial U$, so near
the boundary, $- \log \, \operatorname{dist}(z, \partial U)$ is
plurisubharmonic.

If $U$ is bounded, then $\partial U$ is compact.  So there is some
$\epsilon > 0$ such that $- \log \, \operatorname{dist}(z, \partial U)$
is plurisubharmonic if $\operatorname{dist}(z, \partial U) < 2\epsilon$.
The function
\begin{equation*}
\varphi(z) = \max \bigl\{
- \log \, \operatorname{dist}(z, \partial U) , - \log \epsilon \bigr\}
\end{equation*}
is a continuous plurisubharmonic exhaustion function.
The proof for
unbounded $U$ is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
Finish the proof of the lemma for unbounded domains.
See~\exerciseref{exercise:nestedunions}.
\end{exercise}
\end{exbox}



It may seem that we defined a totally different concept, but it turns
out that Levi and Hartogs pseudoconvexity are one and the same on domains
where both concepts make sense.
As a consequence of the following theorem we say simply ``pseudoconvex'' and there
is no ambiguity.

\begin{thm}
Let $U \subset \C^n$ be a domain with smooth boundary.
Then $U$ is Hartogs pseudoconvex if and only if $U$ is Levi pseudoconvex.
\end{thm}


\begin{proof}
Suppose
$U \subset \C^n$ is a domain with smooth boundary that is not
Levi pseudoconvex at $p \in \partial U$.
As in
\thmref{thm:tomatocan}, change coordinates so that $p=0$ and $U$ is defined
by
\begin{equation*}
\Im z_n > - \sabs{z_1}^2 + \sum_{k=2}^{n-1} \epsilon_k \sabs{z_k}^2 + O(3) .
\end{equation*}
For a small fixed $\lambda > 0$, the
closed analytic discs defined by $\xi \in \overline{\D} \mapsto (\lambda \xi, 0, \cdots, 0, is)$
are in $U$ for all small enough $s > 0$.  The origin
is a limit point of the insides of the discs, but not a limit point of their boundaries.
\hyperref[thm:contprinciple2]{Kontinuit\"atssatz} (second version) is not satisfied,
and $U$ is not
convex with respect to the plurisubharmonic functions.  Therefore,
$U$ is not Hartogs pseudoconvex.

Next suppose $U$ is Levi pseudoconvex.  Take any $p \in \partial U$.
After translation and rotation by a unitary, assume $p=0$ and
write a defining function $r$ as
\begin{equation*}
r(z) = \varphi(z',\bar{z}',\Re z_n) - \Im z_n ,
\end{equation*}
where $z' = (z_1,\ldots,z_{n-1})$ and $\varphi \in O(2)$.
Levi pseudoconvexity says
\begin{equation} \label{eq:psconvcond}
\sum_{k=1,\ell=1}^n
\bar{a}_k a_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q \geq 0
\quad \text{whenever} \quad
\sum_{k=1}^n
a_k \frac{\partial r}{\partial z_k} \Big|_q = 0 ,
\end{equation}
for all $q \in \partial U$ near $0$.
Let $s$ be a small real constant,
and let $\widetilde{q} = (q_1,\ldots,q_{n-1},q_n + is)$.
As no derivatives of $r$ depend on $\Im z_n$, we have
$\frac{\partial r}{\partial z_\ell} \big|_{\widetilde{q}} =
\frac{\partial r}{\partial z_\ell} \big|_{q}$ and
$\frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \big|_{\widetilde{q}} =
\frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \big|_{q}$
for all $k$ and $\ell$.
So condition \eqref{eq:psconvcond} holds for all $q \in U$ near $0$.
We will use $r$ to manufacture a plurisubharmonic exhaustion function.
We need a
positive semidefinite complex Hessian,
and $r$ already has what we need in all but one direction.

Let $\nabla_z r|_q =
\bigl(
\frac{\partial r}{\partial z_1}\big|_q,\ldots,
\frac{\partial r}{\partial z_n}\big|_q \bigr)$ denote the gradient of $r$ in
the holomorphic directions only.
Given $q \in U$ near $0$,
decompose an arbitrary $c \in \C^n$ as $c = a+b$,
where $a$ and $b$ are
orthogonal and $b$ is a scalar multiple of
$\overline{\nabla_z r|_q}$.  That is,
$a = (a_1,\ldots,a_n)$ satisfies
\begin{equation*}
\sum_{k=1}^n
a_k \frac{\partial r}{\partial z_k} \Big|_q =
\blinnprod{a}{\overline{\nabla_z r|_q}}
=
0 .
\end{equation*}
By the equality part of Cauchy--Schwarz,
\begin{equation*}
\BBabs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k} \Big|_q}
=
\BBabs{\sum_{k=1}^n b_k \frac{\partial r}{\partial z_k} \Big|_q}
=
\Babs{\blinnprod{b}{\overline{\nabla_z r|_q}}}
=
\snorm{b} \, \bnorm{\nabla_z r|_q} .
\end{equation*}
As $\nabla_z r|_0 = (0,\ldots,0,-\nicefrac{1}{2i})$, then for $q$
sufficiently near $0$, we have
$\bnorm{\nabla_z r|_q} \geq \nicefrac{1}{3}$, and
\begin{equation} \label{eq:25boundingb}
\snorm{b} =
\frac{1}{\bnorm
{\nabla_z r|_q}}
\BBabs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k} \Big|_q}
%\frac{\abs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k} \Big|_q}}{\snorm
%{\nabla_z r|_q}}
\leq
3 \BBabs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k} \Big|_q}
.
\end{equation}


As $c = a+b$ is the orthogonal decomposition, $\snorm{c} \geq \snorm{b}$.
The complex Hessian matrix of $r$ is continuous, and so let
$M \geq 0$ be an upper bound on its operator norm for $q$ near the origin.
Note that
$\bar{c}_k c_\ell =
( \bar{a}_k + \bar{b}_k )  (a_\ell + b_\ell)
=
\bar{a}_k a_\ell
+ \bar{b}_k (a_\ell + b_\ell)
+ ( \bar{a}_k + \bar{b}_k ) b_\ell
- \bar{b}_k b_\ell
=
\bar{a}_k a_\ell
+ \bar{b}_k c_\ell
+ \bar{c}_k b_\ell
- \bar{b}_k b_\ell$.
Using Cauchy--Schwarz,
\begin{equation} \label{eq:25boundingchr}
\begin{split}
\sum_{k=1,\ell=1}^n
\bar{c}_k c_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
%& =
%\sum_{k=1,\ell=1}^n
%( \bar{a}_k + \bar{b}_k )  (a_\ell + b_\ell) \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
%\\
& =
\smashoperator{\sum_{k=1,\ell=1}^n}
\bar{a}_k a_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
+
\smashoperator{\sum_{k=1,\ell=1}^n}
\bar{b}_k c_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
\\
& \phantom{=} \quad
+
\smashoperator{\sum_{k=1,\ell=1}^n}
\bar{c}_k  b_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
-
\smashoperator{\sum_{k=1,\ell=1}^n}
\bar{b}_k  b_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
\\
& \geq
\smashoperator{\sum_{k=1,\ell=1}^n}
\bar{a}_k a_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
-
M\snorm{b}\snorm{c}
-
M\snorm{c}\snorm{b}
-
M\snorm{b}^2
\\
& \geq
-
3 M\snorm{c}\snorm{b} .
\end{split}
\end{equation}

Putting \eqref{eq:25boundingchr} and \eqref{eq:25boundingb} together,
for $q \in U$ near the origin,
\begin{equation*}
\sum_{k=1,\ell=1}^n
\bar{c}_k c_\ell \frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
\geq -3M \snorm{c} \snorm{b} %=
%-M \frac{\snorm{c}\Babs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k}
%\big|_q}}{\snorm{\nabla_z r |_q}}
\geq
-3^2 M \snorm{c}\BBabs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k}
\Big|_q} .
\end{equation*}
For $z \in U$ sufficiently close to $0$, define
\begin{equation*}
f(z) = -\log \bigl(-r(z)\bigr) + A \snorm{z}^2 ,
\end{equation*}
where $A > 0$ is some constant we will choose later.
The log is there to make $f$ blow up as we approach the boundary.
The $A \snorm{z}^2$ is there to add a constant diagonal matrix to the complex
Hessian of $f$, which we hope is enough to make it positive semidefinite at
all $z$ near $0$.
Compute:
\begin{equation*}
\frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell}
=
\frac{1}{r^2}
\frac{\partial r}{\partial \bar{z}_k}
\frac{\partial r}{\partial z_\ell}
-
\frac{1}{r}
\frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell}
+
A\delta_{k}^{\ell} ,
\end{equation*}
where $\delta_k^\ell$ is the Kronecker delta\footnote{%
Recall $\delta_k^\ell = 0$ if $k\not= \ell$ and $\delta_k^\ell = 1$ if $k =
\ell$.}.
Apply the complex Hessian of $f$ to $c$ at $q \in U$ near the origin
(recall that $r$ is negative on $U$ and so for $q \in U$, $-r = \sabs{r}$):
\begin{equation*}
\begin{split}
\sum_{k=1,\ell=1}^n
\bar{c}_k c_\ell \frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell} \Big|_q
& =
\frac{1}{r^2}
\BBabs{
\sum_{\ell=1}^n
c_\ell
\frac{\partial r}{\partial z_\ell} \Big|_q
}^2
+
\frac{1}{\sabs{r}}
\sum_{k=1,\ell=1}^n
\bar{c}_k
c_\ell
\frac{\partial^2 r}{\partial \bar{z}_k \partial z_\ell} \Big|_q
+
A \snorm{c}^2
\\
& \geq
\frac{1}{r^2}
\BBabs{
\sum_{\ell=1}^n
c_\ell
\frac{\partial r}{\partial z_\ell} \Big|_q
}^2
-
\frac{3^2 M}{\sabs{r}}
\snorm{c}
\BBabs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k} \Big|_q}
+
A \snorm{c}^2 .
\end{split}
\end{equation*}
Now comes a somewhat funky trick.
As a quadratic polynomial in $\snorm{c}$, the right-hand side of the
inequality
is always nonnegative if $A > 0$ and if the discriminant is negative or zero.
Let us see if we can make the discriminant zero:
\begin{equation*}
0 =
{\Biggl(
\frac{3^2 M}{\sabs{r}}
\BBabs{\sum_{k=1}^n c_k \frac{\partial r}{\partial z_k} \Big|_q}
\Biggr)}^2
- 4A
\frac{1}{r^2}
\BBabs{
\sum_{\ell=1}^n
c_\ell
\frac{\partial r}{\partial z_\ell} \Big|_q
}^2 .
\end{equation*}
All the nonconstant terms go away and
$A=\frac{3^4 M^2}{4}$ makes the discriminant zero.
Any larger $A$ would also work
by making the discriminant negative.
Thus for that $A$,
\begin{equation*}
\sum_{k=1,\ell=1}^n
\bar{c}_k c_\ell \frac{\partial^2 f}{\partial \bar{z}_k \partial z_\ell} \Big|_q
\geq 0.
\end{equation*}
In other words, the complex Hessian
of $f$ is positive semidefinite at all points $q \in U$ near $0$.
The function $f(z)$ goes to infinity as $z$ approaches $\partial U$.
So for every $t \in \R$, the $t$-sublevel set
(the set where $f(z) < t$) is a positive
distance away from $\partial U$ near $0$.

We have constructed a local continuous plurisubharmonic
exhaustion function for $U$ near $p$.  If we intersect
with a small ball $B$ centered at $p$, then $U \cap B$ is
Hartogs pseudoconvex.  This is true at all
$p \in \partial U$, so $U$ is Hartogs pseudoconvex.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that any defining function works in the construction above.
Suppose
$U$ is Levi pseudoconvex, $p \in \partial U$,
$r$ is the defining function from the proof, and let
$\tilde{r} = \varphi r$ for some smooth $\varphi$
defined near $p$, $\varphi(p)\not=0$.
Show that for $z \in U$ near $p$, the function
$f(z) = -\log(-\tilde{r}(z)) + A \snorm{z}^2$
is plurisubharmonic if $A$ is sufficiently large.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Holomorphic convexity}

\begin{defn}
Let $U \subset \C^n$ be a domain.  For $K \subset U$,
define the
\emph{\myindex{holomorphic hull}}
\glsadd{not:KhatU}%
\begin{equation*}
\widehat{K}_U \overset{\text{def}}{=} \Bigl\{ z \in U : \sabs{f(z)} \leq
\sup_{w\in K} \sabs{f(w)}
\text{ for all $f \in \sO(U)$}  \Bigr\} .
\end{equation*}
A domain $U$ is \emph{\myindex{holomorphically convex}} if whenever
$K \subset \subset U$, then $\widehat{K}_U \subset \subset U$.
In other words, $U$ is holomorphically convex if it is convex with
respect to moduli of holomorphic functions on $U$.\footnote{Sometimes simply
$\widehat{K}$ is used, but we use
$\widehat{K}_U$ to emphasize the dependence on $U$.}
\end{defn}


It is a simple exercise (see below) to show that a holomorphically convex
domain is Hartogs pseudoconvex.  We will prove that holomorphic convexity is
equivalent to being a domain of holomorphy.  That a Hartogs pseudoconvex
domain is holomorphically convex is the
Levi problem\index{Levi problem}
for Hartogs pseudoconvex domains and is considerably more
difficult.  The thing is, there are lots of plurisubharmonic functions,
and they are easy to construct; we can even construct them locally, and then
piece them together by taking maxima.  There are far fewer holomorphic functions, and
we cannot just construct them locally and expect the pieces to somehow fit
together.  As it is so fundamental, let us state it as a theorem.

\begin{thm}[Solution of the Levi problem]
A domain $U \subset \C^n$ is holomorphically convex
if and only it is Hartogs pseudoconvex.
\end{thm}

\begin{proof}
The forward direction follows from an exercise below, which is sometimes
called Oka's lemma.
We skip the proof of the backward direction
in order to save some hundred pages or so.
See H\"ormander's book~\cite{Hormander} for the proof.
\end{proof}

\begin{exbox}
\begin{exercise}[Oka's lemma\index{Oka's lemma}]
Prove that if a domain $U \subset \C^n$ is holomorphically convex,
then it is Hartogs pseudoconvex.
See \exerciseref{exercise:modholplush}.
\end{exercise}

\begin{exercise}
Prove that every domain $U \subset \C$ is holomorphically convex by
giving a topological description of $\widehat{K}_U$ for every
compact $K \subset \subset U$.  Hint: Runge may be useful.
\end{exercise}

\begin{exercise}
Suppose $f \colon \C^n \to \C$ is holomorphic and $U$ is a topological
component of $\bigl\{ z \in \C^n : \sabs{f(z)} < 1 \bigr\}$.  Prove
that $U$ is a holomorphically convex domain.
\end{exercise}

\begin{exercise}
Compute the hull
$\widehat{K}_{\D^n}$ of the set $K = \bigl\{ z \in \D^n : \sabs{z_\ell} =
\lambda_\ell \text{ for } \ell=1,\ldots,n \bigr\}$, where $0 \leq \lambda_\ell < 1$.
Prove that the unit polydisc is holomorphically convex.
\end{exercise}

\begin{exercise}
Prove that a geometrically convex domain $U \subset \C^n$
is holomorphically convex.
\end{exercise}

\begin{exercise}
Prove the Hartogs figure (see \thmref{thm:extensionhartogsfigure})
is not holomorphically convex.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain, $f \in \sO(U)$, and $f$ is not identically
zero.  Show that if
$U$ is holomorphically convex, then
$\widetilde{U} = \bigl\{ z \in U : f(z) \not= 0 \bigr\}$
is holomorphically convex.
Hint: First see \exerciseref{exercise:connectedcomplement}.
\end{exercise}

\begin{exercise} \label{exercise:biholholconvex}
Suppose $U,V \subset \C^n$ are biholomorphic domains.
Prove that $U$ is holomorphically convex if and only if $V$ is
holomorphically convex.
\end{exercise}

\begin{exercise}
In the definition of holomorphic hull of $K$, replace $U$ with $\C^n$
and $\sO(U)$ with holomorphic polynomials on $\C^n$, to get the
\emph{\myindex{polynomial hull}} of $K$.  Prove that the polynomial hull of
$K \subset \subset \C^n$ is the same as the holomorphic hull $\widehat{K}_{\C^n}$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item Prove the Hartogs triangle $T$ (see \exerciseref{exercise:hartogstriangle})
is holomorphically convex.
\item Prove $T \cup B_{\epsilon}(0)$ (for a small enough $\epsilon > 0$) is
not holomorphically convex.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that if domains $U_1 \subset \C^n$ and $U_2 \subset \C^n$ are
holomorphically convex,
then so are all the topological components of $U_1 \cap U_2$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $n \geq 2$ and $U \subset \C^n$ a domain.
\begin{exparts}
\item
Let $K \subset \subset U$ be
nonempty and compact. Prove $U \setminus K$ is not
holomorphically convex.
\item
Prove that if
$U$ is bounded and holomorphically convex, then $\C^n \setminus U$ is connected.
\item
Find an unbounded holomorphically convex domain $U$ where
$\C^n \setminus U$ is disconnected.
\end{exparts}
\end{exercise}
\end{exbox}

\pagebreak[2]
The set $\C^n$ is both holomorphically convex and
a domain of holomorphy.  These two notions are equivalent also for all
other domains in $\C^n$.

\begin{thm}[Cartan--Thullen\index{Cartan--Thullen theorem}]
\pagebreak[2]
\label{thm:cartthul}
Let $U \subsetneq \C^n$ be a domain.  The following are equivalent:
\begin{enumerate}[(i)]
\item \label{thm:cartthul:domhol}
$U$ is a domain of holomorphy.
\item \label{thm:cartthul:disthull}
For all $K \subset \subset U$,
$\operatorname{dist}(K,\partial U) = \operatorname{dist}(\widehat{K}_U,\partial U)$.
\item \label{thm:cartthul:holconv}
$U$ is holomorphically convex.
\end{enumerate}
\end{thm}

\begin{proof}
We start with \ref{thm:cartthul:domhol} $\Rightarrow$
\ref{thm:cartthul:disthull}.
Consider a $K \subset
\subset U$ with $\operatorname{dist}(K,\partial U) > \operatorname{dist}(\widehat{K}_U,\partial U)$.
After a possible rotation by a unitary,
there is a point $p \in \widehat{K}_U$ and a polydisc
$\Delta = \Delta_r(0)$ with polyradius $r = (r_1,\ldots,r_n)$ such that
$p + \Delta = \Delta_r(p)$ contains a point of $\partial U$, but
\begin{equation*}
K + \Delta = \bigcup_{q \in K} \Delta_r(q) \subset \subset U.
\end{equation*}
See \figureref{fig:cart-thul-fig}.

\begin{myfig}
\subimport*{figures/}{cart-thul-fig.pdf_t}
\caption{Point in the hull closer to the boundary than closest
point of $K$.\label{fig:cart-thul-fig}}
\end{myfig}

If $f \in \sO(U)$, then there is an $M > 0$ such that $\sabs{f} \leq M$ on
$K + \Delta$ as that is a relatively compact set.  By the Cauchy estimates
for each $q \in K$, we get
\begin{equation*}
\abs{\frac{\partial^\alpha f}{\partial z^\alpha}(q)} \leq \frac{M
\alpha!}{r^\alpha} .
\end{equation*}
This inequality therefore holds on $\widehat{K}_U$ and hence at $p$.
The series
\begin{equation*}
\sum_{\alpha}
\frac{1}{\alpha !}\frac{\partial^\alpha f}{\partial z^\alpha}(p) {(z-p)}^\alpha
\end{equation*}
converges in $\Delta_r(p)$.  The function $f$ extends to all of $\Delta_r(p)$ and
$\Delta_r(p)$ contains points outside of $U$.
In other words, $U$ is not a domain of holomorphy.

The implication \ref{thm:cartthul:disthull} $\Rightarrow$
\ref{thm:cartthul:holconv} is immediate.

Finally, we prove
\ref{thm:cartthul:holconv} $\Rightarrow$
\ref{thm:cartthul:domhol}.
Suppose $U$ is holomorphically convex.  Let $p \in \partial U$.
By convexity, choose nested compact sets $K_{\ell-1} \subsetneq K_\ell \subset
\subset U$ such that $\bigcup_\ell K_\ell = U$, and $\widehat{(K_\ell)}_U =
K_\ell$.
As the sets exhaust $U$, we can perhaps pass to a subsequence
to ensure that
there exists a
sequence of points $p_\ell \in K_\ell \setminus K_{\ell-1}$ such that
$\lim_{\ell\to\infty} p_\ell = p$.

Choose $f_1 \in \sO(U)$ so that $f_1(p_1) \geq 1$.
Proceed inductively.
As $p_\ell$ is not in the hull of $K_{\ell-1}$, there is a function $f_\ell \in
\sO(U)$ such that $\sabs{f_\ell} \leq 2^{-\ell}$ on $K_{\ell-1}$, but
\begin{equation*}
\sabs{f_\ell(p_\ell)} \geq \ell + \abs{\sum_{k=1}^{\ell-1} f_k(p_\ell)} .
\end{equation*}
Finding such a function is left as an exercise below.
For every $\ell$, the series $\sum_{k=1}^\infty f_k(z)$ converges uniformly on
$K_\ell$
as for all $k > \ell$, $\sabs{f_k} \leq 2^{-k}$ on $K_\ell$.
As the $K_\ell$ exhaust $U$, the series converges uniformly on compact
subsets of $U$.  Consequently,
\begin{equation*}
f(z) = \sum_{k=1}^\infty f_k(z)
\end{equation*}
is a holomorphic function on $U$.  We bound
\begin{equation*}
\sabs{f(p_\ell)} \geq
\sabs{f_\ell(p_\ell)}
-
\abs{\sum_{k=1}^{\ell-1} f_k(p_\ell)}
-
\abs{\sum_{k=\ell+1}^\infty f_k(p_\ell)}
\geq
\ell
-
\sum_{k=\ell+1}^\infty 2^{-k}
\geq \ell-1 .
\end{equation*}
So $\lim_{\ell\to\infty} f(p_\ell) = \infty$.
Clearly there cannot be any open $W \subset \C^n$
containing $p$ to which $f$ extends (see
\hyperref[defn:domainofhol]{definition of domain of holomorphy}).  As every
connected open $W$ such that
$W \setminus U \not= \emptyset$
and
$W \setminus U^c \not= \emptyset$
must contain a
point of $\partial U$, we are done.
\end{proof}

By \exerciseref{exercise:biholholconvex},
holomorphic convexity is a biholomorphic invariant.
Thus,
being a domain of holomorphy is also a biholomorphic invariant.  This
fact is not easy to prove from the definition of a domain of
holomorphy, as the
biholomorphism is defined only on the interior of our domains.

Holomorphic convexity is an intrinsic notion; it does not require
knowing anything about points outside of $U$.  It is a
better way to think about domains of holomorphy.  Holomorphic
convexity generalizes easily to more complicated complex
manifolds\footnote{Manifolds with complex structure, i.e., ``manifolds
with multiplication by $i$ on the tangent space.''}, while
the notion of a domain of holomorphy only makes sense for domains in $\C^n$.

\begin{exbox}
\begin{exercise}[Behnke--Stein again]
\index{Behnke--Stein theorem}\label{exercise:behnkestein}
\pagebreak[2]
Show that the union $\bigcup_\ell U_\ell$ of a nested sequence of holomorphically
convex domains $U_{\ell-1} \subset U_\ell \subset \C^n$ is holomorphically convex.
\end{exercise}

\begin{exercise}
Prove the existence of the function $f_\ell \in \sO(U)$ as indicated in the proof
of Cartan--Thullen above.
\end{exercise}

\begin{exercise}
Show that if $U \subset \C^n$
is holomorphically convex, then there
exists a single function $f \in \sO(U)$ that does not extend through any
point $p \in \partial U$.
\end{exercise}

\begin{exercise}
We know $U = \C^2 \setminus \{ z \in \C^2 : z_1 = 0 \}$ is a domain of
holomorphy.  Use part~\ref{thm:cartthul:disthull}
of the theorem to show that if $W \subset \C^2$ is a domain of holomorphy
and $U \subset W$, then either $W=U$ or $W = \C^2$.  Hint:  Suppose $L
\subset W$
is a complex line and $K$ is a circle in $L$.  What is $\widehat{K}_W$?
\end{exercise}
\end{exbox}

In the following series of exercises, which you should most definitely do in
order, you will solve the
Levi problem\index{Levi problem!for complete Reinhardt domains}
(and more) for complete Reinhardt domains.
Recall that a domain $U$ is a
\myindex{complete Reinhardt domain}\index{Reinhardt domain!complete}
if whenever $(z_1,\ldots,z_n)$ is in $U$ and $r_k = \sabs{z_k}$, then the
entire closed polydisc $\overline{\Delta_r(0)} \subset U$.
We say a complete Reinhardt domain $U$ is \emph{\myindex{logarithmically convex}}%
\index{Reinhardt domain!logarithmically convex}
if there exists a
(geometrically) convex $C \subset \R^n$ such that $z \in U$ if and only if
$\bigl(\log \sabs{z_1},\ldots,\log\sabs{z_n}\bigr) \in C$.

\begin{exbox}
\begin{exercise}
Prove that a logarithmically convex complete Reinhardt domain
is the intersection of sets of the form
\begin{equation*}
\bigl\{ z \in \C^n : \alpha_1 \log \sabs{z_1} + \cdots + \alpha_n \log
\sabs{z_n} < \beta  \bigr\}
=
\bigl\{ z \in \C^n : \sabs{z_1}^{\alpha_1} \cdots \sabs{z_n}^{\alpha_n}
< e^\beta \bigr\}
\avoidbreak
\end{equation*}
for some nonnegative $\alpha_1,\ldots,\alpha_n$, and $\beta \in \R$.
\end{exercise}

\begin{exercise}
Prove that if a complete Reinhardt domain is Hartogs
pseudoconvex, then it is logarithmically convex.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $\alpha_1,\ldots,\alpha_n \geq 0$ and $\beta \in \R$.
For each $k \in \N_0$, let $\ell_m^k \in \N_0$ be the smallest nonnegative integer such that
$\ell_m^k \geq k \alpha_m$.
Prove that the domain of convergence of the power series
\begin{equation*}
\sum_{k=0}^\infty e^{-k\beta}
z_1^{\ell_1^k}
\cdots
z_n^{\ell_n^k}
\end{equation*}
is precisely
$\bigl\{ z \in \C^n :
\sabs{z_1}^{\alpha_1} \cdots \sabs{z_n}^{\alpha_n}
< e^\beta \bigr\}$.
Hint: That it diverges outside is easy, what is hard is that it converges
inside.  Perhaps useful is to notice
$\frac{\ell_m^k}{k}-\alpha_m \leq \frac{1}{k}$, and
that if $z$ is in the set, there is some $\epsilon > 0$ such that
$(1+\epsilon)\sabs{z_1}^{\alpha_1} \cdots \sabs{z_n}^{\alpha_n} =
e^{\beta}$.
\end{exercise}

\begin{exercise}
Prove that if a complete Reinhardt domain is 
logarithmically convex,
then it is holomorphically
convex and therefore a domain of holomorphy.
\end{exercise}

\begin{exercise}
Prove that a complete Reinhardt domain is
a domain of holomorphy if and only if it is the
domain of convergence of some power series at the origin.  Hint: There is a
function that does not extend past any boundary point of a holomorphically
convex domain.
\end{exercise}
\end{exbox}

\pagebreak[2]
We (you) have proved the following proposition.

\begin{prop}
\pagebreak[2]
Let $U \subset \C^n$ be a complete Reinhardt domain.  Then the following are
equivalent:
\begin{enumerate}[(i)]
\item
$U$ is logarithmically convex.
\item
$U$ is a domain of holomorphy.
\item
$U$ is a domain of convergence of some power series at the origin.
\item
$U$ is Hartogs pseudoconvex.
\end{enumerate}
\end{prop}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{CR Functions} \label{ch:crfunctions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Real-analytic functions and complexification}

\begin{defn}
Let $U \subset \R^n$ be open.
A function $f \colon U \to \C$ is
\emph{\myindex{real-analytic}} (or simply \emph{analytic} if
clear from context) if at each point $p \in U$, the function $f$
has a convergent power series that converges (absolutely) to $f$ in some
neighborhood of $p$.
\glsadd{not:Comega}%
A common notation for real-analytic is $C^\omega$.
\end{defn}

Before we discuss the connection between real-analytic and holomorphic functions,
we prove a simple lemma.

\begin{lemma} \label{lemma:sameonRn}
Let $\R^n \subset \C^n$ be the natural inclusion and
$V \subset \C^n$ a domain such that $V \cap \R^n \not= \emptyset$.
Suppose $f,g \colon V \to \C$ are holomorphic functions such that
$f=g$ on $V \cap \R^n$.  Then $f=g$ on $V$.
\end{lemma}

\begin{proof}
Considering $f-g$, we may assume that $g=0$.
Let $z = x+iy$ as usual so that $\R^n$ is given by $y=0$.
Our assumption is that $f = 0$ when $y=0$,
so the derivative of $f$ with respect to $x_k$ is zero.
When $y=0$,
the Cauchy--Riemann equations say
\begin{equation*}
0 = \frac{\partial f}{\partial x_k} =
-i \frac{\partial f}{\partial y_k} .
\end{equation*}
Therefore, on $y=0$,
\begin{equation*}
\frac{\partial f }{\partial z_k} = 0 .
\end{equation*}
The derivative $\frac{\partial f }{\partial z_k}$ is holomorphic
and $\frac{\partial f }{\partial z_k} = 0$ on $y=0$.
By induction, all holomorphic derivatives of $f$ at $p \in \R^n \cap V$ vanish,
and $f$ has a zero
power series.  Hence $f$ is identically zero in a neighborhood of
$p$ in $\C^n$.
By the identity theorem, $f$ is zero on all of $V$.
\end{proof}


We return to $\R^n$ for a moment.
We write a power series in $\R^n$ in multi-index notation as usual.
Suppose that for some
$a \in \R^n$
and some polyradius
$r=(r_1,\ldots,r_n)$,
the series
\begin{equation*}
\sum_{\alpha} c_{\alpha} {(x-a)}^\alpha
\end{equation*}
converges whenever $\sabs{x_k-a_k} < r_k$ for all $k$.
Here convergence is absolute convergence.  That is,
\begin{equation*}
\sum_{\alpha} \sabs{c_{\alpha}}\, \sabs{x-a}^\alpha
\end{equation*}
converges.
If we replace $x_k \in \R$ with $z_k \in \C$ such that
$\sabs{z_k-a_k} \leq \sabs{x_k-a_k}$, then the series still converges.
Hence the series
\begin{equation*}
\sum_{\alpha} c_{\alpha} {(z-a)}^\alpha
\end{equation*}
converges absolutely in $\Delta_r(a) \subset \C^n$.

\begin{prop}[Complexification part I\index{complexification}]
Suppose $U \subset \R^n$ is a domain and
$f \colon U \to \C$ is real-analytic.
Let $\R^n \subset \C^n$ be the natural inclusion.
Then there exists a domain $V \subset \C^n$ such that $U \subset V$
and a unique holomorphic function $F \colon V \to \C$ such that $F|_U = f$.
\end{prop}

One of many consequences
of this proposition is that
a real-analytic function is $C^\infty$ smooth.  Be careful
and notice that $U$ is a domain in $\R^n$, but it is not an open set
when considered as a subset of $\C^n$.  Furthermore, $V$ may be a very
``thin'' neighborhood around $U$ and there is no way of finding $V$ just from
knowing $U$.  You need to also know $f$.
As an example, consider
$f(x) = \frac{1}{\epsilon^2+x^2}$ for $\epsilon > 0$, which is
real-analytic on $\R$, but the complexification is not holomorphic at $\pm
\epsilon i$.

\begin{proof}
We proved the local version already.  We must prove that if we
extend our $f$ near every point, we always get the same function.
That follows from \lemmaref{lemma:sameonRn}; any two such functions are
equal on $\R^n$, and hence equal.  There is a subtle topological technical
point in this, so let us elaborate.  A key topological fact is that we define
$V$ as a union of the polydiscs where the series converges.  If
a point $p$ is in two different such polydiscs, we need to show that
the two definitions of $F$ are the same at $p$.  The intersection
of two polydiscs is connected, and in this case it also contains a piece
of $\R^n$, and we may apply the lemma.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the identity theorem for real-analytic functions.  That is, if $U
\subset \R^n$ is a domain, $f \colon U \to \R$ a real-analytic function, and
$f$ is zero on a nonempty open subset of $U$, then $f$ is identically zero.
\end{exercise}

\begin{exercise}
Suppose $U \subset \R^n$ is a domain and $f \colon U \to \R$ is
a real-analytic function such that $f|_W$ is harmonic for
some nonempty open $W \subset U$.
Prove that $f$ is harmonic.
\end{exercise}

\begin{exercise}
Let $(0,1) \subset \R$.  Construct a real-analytic function
on $(0,1)$ that does not complexify to the rectangle $(0,1) + i(-\epsilon,\epsilon)
\subset \C$ for every $\epsilon > 0$.  Why does this not contradict the
proposition?
\end{exercise}
\end{exbox}

\pagebreak[2]
A polynomial $P(x)$ in $n$ real variables $(x_1,\ldots,x_n)$ is homogeneous of degree $d$ if
$P(s x) = s^d P(x)$ for all $s \in \R$ and $x \in \R^n$.
A homogeneous polynomial of degree $d$ is a polynomial whose
every monomial
is of total degree $d$.
If $f$ is real-analytic near $a \in \R^n$, then
write the power series of $f$ at $a$ as
\begin{equation*}
\sum_{m=0}^{\infty} f_m(x-a) ,
\end{equation*}
where $f_m$ is a homogeneous polynomial of degree $m$.  The $f_m$ is
called the
\emph{\myindex{degree $m$ homogeneous part}}\index{homogeneous part} of $f$
at $a$.

There is usually a better way to complexify
real-analytic functions in $\C^n$.
Suppose $U \subset \C^n \cong \R^{2n}$, and $f \colon U \to
\C$ is real-analytic.  Assume $a=0 \in U$ for simplicity.
Writing $z = x+iy$, near $0$,
\begin{equation*}
f(x,y)
=
\sum_{m=0}^\infty
f_m(x,y)
=
\sum_{m=0}^\infty
f_m\left(
\frac{z+\bar{z}}{2},
\frac{z-\bar{z}}{2i}\right) .
\end{equation*}
The polynomial $f_m$ becomes a homogeneous polynomial of degree $m$
in the variables $z$ and $\bar{z}$.  The
series becomes a power series in $z$ and $\bar{z}$.
We simply write the function as $f(z,\bar{z})$, and we consider the
power series representation in $z$ and $\bar{z}$ rather than
in $x$ and $y$.
In multi-index notation, we write a power series at $a \in \C^n$ as
\begin{equation*}
\sum_{\alpha,\beta} c_{\alpha,\beta} {(z-a)}^\alpha
{(\bar{z}-\bar{a})}^\beta .
\end{equation*}

A holomorphic function
is real-analytic, but not vice versa.  A holomorphic function
is a real-analytic function that does not depend on $\bar{z}$.

Before we discuss complexification in terms of $z$ and $\bar{z}$, we need
a lemma.

\begin{lemma}
Let $V \subset \C^n \times \C^n$ be a domain, let the coordinates be $(z,\zeta) \in \C^n \times
\C^n$, let
\begin{equation*}
D = \bigl\{ (z,\zeta) \in \C^n \times \C^n : \zeta = \bar{z} \bigr\},
\end{equation*}
and suppose $D \cap V \not= \emptyset$.
Suppose $f,g \colon V \to \C$ are holomorphic functions such that
$f=g$ on $D \cap V$.  Then $f=g$ on all of $V$.
\end{lemma}

The set $D$ is sometimes called the \emph{\myindex{diagonal}}.

\begin{proof}
Without loss of generality, assume that $g=0$.
For $(z,\bar{z}) \in V$, we have $f(z,\bar{z}) = 0$, which is really
$f$ composed with the map taking $z$ to $(z,\bar{z})$.  This
composition is identically zero, so applying Wirtinger operators yields zero.
Using the chain rule,
\begin{equation*}
0 =
\frac{\partial}{\partial \bar{z}_k} \Bigl[f(z,\bar{z})\Bigr]
=
\frac{\partial f}{\partial \zeta_k}(z,\bar{z}) .
\end{equation*}
Similarly with the $z_k$,
\begin{equation*}
0 =
\frac{\partial}{\partial z_k} \Bigl[f(z,\bar{z})\Bigr]
=
\frac{\partial f}{\partial z_k}(z,\bar{z}) .
\end{equation*}
Either way, we get another holomorphic function in $z$ and $\zeta$
that is zero on $D$.
By induction, for all $\alpha$ and $\beta$ we get
\begin{equation*}
0 =
\frac{\partial^{\sabs{\alpha}+\sabs{\beta}}}{\partial z^\alpha \partial \bar{z}^\beta} \Bigl[f(z,\bar{z})\Bigr]
=
\frac{\partial^{\sabs{\alpha}+\sabs{\beta}} f}{\partial z^\alpha \partial
\zeta^\beta}(z,\bar{z}) .
\end{equation*}
All holomorphic derivatives in $z$ and $\zeta$ of $f$ are zero on every point
$(z,\bar{z})$, so the power series is zero at every point $(z,\bar{z})$,
and so $f$ is identically zero in a neighborhood of every
point $(z,\bar{z})$.  The lemma follows by the identity
theorem.
\end{proof}

Let $f$ be a real-analytic function.  Suppose
the series (in multi-index notation)
\begin{equation*}
f(z,\bar{z}) =
\sum_{\alpha,\beta} c_{\alpha,\beta} {(z-a)}^\alpha
{(\bar{z}-\bar{a})}^\beta
\end{equation*}
converges in a polydisc $\Delta_r(a) \subset \C^n$.
By convergence we mean absolute
convergence,
\begin{equation*}
\sum_{\alpha,\beta} \sabs{c_{\alpha,\beta}} \, \sabs{z-a}^\alpha
\sabs{\bar{z}-\bar{a}}^\beta
\end{equation*}
converges.
The series still converges if we replace $\bar{z}_k$  with
$\zeta_k$ where $\sabs{\zeta_k-\bar{a}} \leq \sabs{\bar{z}_k-\bar{a}}$.
So the series
\begin{equation*}
F(z,\zeta) =
\sum_{\alpha,\beta} c_{\alpha,\beta} {(z-a)}^\alpha
{(\zeta-\bar{a})}^\beta
\end{equation*}
converges (absolutely) for all $(z,\zeta) \in \Delta_r(a) \times \Delta_r(\bar{a})$.

Putting together the discussion above with the lemma we obtain:

\begin{prop}[Complexification part II\index{complexification}] \label{prop:complexificationpt2}
Suppose $U \subset \C^n$ is a domain and $f \colon U \to \C$ is
real-analytic.
Then there exists a domain $V \subset \C^n \times \C^n$ such that
\begin{equation*}
\bigl\{ (z,\zeta) : \zeta = \bar{z} \text{ and } z \in U \bigr\} \subset V ,
\end{equation*}
and a unique holomorphic function $F \colon V \to \C$ such that
$F(z,\bar{z}) = f(z,\bar{z})$ for all $z \in U$.
\end{prop}

The function $f$ can be thought of as the restriction of $F$ to the set
where $\zeta = \bar{z}$.  We will abuse notation and write
simply $f(z,\zeta)$ both for $f$ and its extension.
The reason for this abuse is evident from the computations above.
What we are calling $f$ is a function of $(z,\bar{z})$ if thinking
of it as a function on the diagonal where $\zeta=\bar{z}$, or it is a function of
$z$ if thinking of it as just the function $z \mapsto f(z,\bar{z})$, or
it is the function $(z,\zeta) \mapsto f(z,\zeta)$.  We have the
following commutative diagram:
\begin{equation*}
\begin{tikzcd}
U \subset \C^n \ar[rr, "{z \, \mapsto \, (z,\bar{z})}"] \ar[dr, "f"']
&
&
V \subset \C^n \times \C^n \ar[dl, "f ~~ {(=F)}"]
\\
&
\C
\end{tikzcd}
\end{equation*}
All three ways of going from one place to another in the diagram
we are calling $f$.  The arrow from $V$ was called $F$ in the proposition.
The notation plays well with differentiation and the Wirtinger operators.
Differentiating $f$ (really the $F$ in the proposition) in $\zeta_k$ and
evaluating at $(z,\bar{z})$ is the same thing as evaluating at
$(z,\bar{z})$ and then differentiating in $\bar{z}_k$ using the Wirtinger
operator:
\begin{equation*}
\frac{\partial F}{\partial \zeta_k}(z,\bar{z}) =
\frac{\partial f}{\partial \zeta_k}(z,\bar{z}) =
\frac{\partial}{\partial \bar{z}_k}\Bigl[ f(z,\bar{z}) \Bigr] =
\frac{\partial f}{\partial \bar{z}_k}(z,\bar{z}) .
\end{equation*}
If we squint our mind's eye,
we can't quite see the difference between $\bar{z}$ and $\zeta$.
We have already used this idea for smooth functions, but for
real-analytic functions we can
treat $z$ and $\bar{z}$ as truly independent variables.
The abuse of notation is entirely justified, at least once it is
understood well.

\begin{remark}
The domain $V$ in the proposition is not simply $U$ times the conjugate of $U$.
In general, it is smaller.  For example, a real-analytic $f \colon \C^n \to
\C$ does not necessarily complexify to all of $\C^n \times \C^n$.
That is
because the domain of convergence for a real-analytic function on $\C^n$
is not necessarily all of $\C^n$.  In one dimension,
\begin{equation*}
f(z,\bar{z})
= \frac{1}{1+\sabs{z}^2}
\end{equation*}
is real-analytic on $\C$, but it is not a restriction to the diagonal
of a holomorphic function defined on all of $\C^2$.  The problem is that the complexified
function
\begin{equation*}
f(z,\zeta)
= \frac{1}{1+z \zeta}
\end{equation*}
is undefined on the set where $z \zeta = -1$, which by a fluke
never happens when $\zeta = \bar{z}$.
\end{remark}

\begin{remark}
This form of complexification is sometimes called
\emph{\myindex{polarization}} due to its relation to the polarization
identities\footnote{Such as $4 \linnprod{z}{w} =
\snorm{z+w}^2-\snorm{z-w}^2 +i \bigl( \snorm{z+iw}^2 - \snorm{z-iw}^2
\bigr)$.}:  We can recover a Hermitian matrix $A$,
and therefore the sesquilinear form $\linnprod{Az}{w}$ for
$z,w\in \C^n$, by simply knowing the value of
\begin{equation*}
\linnprod{Az}{z} = z^*Az = \sum_{k,\ell=1}^n a_{k\ell} \, \bar{z}_k z_\ell
\end{equation*}
for all $z \in \C^n$.  In fact, under the hood, \propref{prop:complexificationpt2} is
polarization in an infinite-dimensional Hilbert space, but we digress.
\end{remark}

Treating $\bar{z}$ as a separate variable is a very powerful idea, and
as we have just seen it is completely natural for
real-analytic functions.  This is one of the reasons why real-analytic
functions play a special role in complex analysis.

\begin{exbox}
\begin{exercise}
Let $U \subset \C^n$ be an open set and $\varphi \colon U \to \R$ a
pluriharmonic function.  Prove that $\varphi$ is real-analytic.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be an open set, $z_0 \in U$.
Suppose $\varphi \colon U \to \R$ is a pluriharmonic function.
You know that $\varphi$ is real-analytic.
Using complexification, write down a formula for a holomorphic function near
$z_0$ whose real part is $\varphi$.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a domain, and suppose $f, g \in \sO(U)$.
Suppose that $f = \bar{g}$ on $U$.  Use complexification (complexify
$f-\bar{g}$) to show that both $f$ and $g$ are constant.
\end{exercise}
\end{exbox}

\begin{example}
Not every $C^\infty$ smooth function is real-analytic.  For $x \in \R$,
define
\begin{equation*}
f(x) =
\begin{cases}
e^{-1/x} & \text{if $x > 0$,} \\
0 & \text{if $x \leq 0$.}
\end{cases}
\end{equation*}
The function
$f \colon \R \to \R$ is $C^\infty$ and $f^{(k)}(0) = 0$ for all $k$.
The Taylor series of $f$ at the origin does
not converge to $f$ in any neighborhood of the origin; it converges to the
zero function but not to $f$.
Consequently, there is no neighborhood $V$ of the origin in $\C$ such that
$f$ is the restriction to $V \cap \R$ of a holomorphic function in $V$.
\end{example}

\begin{exbox}
\begin{exercise}
Prove the statements of the example above.
\end{exercise}
\end{exbox}

\begin{defn}\label{def:rahypersurfacesimple}
A real hypersurface $M \subset \R^n$ is said to be
\emph{real-analytic}\index{real-analytic hypersurface}\index{hypersurface!real-analytic}
if locally at every point it is the graph of a real-analytic function.  That
is, near every point (locally), after perhaps relabeling coordinates,
$M$ can be written as
a graph
\begin{equation*}
y = \varphi(x) ,
\avoidbreak
\end{equation*}
where $\varphi$ is real-analytic, $(x,y) \in \R^{n-1} \times \R = \R^n$.
\end{defn}

Compare this definition to \defnref{def:hypersurface}.  We could
define a real-analytic hypersurface as in
\defnref{def:hypersurface} and then prove an analogue of
\propref{prop:realgraphcoords} to show that this new definition
is identical to the definition above.
We avoid this complication, leaving it to the interested reader.

\begin{exbox}
\begin{exercise}
Show that \defnref{def:rahypersurfacesimple}
is equivalent to an analogue of
\defnref{def:hypersurface}.  That is, state the alternative definition of
real-analytic hypersurface and prove the analogue of
\propref{prop:realgraphcoords}.
\end{exercise}
\end{exbox}

A mapping to $\R^m$ is real-analytic if all the components are real-analytic
functions.  Via complexification we give a simple proof of the following
result.

\begin{prop}
Let $U \subset \R^n$, $V \subset \R^k$ be open and let
$f \colon U \to V$ and $g \colon V \to \R^m$ be real-analytic.
Then $g \circ f$ is real-analytic.
\end{prop}

\begin{proof}
Let $x \in \R^n$ be our coordinates in $U$ and $y \in \R^k$ be
our coordinates in $V$.  Complexify $f(x)$ and $g(y)$ by
allowing $x$ to be a complex vector in a small neighborhood of $U$ in
$\C^n$
and $y$ to be a complex vector in a small neighborhood of $V$ in $\C^k$.
So treat $f$ and $g$ as holomorphic functions.  On a certain
neighborhood of $U$ in $\C^n$, the composition $f \circ g$ makes sense
and it is holomorphic, as composition of holomorphic mappings is holomorphic.
Restricting the complexified $f \circ g$ back to $\R^n$ we obtain a
real-analytic function.
\end{proof}

The proof demonstrates a simple application of complexification.  Many
properties of holomorphic functions are easy to prove because
holomorphic functions are solutions to certain PDE (the Cauchy--Riemann
equations).  There is no PDE
that defines real-analytic functions, so complexification provides a useful
tool to transfer certain properties of holomorphic functions to
real-analytic functions.  We must be careful, however.  Hypotheses on
real-analytic functions only give us hypotheses on certain points of the
complexified holomorphic functions.

\begin{exbox}
\begin{exercise}
Demonstrate the point about complexification we made just above.
Find a nonconstant bounded real-analytic $f \colon \R^n \to \R$
that happens to complexify to $\C^n$.
\end{exercise}

\begin{exercise}
Let $U \subset \R^n$ be open.  Let $\varphi \colon (0,1) \to U$ be a
real-analytic function (curve), and let $f \colon U \to \R$ be
real-analytic.  Suppose that $(f \circ \varphi)(t) = 0$ for all $t \in
(0,\epsilon)$ for some $\epsilon > 0$.  Prove that $f$ is zero on the
image $\varphi\bigl((0,1)\bigr)$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CR functions}

We first need to know what it means for a function $f \colon X \to \C$
to be smooth if $X$ is not an open set, for example, if $X$ is a hypersurface.

\begin{defn}
Let $X \subset \R^n$ be a set.
The function $f \colon X \to \C$ is smooth (resp.\
real-analytic) if for each point $p \in X$ there is a
neighborhood $U \subset \R^n$ of $p$ and a smooth (resp.\ real-analytic) $F
\colon U \to \C$ such that $F(q) = f(q)$ for $q \in X \cap U$.
\end{defn}

For an arbitrary set $X$, issues surrounding this definition can be
rather subtle.  The definition
is easy to work with,
however, if $X$ is nice, such as a hypersurface, or if $X$ is
a closure of a domain with smooth boundary.

\begin{prop}
Suppose $M \subset \R^n$ is a smooth (resp.\ real-analytic) real hypersurface.
A function $f \colon M \to \C$
is smooth (resp.\ real-analytic) if and only if whenever near any point we write
$M$ in coordinates $(x,y) \in \R^{n-1} \times \R$ as
\begin{equation*}
y = \varphi(x) ,
\avoidbreak
\end{equation*}
for a smooth (resp.\ real-analytic) function $\varphi$, then
$f\bigl(x,\varphi(x)\bigr)$ is a smooth (resp.\ real-analytic) function of $x$.
\end{prop}

\begin{exbox}
\begin{exercise}
Prove the proposition.
\end{exercise}

\begin{exercise}
Prove that if $M$ is a smooth or real-analytic
hypersurface, and $f \colon M \to \C$ is smooth or real-analytic, then the function $F$ from the definition is never unique,
even for a fixed neighborhood $U$.
\end{exercise}

\begin{exercise}
Suppose $M \subset \R^n$ is a smooth hypersurface, $f \colon M \to \C$
is a smooth function, $p \in M$, and $X_p \in T_p M$.  Prove that
$X_p f$ is well-defined.
That is, suppose
$U$ is a neighborhood of $p$,
$F \colon U \to \C$ and $G \colon U \to \C$
are smooth functions that both equal $f$ on $U \cap M$.
Prove that
$X_p F = X_p G$.
\end{exercise}
\end{exbox}

Due to the last exercise, we can apply vectors of $T_p M$ to
a smooth function on a hypersurface by simply applying
them to any smooth extension.  We can similarly apply vectors of
$\C T_p M$ to smooth functions on $M$, as
$\C T_p M$ is simply the complex span of vectors in $T_p M$.

\begin{defn}
Let $M \subset \C^n$ be a smooth real hypersurface.
A smooth $f \colon M \to \C$ is a
\emph{\myindex{smooth CR function}}\index{CR function} if
\begin{equation*}
X_p f = 0
\avoidbreak
\end{equation*}
for all $p \in M$ and all vectors $X_p \in T^{(0,1)}_p M$.
\end{defn}

\begin{remark}
One only needs one derivative (rather than $C^\infty$) in the definition
above.
One can even
define a continuous CR function if the derivative is taken in the
distribution sense, but we digress.
\end{remark}

\begin{remark}
When $n=1$, a real hypersurface $M \subset \C$ is a curve and $T^{(0,1)}_p M$
is trivial.  Therefore, all smooth functions $f \colon M \to \C$ are CR functions.
\end{remark}

\begin{prop}
Let $M \subset U$ be a smooth (resp.\ real-analytic) real hypersurface in an
open $U
\subset \C^n$.  Suppose $F \colon U \to \C$ is a holomorphic function,
then the restriction $f = F|_M$ is a smooth (resp.\ real-analytic) CR function.
\end{prop}

\begin{proof}
First let us prove that $f$ is smooth.  The function $F$ is smooth and
defined on a neighborhood of every point of $M$, and so it can be used in the
definition.
Similarly for real-analytic.

Let us show $f$ is CR at some $p \in M$.
Differentiating $f$ with vectors in $\C T_pM$
is the same as differentiating $F$.
As $T_p^{(0,1)} M \subset T_p^{(0,1)} \C^n$, we have
\begin{equation*}
X_p f = X_p F = 0 \qquad \text{for all} \quad X_p \in T_p^{(0,1)} M.
\qedhere
\end{equation*}
\end{proof}

On the other hand, not every smooth CR function is a restriction of a holomorphic function.

\begin{example}
Take the smooth function $f \colon \R \to \R$ we defined before
that is not real-analytic at the origin.
Let $M \subset \C^2$ be the real-analytic hypersurface defined by
$\Im z_2 = 0$.
Clearly,
$T_p^{(0,1)} M$ is one-complex-dimensional, and at each $p \in M$,
$\frac{\partial}{\partial \bar{z}_1}\big|_p$ is tangent and spans
$T_p^{(0,1)} M$.  Define $g \colon M \to \C$ by
\begin{equation*}
g(z_1,z_2,\bar{z}_1,\bar{z}_2) = f(\Re z_2) .
\end{equation*}
Then $g$ is CR as it is independent of $\bar{z}_1$.
If $G \colon U \subset \C^2 \to \C$ is a holomorphic
function where $U$ is some open set containing the origin, then $G$
restricted to $M$ must be real-analytic (a power series in $\Re z_1$, $\Im
z_1$, and $\Re z_2$) and therefore $G$ cannot equal to
$g$ on $M$.
\end{example}

\begin{exbox}
\begin{exercise}
Suppose $M \subset \C^n$ is a smooth real hypersurface
and $f \colon M \to \C$ is a CR function that is a restriction
of a holomorphic function $F \colon U \to \C$ defined in
some neighborhood $U \subset \C^n$ of $M$.  Show that $F$ is unique,
that is, if $G \colon U \to \C$ is another holomorphic function such that
$G|_M = f = F|_M$, then $G=F$.
\end{exercise}

\begin{exercise}
Show that there is no maximum principle of CR functions.  In fact, find a
smooth real hypersurface $M \subset \C^n$, $n \geq 2$, and a smooth CR function
$f$ on $M$ such that $\sabs{f}$ attains a strict maximum at a point.
\end{exercise}

\begin{exercise}
Suppose $M \subset \C^n$, $n \geq 2$, is the real hypersurface given
by $\Im z_n = 0$.
Show that every smooth CR function on $M$ is holomorphic in the variables
$z_1,\ldots,z_{n-1}$.
Use this to show that for no smooth CR function $f$ on $M$ can
$\sabs{f}$ attain a \emph{strict} maximum on $M$.  Show that there do
exist nonconstant functions such that $\sabs{f}$ attains a (nonstrict) maximum $M$.
\end{exercise}
\end{exbox}

Real-analytic CR functions on a real-analytic
hypersurface $M$ always extend to holomorphic functions of a neighborhood of $M$.
To prove this fact, we complexify everything, that is, we treat the
$z$s and $\bar{z}$s as separate variables.  The standard way of
writing a hypersurface as a graph is not as convenient for this setting, so
we prove that a real-analytic hypersurface is a
graph of a holomorphic function in the complexified variables restricted to
the diagonal.  That is,
using variables $(z,w)$,
we write $M$ as a graph of $\bar{w}$ over $z$, $\bar{z}$, and $w$.
We can then eliminate $\bar{w}$ in any real-analytic
expression.

\begin{prop} \label{prop:complexificationofrasurface}
Suppose $M \subset \C^n$ is a real-analytic hypersurface and $p \in M$.
Then after a translation and rotation by a unitary matrix, $p=0$, and near
the origin in coordinates $(z,w) \in \C^{n-1} \times \C$,
the hypersurface $M$ is given by
\begin{equation*}
\bar{w} = \Phi(z,\bar{z},w) ,
\end{equation*}
where $\Phi(z,\zeta,w)$ is a holomorphic function defined on a neighborhood of the origin
in $\C^{n-1} \times \C^{n-1} \times \C$,
such that
$\Phi$,
$\frac{\partial \Phi}{\partial z_k}$,
$\frac{\partial \Phi}{\partial \zeta_k}$
vanish at the origin for all $k$,
and $w = \bar{\Phi}\bigl(\zeta,z,\Phi(z,\zeta,w)\bigr)$
for all $z$, $\zeta$, and $w$.

A local basis for $T^{(0,1)} M$ vector fields is given by
\begin{equation*}
\frac{\partial}{\partial \bar{z}_k}
+\frac{\partial \Phi}{\partial \bar{z}_k} \frac{\partial}{\partial \bar{w}}
\quad
\left(
=
\frac{\partial}{\partial \bar{z}_k}
+\frac{\partial \Phi}{\partial \zeta_k} \frac{\partial}{\partial \bar{w}}
\right)
,
\qquad k=1,\ldots,n-1.
\end{equation*}

Finally, let $\sM$ be the set in
$(z,\zeta,w,\omega) \in \C^{n-1} \times \C^{n-1} \times \C \times \C$
coordinates given near the origin
by $\omega = \Phi(z,\zeta,w)$.
Then $\sM$ is the unique \emph{complexification}
of $M$\index{complexification of a real hypersurface}%
\index{hypersurface!complexification}
near the origin in the sense that if $f(z,\bar{z},w,\bar{w})$
is a real-analytic function vanishing on $M$ near the origin, then
$f(z,\zeta,w,\omega)$ vanishes on $\sM$ near the origin.
\end{prop}

Again as a slight abuse of notation, $\Phi$ refers to both the function
$\Phi(z,\zeta,w)$ and $\Phi(z,\bar{z},w)$.

\begin{proof}
Translate and rotate so that $M$ is given by
\begin{equation*}
\Im w = \varphi(z,\bar{z},\Re w) ,
\end{equation*}
where $\varphi$ is $O(2)$.
Write the defining function
as $r(z,\bar{z},w,\bar{w}) = -\frac{w-\bar{w}}{2i}
+\varphi\bigl(z,\bar{z},\frac{w+\bar{w}}{2}\bigr)$.
Complexifying, consider
$r(z,\zeta,w,\omega)$ as
a holomorphic function of $2n$ variables,
and let $\sM$ be the set defined by
$r(z,\zeta,w,\omega) = 0$.
The derivative of $r$ in
$\omega$ (that is $\bar{w}$) does not vanish near the origin.
Use the implicit function theorem for holomorphic functions to write $\sM$
near the origin as
\begin{equation*}
\omega = \Phi(z,\zeta,w) .
\end{equation*}
Restrict to the diagonal, $\bar{w} = \omega$ and $\bar{z}=\zeta$,
to get
$\bar{w} = \Phi(z,\bar{z},w)$.  This is order $2$ in the $z$ and the $\bar{z}$
since $\varphi$ is $O(2)$.

Because $r$ is real-valued, then
$r(z,\bar{z},w,\bar{w}) =
\overline{r(z,\bar{z},w,\bar{w})} = \bar{r}(\bar{z},z,\bar{w},w)$.
Complexify to obtain
$r(z,\zeta,w,\omega) =
\bar{r}(\zeta,z,\omega,w)$ for all $(z,\zeta,w,\omega)$ near the origin.
If $r(z,\zeta,w,\omega) = 0$,
then
\begin{equation*}
0 = \overline{r(z,\zeta,w,\omega)} =
\overline{\bar{r}(\zeta,z,\omega,w)} =
r(\bar{\zeta},\bar{z},\bar{\omega},\bar{w}) =0.
\end{equation*}
So,
$(z,\zeta,w,\omega) \in \sM$
if and only if
$(\bar{\zeta},\bar{z},\bar{\omega},\bar{w}) \in \sM$.
Near the origin,
$(z,\zeta,w,\omega) \in \sM$ if and only if
$\omega = \Phi(z,\zeta,w)$, and hence
if and only if
$\bar{w} = \Phi(\bar{\zeta},\bar{z},\bar{\omega})$.
Conjugating, we get that $\sM$ is also given by
\begin{equation*}
w = \bar{\Phi}(\zeta,z,\omega).
\end{equation*}
As $\bigl(z,\zeta,w,\Phi(z,\zeta,w)\bigr) \in \sM$, then
for all $z$, $\zeta$, and $w$,
\begin{equation*}
w = \bar{\Phi}\bigl(\zeta,z,\Phi(z,\zeta,w)\bigr).
\end{equation*}

The vector field
$X_k = \frac{\partial}{\partial \bar{z}_k}
+\frac{\partial \Phi}{\partial \bar{z}_k} \frac{\partial}{\partial \bar{w}}$
annihilates
the function $\Phi(z,\bar{z},w)-\bar{w}$, but that is not enough.
The vector field must annihilate a real defining function such as the
real part of $\Phi(z,\bar{z},w)-\bar{w}$.  So $X_k$ must also
annihilate the conjugate
$\bar{\Phi}(\bar{z},z,\bar{w})-w$, at least on $M$.  Compute, for $(z,w) \in M$,
\begin{equation*}
\begin{split}
X_k \bigl[\bar{\Phi}(\bar{z},z,\bar{w})-w\bigr]
&=
\frac{\partial \bar{\Phi}}{\partial \bar{z}_k}
%\Big|_
{(\bar{z},z,\bar{w})}
+
\frac{\partial \Phi}{\partial \bar{z}_k}
%\Big|_
{(z,\bar{z},w)}
\frac{\partial \bar{\Phi}}{\partial \bar{w}}
%\Big|_
{(\bar{z},z,\bar{w})}
\\
& =
\frac{\partial \bar{\Phi}}{\partial \bar{z}_k}
%\Big|_
{\bigl(\bar{z},z,\Phi(z,\bar{z},\bar{w})\bigr)}
+
\frac{\partial \Phi}{\partial \bar{z}_k}
%\Big|_
{(z,\bar{z},w)}
\frac{\partial \bar{\Phi}}{\partial \bar{w}}
%\Big|_
{\bigl(\bar{z},z,\Phi(z,\bar{z},\bar{w})\bigr)}
\\
& =
\frac{\partial}{\partial \bar{z}_k}
\Bigl[
\bar{\Phi}\bigl(\bar{z},z,\Phi(z,\bar{z},w)\bigr)
\Bigr]
=
\frac{\partial}{\partial \bar{z}_k}
\Bigl[
w
\Bigr]
= 0 .
\end{split}
\end{equation*}

The last claim of the proposition is left as an exercise.
\end{proof}

Why do we say the last claim in the proposition
proves the ``uniqueness'' of the complexification?
Suppose we defined a complexification $\sM'$ by another holomorphic
equation $f=0$.
By the claim, $\sM \subset \sM'$, at least near the origin.
If the derivative $df$ is nonzero at the origin, then
$f\bigl(z,\zeta,w,\Phi(z,\zeta,w)\bigr) = 0$ implies that
$\frac{\partial f}{\partial \omega}$ is nonzero at the origin.
Using the holomorphic implicit function theorem we can uniquely solve $f=0$
for $\omega$ near the origin, that unique solution is $\Phi$,
and hence $\sM' = \sM$ near the origin.

As an example, recall that the sphere (minus a point) in $\C^2$ is biholomorphic to the
hypersurface
given by $\Im w = \sabs{z}^2$.  That is, $\frac{w-\bar{w}}{2i} = z \bar{z}$.  Solving for
$\bar{w}$ and using $\zeta$ and $\omega$ obtains the equation for the
complexification $\omega = -2iz \zeta + w$.  Then
$\Phi(z,\zeta,w) =
-2iz \zeta + w$, and
$\bar{\Phi}(\zeta,z,\omega) = 2i\zeta z + \omega$.  Let us check
that $\Phi$ is the right sort of function:
$\bar{\Phi}\bigl(z,\zeta,\Phi(z,\zeta,w)\bigr)
=
2i\zeta z + (-2i z \zeta + w) = w$.  The CR vector field is
given by
$\frac{\partial}{\partial \bar{z}}
+2i z \frac{\partial}{\partial \bar{w}}$.

\begin{exbox}
\begin{exercise}
Finish the proof of the proposition:
Let $M\subset \C^n$ be a real-analytic hypersurface given by
$\bar{w} = \Phi(z,\bar{z},w)$ near the origin, as in the proposition.  Let
$f(z,\bar{z},w,\bar{w})$ be a real-analytic function such that $f=0$
on $M$.  Prove that the complexified $f(z,\zeta,w,\omega)$ vanishes on
$\sM$.
\end{exercise}

\begin{exercise}
In the proposition we only rotated and translated.  Sometimes the following
change of coordinates is also done.  Prove that one can change coordinates
(no longer linear) so that the $\Phi$ in the proposition is
such that $\Phi(z,0,w) = \Phi(0,\zeta,w) = w$ for all $z$, $\zeta$, and $w$.
These coordinates are called \emph{\myindex{normal coordinates}}.
\end{exercise}

\begin{exercise}
Suppose
$\Phi$ is a holomorphic function defined on a neighborhood of the origin
in $\C^{n-1} \times \C^{n-1} \times \C$.
\begin{exparts}
\item
Show that $\bar{w} = \Phi(z,\bar{z},w)$ defines a real-analytic hypersurface
near the origin if and only
$w = \bar{\Phi}\bigl(\zeta,z,\Phi(z,\zeta,w)\bigr)$
for all $z$, $\zeta$, and $w$.  Hint: One direction was proved already.
\item
As an example, show that $\bar{w} = z\bar{z}$ does not satisfy the
condition above, nor does it define a real hypersurface.
\end{exparts}
\end{exercise}
\end{exbox}

Let us prove that real-analytic CR functions on real-analytic
hypersurfaces are restrictions
of holomorphic functions.  To motivate the proof, consider
a real-analytic function $f$ on the circle $\sabs{z}^2 = z \bar{z} = 1$
($f$ is vacuously CR).  This $f$
is a restriction of a real-analytic function on a
neighborhood of the circle, that we write $f(z,\bar{z})$.
On the circle
$\bar{z} = \nicefrac{1}{z}$.  Thus,
$F(z) = f\bigl(z,\nicefrac{1}{z}\bigr)$ is a holomorphic function
defined on a neighborhood of the circle and
equal to $f$ on the circle.
Our strategy then is to solve for one of the barred variables via
\propref{prop:complexificationofrasurface} and hope
the CR conditions take care of the rest of the barred variables
in more than one dimension.

\begin{thm}[Severi] \label{thm:severi}\index{Severi's theorem}
Suppose $M \subset \C^n$ is a real-analytic hypersurface and $p \in M$.
For every real-analytic CR function $f \colon M \to \C$, there exists
a holomorphic function $F \in \sO(U)$ for a neighborhood $U$ of $p$
such that $F(q) = f(q)$ for all $q \in M \cap U$.
\end{thm}

\begin{proof}
Write $M$ near $p$ as $\bar{w} = \Phi(z,\bar{z},w)$.
Let $\sM$ be the set in the $2n$ variables $(z,w,\zeta,\omega)$ given by
$\omega = \Phi(z,\zeta,w)$.
Take $f$ and consider any real-analytic extension of $f$
to a neighborhood of $p$ and write it
$f(z,w,\bar{z},\bar{w})$.  Complexify\footnote{At this point
$f$ stands for three distinct objects:
the function on $M$, its real-analytic extension to
a neighborhood in $\C^n$, and its complexification
to a neighborhood of $(p,\bar{p})$ in $\C^n \times \C^n$.}
as before to
$f(z,w,\zeta,\omega)$.  On $\sM$ we have
$f(z,w,\zeta,\omega) = f\bigl(z,w,\zeta,\Phi(z,\zeta,w)\bigr)$.  Let
\begin{equation*}
F(z,w,\zeta) = f\bigl(z,w,\zeta,\Phi(z,\zeta,w)\bigr).
\end{equation*}
Clearly $F(z,w,\bar{z})$ equals $f$ on $M$.
As $f$ is a CR function, it is annihilated by
$\frac{\partial}{\partial \bar{z}_k}
+\frac{\partial \Phi}{\partial \bar{z}_k} \frac{\partial}{\partial
\bar{w}}$ on $M$.  So
\begin{equation*}
\frac{\partial F}{\partial \zeta_k}
+\frac{\partial \Phi}{\partial \zeta_k} \frac{\partial F}{\partial
\omega}
=
\frac{\partial F}{\partial \zeta_k} = 0
\end{equation*}
on $M \subset \sM$.  We have a real analytic function
$\frac{\partial F}{\partial \zeta_k}(z,w,\bar{z})$
that is zero on $M$, so
$\frac{\partial F}{\partial \zeta_k}(z,w,\zeta) = 0$
on $\sM$
(\propref{prop:complexificationofrasurface} again).
As $\frac{\partial F}{\partial \zeta_k}$ is a function only of
$z$, $w$, and $\zeta$ (and not of $\omega$),
$\frac{\partial F}{\partial \zeta_k} = 0$
for all
$(z,w,\zeta)$ in a neighborhood of the origin.  Consequently,
$F$ does not depend on $\zeta$, and
$F$ is actually a holomorphic function of $z$ and $w$ only
and $F = f$ on $M$.
\end{proof}

The most important place where we find CR functions that aren't necessarily
real-analytic is as boundary values of holomorphic functions.

\begin{prop} \label{prop:boundaryvaluesCR}
Suppose $U \subset \C^n$ is an open set with smooth boundary.  Suppose
$f \colon \widebar{U} \to \C$ is a smooth function, holomorphic on $U$.
Then $f|_{\partial U}$ is a smooth CR function.
\end{prop}

\begin{proof}
The function $f|_{\partial U}$ is clearly smooth.

Suppose $p \in \partial U$.
If $X_p \in T_p^{(0,1)} \partial U$ is such that
\begin{equation*}
X_p = \sum_{k=1}^n a_k \frac{\partial}{\partial \bar{z}_k} \Big|_p ,
\end{equation*}
then take a sequence $\{ q_\ell \}$ in $U$ that approaches $p$.  Consider
\begin{equation*}
X_{q_\ell} = \sum_{k=1}^n a_k \frac{\partial}{\partial \bar{z}_k}
\Big|_{q_\ell} .
\end{equation*}
Then $X_{q_\ell} f = 0$ for all $\ell$ and by continuity $X_p f = 0$.
\end{proof}

\pagebreak[2]
The boundary values of a holomorphic function define the function uniquely.
That is, if two holomorphic functions continuous up to the (smooth) boundary
are equal on an open set of the boundary, then they are equal in the domain:

\begin{prop} \label{prop:boundaryvaluesdeterminef}
Suppose $U \subset \C^n$ is a domain with smooth boundary and $f \colon
\widebar{U} \to \C$ is
a continuous function, holomorphic on $U$.  If $f=0$ on a nonempty open subset of $\partial
U$, then $f=0$ on all of $U$.
\end{prop}

\begin{proof}
Take $p \in \partial U$ such that $f=0$ on a neighborhood of $p$ in
$\partial U$.  Consider a small neighborhood $\Delta$ of $p$ such
that $f$ is zero on $\partial U \cap \Delta$.  Define $g \colon \Delta \to
\C$ by setting $g(z) = f(z)$ if $z \in U$ and $g(z) = 0$ otherwise.
See \figureref{fig:zero-onbound}.
It is not hard to see that $g$ is continuous, and it is clearly holomorphic
where it is not zero.  Rad{\'o}'s theorem
(\thmref{thm:rado}) says that $g$ is holomorphic, and as it is zero on a
nonempty open subset of $\Delta$, it is identically zero on $\Delta$,
meaning $f$ is zero on a nonempty open subset of $U$, and we are done by
identity.

\begin{myfig}
\subimport*{figures/}{zero-onbound.pdf_t}
\caption{Extending a function zero on the boundary.\label{fig:zero-onbound}}
\end{myfig}
\end{proof}

\begin{exbox}
\begin{exercise}
Find a domain $U \subset \C^n$, $n \geq 2$, with smooth boundary and a smooth
CR function $f \colon \partial U \to \C$ such that there is no holomorphic function
on $U$ or $\C^n \setminus U$ continuous up to the boundary and whose boundary values are $f$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Suppose $U \subset \C^n$ is a bounded open set with smooth boundary,
$f \colon \widebar{U} \to \C$ is a continuous function, holomorphic in
$U$, and $f|_{\partial U}$ is real-valued.  Show that $f$ is
constant.
\item
Find a counterexample to the statement if you allow $U$
to be unbounded.
\end{exparts}
\end{exercise}

\begin{exercise}
Find a smooth CR function on the sphere $S^{2n-1} \subset \C^n$ that is not
a restriction of a holomorphic function of a neighborhood of $S^{2n-1}$.
\end{exercise}

\begin{exercise}
Show a global version of Severi.  Given a real-analytic hypersurface $M
\subset \C^n$ and a real-analytic CR function $f \colon M \to \C$,
show that there exists a neighborhood $U$ of $M$, and an $F \in \sO(U)$
such that $F|_U = f$.
\end{exercise}
\end{exbox}

A problem we tackle next is to try to extend
a smooth CR function from the boundary of a domain to a holomorphic
function inside.  This is a PDE problem where the PDE are
the Cauchy--Riemann equations, and the function on the boundary is
the boundary condition.
Cauchy--Riemann equations are
\emph{\myindex{overdetermined}}, that is, there are too many
equations.  Not every data on the boundary gives a solution.
\propref{prop:boundaryvaluesCR} says that the data being CR is a necessary condition
for a solution (it is not sufficient in general).
\propref{prop:boundaryvaluesdeterminef} says the solution is unique
if it exists.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Approximation of CR functions}

The following theorem (proved circa 1980) holds in much more generality, but
we state its simplest version.  One of the simplifications we make is that
we consider only smooth CR functions here, although the theorem holds even
for continuous CR functions where the CR conditions are interpreted in the
sense of distributions.

\begin{thm}[Baouendi--Tr{\`e}ves]%
\pagebreak[2]
\index{Baouendi--Tr{\`e}ves approximation theorem}
Suppose $M \subset \C^n$ is a smooth real hypersurface,
$p \in M$ is a point,
and
$z=(z_1,\dots,z_n)$ are the holomorphic coordinates of $\C^n$.
Then there exists a compact
neighborhood $K \subset M$ of $p$, such that for every smooth CR function $f \colon M \to \C$,
there exists a sequence $\{ p_\ell \}$ of polynomials in $z$ such that
\begin{equation*}
p_\ell(z) \to f(z)
\qquad \text{uniformly in $K$.}
\end{equation*}
\end{thm}

A key point is that $K$ cannot be chosen arbitrarily, it depends on $p$ and $M$.
On the other hand, $K$ does not depend on $f$.  Given $M$ and $p \in M$
there is a $K$ such that \emph{every} CR function on $M$ is approximated
uniformly on $K$ by holomorphic polynomials.  The theorem applies in one
dimension, although in that case the theorem of Mergelyan
(see \thmref{thm:mergelyan}) is much more general.

\begin{example}
Let us show that $K$ cannot possibly be arbitrary.  For simplicity $n=1$.
Let $S^1 \subset \C$ be the unit circle (boundary of the disc),
then every smooth function on $S^1$ is a smooth CR function.  Let $f$
be a nonconstant real function such as $\Re z$.  Suppose for
contradiction that we could take $K = S^1$ in the theorem.  Then $f(z) = \Re
z$ could be uniformly
approximated on $S^1$ by holomorphic polynomials.
By the maximum principle, the
polynomials would converge on $\D$ to a holomorphic function on $\D$
continuous on $\overline{\D}$.  This function would have nonconstant real boundary values,
which is impossible.  Clearly $K$ cannot be the entire circle.

The example is easily extended to $\C^n$
by considering
$M = S^1 \times \C^{n-1}$, then $\Re z_1$ is a smooth CR function on $M$ that cannot be
approximated uniformly on $S^1 \times \{ 0 \}$ by holomorphic polynomials.
\end{example}

The technique of the example above will be used later in a more general
situation, to extend CR functions using Baouendi--Tr{\`e}ves.

\begin{remark}
It is important to note the difference
between Baouendi--Tr{\`e}ves (and similar theorems
in complex analysis)
and the Weierstrass approximation theorem.  In Baouendi--Tr{\`e}ves we obtain
an approximation by holomorphic polynomials, while Weierstrass gives us
polynomials in the real variables, or in $z$ and $\bar{z}$.  For example,
via Weierstrass, every continuous function is uniformly approximable on $S^1$ via polynomials
in $\Re z$ and $\Im z$, and therefore by polynomials in $z$ and $\bar{z}$.
These polynomials do not in general converge anywhere but on $S^1$.
\end{remark}

\begin{exbox}
\begin{exercise}
Let $z=x+iy$ as usual in $\C$.
Find a sequence of polynomials in $x$ and $y$ that converge uniformly to $e^{x-y}$ on $S^1$,
but diverge everywhere else.
\end{exercise}
\end{exbox}


The proof is an ingenious
use of the standard technique used to prove the Weierstrass approximation
theorem.  Also, as we have seen mollifiers before, the technique will not be
completely foreign even to the reader who does not know the Weierstrass
approximation theorem.  Basically what we do is use the standard convolution
argument, this time against a holomorphic function.  Letting $z=x+iy$
we only do the convolution in the
$x$ variables keeping $y=0$.  Then we use the fact that the
function is CR to show that we get an approximation even for other $y$.

In the formulas below, given a vector $v = (v_1,\ldots,v_n)$,
it will be useful to write
\glsadd{not:bracketsquare}%
\begin{equation*}
[v]^2 \overset{\text{def}}{=} v_1^2 + \cdots + v_n^2 .
\end{equation*}

The following lemma is a neat application of ideas from several complex
variables to solve a problem that does not at first seems to involve
holomorphic functions.

\begin{lemma} \label{lemma:matrixint}
Let $W$ be the set of $n \times n$ complex matrices $A$ such that
\begin{equation*}
\snorm{(\Im A)x} < \snorm{(\Re A)x}
\end{equation*}
for all nonzero $x \in \R^n$ and $\Re A$ is positive definite.
Then for all $A \in W$,
\begin{equation*}
\int_{\R^n} e^{-{[Ax]}^2} \det A \, dx  = \pi^{n/2} .
\end{equation*}
\end{lemma}

\begin{proof}
Suppose $A$ has real entries and $A$ is positive definite (so
$A$ is also invertible).  By a
change of coordinates
\begin{equation*}
\int_{\R^n} e^{-{[Ax]}^2} \det A \, dx  =
\int_{\R^n} e^{-{[x]}^2} \, dx  =
\left(\int_\R e^{-x_1^2} \, dx_1 \right)
\cdots
\left(\int_\R e^{-x_n^2} \, dx_n \right)
=
{(\sqrt{\pi})}^n .
\end{equation*}
Next suppose $A$ is any matrix in $W$.
There is some $\epsilon > 0$
such that $\snorm{(\Im A) x}^2 \leq
(1-\epsilon^2) \snorm{(\Re A) x}^2$ for all $x \in \R^n$.  That is because
we only need to check this for $x$ in the unit sphere, which is compact
(exercise).  By reality of $\Re A$, $\Im A$, and $x$
we get
${[(\Re A)x]}^2 = \snorm{(\Re A)x}^2$ and
${[(\Im A)x]}^2 = \snorm{(\Im A)x}^2$.  So
\begin{equation*}
\abs{e^{-{[Ax]}^2}}
=
e^{-\Re {[Ax]}^2}
\leq
e^{-{[(\Re A)x]}^2 + {[(\Im A)x]}^2}
\leq
e^{-\epsilon^2 {[(\Re A)x]}^2} .
\end{equation*}
Therefore, the integral exists for all $A$ in $W$ by a similar computation as
above.

The expression
\begin{equation*}
\int_{\R^n} e^{-{[Ax]}^2} \det A \, dx
\end{equation*}
is a well-defined holomorphic function in the entries of $A$, thinking of
$W$ as a domain (see exercises below) in $\C^{n^2}$.  We have a holomorphic function that is
constantly equal to $\pi^{n/2}$ on $W \cap \R^{n^2}$ and hence it is equal
to $\pi^{n/2}$ everywhere on $W$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the existence of $\epsilon > 0$ in the proof above.
\end{exercise}

\begin{exercise}
Show that $W \subset \C^{n^2}$ in the proof above is a domain (open and connected).
\end{exercise}

\begin{exercise}
Prove that we can really differentiate under the integral to show that the
integral is holomorphic in the entries of $A$.
\end{exercise}

\begin{exercise}
Show that some hypotheses are needed for the lemma.  In particular, take
$n=1$ and find the exact set of $A$ (now a complex number) for which
the conclusion of the lemma is true.
\end{exercise}
\end{exbox}

Given an $n \times n$ matrix $A$, let $\snorm{A}$ denote the operator norm,
\begin{equation*}
\snorm{A} = \sup_{\snorm{v}=1} \snorm{Av} = \sup_{v \in \C^n, v\not= 0}
\frac{\snorm{Av}}{\snorm{v}} .
\end{equation*}

\begin{exbox}
\begin{exercise}
Let $W$ be as in \lemmaref{lemma:matrixint}.  Let $B$ be an $n \times n$
real matrix such that $\snorm{B} < 1$.   Show that $I + iB \in W$.
\end{exercise}
\end{exbox}

We will be using differential forms, and the following lemma says
that as far as the exterior
derivative is concerned, all CR functions behave as
restrictions of holomorphic functions.

\begin{lemma} \label{lemma:crdf}
Let $M \subset \C^n$ be a smooth real hypersurface, $f \colon M \to \C$
be a smooth CR function, and $(z_1,\ldots,z_n)$ be the holomorphic
coordinates of $\C^n$.  Then at each point $p \in M$,
the exterior derivative
$df$ is a linear combination of $dz_1,\ldots,dz_n$,
thinking of $z_1,\ldots,z_n$ as functions on $M$.\linebreak[1]
Namely,
\begin{equation*}
d(f \, dz) = df \wedge dz = 0.
\end{equation*}
\end{lemma}

Recall the notation $dz = dz_1 \wedge dz_2 \wedge \cdots \wedge dz_n$.

\begin{proof}
After a complex affine change of coordinates, we simply
need to prove the result at the origin.  Let
$\xi_1,\ldots,\xi_n$ be
the new holomorphic coordinates
and suppose the $T^{(1,0)}_0 M$ tangent space is spanned
by
$\frac{\partial}{\partial \xi_1}\big|_0,
\ldots,
\frac{\partial}{\partial \xi_{n-1}}\big|_0$,
and such that $\frac{\partial}{\partial \Re \xi_n}\big|_0$ is tangent
and $\frac{\partial}{\partial \Im \xi_n}\big|_0$ is normal.
At the origin, the CR conditions are
$\frac{\partial f}{\partial \bar{\xi}_k}(0) = 0$ for all $k$, so
\begin{equation*}
df(0) =
\frac{\partial f}{\partial \xi_1}(0) \, d\xi_1(0) + \cdots +
\frac{\partial f}{\partial \xi_{n-1}}(0) \, d\xi_{n-1}(0)  +
\frac{\partial f}{\partial \Re \xi_{n}}(0) \, d(\Re \xi_{n})(0)
.
\end{equation*}
Also, at the origin $d\xi_n(0) = d(\Re \xi_n)(0) + i d(\Im \xi_n)(0) = d(\Re \xi_n)(0)$.
So $df(0)$ is a linear combination of $d\xi_1(0),\ldots,d\xi_n(0)$.
As $\xi$ is a complex affine function of $z$, then each $d\xi_k$ is a linear
combination of $dz_1$ through $dz_n$, and the claim follows.
So if $f$ is a CR function, then
$d(f\,dz) = df \wedge dz = 0$ since $dz_k \wedge dz_k = 0$.
\end{proof}

\begin{proof}[Proof of the theorem of Baouendi--Tr{\`e}ves]
Suppose $M \subset \C^n$ is a smooth real hypersurface, and without loss
of generality suppose $p=0 \in M$.
Let $z=(z_1,\ldots,z_n)$ be the holomorphic coordinates, write $z=x+iy$,
$y=(y',y_n)$, and
suppose $M$ is given by
\begin{equation*}
y_n = \psi(x,y') ,
\end{equation*}
where $\psi$ is $O(2)$.
The variables $(x,y')$ parametrize $M$ near $0$:
\begin{equation*}
z_k = x_k+iy_k , \quad \text{ for $k = 1,\ldots,n-1$,} \quad \text{and} \quad
z_n = x_n + i \psi(x,y') .
\end{equation*}
Define
\begin{equation*}
\varphi(x,y') = \bigl(y_1,\ldots,y_{n-1},\psi(x,y')\bigr) .
\end{equation*}
Write $(x,y') \mapsto z = x + i\varphi(x,y')$ as the parametrization.
That is, think of $z$ as a function of $(x,y')$.

Let $r > 0$ and $d > 0$ be small numbers to be determined later.
Assume they are small enough so
that $f$ and $\varphi$ are defined and smooth on some neighborhood of the
set where $\snorm{x} \leq r$ and $\snorm{y'} \leq d$.
There exists a smooth $g \colon \R^n \to [0,1]$ such that $g \equiv 1$ on
$B_{r/2}(0)$ and $g \equiv 0$ outside of $B_{r}(0)$.
See \figureref{fig:cutoff-bt}.
Explicit formula
can be given.  Alternatively we obtain such a $g$ by use of
mollifiers on a function that is identically one on
$B_{3r/4}(0)$ and zero elsewhere.  Such a $g$ is commonly called a
\emph{\myindex{cutoff function}}.

\begin{myfig}
\subimport*{figures/}{cutoff-bt.pdf_t}
\caption{Cutoff function.\label{fig:cutoff-bt}}
\end{myfig}

\begin{exbox}
\begin{exercise}
Find an explicit formula for $g$ without using mollifiers.
\end{exercise}
\end{exbox}

Let
\begin{equation*}
K' = \bigl\{ (x,y') : \snorm{x} \leq \nicefrac{r}{4} , \snorm{y'} \leq d
\bigr\} .
\end{equation*}
Let $K = z(K')$, that is the image of $K'$ under the mapping $z(x,y')$.

Consider the CR function $f$ a function of $(x,y')$
and write $f(x,y')$.
For $\ell \in \N$,
let $\alpha_{\ell}$ be a differential $n$-form defined (thinking
of $w \in \C^n$ as a constant parameter) by
\begin{equation*}
\begin{split}
\alpha_{\ell}(x,y')
& =
{\left(\frac{\ell}{\pi}\right)}^{n/2}
e^{-\ell [w - z]^2} g(x) f(x,y')
\,
dz
\\
& =
{\left(\frac{\ell}{\pi}\right)}^{n/2}
e^{-\ell [w - x-i\varphi(x,y')]^2} g(x) f(x,y')
\\
& \qquad \qquad
(dx_1 + idy_1)  \wedge
\cdots \wedge
(dx_{n-1} + i dy_{n-1})
\wedge
\bigl(dx_{n} + i d \psi (x,y') \bigr) .
\end{split}
\end{equation*}

The key is the exponential, which looks like the bump function
mollifier, except that now we have $w$ and $z$
possibly complex.  The exponential is also holomorphic in $w$, and that will
give us entire holomorphic approximating functions.

Fix $y'$ with $0 < \snorm{y'} < d$ and let $D$ be defined by
\begin{equation*}
D = \bigl\{ (x,s) \in \R^n \times \R^{n-1} : \snorm{x} < r \text{ and } s = t y' \text{ for
$t \in (0,1)$} \bigr\} .
\end{equation*}
$D$ is an $(n+1)$-dimensional ``cylinder.''  We take a ball in the
$x$ directions, then take a single fixed point $y'$ in the $s$ variables and make a
cylinder.  See \figureref{fig:cylinder-bt}.

\begin{myfig}
\subimport*{figures/}{cylinder-bt.pdf_t}
\caption{Cylinder $D$.\label{fig:cylinder-bt}}
\end{myfig}

Orient $D$ in
the standard way as if it sat in the $(x,t)$ variables in $\R^n \times \R$.
Stokes' theorem says
\begin{equation*}
\int_D d \alpha_{\ell} (x,s)
=
\int_{\partial D} \alpha_{\ell} (x,s) .
\end{equation*}
Since $g(x) = 0$ if $\snorm{x} \geq r$, $\alpha_\ell$ is zero on the sides
of the cylinder $D$, so the integral over $\partial D$ only needs to
consider the top and bottom of the cylinder.
And because of $g$, the integral over the top and bottom
can be taken over $\R^n$.
As is usual in these sorts of arguments, we do the slight abuse of notation
ignoring that $f$ and $\varphi$ are undefined
where $g$ is identically zero:
\begin{multline} \label{eq:BTstokes}
\int_{\partial D} \alpha_{\ell}(x,s)
\\
\begin{aligned}
=
&
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
\!\!\!
e^{-\ell [w - x-i\varphi(x,y')]^2 } g(x) f(x,y')
\,
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,y') \bigr)
\\
&
-
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
\!\!\!
e^{  -\ell [w - x-i\varphi(x,0)]^2 } g(x) f(x,0)
\,
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,0) \bigr) ,
\end{aligned}
\end{multline}
where $d_x$ means the derivative in the $x$ directions only.
I.e., $d_x \psi =
\frac{\partial \psi}{\partial x_1} dx_1
+ \cdots +
\frac{\partial \psi}{\partial x_n} dx_n$.


We will show that as $\ell \to \infty$, the left-hand side of \eqref{eq:BTstokes}
goes to zero uniformly for $w \in K$
and the first term on the right-hand side goes to $f(\tilde{x},y')$
if $w = z(\tilde{x},y')$ is in $M$.  Hence,
we define entire functions that we will show approximate $f$:
\begin{equation*}
f_\ell(w)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
e^{  -\ell [w - x-i\varphi(x,0)]^2 } g(x) f(x,0)
\,
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,0) \bigr) .
\end{equation*}
Clearly each $f_\ell$ is holomorphic and defined for all $w \in \C^n$.

In the next claim it is important that $f$ is a CR function.

\begin{claim}
\pagebreak[2]
We have
\begin{equation*}
%\int_D
d \alpha_\ell(x,s)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
%\int_D
e^{-\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x)
\wedge
dz(x,s) ,
\end{equation*}
and for sufficiently small $r>0$ and $d>0$,
\begin{equation*}
\lim_{\ell\to\infty}
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{(x,s)\in D}
e^{-\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x)
\wedge
dz(x,s)
= 0
\end{equation*}
uniformly as a function of $w \in K$ and $y' \in B_d(0)$ (recall that $D$ depends on
$y'$).
\end{claim}

\begin{proof}
The function
$(x,s) \mapsto e^{-\ell [w - z(x,s)]^2}$ is CR (as a function on $M$), and so
is $f(x,s)$.  Therefore, using \lemmaref{lemma:crdf},
\begin{equation*}
d \alpha_{\ell}(x,s)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
e^{-\ell [w - z(x,s)]^2 } f(x,s)
\,
dg(x)
\wedge
dz(x,s) .
\end{equation*}
Since $dg$ is zero for $\snorm{x} \leq \nicefrac{r}{2}$, the integral
\begin{equation*}
\int_D
d \alpha_\ell(x,s)
=
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_D
e^{ -\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x)
\wedge
dz(x,s)
\end{equation*}
is only evaluated for the subset of $D$ where $\snorm{x} > \nicefrac{r}{2}$.

Suppose $w \in K$ and $(x,s) \in D$ with $\snorm{x} > \nicefrac{r}{2}$.
Let $w = z(\tilde{x},\tilde{s})$.
We need to estimate
\begin{equation*}
\babs{e^{ -\ell {[w - z(x,s)]}^2 }} =
e^{ -\ell \Re {[w - z(x,s)]}^2 } .
\end{equation*}
Then
\begin{equation*}
-\Re {[w - z]}^2 =
-\snorm{\tilde{x}-x}^2
+
\snorm{\varphi(\tilde{x},\tilde{s})-\varphi(x,s)}^2 .
\end{equation*}
By the mean value theorem
\begin{equation*}
\snorm{\varphi(\tilde{x},\tilde{s})-\varphi(x,s)}
\leq
\snorm{\varphi(\tilde{x},\tilde{s})-\varphi(x,\tilde{s})}
+
\snorm{\varphi(x,\tilde{s})-\varphi(x,s)}
\leq
a \snorm{\tilde{x}-x}
+
A \snorm{\tilde{s}-s} ,
\end{equation*}
where $a$ and $A$ are
\begin{equation*}
a = \sup_{\snorm{\hat{x}} \leq r, \snorm{\hat{y}'} \leq d}
\norm{\,\left[\frac{\partial \varphi}{\partial x}(\hat{x},\hat{y}')\right]\,},
\qquad
A = \sup_{\snorm{\hat{x}} \leq r, \snorm{\hat{y}'} \leq d}
\norm{\,\left[\frac{\partial \varphi}{\partial y'}(\hat{x},\hat{y}'))\right]\,}.
\end{equation*}
Here $\bigl[ \frac{\partial \varphi}{\partial x} \bigr]$ and
$\bigl[ \frac{\partial \varphi}{\partial y'} \bigr]$ are
the derivatives (matrices) of $\varphi$ with respect to $x$ and $y'$
respectively, and the norm we are taking is the operator norm.
Because $\bigl[ \frac{\partial \varphi}{\partial x} \bigr]$ is zero
at the origin, we pick $r$ and $d$ small
enough (and hence $K$ small enough) so that $a \leq \nicefrac{1}{4}$.
We furthermore pick $d$ possibly even smaller to ensure
that $d \leq \frac{r}{32A}$.  We have that $\nicefrac{r}{2} \leq \snorm{x} \leq
r$, but $\snorm{\tilde{x}} \leq \nicefrac{r}{4}$ (recall $w \in K$), so
\begin{equation*}
\frac{r}{4} \leq \snorm{\tilde{x}-x} \leq \frac{5r}{4} .
\end{equation*}
Also, $\snorm{\tilde{s}-s} \leq 2d$ by triangle inequality.

Therefore,
\begin{equation*}
\begin{split}
-\Re {[w - z(x,s)]}^2 & \leq
- \snorm{\tilde{x}-x}^2
+
a^2 \snorm{\tilde{x}-x}^2
+
A^2 \snorm{\tilde{s}-s}^2
+
2aA \snorm{\tilde{x}-x}\snorm{\tilde{s}-s}
\\
& \leq
\frac{-15}{16} \snorm{\tilde{x}-x}^2
+
A^2 \snorm{\tilde{s}-s}^2
+
\frac{A}{2} \snorm{\tilde{x}-x}\snorm{\tilde{s}-s}
\\
& \leq \frac{-r^2}{64} .
%(
%(-15/16)(1/16)
%+
%(2^2)/(2^2 * 16^2)
%+
%(1/2)(5/4)2(1/(2*16))
%) r^2
\end{split}
\end{equation*}
In other words,
\begin{equation*}
\babs{
e^{-\ell[w-z(x,s)]^2}}
\leq
e^{-\ell r^2  / 64} ,
\end{equation*}
or
\begin{equation*}
\abs{
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{(x,s)\in D}
e^{-\ell [w - z(x,s)]^2} f(x,s)
\,
dg(x)
\wedge
dz(x,s)
}
\leq
C
\ell^{n/2}
e^{-\ell r^2  / 64} ,
\end{equation*}
for some constant $C$.  Note that $D$ depends on $y'$.  The set of
all $y'$ with $\snorm{y'} \leq d$,
is a compact set, so we can make $C$
large enough to not depend on the $y'$ that was chosen.
The claim follows.
\end{proof}

\begin{claim}
For the given $r>0$ and $d>0$,
\begin{multline*}
\lim_{\ell\to\infty}
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
e^{  -\ell [\tilde{x}+i\varphi(\tilde{x},y') - x-i\varphi(x,y')]^2 }
\\
g(x) f(x,y')
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,y') \bigr)
%\\
= f(\tilde{x},y')
\end{multline*}
uniformly in $(\tilde{x},y') \in K'$.
\end{claim}

That is, we look at \eqref{eq:BTstokes} and we plug in $w = z(\tilde{x},y') \in K$.
The $g$ (as usual) makes sure we never evaluate $f$, $\psi$, or
$\varphi$ at
points where they are not defined.

\begin{proof}
The change of variables formula implies
\begin{equation*}
dx_1  \wedge
\cdots \wedge
dx_{n-1}
\wedge
\bigl(dx_{n} + i d_x \psi (x,y') \bigr)
=
d_x z(x,y')
=
\det \left[\frac{\partial z}{\partial x}(x,y')\right] dx ,
\end{equation*}
where $\bigl[\frac{\partial z}{\partial x}(x,y')\bigr]$ is the matrix
corresponding to the derivative of the mapping $z$ with respect to the $x$
variables evaluated at $(x,y')$.

Let us change variables of integration via $\xi = \sqrt{\ell} ( x-\tilde{x})$:
\begin{multline*}
{\left(\frac{\ell}{\pi}\right)}^{n/2}
\int_{x \in \R^n}
e^{  -\ell [\tilde{x}+i\varphi(\tilde{x},y') - x-i\varphi(x,y')]^2 } g(x) f(x,y')
\det \left[\frac{\partial z}{\partial x}(x,y')\right] dx
=
\\
{\left(\frac{1}{\pi}\right)}^{n/2}
\int_{\xi \in \R^n}
e^{-{\left[\xi +
i\sqrt{\ell}\left(\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')\right)\right  ]}^2}
\\
g\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}}\right)
f\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right)
\det \left[\frac{\partial z}{\partial
x}\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right)\right] d\xi .
\end{multline*}
We now wish to take a limit as $\ell \to \infty$ and for this we apply
the dominated convergence theorem.
So we need to dominate the integrand.
The second half of the integrand is uniformly bounded independent of
$\ell$ as
\begin{equation*}
x \mapsto g(x) f(x,y') \det \left[\frac{\partial z}{\partial x}(x,y')\right]
\end{equation*}
is a continuous function with compact support (because of $g$).
Hence it is enough to worry about the exponential term.
We also only consider those $\xi$ where the integrand is not zero.
Recall that $r$ and $d$ are small enough that
\begin{equation*}
\sup_{\snorm{\hat{x}} \leq r, \snorm{\hat{y}'} \leq d}
\norm{\,\left[
\frac{ \partial \varphi}{\partial  x}(\hat{x},\hat{y}')
\right]\,} \leq \frac{1}{4} ,
\end{equation*}
and as $\snorm{\tilde{x}} \leq \nicefrac{r}{4}$ (as
$(\tilde{x},y') \in K$) and
$\norm{\tilde{x}+\frac{\xi}{\sqrt{\ell}}} \leq r$ (because $g$ is
zero otherwise), then
\begin{equation*}
\norm{\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')}
\leq \frac{1}{4} \norm{\tilde{x}+\frac{\xi}{\sqrt{\ell}}-\tilde{x}} =
\frac{\snorm{\xi}}{4 \sqrt{\ell}} .
\end{equation*}

So under the same conditions,
\begin{equation*}
\begin{split}
\bbabs{e^{-{\left[\xi +
i\sqrt{\ell}\left(\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')\right)\right]}^2}}
& =
e^{-\Re {\left[\xi +
i\sqrt{\ell}\left(\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')\right)\right]}^2}
\\
& =
e^{-\snorm{\xi}^2 + \ell
\norm{\varphi\left(\tilde{x}+\frac{\xi}{\sqrt{\ell}},y'\right) -
\varphi(\tilde{x},y')}^2}
\\
& \leq
e^{-(15/16)\snorm{\xi}^2} .
\end{split}
\end{equation*}
And that is integrable.  Thus, take the pointwise limit under the integral to obtain
\begin{equation*}
{\left(\frac{1}{\pi}\right)}^{n/2}
\int_{\xi \in \R^n}
e^{-{\left[\xi + i\left[ \frac{\partial \varphi}{\partial x}(\tilde{x},y') \right] \xi \right]}^2}
g(\tilde{x})
f(\tilde{x},y')
\det \left[\frac{\partial z}{\partial
x}(\tilde{x},y')\right] d\xi .
\end{equation*}
In the exponent, we have an expression for the derivative
in the $\xi$ direction with $y'$ fixed.  If $(\tilde{x},y') \in K'$, then
$g(\tilde{x}) = 1$, and so we can ignore $g$.

Let $A = I + i \bigl[ \frac{\partial \varphi}{\partial x}(\tilde{x},y')
\bigr]$.  \lemmaref{lemma:matrixint} says
\begin{equation*}
{\left(\frac{1}{\pi}\right)}^{n/2}
\int_{\xi \in \R^n}
e^{-{\left[\xi + i\left[ \frac{\partial \varphi}{\partial x}(\tilde{x},y') \right] \xi \right]}^2}
f(\tilde{x},y')
\det \left[\frac{\partial z}{\partial
x}(\tilde{x},y')\right] d\xi  = f(\tilde{x},y') .
\end{equation*}

That the convergence is uniform in
$(\tilde{x},y') \in K'$ is left as an exercise.
\end{proof}

\begin{exbox}
\begin{exercise}
In the claim above, finish the proof that the convergence is
uniform in $(\tilde{x},y') \in K'$.
Hint: It may be easier to
use the form of the integral before the change of variables
and prove that the sequence is uniformly Cauchy.
\end{exercise}
\end{exbox}

We are essentially done with the proof of the theorem.
The two claims together with \eqref{eq:BTstokes} show that $f_\ell$ are entire
holomorphic functions that approximate $f$ uniformly on $K$.  Entire holomorphic
functions can be approximated by polynomials uniformly on compact subsets;
simply take the partial sums of Taylor series at the origin.
\end{proof}

\begin{exbox}
\begin{exercise}
Explain why being approximable on $K$ by (holomorphic) polynomials does not
necessarily mean that
$f$ is real-analytic.
\end{exercise}

\begin{exercise}
Suppose $M \subset \C^n$ is given by $\Im z_n = 0$.  Use the standard
Weierstrass approximation theorem to show that given a $K \subset \subset M$,
and a smooth CR function $f \colon M \to \C$, then $f$ can be uniformly approximated
by holomorphic polynomials on $K$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extension of CR functions}

We will apply the so-called ``technique of analytic discs'' together
with
Baouendi--Tr{\`e}ves to prove the
Lewy extension theorem.  Lewy's original proof was different
and predates Baouendi--Tr{\`e}ves.  A local extension theorem of this type
was first proved by Helmut Knesser in 1936.

\begin{thm}[Lewy]%
\index{Lewy extension theorem}
Suppose $M \subset \C^n$ is a smooth real hypersurface and $p \in M$.
There exists a neighborhood $U$ of $p$ with the following
property.
Suppose $r \colon U \to \R$ is
a smooth defining function for $M \cap U$, denote by $U_- \subset U$ the set where $r$
is negative and $U_+ \subset U$ the set where $r$ is positive.
Let $f \colon M \to \C$ be a smooth CR function.
Then:

\begin{enumerate}[(i)]
\item
If the Levi form with respect to $r$ has a positive eigenvalue at $p$, then
$f$ extends to a holomorphic function on $U_-$ continuous up to $M$
(continuous on $\{ z \in U : r(z) \leq 0 \}$).
\item
If the Levi form with respect to $r$ has a negative eigenvalue at $p$, then
$f$ extends to a holomorphic function on $U_+$ continuous up to $M$
(continuous on $\{ z \in U : r(z) \geq 0 \}$).
\item
If the Levi form with respect to $r$ has eigenvalues of both signs at $p$, then
$f$ extends to a function holomorphic on $U$.
\end{enumerate}
\end{thm}

So if the Levi form has eigenvalues of both signs,
then near $p$ all CR functions are restrictions of holomorphic
functions.
The function $r$ can be any defining
function for $M$.  Either we can extend it to all of $U$ or we could take a
smaller $U$ such that $r$ is defined on $U$.  As we noticed before,
once we pick sides (where $r$ is positive and where it is negative), then
the number of positive eigenvalues and the number of negative eigenvalues of
the Levi form is fixed.  A different $r$ may flip $U_-$
and $U_+$, but the conclusion of the theorem is exactly the same.

\begin{proof}
We prove the first item, and the second item follows by considering $-r$.
Suppose $p = 0$ and $M$ is given in some neighborhood
$\Omega$ of the origin as
\begin{equation*}
\Im w = \sabs{z_1}^2 + \sum_{k=2}^{n-1} \epsilon_k \sabs{z_k}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) ,
\end{equation*}
where $z' = (z_2,\ldots,z_{n-1})$, $\epsilon_k = -1,0,1$,
and $E$ is $O(3)$.
Let $\Omega_-$ be given by
\begin{equation*}
0 > r = \sabs{z_1}^2 + \sum_{k=2}^{n-1} \epsilon_k \sabs{z_k}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) - \Im w .
\end{equation*}
The (real) Hessian of the function (of one variable)
\begin{equation*}
z_1 \mapsto \sabs{z_1}^2 +
E(z_1,0,\bar{z}_1,0,0)
\end{equation*}
is positive definite in a
neighborhood of the origin and the function has a strict minimum at $0$.
There is some small disc $D \subset \C$ such
that this function is strictly positive on $\partial D$.

Therefore, for $(z',w) \in W$ in some
small neighborhood $W \subset \C^{n-1}$ of the origin,
the function
\begin{equation*}
z_1 \mapsto \sabs{z_1}^2 + \sum_{k=2}^n \epsilon_k \sabs{z_k}^2 +
E(z_1,z',\bar{z}_1,\bar{z}',\Re w) - \Im w
\end{equation*}
is still strictly positive on $\partial D$.

We wish to apply
Baouendi--Tr{\`e}ves and so let $K$ be the compact neighborhood of the
origin from the theorem.  Take $D$ and $W$ small enough such
that $(D \times W) \cap M \subset K$.
Find the polynomials $p_\ell$ that approximate $f$ uniformly on $K$.
Consider $z_1 \in D$ and $(z',w) \in W$ such that
$(z_1,z',w) \in \Omega_-$.
Let
$\Delta = \bigl( D \times \{ (z',w) \} \bigr) \cap \Omega_-$.
Denote by $\partial \Delta$ the boundary of $\Delta$ in the subspace topology
of $\C \times \{ (z',w) \}$.

The set $\Omega_+$ where $r > 0$ is open and it contains
$(\partial D) \times \{ (z',w) \}$.  Therefore,
$\partial \Delta$ contains no points of $(\partial D) \times \{ (z',w) \}$.
Consequently, $\partial \Delta$ contains only points where
$r = 0$, that is $\partial \Delta \subset M$, and
also
$\partial \Delta \subset D \times W$.
As $(D \times W) \cap M \subset K$, we have $\partial \Delta \subset K$.
See \figureref{fig:lewy-extension-figure}.

\begin{myfig}
\subimport*{figures/}{lewy-extension-figure.pdf_t}
\caption{Proof of Lewy extension.\label{fig:lewy-extension-figure}}
\end{myfig}

As $p_\ell \to f$ uniformly on $K$, then $p_\ell \to f$ uniformly on
$\partial \Delta$.  As  $p_\ell$ are holomorphic, then by the maximum
principle, $p_\ell$ converge uniformly on all of $\Delta$.  In fact, as $(z_1,z',w)$ was
an arbitrary point in $(D \times W) \cap \Omega_-$,
the polynomials $p_\ell$ converge uniformly on
$(D \times W) \cap \overline{\Omega_-}$.
Let $U = D \times W$, then $U_- = (D \times W) \cap \Omega_-$.
Notice $U$ depends on $K$, but not on $f$.
So $p_\ell$ converge to a continuous function $F$ on $\overline{U_-} \cap
U$ and $F$ is holomorphic on $U_-$.  Clearly $F$ equals $f$ on $M \cap U$.

To prove the last item, pick a side, and then use one of the first two
items to extend the function to that side.  Via the tomato can
principle (\thmref{thm:tomatocan}) the function also extends across $M$ and
therefore to a whole neighborhood of $p$.
\end{proof}

If you were wondering what happened to the analytic discs we promised,
the $\Delta$ in the above is an analytic disc (simply connected) for a small enough $U$, but it was
not necessary to prove that fact.

We state the next corollary for a strongly convex domain, even though it
holds with far more generality.
It is a special case of the \emph{\myindex{Hartogs--Bochner}}\footnote{What is called
Hartogs--Bochner is
the $C^1$ version of this theorem where the domain is only assumed to be bounded
and the boundary connected,
and it was proved by neither Hartogs nor
Bochner, but by Martinelli in 1961.} theorem.
Later, in \exerciseref{exercise:HartogsBochnerSPCVX},
you will prove it for strongly pseudoconvex
domains.  However, the theorem is true for every bounded domain with connected smooth
boundary
with no assumptions on the Levi form, but
a different approach would have to be taken.

\begin{cor} \label{cor:bochnerhartogsstrconvex}
Suppose $U \subset \C^n$, $n \geq 2$, is a bounded domain with smooth boundary that is
strongly convex
and $f \colon \partial U \to \C$ is a smooth CR function, then
there exists a continuous function $F \colon \widebar{U} \to \C$
holomorphic in $U$
such that $F|_{\partial U} = f$.
\end{cor}

\begin{proof}
A strongly convex domain is strongly pseudoconvex, so $f$ must extend to the
inside locally near every point.  The extension is locally unique as any two
extensions have the same boundary values.  Therefore, there exists a set
$K \subset \subset U$ such that $f$ extends to $U \setminus K$.
Via an exercise below we can assume that $K$ is strongly convex and
therefore we can apply the special case of Hartogs phenomenon
that you proved in \exerciseref{exercise:convexhartogs} to find an
extension holomorphic in $U$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the existence of the strongly convex $K$ in the proof of
\corref{cor:bochnerhartogsstrconvex}.
\end{exercise}

\begin{exercise}
Show by example that the corollary is not true when $n=1$.  Explain where in
the proof have we used that $n \geq 2$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \partial \bB_2 \to \C$ is a smooth CR function.
Write down an explicit formula for the extension $F$.
\end{exercise}

\begin{exercise}
A smooth real hypersurface $M \subset \C^3$ is defined by $\Im w = \sabs{z_1}^2-\sabs{z_2}^2 + O(3)$
and $f$ is a real-valued smooth CR function on $M$.  Show
that $\sabs{f}$ does not attain a maximum at the origin.
\end{exercise}

\begin{exercise}
A real-analytic hypersurface
$M \subset \C^n$, $n \geq 3$, is
such that the Levi form at $p \in M$ has eigenvalues of both signs.
Show that every smooth CR function $f$ on $M$ is, in fact, real-analytic in
a neighborhood of $p$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $M \subset \C^3$ be defined by $\Im w = \sabs{z_1}^2-\sabs{z_2}^2$.
\begin{exparts}
\item
Show that for this $M$,
the conclusion of Baouendi--Tr{\`e}ves holds with
an arbitrary compact subset $K \subset \subset M$.
\item
Use this to show that every
smooth CR function $f \colon M \to \C$ is a restriction of an entire holomorphic function
$F \colon \C^3 \to \C$.
\end{exparts}
\end{exercise}

\begin{exercise}
Find an $M \subset \C^n$, $n \geq 2$, such that near some $p \in M$,
for every neighborhood $W$ of $p$ in $M$, there is a CR function $f \colon
W \to \C$ that does not extend holomorphically to either side of $M$ at $p$.
\end{exercise}

\begin{exercise}
Suppose $f \colon \partial \bB_n \to \C$ is a smooth function and $n \geq 2$.
Prove that $f$ is a CR function if and only if
\begin{equation*}
\int_0^{2\pi} f(e^{i\theta}v) \, e^{ik\theta} d\theta = 0
\qquad
\text{for all $v \in \partial \bB_n$ and all $k \in \N$.}
\end{equation*}
\end{exercise}

\begin{exercise}
Prove the third item in the Lewy extension theorem without the use
of the tomato can principle.  That is, prove in a more elementary
way that if $M \subset U \subset \C^n$ is a smooth real hypersurface
in an open set $U$ and $f \colon U \to \C$ is continuous
and holomorphic in $U \setminus M$, then $f$ is holomorphic.
\end{exercise}
\end{exbox}

\begin{remark}
Studying solutions to nonhomogeneous CR equations of the form $X f = \psi$
for a CR vector field $X$,
and the fact that such conditions can guarantee
that a function must be real-analytic, led Lewy to a famous, very
surprising, and rather simple
example of a
linear partial differential equation with smooth coefficients
that has no solution on any open set\footnote{%
Lewy, Hans, \emph{An example of a smooth linear partial differential
equation without solution}, Annals of Mathematics, \textbf{66} (1957), 155--158.}.
The example is surprising because when a linear PDE has real-analytic
coefficients, a solution always exists by the theorem of Cauchy--Kowalevski.
Furthermore, if $X$ is a real vector field ($X$ is in $TM$ not in $\C TM$),
then a solution to $Xf = \psi$ exists by the method of characteristics, even
if $X$ and $\psi$ are only smooth.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The \texorpdfstring{$\bar{\partial}$}{dbar}-problem} \label{ch:dbar}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The generalized Cauchy integral formula}
\index{generalized Cauchy integral formula}

Before we get into the $\bar{\partial}$-problem, let us prove a more general
version of Cauchy's formula using Stokes' theorem (really Green's theorem).
This version is called the \emph{\myindex{Cauchy--Pompeiu integral formula}}.
We only need the theorem for smooth functions, but as it is often
applied in less regular contexts and it is just an application of Stokes'
theorem, let us state it more generally.  In applications, the boundary is often
only piecewise smooth, and again that is all we need for Stokes.

\begin{thm}[Cauchy--Pompeiu] \label{thm:generalizedcauchy}
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively (see \appendixref{ap:onevarresults}),
and let $f \colon \widebar{U} \to \C$ be continuous
with bounded continuous partial derivatives in $U$.
Then for $z \in U$:
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
+
\frac{1}{2\pi i}
\int_{U}
\frac{\frac{\partial f}{\partial \bar{\zeta}}(\zeta)}{\zeta-z}
\,
d\zeta \wedge d\bar{\zeta} .
\end{equation*}
\end{thm}

If $f$ is holomorphic, then the second term is zero, and we
obtain the standard Cauchy formula.
\glsadd{not:dA}%
If $\zeta = x+iy$, then the standard orientation on $\C$ is the one
corresponding to the area form $dA = dx \wedge dy$.
The form $d\zeta \wedge d\bar{\zeta}$ is the area form up to a scalar.
That is,
\begin{equation*}
d\zeta \wedge d\bar{\zeta}
=
(dx+i\,dy)\wedge (dx-i\,dy)
=
(- 2 i ) dx \wedge dy = (-2i) dA .
\end{equation*}

As we want to use Stokes, we need to write the standard
exterior derivative in terms of $z$ and $\bar{z}$.
For $z = x+iy$, we compute:
\begin{equation*}
d \psi
=
\frac{\partial \psi}{\partial x} dx
+
\frac{\partial \psi}{\partial y} dy
=
\frac{\partial \psi}{\partial z} dz
+
\frac{\partial \psi}{\partial \bar{z}} d\bar{z}.
\end{equation*}

\begin{exbox}
\begin{exercise}
Observe the singularity in the second term of the Cauchy--Pompeiu formula,
and prove that the integral still makes
sense (the function is integrable).  Hint: polar coordinates.
\end{exercise}

\begin{exercise}
Why can we not differentiate in $\bar{z}$ under the integral in the second
term of the Cauchy--Pompeiu formula?
Notice that it would lead to an impossible result.
\end{exercise}
\end{exbox}

\begin{proof}
Fix $z \in U$.  We wish to apply Stokes' theorem\footnote{%
We are really using Green's theorem, which is the generalized
Stokes' theorem in $2$ dimensions, see \thmref{thm:greens}.},
but the integrand is not smooth at $z$.
Let $\Delta_r(z)$ be a small disc such that
$\Delta_r(z) \subset
\subset U$.  See \figureref{fig:cauchy-pompeiu}.
Stokes now applies on $U \setminus \Delta_r(z)$.

\begin{myfig}
%NOTE: Same as the figure for fig:usingstokes
\subimport*{figures/}{cauchy-pompeiu.pdf_t}
\caption{Proof of Cauchy--Pompeiu.\label{fig:cauchy-pompeiu}}
\end{myfig}

Via Stokes, where the exterior derivative inside is with respect to $\zeta$
and $\bar{\zeta}$ of
course,
\begin{equation*}
\int_{\partial U} \frac{f(\zeta)}{\zeta-z}\,  d\zeta -
\int_{\partial \Delta_r(z)} \frac{f(\zeta)}{\zeta-z}\,  d\zeta
=
\int_{U \setminus \Delta_r(z)} d\left( \frac{f(\zeta)}{\zeta-z} \, d\zeta \right)
=
\int_{U \setminus \Delta_r(z)} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta .
\end{equation*}
The second equality follows because the holomorphic derivative in $\zeta$
has a $d\zeta$ and when we wedge it with $d\zeta$ we get zero.
We now wish to let the radius $r$ go to zero.
Via the exercise above,
$\frac{\frac{\partial f}{\partial \bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta$
is integrable over all of $U$.  Therefore,
\begin{equation*}
\lim_{r \to 0}
\int_{U \setminus \Delta_r(z)} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta
=
\int_{U} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\bar{\zeta} \wedge d\zeta
=
-
\int_{U} \frac{\frac{\partial f}{\partial
\bar{\zeta}}(\zeta)}{\zeta-z} \, d\zeta \wedge d\bar{\zeta} .
\end{equation*}
The second equality is simply swapping the order of $d\zeta$ and
$d\bar{\zeta}$.
By continuity of $f$,
\begin{equation*}
\lim_{r \to 0}
\frac{1}{2\pi i}
\int_{\partial \Delta_r(z)} \frac{f(\zeta)}{\zeta-z}\,  d\zeta
=
\lim_{r \to 0}
\frac{1}{2\pi}
\int_0^{2\pi} f(z + r e^{i\theta})\, d\theta
=
f(z) .
\end{equation*}
The theorem follows.
\end{proof}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary and
suppose $f \colon \widebar{U} \to \C$ is a $C^1$ function such
that
$\int_{U} \frac{\frac{\partial f}{\partial \bar{z}}(\zeta)}{\zeta-z} \,
dA(\zeta) =
0$ for every $z \in \partial U$.  Prove that $f|_{\partial U}$ are the boundary
values of a function continuous on $\widebar{U}$ and holomorphic in $U$.
\item
Given arbitrary $\epsilon > 0$, find a $C^1$ function $f$ on the closed unit disc
$\overline{\D}$,
such that $\frac{\partial f}{\partial \bar{z}}$ is identically zero
outside an $\epsilon$-neighborhood of the origin, yet $f|_{\partial \D}$
are not the boundary values of a holomorphic function.
\end{exparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C$ and $f$ be as in the theorem, but let $z \notin
\widebar{U}$.  Show that
\begin{equation*}
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta
+
\frac{1}{2\pi i}
\int_{U}
\frac{\frac{\partial f}{\partial \bar{\zeta}}(\zeta)}{\zeta-z}
\,
d\zeta \wedge d\bar{\zeta}
= 0 .
\end{equation*}
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Compactly supported \texorpdfstring{$\bar{\partial}$}{dbar}-problem}
\label{sec:compactdbar}

For a smooth function $\psi$, consider the exterior
derivative in terms of $z$ and $\bar{z}$,
\glsadd{not:dpsi}%
\begin{equation*}
d \psi =
\frac{\partial \psi}{\partial z_1} dz_1 + \cdots +
\frac{\partial \psi}{\partial z_n} dz_n
+
\frac{\partial \psi}{\partial \bar{z}_1} d\bar{z}_1 + \cdots +
\frac{\partial \psi}{\partial \bar{z}_n} d\bar{z}_n .
\end{equation*}
Let us give a name to the two parts of the derivative:
\glsadd{not:d}%
\glsadd{not:dbar}%
\begin{equation*}
\partial \psi \overset{\text{def}}{=}
\frac{\partial \psi}{\partial z_1} dz_1 + \cdots +
\frac{\partial \psi}{\partial z_n} dz_n, \qquad
\bar{\partial} \psi \overset{\text{def}}{=}
\frac{\partial \psi}{\partial \bar{z}_1} d\bar{z}_1 + \cdots +
\frac{\partial \psi}{\partial \bar{z}_n} d\bar{z}_n .
\end{equation*}
Then $d \psi = \partial \psi + \bar{\partial} \psi$.
Notice $\psi$ is holomorphic if and only if $\bar{\partial} \psi = 0$.

The so-called
\emph{\myindex{inhomogeneous $\bar{\partial}$-problem}}\index{$\bar{\partial}$-problem}
($\bar{\partial}$ is pronounced ``dee bar'') is to
solve the equation
\begin{equation*}
\bar{\partial} \psi = g ,
\end{equation*}
for $\psi$, given a one-form
\begin{equation*}
g = g_1 d\bar{z}_1 + \cdots + g_n d\bar{z}_n .
\end{equation*}
Such a $g$ is called a \emph{\myindex{$(0,1)$-form}}.
The fact that the partial
derivatives of $\psi$ commute, forces certain compatibility conditions
on $g$ for us to have any hope of getting a solution (see below).

\begin{exbox}
\begin{exercise}
Find an explicit example of a $g$ in $\C^2$ such that no corresponding
$\psi$ exists.
\end{exercise}
\end{exbox}

On any open set where $g = 0$, $\psi$ is holomorphic.  So
for a general $g$, what we are doing is finding a function
that is not holomorphic in a specific way.

\begin{thm} \label{thm:compactdbar}
Suppose $g$ is a $(0,1)$-form
on $\C^n$, $n \geq 2$, given by
\begin{equation*}
g = g_1 d\bar{z}_1 + \cdots + g_n d\bar{z}_n ,
\end{equation*}
where $g_j \colon \C^n \to \C$ are compactly supported smooth functions
satisfying the \emph{\myindex{compatibility conditions}}
\begin{equation} \label{eq:compatconds}
\frac{\partial g_k}{\partial \bar{z}_\ell} =
\frac{\partial g_\ell}{\partial \bar{z}_k}  \qquad \text{for all $k,\ell =
1,2,\ldots,n$.}
\end{equation}
Then there exists a unique compactly supported smooth function $\psi \colon
\C^n \to \C$ such that
\begin{equation*}
\bar{\partial} \psi = g .
\end{equation*}
\end{thm}

The compatibility conditions on $g$ are necessary, but the compactness is not.
Without compactness, the domain where the equation lives
would come into play.  Let us not worry about this, and prove that this simple
compactly supported version always has a solution.
The compactly supported solution
is unique:
If $\psi_1$ and $\psi_2$ are solutions, then
$\bar{\partial}(\psi_1-\psi_2) = g-g = 0$, and so $\psi_1-\psi_2$
is holomorphic.
The only holomorphic compactly supported function is $0$, and hence the compactly
supported solution $\psi$ is unique.

\begin{proof}
Really there are $n$
smooth functions, $g_1,\ldots,g_n$, so the equation $\bar{\partial} \psi = g$
consists of the $n$ equations
\begin{equation*}
\frac{\partial \psi}{\partial \bar{z}_k} = g_k ,
\end{equation*}
where the functions $g_k$ satisfy the compatibility conditions
\eqref{eq:compatconds}.

We claim that the following is an explicit solution:
\begin{equation*}
\begin{split}
\psi(z)
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 g_1(\zeta,z_2,\ldots,z_n)
}{\zeta - z_1}
d\zeta \wedge d\bar{\zeta}
\\
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 g_1(\zeta+z_1,z_2,\ldots,z_n)
}{\zeta}
d\zeta \wedge d\bar{\zeta} .
\end{split}
\end{equation*}
To show that the singularity does not matter for integrability is the same
idea as for the generalized Cauchy formula.

Let us check that $\psi$ is the solution.
We use the generalized Cauchy formula on the $z_1$
variable.
Take $R$ large enough so that
$g_k(\zeta,z_2,\ldots,z_n)$ is zero when $\sabs{\zeta}\geq R$ for all $k$.
For every $k$,
\begin{equation*}
\begin{split}
g_k(z) & =
\frac{1}{2\pi i}
\int_{\abs{\zeta}=R}
\frac{g_k(\zeta,z_2,\ldots,z_n)}{\zeta-z_1}
d \zeta
+
\frac{1}{2\pi i}
\int_{\abs{\zeta} \leq R}
\frac{\frac{\partial g_k}{\partial \bar{z}_1}(\zeta,z_2,\ldots,z_n)}{\zeta-z_1}
d\zeta \wedge d\bar{\zeta}
\\
& =
\frac{1}{2\pi i}
\int_{\C}
\frac{\frac{\partial g_k}{\partial \bar{z}_1}(\zeta,z_2,\ldots,z_n)}{\zeta-z_1}
d\zeta \wedge d\bar{\zeta}  .
\end{split}
\end{equation*}

Using the second form of the definition of $\psi$, the
compatibility conditions \eqref{eq:compatconds}, and the computation above we get
\begin{equation*}
\begin{split}
\frac{\partial\psi}{\partial \bar{z}_k}(z)
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 \frac{\partial g_1}{\partial \bar{z}_k}(\zeta+z_1,z_2,\ldots,z_n)
}{\zeta}
d\zeta \wedge d\bar{\zeta}
\\
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 \frac{\partial g_k}{\partial \bar{z}_1}(\zeta+z_1,z_2,\ldots,z_n)
}{\zeta}
d\zeta \wedge d\bar{\zeta}
\\
& =
\frac{1}{2\pi i}
\int_\C
\frac{
 \frac{\partial g_k}{\partial \bar{z}_1}(\zeta,z_2,\ldots,z_n)
}{\zeta-z_1}
d\zeta \wedge d\bar{\zeta}
=
g_k(z) .
\end{split}
\end{equation*}

\begin{exbox}
\begin{exercise}
Show that we were allowed to differentiate $\psi$
under the integral in the computation above, but only in the second form
of the integral.
\end{exercise}
\end{exbox}

That $\psi$ has compact support follows because $g_1$ has compact
support together with the identity theorem.  In particular, $\psi$ is
holomorphic for large $z$ since $\bar{\partial} \psi = g = 0$ when $z$
is large.  When at least one of $z_2,\ldots,z_n$ is large,
then $\psi$ is identically zero
simply from its definition.  See \figureref{fig:dbarcpt-fig}.

\begin{myfig}
\subimport*{figures/}{dbarcpt-fig.pdf_t}
\caption{Far enough, $\bar{\partial} \psi = 0$.\label{fig:dbarcpt-fig}}
\end{myfig}

As $\bar{\partial} \psi = 0$ on the light gray and white areas in the
diagram, $\psi$ is holomorphic there. As $\psi$ is zero on the light
gray region, it is zero also on the white region by the identity theorem.
That is, $\psi$ is zero on the unbounded component of the set where $g=0$,
and so $\psi$ has compact support.
\end{proof}

The first part of the proof still works when $n=1$; we get a solution
$\psi$.  However, the last bit of the proof does not work in one dimension, so
$\psi$ does not have compact support.

\begin{exbox}
\begin{exercise} \label{exercise:supportofpsi}
\begin{exparts}
\item
Show that if $g$ is supported in $K \subset \subset \C^n$, $n \geq 2$,
then $\psi$ is supported in the complement of the unbounded component
of $\C^n \setminus K$.  In particular, show that if $K$ is the support of
$g$ and $\C^n \setminus K$ is connected, then the support of
$\psi$ is~$K$.
\item
Find an explicit example where the support of $\psi$ is strictly larger
than the support of~$g$.
\end{exparts}
\end{exercise}

\begin{exercise}
Find an example of a smooth function $g \colon \C \to \C$ with compact
support, such that no solution $\psi \colon \C \to \C$ to
$\frac{\partial \psi}{\partial \bar{z}} = g$ (at least one of which always exists) is
of compact support.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The general Hartogs phenomenon}
\label{sec:hartogsphenom}

We can now prove the general Hartogs phenomenon as an application of the
solution of the compactly supported
inhomogeneous $\bar{\partial}$-problem.  We proved special
versions of this phenomenon using Hartogs figures before.
The proof of the theorem has a complicated history as
Hartogs' original proof from 1906 contained gaps.
A fully working proof was finally supplied by Fueter in 1939 for $n=2$
and independently by Bochner and Martinelli for higher $n$
in the early 40s.
The proof we give is the
standard one given nowadays due to Leon Ehrenpreis from 1961.

\begin{thm}[Hartogs phenomenon]\index{Hartogs phenomenon}
Let $U \subset \C^n$ be a domain, $n \geq 2$, and let
$K \subset \subset U$ be a compact set such that
$U \setminus K$ is connected.  Every holomorphic $f \colon U \setminus K \to \C$
extends uniquely to a holomorphic function on $U$.
See \figureref{fig:hartogs-fig}.
\end{thm}

\begin{myfig}
\subimport*{figures/}{hartogs-fig.pdf_t}
\caption{Hartogs phenomenon.\label{fig:hartogs-fig}}
\end{myfig}

The idea of the proof is extending in any way whatsoever and then using the solution
to the $\bar{\partial}$-problem to correct the result to make it
holomorphic.

\begin{proof}
First find a smooth function $\varphi$ that is $1$ in a neighborhood of
$K$ and is compactly supported in $U$ (exercise below).  Let
$f_0 = (1-\varphi)f$ on $U \setminus K$ and $f_0 = 0$ on $K$.  The function $f_0$
is smooth on $U$ and it is holomorphic
and equal to $f$ near the boundary of $U$, where $\varphi$ is $0$.
We let $g = \bar{\partial} f_0$ on $U$, that is $g_k = \frac{\partial
f_0}{\partial \bar{z}_k}$,
and we let $g=0$ outside $U$.
As $g_k$ are identically zero near $\partial U$, we find that each
$g_k$ is smooth on $\C^n$.
The compatibility conditions
\eqref{eq:compatconds} are satisfied
because partial derivatives commute.
Let us see why $g$ is compactly supported.  The
only place to check is on $U \setminus K$ as elsewhere we have $g = 0$
automatically.  Note that $f$ is holomorphic on $U \setminus K$ and compute
\begin{equation*}
\frac{\partial f_0}{\partial \bar{z}_k}
=
\frac{\partial }{\partial \bar{z}_k}
\bigl((1-\varphi)f\bigr)
=
\frac{\partial f}{\partial \bar{z}_k}
- \varphi \frac{\partial f}{\partial \bar{z}_k}
- \frac{\partial \varphi}{\partial \bar{z}_k} f
=
- \frac{\partial \varphi}{\partial \bar{z}_k} f .
\end{equation*}
The function $\frac{\partial \varphi}{\partial \bar{z}_k}$ is compactly supported in
$U \setminus K$ by construction.
Apply the solution of the compactly supported $\bar{\partial}$-problem
to find a
compactly supported function $\psi$ such that $\bar{\partial}\psi = g$.
Set $F = f_0 - \psi$.  We check that $F$ is the desired
extension.  First, it is holomorphic:
\begin{equation*}
\frac{\partial F}{\partial \bar{z}_k}
=
\frac{\partial f_0}{\partial \bar{z}_k}
-
\frac{\partial \psi}{\partial \bar{z}_k}
=
g_k
-
g_k
= 0 .
\end{equation*}
Next, \exerciseref{exercise:supportofpsi} and the fact that $U \setminus
K$ is connected reveals that $\psi$ must be compactly supported in $U$.
This means that $F$ agrees with $f$ near the boundary (in particular
on an open set) and thus everywhere in $U \setminus K$ since $U \setminus K$
is connected.
\end{proof}

The hypotheses on dimension and on connectedness of $U \setminus K$
are necessary.
No such theorem is true in one dimension.
If $U \setminus K$ is disconnected, a simple
counterexample can be constructed.
See the exercise below.

\begin{exbox}
\begin{exercise}
Show that $\varphi$ exists.  Hint: Use mollifiers.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is a domain and
$K \subset U$ is a compact set (perhaps $U \setminus
K$ is disconnected).
Prove that given $f \in \sO(U \setminus K)$ there exists an $F \in \sO(U)$ that equals
to $f$ on the intersection of $U$ and the unbounded component
of $\C^n \setminus K$.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is a domain and
$K \subset U$ is a compact set such that $U \setminus K$ is disconnected.
Find a counterexample to the conclusion to Hartogs.
\end{exercise}
\end{exbox}

One of many consequences of the Hartogs phenomenon is
that
the zero set of a holomorphic function $f$ is never compact in
dimension $2$ or higher, although there exist
easier proofs of that fact, see \exerciseref{exercise:zerosetnotcompact}.  If it
were compact, $\frac{1}{f}$ would provide a contradiction, see also
\exerciseref{exercise:connectedcomplement}.

\begin{cor}
Suppose $U \subset \C^n$, $n \geq 2$, is a domain and $f \colon U \to \C$ is
holomorphic.  If the zero set $f^{-1}(0)$ is not empty, then it is not compact.
\end{cor}

Replacing $U \setminus K$ with a hypersurface
is usually called the Hartogs--Bochner theorem (when the hypersurface is
$C^1$ or smooth).
The real-analytic case was stated first by Severi in 1931.

\begin{cor}[Severi]\index{Severi's theorem}
Suppose $U \subset \C^n$, $n \geq 2$, is a bounded domain with connected real-analytic boundary and
$f \colon \partial U \to \C$ is a real-analytic CR function.  Then
there exists some neighborhood $U' \subset \C^n$ of $\widebar{U}$
and a holomorphic function $F \colon U' \to \C$ for which
$F|_{\partial U} = f$.
\end{cor}

\begin{proof}
By Severi's result (\thmref{thm:severi}), for every $p \in \partial U$,
there is a small ball $B_p$ centered at $p$,
such that $f$ extends to $B_p$.  Cover $\partial
U$ by finitely many such balls so that if $B_p$ intersects $B_q$, then
the (connected) intersection $B_p \cap B_q$ contains points of $\partial U$.
The extension in $B_p$ and in $B_q$ then agree on a piece of a hypersurface
$\partial U$, and hence agree.  Taking a union of the $B_p$, we find
a unique extension in single neighborhood of $\partial U$.
We write this neighborhood as $U' \setminus K$ for some compact $K$
and a connected $U'$ such that $\widebar{U} \subset U'$.
Consider the topological components of $\C^n \setminus K$.  As
$\partial U$ is connected and $U$ is bounded,
the unbounded component of $\C^n \setminus K$ must contain all of $\partial U$.
By boundedness of $U$, all the other components are
relatively compact in $U$.  If we add them to $K$, then $K$
is still compact and $U' \setminus K$ is connected.
We apply the Hartogs phenomenon.
\end{proof}

\begin{exbox}
\begin{exercise}[Hartogs--Bochner again] \label{exercise:HartogsBochnerSPCVX}
Let $U \subset \C^n$, $n \geq 2$, be a bounded domain with connected strongly
pseudoconvex smooth boundary
and let $f \colon \partial U \to \C$ be a smooth CR function.  Prove
that there exists a continuous function $F \colon \widebar{U} \to \C$
holomorphic in $U$
such that $F|_{\partial U} = f$.
Note: Strong pseudoconvexity is not needed (``bounded with smooth boundary''
will do), but that is much more difficult to prove.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$, $n \geq 2$, is a bounded domain of
holomorphy.  Show that $\C^n \setminus U$ is connected using the
Hartogs phenomenon.
\end{exercise}

\begin{exercise}
Suppose $W \subset U \subset \C^n$, $n \geq 3$, are domains such that
for each fixed $z_3^0,z_4^0,\ldots,z_n^0$,
\begin{multline*}
\bigl\{ (z_1,z_2) \in \C^2 :
(z_1,z_2,z_3^0,\ldots,z_n^0) \in U \setminus W
\bigr\}
\\
\subset \subset
\bigl\{ (z_1,z_2) \in \C^2 :
(z_1,z_2,z_3^0,\ldots,z_n^0) \in U
\bigr\} .
\end{multline*}
Prove that every $f \in \sO(W)$ extends to a holomorphic function on
$U$.  Note: The fact that $W$ is connected is important.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Prove that if $n \geq 2$, no domain of the form $U = \C^n \setminus K$
for a compact $K$ is biholomorphic to a bounded domain.
\item
Prove that every domain of the form $U = \C \setminus K$
for a compact $K$ with nonempty interior is biholomorphic to a bounded domain.
\end{exparts}
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$, $n \geq 2$, is a domain such that
for some affine $A \colon \C^2 \to \C^n$
the set $A^{-1}\bigl(\C^n \setminus U\bigr)$ has a bounded
topological component.  Prove that $U$ is not a domain of holomorphy.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Solvability of the \texorpdfstring{$\bar{\partial}$}{dbar}-problem in the polydisc} \label{sec:dbarpoly}

Let us tackle the solvability of the $\bar{\partial}$-problem for differential forms.
In general, the problem is equivalent to holomorphic convexity, although
it is rather involved, and thus we content ourselves with polydiscs and
other simple examples.
To work with differential forms, we, as before, split the derivatives
into the holomorphic and antiholomorphic parts.  For higher order forms,
we work with multi-indices for simplicity, although the way that
multi-indices are applied is slightly different.

\begin{defn} \label{defn:danddbarform}
\pagebreak[2]
Suppose $U \subset \C^n$ is open and
let $p$ and $q$ be integers, $0 \leq p,q \leq n$.
Let $\alpha$ and $\beta$ be ordered $p$- and $q$-tuples of
distinct integers such that
$1 \leq \alpha_1 < \alpha_2 < \dots < \alpha_p \leq n$ and
$1 \leq \beta_1 < \beta_2 < \dots < \beta_q \leq n$.
Write
\begin{equation*}
dz_{\alpha} =
dz_{\alpha_1}
\wedge
\cdots
\wedge
dz_{\alpha_p}
\quad
\text{and}
\quad
d\bar{z}_{\beta} =
d\bar{z}_{\beta_1}
\wedge
\cdots
\wedge
d\bar{z}_{\beta_q} .
\end{equation*}
Then a differential form
\begin{equation*}
\eta =
\sum_{\alpha,\beta}
\eta_{\alpha \beta} \, dz_\alpha \wedge d\bar{z}_\beta ,
\end{equation*}
where the $\alpha$ and $\beta$ run over all $p$- and $q$-tuples
as above and $\eta_{\alpha \beta}$ are smooth functions on $U$,
is called a
\emph{\myindex{$(p,q)$-form}} or a differential form of
\emph{\myindex{bidegree}} $(p,q)$.
Note that $(0,0)$-forms are simply smooth functions on $U$.
A general $k$-form can be written as a sum of $(p,q)$-forms
for different $p$ and $q$ where $p+q = k$.
Define
\glsadd{not:d}%
\glsadd{not:dbar}%
\begin{equation*}
\partial \eta \overset{\text{def}}{=}
\sum_{\alpha,\beta}
\,
\sum_{k=1}^n
\frac{\partial \eta_{\alpha \beta}}{\partial z_k} dz_k \wedge dz_\alpha
\wedge d\bar{z}_\beta ,
\quad \text{and} \quad
\bar{\partial} \eta \overset{\text{def}}{=}
\sum_{\alpha,\beta}
\,
\sum_{k=1}^n
\frac{\partial \eta_{\alpha \beta}}{\partial \bar{z}_k} d\bar{z}_k \wedge dz_\alpha
\wedge d\bar{z}_\beta .
\end{equation*}
\end{defn}

If $\eta$ is of bidegree $(p,q)$, then
$\partial \eta$ is of bidegree $(p+1,q)$ and
$\bar{\partial} \eta$ is of bidegree $(p,q+1)$.
We get the total exterior derivative $d \eta = \partial \eta + \bar{\partial} \eta$ as before.

\begin{exbox}
\begin{exercise}
Prove $d \eta = \partial \eta + \bar{\partial} \eta$
and prove the \emph{\myindex{Leibniz rule}}:
If $\eta$ is a $(p,q)$-form, then
\begin{equation*}
\partial(\eta \wedge \omega) = 
\partial\eta \wedge \omega + {(-1)}^{p+q} \eta \wedge \partial \omega
\quad
\text{and}
\quad
\bar{\partial}(\eta \wedge \omega) = 
\bar{\partial}\eta \wedge \omega + {(-1)}^{p+q} \eta \wedge
\bar{\partial}\omega .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that $\bar{\partial}^2=0$, that is, prove that $\bar{\partial}^2 \eta =
\bar{\partial} \bar{\partial} \eta = 0$ for every form $\eta$.
Similarly, show $\partial^2 = 0$.
\end{exercise}

\begin{exercise}
Given a hypersurface $M \subset \C^n$ with a defining function $r$,
compute
$\partial \bar{\partial} r$ and show that it gives the Levi form.
That is, for a $T^{(1,0)} M$ vector field $Z$,
the Levi form is given by
$\langle\partial \bar{\partial} r , Z \wedge \bar{Z} \rangle$.
Hint: See
\appendixref{ap:diffforms} on how to evaluate differential forms
as multilinear forms.
\end{exercise}
\end{exbox}

A form $\eta$ is a
\emph{\myindex{$\bar{\partial}$-exact form}}\index{exact form}
if there exists a form $\omega$ such that
$\bar{\partial} \omega = \eta$.  A form $\eta$ is a
\emph{\myindex{$\bar{\partial}$-closed form}}\index{exact form}
if $\bar{\partial} \eta = 0$.
For an open set $U \subset \C^n$,
we define the
\emph{\myindex{Dolbeault cohomology groups}}\index{cohomology}
(quotient of complex vector spaces)%
\glsadd{not:dolbeault}%
\begin{equation*}
H^{(p,q)}(U)
=
\frac{
\bigl\{
\bar{\partial}\text{-closed forms of bidegree } (p,q) \text{ on } U
\bigr\}
}{
\bigl\{
\bar{\partial}\text{-exact forms of bidegree } (p,q) \text{ on } U
\bigr\}
} .
\end{equation*}
By convention, the only $(0,0)$-form that is exact is the identically zero form.

\begin{exbox}
\begin{exercise}
Show that the $\bar{\partial}$-closed forms of degree $(p,q)$
are a subspace of the vector space of all $(p,q)$-forms and similarly
that the
$\bar{\partial}$-exact forms of degree $(p,q)$
are a subspace.
\end{exercise}

\begin{exercise}
Prove that if two domains $U,V \subset \C^n$ are biholomorphic,
then for all $(p,q)$, then $H^{(p,q)}(U)$ and $H^{(p,q)}(V)$
are isomorphic as vector spaces.
\end{exercise}
\end{exbox}

The $\bar{\partial}$-problem for $(p,q)$-forms is then the solvability
of the equation $\bar{\partial} \omega = \eta$ for every $(p,q)$-form
$\eta$ given the necessary
compatibility conditions $\bar{\partial} \eta = 0$.
This problem
can then be stated as the cohomology condition $H^{(p,q)}(U) = 0$,
where by $0$, we mean the trivial vector space.
The Dolbeault cohomology is, in a sense, a refinement
of the so-called de Rahm cohomology,
which is the usual smooth cohomology measuring the normal topology of $U$,
where the Dolbeault cohomology also takes into account the complex structure.
Note that $H^{(0,0)}(U)$ is just the set of $\bar{\partial}$-closed
$(0,0)$-forms, that is, it is the set $\sO(U)$ of holomorphic functions on $U$.
More generally, $H^{(p,0)}(U)$ is the set of $\bar{\partial}$-closed
$(p,0)$-forms, that is, forms
\begin{equation*}
\eta =
\sum_{\alpha}
\eta_{\alpha} \, dz_\alpha
\quad \text{such that}
\quad
\bar{\partial} \eta
=
\sum_{\alpha}
\,
\sum_{k=1}^n
\frac{\partial \eta_{\alpha}}{\partial \bar{z}_k} d\bar{z}_k \wedge
dz_\alpha = 0.
\avoidbreak
\end{equation*}
That is, all the functions $\eta_{\alpha}$ are holomorphic.

Solvability of the equation $\bar{\partial} \omega = \eta$ for every $(p,q)$-form
$\eta$ such that $\bar{\partial} \eta = 0$ whenever $q \geq 1$
is equivalent to holomorphic convexity, although
the complete proof is beyond the scope of this book.  You will prove
one direction of this theorem in $\C^2$ in the exercises in this section,
and we will prove the general version of this direction in the next section;
see \thmref{thm:dolhomimpliesdoh}.  That is, we will prove that
the vanishing of the cohomology groups implies domain of holomorphy.
Let us state this theorem without proof.

\begin{thm}
A domain $U \subset \C^n$ is a domain of holomorphy
(and hence holomorphically convex) if and only if $H^{(0,q)}(U) = 0$
whenever $1 \leq q \leq n-1$.
\end{thm}

If $U$ is a domain of holomorphy, it is in fact true that $H^{(p,q)}(U) = 0$
whenever $q \geq 1$.  We will not prove this fact, but we will prove it for a
polydisc, and we saw above that $H^{(p,0)}(U)$ is never trivial.

\begin{exbox}
\begin{exercise} \label{exercise:zeropsufficient}
Prove that
it is sufficient to consider $p=0$,
that is,
$H^{(0,q)}(U) = 0$ if and only if $H^{(p,q)}(U) = 0$ for all $p$.
\end{exercise}

\begin{exercise} \label{exercise:dobeaultdisconnected}
Suppose $U \subset \C^n$ is open.
Show that
$H^{(p,q)}(U) = 0$ if and only if $H^{(p,q)}(W) = 0$ for
every connected component $W$ of $U$.
\end{exercise}
\end{exbox}

\begin{example}
As we mentioned, if $U$ is not a domain of holomorphy, the $\bar{\partial}$
problem is not always solvable, and hence the Dolbeault cohomology groups
may be nonzero.  Let us show that
$H^{(0,1)}\bigl(\C^2 \setminus \{ 0 \}\bigr)$ contains a nonzero element.

Let $r = \sabs{z}^2+\sabs{w}^2$.  Write
\begin{equation*}
\frac{1}{zw} = \frac{\bar{w}}{z r} - \frac{-\bar{z}}{w r} .
\end{equation*}
That is, the two functions on the right hand side differ by a holomorphic
function wherever $z$ and $w$ are both not zero and hence their
$\bar{\partial}$'s are equal (where they are both defined).  The left hand
term is defined when $z\not=0$ and the right hand term is defined when $w
\not=0$.  So the following form is well-defined on $\C^2 \setminus \{ 0 \}$
\begin{equation*}
\eta =
\bar{\partial}
\left(
\dfrac{\bar{w}}{z r}
\right)
\quad \text{if $z\not=0$,}
\qquad \text{and} \qquad
\eta =
\bar{\partial}
\left(
\dfrac{-\bar{z}}{w r}
\right)
\quad \text{if $w\not=0$} .
\end{equation*}
That $\eta$ is $\bar{\partial}$-closed follows by $\bar{\partial}^2 = 0$.
Suppose for contradiction that there existed a smooth
$f \colon \C^2 \setminus \{ 0 \} \to \C$
such that $\bar{\partial} f = \eta$.
Define $g = z f - \nicefrac{\bar{w}}{r}$.  When $z\not=0$, then
$\nicefrac{g}{z} = f - \nicefrac{\bar{w}}{zr}$, and so
$\bar{\partial}\bigl(\nicefrac{g}{z}\bigr) = 0$.  Therefore, $g$ is holomorphic
where $z\not=0$, but $g$ is, and this is really where the contradiction
comes in, smooth on $\C^2 \setminus \{ 0 \}$ and hence satisfies the
Cauchy--Riemann equations on $\C^2 \setminus \{ 0 \}$ and so is holomorphic.
By Hartogs phenomenon (any version) $g$ extends to be holomorphic in $\C^2$,
in particular, near $0$, which contradicts the fact that $g(0,w) =
\nicefrac{-1}{w}$.
\end{example}

\begin{example}
It is not simply the topology (as we know already) that determines the
$H^{(p,q)}$ groups.  The previous example still works in $\C^2 \setminus
\R^2 = \bigl\{ (z,w) \in \C^2 :
\Im z \not= 0 \text{ or } \Im w \not= 0 \bigr\}$.
That is, the function $g$ from the previous example can be defined in
$\C^2 \setminus \R^2$ and hence extends to $\C^2$
(see \exerciseref{exercise:C2minusR2}),
leading again to a contradiction.
Notice that the domain
$\C^2 \setminus \{ z = 0\}$
has the same exact topology as $\C^2 \setminus \R^2$.
However,
$\C^2 \setminus \{ z = 0\}$
is a domain of holomorphy, and via an exercise below,
$H^{(0,1)}\bigl(\C^2 \setminus \{ z = 0\}\bigr) = 0$.
The thing is that the $f$ actually exists
by construction: $f = \nicefrac{\bar{w}}{z r}$.  Then $g$ is identically
zero, so we do not get any contradiction.
\end{example}

\begin{exbox}
\begin{exercise}
Prove that if $U \subset \C^2$ is a domain and $K \subset U$ is compact,
then $H^{(0,1)}(U \setminus K)$ is nontrivial.
\end{exercise}

\begin{exercise}
Give another example for why topology is not enough.  Consider the
Hartogs figure
$H =
\bigl\{ (z,w) \in \D^{2} : \sabs{z} > \nicefrac{1}{2} \text{ or }
\sabs{w} < \nicefrac{1}{2}
\bigr\}$.
Show that while $H$ is homeomorphic to the polydisc (has trivial topology),
$H^{(0,1)}(H)$ is nontrivial.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^2$ is a domain such that $H^{(0,1)}(U) = 0$,
then $U$ is a domain of holomorphy.
Hint: Prove the contrapositive, suppose that $U$ is not a domain of
holomorphy and show that $H^{(0,1)}(U) \not= 0$ using the reasoning from
the examples.
\end{exercise}

\begin{exercise} \label{exercise:Levinegdol}
Suppose $U \subset \C^n$ is a domain with smooth boundary,
$0 \in \partial U$, $\{ z_1=z_2 =0\} \cap U = \emptyset$, and the Levi form
at the origin has a negative eigenvalue.  Show that
$H^{(0,1)}(U) = 0$.
\end{exercise}

\begin{exercise}
Prove that $H^{(0,1)}\bigl(\C^2 \setminus \{ 0 \} \bigr)$ is not just
nontrivial, it is an infinite-dimensional vector space.
\end{exercise}
\end{exbox}

We will prove the solvability of the $\bar{\partial}$-problem for the
polydisc.
We will allow some of the factors in the polydisc to be $\C$,
so we define \emph{\myindex{possibly unbounded polydisc}}
$\Delta \subset \C^n$
to mean $\Delta = D_1 \times \cdots \times D_n$ where
each $D_k$ is either a disc or $\C$.  In this way, we also achieve a
solution on $\C^n$ itself.  This is the theorem we will actually prove:

\begin{thm} \label{thm:dbaronpoly}
Let $\Delta \subset \C^n$ be a possibly unbounded polydisc,
let $p \geq 0$ and $q \geq 1$ be integers, and
let $\eta$ be a smooth $(p,q)$-form on $\Delta$ such that
$\bar{\partial} \eta = 0$, then there exists a $(p,q-1)$-form $\omega$
such that $\bar{\partial} \omega = \eta$.
In other words, $H^{(p,q)}(\Delta) = 0$ for $q \geq 1$.
\end{thm}

Before tackling the proof of the theorem, let us
solve a simple $\bar{\partial}$-problem in one dimension
using the Cauchy--Pompeiu formula.

\begin{lemma} \label{lemma:dbaronevar}
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary,
and let $g \colon \widebar{U} \to \C$ be a smooth function (restriction of a
smooth function on a neighborhood of $\widebar{U}$).
Then $\psi \colon U \to \C$ given by
\begin{equation*}
\psi(z) =
\frac{1}{2\pi i} \int_U \frac{g(\zeta)}{\zeta-z} d\zeta \wedge d\bar{\zeta}
\end{equation*}
is a smooth function such that $\frac{\partial \psi}{\partial \bar{z}} = g$.
\end{lemma}

By taking conjugates, we can similarly solve the
$\frac{\partial \psi}{\partial z} = g$ problem.  Moreover, since the
solution is given as an integral, we can solve the problem with
parameters.  That is, if $g$ depends on some other variables in a smooth or
holomorphic way, then the solution $\psi$ also depends on those variables
smoothly or holomorphically.
Compare the expression for $\psi$ to the one used in the proof of
\thmref{thm:compactdbar}.
The proof is based on the fact that $\psi(z) = \log \sabs{z}^2$
solves the problem for $g(z) = \nicefrac{1}{\bar{z}}$.
This fact may be surprising, as doing calculus blindly, one would arrive
at the multivalued function $\log \bar{z} + \text{holomorphic function}$,
but $\log \bar{z} + \log z = \log \sabs{z}^2$ is single valued as needed.
The lemma, via the exercises, leads to showing that $H^{(0,1)}(U) = 0$ for
every domain $U \subset \C$.  Every $(0,1)$-form
$g \, d\bar{z}$ in $U$ is $\bar{\partial}$-closed, so $H^{(0,1)}(U) = 0$ is
equivalent to showing that $\frac{\partial \psi}{\partial \bar{z}} = g$
is solvable for any smooth function $g$ on $U$.

\begin{exbox}
\begin{exercise}
The fact that the functions are complex-valued is important.  Show that
for $g(x+iy) = \frac{y}{x^2+y^2}$ for $\C \setminus \{ 0 \}$,
there is no real-valued $\psi \colon \C \setminus \{ 0 \} \to \R$
such that
$\frac{\partial \psi}{\partial z} = g$ or
$\frac{\partial \psi}{\partial \bar{z}} = g$.
\end{exercise}
\end{exbox}

\begin{proof}[Proof of the lemma]
Fix $z \in U$ for a moment and take a small disc $\Delta_r(z)$ such that
$\overline{\Delta_r(z)} \subset U$, see \figureref{fig:usingstokes}.  Then via Stokes,
\begin{multline*}
\int_{\partial U}
g(\zeta) \log \sabs{\zeta-z}^2 d \bar{\zeta}
-
\int_{\partial \Delta_r(z)}
g(\zeta) \log \sabs{\zeta-z}^2 d \bar{\zeta}
=
\int_{U \setminus \Delta_r(z)}
d\bigl( g(\zeta) \log \sabs{\zeta-z}^2 d \bar{\zeta} \bigr)
\\
=
\int_{U \setminus \Delta_r(z)}
\frac{\partial g}{\partial \zeta}(\zeta)
\log \sabs{\zeta-z}^2
d \zeta \wedge d \bar{\zeta}
+
\int_{U \setminus \Delta_r(z)}
\frac{g(\zeta)}{\zeta-z}
d \zeta \wedge d \bar{\zeta} .
\end{multline*}
Note that both $\log \sabs{\zeta-z}^2$ and $\frac{1}{\zeta-z}$ are integrable
over $U$ with respect to the area measure.
As $r \to 0$, the last term above goes to $\int_U \frac{g(\zeta)}{\zeta-z}
d\zeta \wedge d\bar{\zeta}$, which is $2\pi i \psi(z)$.
Next, the function $g$ is bounded on $\widebar{U}$, by say $M$, so
\begin{equation*}
\abs{
\int_{\partial \Delta_r(z)}
g(\zeta) \log \sabs{\zeta-z}^2 d \bar{\zeta}
}
\leq
\int_{\partial \Delta_r(z)}
M
\abs{\log (r^2)} \sabs{d \bar{\zeta}}
=
2\pi r M \abs{\log(r^2)}
\underset{\text{as } r \to 0}{\to} 0 .
\end{equation*}
Thus, taking the limit we get
\begin{equation*}
\psi(z)
=
\frac{1}{2\pi i}
\int_{\partial U}
g(\zeta) \log \sabs{\zeta-z}^2 d \bar{\zeta}
-
\frac{1}{2\pi i}
\int_{U}
\frac{\partial g}{\partial \zeta}(\zeta)
\log \sabs{\zeta-z}^2
d \zeta \wedge d \bar{\zeta} .
\end{equation*}
\begin{myfig}
%NOTE: Same as the figure for proof of Cauchy--Pompeiu
\subimport*{figures/}{cauchy-pompeiu.pdf_t}
\caption{Using Stokes.\label{fig:usingstokes}}
\end{myfig}

Taking partial derivatives (in $\Re z$ and $\Im z$) still leaves the
integrands integrable, and hence we can pass them under the integral sign.
In particular, the function $\psi$ is $C^1$ and we can
take the $\bar{z}$ derivative.
We then apply
the Cauchy--Pompeiu formula (actually its conjugate applied to $\bar{g}$)
\begin{equation*}
\frac{\partial \psi}{\partial \bar{z}}(z)
=
\frac{-1}{2\pi i}
\int_{\partial U}
\frac{g(\zeta)}{\bar{\zeta}-\bar{z}} d \bar{\zeta}
+
\frac{1}{2\pi i}
\int_{U}
\frac{\frac{\partial g}{\partial \zeta}(\zeta)}{\bar{\zeta}-\bar{z}}
d \zeta \wedge d \bar{\zeta}
= g(z) .
\end{equation*}
So we are done with the existence of this solution, we need to show that it
is also smooth.
As $g$ is smooth, then $\frac{\partial \psi}{\partial \bar{z}}$ is also
smooth.  If we prove that
$\frac{\partial \psi}{\partial z}$ is also smooth, then $\psi$ must be
smooth.  We take the $z$ derivative instead of the $\bar{z}$
derivative to find
\begin{equation*}
\frac{\partial \psi}{\partial z}(z)
=
\frac{-1}{2\pi i}
\int_{\partial U}
\frac{g(\zeta)}{\zeta-z} d \bar{\zeta}
+
\frac{1}{2\pi i}
\int_{U}
\frac{\frac{\partial g}{\partial \zeta}(\zeta)}{\zeta-z}
d \zeta \wedge d \bar{\zeta} .
\end{equation*}
The first integral is clearly smooth.  The second integral is precisely the
sort of integral we have just shown is $C^1$ (with $g$ replaced by
$\frac{\partial g}{\partial \zeta}$), so $\psi$ is $C^2$.  By induction,
$\frac{\partial \psi}{\partial z}$ is smooth, and hence $\psi$ is smooth.
\end{proof}

Moving to several variables,
we prove that we can solve the problem on a subpolydisc of
any polydisc,
which is usually called the \emph{\myindex{Dolbeault lemma}} or
\emph{\myindex{Dolbeault--Grothendieck lemma}}.
As it is the analogue of the Poincar\'e lemma, it is sometimes
called the \emph{\myindex{$\bar{\partial}$-Poincar\'e lemma}}.

\begin{lemma}[Dolbeault--Grothendieck]\label{lemma:dolgro}
Let $\Delta_s(w) \subset \Delta_r(w) \subset \C^n$ be polydiscs
where $0 < s_\ell < r_\ell < \infty$ for each $\ell$.
Let $p \geq 0$ and $q \geq 1$ be integers, and
let $\eta$ be a smooth $(p,q)$-form on $\Delta_r(w)$ such that
$\bar{\partial} \eta = 0$, then there exists a smooth $(p,q-1)$-form $\omega$
on $\Delta_s(w)$ such that $\bar{\partial} \omega = \eta$.
\end{lemma}

\begin{proof}
Le $k$ be an integer such that $\eta$ only involves
$d\bar{z}_1,\ldots,d\bar{z}_k$ from the barred differentials.
If $k=0$, then the lemma is true trivially as $q \geq 1$, so $\eta$ would
just have to be zero.
We will proceed by induction.

Write
\begin{equation*}
\eta = d\bar{z}_k \wedge \tau + \theta,
\end{equation*}
where $\tau$ and $\theta$ only involve the barred differentials
$d\bar{z}_1,\ldots,d\bar{z}_{k-1}$.
Now
\begin{equation*}
0 = \bar{\partial} \eta
= \bar{\partial} (d\bar{z}_k \wedge \tau + \theta)
= -d\bar{z}_k \wedge \bar{\partial} \tau + \bar{\partial} \theta .
\end{equation*}
Hence the coefficients of $\tau$ and $\theta$ must be holomorphic in
$z_{k+1},\ldots,z_n$ as their derivatives in the bars of those variables
are zero.
In particular, if $\tau_{\alpha \beta}$ is one of the coefficients
of $\tau$, then it is a smooth function of the larger polydisc, but also
holomorphic in the variables $z_{k+1},\ldots,z_{n}$.

\begin{exbox}
\begin{exercise}
Check the assertion that the coefficients of
$\tau$ and $\theta$ are holomorphic in the variables $z_{k+1},\ldots,z_n$.
\end{exercise}
\end{exbox}


By \lemmaref{lemma:dbaronevar}, there is a smooth function
$\psi_{\alpha\beta}$ in the
$z_k$ variable such that $\frac{\partial \psi_{\alpha\beta}}{\partial \bar{z}_k} =
\tau_{\alpha \beta}$.  Moreover, each $\psi_{\alpha\beta}$ is also a smooth function
in
\begin{equation*}
\Delta_{r_1}(w_1)
\times \cdots \times
\Delta_{r_{k-1}}(w_{k-1})
\times
\Delta_{t}(w_{k})
\times
\Delta_{r_{k+1}}(w_{k+1})
\times \cdots \times
\Delta_{r_{n}}(w_{n})
\end{equation*}
for some $t$ such that $s_k < t < r_k$.
It is also holomorphic in the variables $z_{k+1},\ldots,z_n$.
From the $\psi_{\alpha\beta}$ functions we construct a $(p,q-1)$-form $\psi$.
We compute $\bar{\partial}\psi$:
\begin{equation*}
\begin{split}
\bar{\partial} \left(
\sum_{\alpha \beta}
\psi_{\alpha\beta} \, dz_\alpha \wedge d\bar{z}_\beta
\right)
& =
\sum_{\alpha \beta}
\frac{\partial \psi_{\alpha\beta}}{\partial \bar{z}_k} d\bar{z}_k
\wedge dz_\alpha \wedge d\bar{z}_\beta
+
\sum_{\alpha \beta}
\sum_{\ell=1}^{k-1}
\frac{\partial \psi_{\alpha\beta}}{\partial \bar{z}_\ell} d\bar{z}_\ell
\wedge dz_\alpha \wedge d\bar{z}_\beta
\\
& =
d\bar{z}_k
\wedge
\left(
\sum_{\alpha \beta}
\tau_{\alpha\beta}
\,
dz_\alpha \wedge d\bar{z}_\beta
\right)
+
\sum_{\alpha \beta}
\sum_{\ell=1}^{k-1}
\frac{\partial \psi_{\alpha\beta}}{\partial \bar{z}_\ell} d\bar{z}_\ell
\wedge dz_\alpha \wedge d\bar{z}_\beta
\\
& =
d\bar{z}_k \wedge \tau + \delta ,
\end{split}
\end{equation*}
where $\delta$ also does not contain
the barred differentials other than $d\bar{z}_1,\ldots,d\bar{z}_{k-1}$.
Now note that
\begin{equation*}
\bar{\partial}(\theta-\delta)
=
\bar{\partial}(\eta-\bar{\partial} \psi)
=
\bar{\partial}\eta-\bar{\partial}^2 \psi = 0 .
\end{equation*}
Since $\theta$ and $\delta$ both only contain the barred differentials
$d\bar{z}_1,\ldots,d\bar{z}_{k-1}$, we can apply the
induction hypothesis to find a $(p,q-1)$-form $\varphi$ such that
$\bar{\partial} \varphi = \theta-\delta$.  Then we let
$\omega = \psi+\varphi$ and note that we are done:
\begin{equation*}
\bar{\partial} \omega
=
\bar{\partial} (\psi+\varphi )
=
d\bar{z}_k \wedge \tau + \delta + \theta - \delta
=
\eta .
\qedhere
\end{equation*}
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that the construction from the proof of the lemma
reproduces the compactly supported
solution for compactly supported $(0,1)$-forms from \sectionref{sec:compactdbar} provided we start with a
sufficiently large polydisc, of course.
\end{exercise}

\begin{exercise} \label{exercise:dolbeaultlemmaballs}
Prove the lemma if you replace the polydiscs $\Delta_s(w)$ and $\Delta_r(w)$
with two nested balls with center $w$.
\end{exercise}
\end{exbox}

We can now prove the theorem itself.

\begin{proof}[Proof of \thmref{thm:dbaronpoly}]
The proof splits in two cases.  First, suppose that $q > 1$.
Pick a sequence of polydiscs $\Delta_k$ all centered at the origin,
such that $\bigcup_k \Delta_k = \Delta$ and such that $\overline{\Delta_k}
\subset \Delta_{k+1}$ for all $k$.
Using the
\hyperref[lemma:dolgro]{Dolbeault--Grothendieck lemma} with $\Delta_3$ and
$\Delta_4$, we find a smooth form $\omega_1$ defined on
$\Delta_3$ such that $\bar{\partial} \omega_1 = \eta$.
We will construct a sequence of forms $\{ \omega_k \}$
each $\omega_k$ defined in $\Delta_{k+2}$
such that $\omega_{k+1}|_{\Delta_k} = \omega_k|_{\Delta_k}$.
Suppose we have defined $\omega_1,\ldots,\omega_k$.
Using $\Delta_{k+3}$ and $\Delta_{k+4}$ with the lemma again,
define a new form $\sigma$ on $\Delta_{k+3}$ such that $\bar{\partial}
\sigma = \eta$.  What we need to do is to correct $\sigma$ so that it equals
$\omega_k$ on $\Delta_k$.  On $\Delta_{k+2}$, we have
$\bar{\partial}(\omega_k-\sigma) = \eta-\eta = 0$, so $\omega_k-\sigma$
is closed.  The lemma gives a new form $\theta$ on $\Delta_{k+1}$ such that
$\bar{\partial}\theta = \omega_k-\sigma$.
Define a smooth bump function $\varphi$ on $\C^n$ such that $\varphi=1$ on
$\overline{\Delta_{k}}$ and $\varphi$ is compactly supported in
$\Delta_{k+1}$.  Define
$\omega_{k+1} = \sigma + \bar{\partial}(\varphi \theta)$, which can be
defined as a smooth form on $\Delta_{k+3}$ as $\varphi$ is identically zero
where $\theta$ is undefined.  The fact that
$\bar{\partial}^2=0$ ensures that $\bar{\partial}\omega_{k+1} =
\bar{\partial} \sigma = \eta$ on $\Delta_{k+3}$.  On $\Delta_k$, we have
$\omega_{k+1} = \sigma + \bar{\partial}\theta = \sigma+\omega_k-\sigma=\omega_k$.
See \figureref{fig:discsdolbeault}.
The sequence $\{ \omega_k \}$ is defined.
We define $\omega$ on $\Delta$ by simply letting $\omega =
\omega_k$ on $\Delta_k$.

\begin{myfig}
\subimport*{figures/}{discsdolbeault.pdf_t}
\caption{Diagram for defining $\omega_{k+1}=\sigma + \bar{\partial}(\varphi \theta)$.
The dotted line gives $\supp \varphi$.\label{fig:discsdolbeault}}
\end{myfig}

Now assume $q=1$.  By \exerciseref{exercise:zeropsufficient}, it is enough
to consider $p=0$ to simplify notation.  That is, we are now looking for
a smooth function $\omega$ so that $\bar{\partial}\omega = \eta$.
We take the polydiscs $\Delta_k$ as before and define $\omega_1$
on $\Delta_2$ using $\eta$ on $\Delta_3$.
But instead of ensuring that $\omega_{k+1}$ and $\omega_k$
are equal on $\Delta_k$, we will ask that
\begin{equation*}
\sabs{\omega_{k+1}(z)-\omega_{k}(z)} < 2^{-k}
\quad
\text{for } z \in \overline{\Delta_k} .
\end{equation*}
Suppose that such $\omega_1,\ldots,\omega_k$ have been defined.
Use the lemma with $\Delta_{k+3}$ and $\Delta_{k+2}$ to obtain a smooth function
$\sigma$ on $\Delta_{k+2}$ such that
$\bar{\partial} \sigma = \eta$.
The function $\sigma-\omega_k$ is holomorphic (on $\Delta_{k+1}$ where
$\omega_k$ is defined) as
$\bar{\partial}(\sigma-\omega_k) = \eta-\eta = 0$, and hence it has a power
series representation converging uniformly on $\overline{\Delta_k}$.  Thus there exists a
holomorphic polynomial $P$ such that
\begin{equation*}
\sabs{\sigma(z)-\omega_{k}(z) - P(z)} < 2^{-k}
\quad
\text{for } z \in \overline{\Delta_k} .
\end{equation*}
So let $\omega_{k+1} = \sigma-P$.  As $P$ is holomorphic,
we have $\bar{\partial}\omega_{k+1} = \eta$, and we satisfied the
required properties.

On any particular $\overline{\Delta_\ell}$, the sequence $\{ \omega_k \}$ is
uniformly Cauchy as if $m > k \geq \ell$, then
$\sabs{\omega_{m}(z)-\omega_k(z)} <
2^{-k}+2^{-k-1} +\cdots+2^{-m+1} < 2^{-k+1}$.  So the sequence converges
uniformly on compact subsets of $\Delta$ to a function $\omega \colon \Delta
\to \C$.  We need to show that
$\omega$ is smooth and satisfies $\bar{\partial} \omega = \eta$.
The functions
$\omega_{m}-\omega_\ell$
converge uniformly as $m \to \infty$ on $\Delta_{\ell}$.
As
$\bar{\partial}(\omega_m-\omega_\ell) = \eta-\eta = 0$, these functions
are holomorphic, and so the limit $\omega -\omega_\ell$ is holomorphic.
In particular, $\omega$ is smooth as $\omega_\ell$ is smooth and
$\bar{\partial}\omega = \bar{\partial}\omega_\ell = \eta$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the theorem also holds for a ball, say the unit ball $\bB_n \subset
\C^n$.  Hint: Use \exerciseref{exercise:dolbeaultlemmaballs}.
\end{exercise}

\begin{exercise}
For any disc $D \subset \C$, let $D^*$ denote the punctured disc
$D^* = D \setminus \{ a \}$ where $a$ is the center.
Prove that the theorem also holds for
$U = D_1 \times \cdots \times D_k \times D_{k+1}^* \times
\cdots \times D_n^*$ for some (possibly unbounded) discs
$D_1,\ldots,D_n$.
Hint: You may have to prove a slightly more general version of the
Dolbeault--Grothendieck lemma.
Also, see \exerciseref{exercise:Laurentser}.
\end{exercise}

\begin{exercise}
Prove that in one variable, for any domain $U \subset \C$, we have
$H^{(0,1)}(U) = 0$.
\end{exercise}

\begin{exercise}
Prove the theorem for the Hartogs triangle
$T = \bigl\{ (z,w) \in \D^{2} : \sabs{z} > \sabs{w} \bigr\}$.
\end{exercise}

\begin{exercise} \label{exercise:dollocal}
Suppose $U \subset \C^n$ is a domain and $p \in \partial U$.  Suppose there
is some polydisc $\Delta$ centered at $p$ such that
$H^{(0,q)}(U \cap \Delta) \not= 0$, then $H^{(0,q)}(U) \not= 0$.
Hint:  Find a smooth function $\varphi$ on $U \cap \Delta$ such that
$\varphi$ is identically zero near the boundary of $U$ and identically one
near the boundary of $\Delta$.  And use the theorem.
\end{exercise}

\begin{exercise}
Use the \exerciseref{exercise:dollocal} and
\exerciseref{exercise:Levinegdol} to prove that if
$U \subset \C^n$, $n \geq 2$, is a domain with smooth boundary and $p \in \partial U$
such that the Levi form at $p$ has a negative eigenvalue and $n-2$ positive
eigenvalues, then $H^{(0,1)}(U) \not= 0$.
\end{exercise}

\begin{exercise}
Being a subset gives us no relation between the Dolbeault
cohomology groups:  Find domains $U \subset V \subset W \subset \C^2$
such that
$H^{(0,1)}(U) = H^{(0,1)}(W) = 0$, but $H^{(0,1)}(V)$ is nontrivial.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extension from an affine subspace} \label{sec:hypextension}

Let $L \subset \C^n$ be a (complex)
\emph{\myindex{affine subspace}} of dimension $k$,
that is,
the set defined by $M z = c$ for an $(n-k) \times n$ matrix
$M$ of rank $n-k$ and $c \in \C^{n-k}$.
After a complex linear
transformation and translation,
we assume that
$L = \{ z \in \C^n : z_{k+1} = \cdots = z_n = 0 \}$.
We say a function $f$ defined on an open subset of $L$ is
holomorphic
if after this change of coordinates it is holomorphic in the
$z_1,\ldots,z_k$ variables.
If $k=n-1$, we call $L$ a (complex) \emph{\myindex{affine hyperplane}},
and if $k=1$, we call $L$ a \emph{\myindex{complex line}}.

It is easy to see that if $U \subset \C^n$ is open, $L$ is an affine
subspace, and $F \in \sO(U)$, then $F|_{U \cap L}$ is holomorphic.
It is not hard to see that being able to do the inverse may be quite
useful in proofs by induction on dimension,
that is, starting with a holomorphic function $f$ on $U \cap L$
and finding a holomorphic function on $U$ whose restriction is $f$.
If $f$ is just smooth, then finding such a smooth extension is not
difficult no matter what $U$ looks like, however, in the holomorphic
category it is not that easy (or even always possible).
A holomorphic extension from an affine subspace is possible for domains of
holomorphy, in fact, it is a defining characteristic of such domains.

\begin{example} \label{example:noextend}
Suppose $U = \C^2 \setminus \{ 0 \}$ and $L = \{ (z,w) \in \C^2 : w = 0 \}$.
Then $U \cap L$ is the punctured plane.
Let $f(z) = \nicefrac{1}{z}$ be the function on $U \cap L$.  Suppose that
there was an $F \in \sO(U)$ whose restriction to $U \cap L$ was $f$.
Such an $F$ extends to all of $\C^2$, and hence $f$ extends to all of $L$
(which is just the $z$-plane), which is impossible.  Note that we proved
that $H^{(0,1)}(U)$ is nontrivial.
\end{example}

\begin{example} \label{example:ballextend}
Suppose $U = B_2\bigl((0,1)\bigr) \subset \C^2$ and
$L = \{ (z,w) \in \C^2 : w = 0 \}$.
Then $U \cap L$ is a disc of radius $\sqrt{3}$ in the $z$-plane.
Take a holomorphic $f \colon \Delta_{\sqrt{3}}(0) \to \C$ that
does not extend past any point in the boundary of the disc.
The obvious way to extend $f$ to an
$F$ would be to simply take a function $F(z,w) = f(z)$, but that is not
defined on all of $U$.
Extension to all of $U$ is somewhat harder to prove.  See
\figureref{fig:extendingtoball}.  It is possible to do this explicitly in
this specific case (exercise below).  Note that
$H^{(0,1)}(U) = 0$, which you proved
as an exercise in the previous section.
\begin{myfig}
\subimport*{figures/}{extendingtoball.pdf_t}
\caption{Extending $f$ from a hyperplane to the
ball,  it is easy to extend to within the dotted lines.\label{fig:extendingtoball}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Explicitly define the extension of the holomorphic function $f$ to $F$
in the setup of \exampleref{example:ballextend}.
\end{exercise}

\begin{exercise}
Come up with an example domain $U$ and hyperplane $L$ (in $\C^2$)
where $H^{(0,1)}(U)$ is nontrivial, but where
every $f \in \sO(U \cap L)$ extends holomorphically to $U$.
\end{exercise}

\begin{exercise}
Multiple hyperplanes are possible.  Let $L$ be the set $\C^2$
where $z=0$, $M$ be the set where $w=0$, and $N$ be the set where
$z=w$.  Let $f$, $g$, $h$ be holomorphic functions on $L$, $M$, and $N$.
Some extra hypothesis is necessary to tie the
three functions together.  For simplicity,
suppose that the functions and all their first derivatives vanish at
the origin.
Prove that there is a holomorphic
$F$ on $\C^2$ such that
$F|_L = f$,
$F|_M = g$,
$F|_N = h$.
\end{exercise}

\begin{exercise}
Show that you cannot replace the hyperplane with something that has no
complex structure.  Find an example of a domain of holomorphy $U \subset
\C^2$, and a real-analytic $f \colon U \cap \R^2 \to \C$
that is not a restriction of a holomorphic function on $U$.
Note that $f$ does extend holomorphically to some
neighborhood of $U \cap \R^2$ in $U$, just perhaps not all of $U$.
\end{exercise}
\end{exbox}

The way to find the extension in general is to start with a simple
extension to some neighborhood of $L$, such as $F(z,w)=f(z)$,
then extend in a smooth way via a cutoff function to all of $U$.
To make the extension to $U$ holomorphic, we
then correct via the appropriate $\bar{\partial}$-problem.
We discussed the extension of holomorphic functions, but it is no harder to
prove a more general version of the problem about
$\bar{\partial}$-closed $(p,q)$-forms---holomorphic functions
are the $\bar{\partial}$-closed $(0,0)$-forms.

To be able to prove this more general theorem, we need to know what it means
to restrict $(p,q)$-forms to $L$, not just functions.
Instead of making this too complicated, let us do the same
simplification as we did above for functions.
Informally, we will restrict the values of the form
to $L$ and only take the parts of the form that ``point along $L$.''
Let us, also for simplicity, suppose that $L$ is a hyperplane; the more
general case then follows.
As before,
after a linear map and a translation, assume $L = \{ z \in \C^n : z_n = 0 \}$.
Write a $(p,q)$-form as
\begin{equation*}
\eta = \sum_{\alpha\beta} \eta_{\alpha\beta} \, dz_{\alpha} \wedge d\bar{z}_{\beta}
+ \omega_2 \wedge dz_n + \omega_3 \wedge d\bar{z}_n ,
\end{equation*}
where $\alpha$ and $\beta$ do not include $n$ (so no $dz_n$ nor $d\bar{z}_n$
in the first term).  Then
\begin{equation*}
\eta|_L = \sum_{\alpha\beta} \eta_{\alpha\beta}|_L \, dz_{\alpha} \wedge
d\bar{z}_{\beta} .
\end{equation*}
Basically, we restrict the components to $L$, and throw out the differentials
that would take vectors that do not point along $L$.
If $\eta$ is a function (a $(0,0)$-form),
then, of course, this
is just the restriction of the function.

\begin{thm}
Suppose that $U \subset \C^n$ is open with $H^{(p,q+1)}(U) = 0$ and
$L$ is an affine hyperplane.
Let $\psi$ be a smooth $\bar{\partial}$-closed $(p,q)$-form on
$U \cap L$, then there exists a smooth $\bar{\partial}$-closed $(p,q)$-form
$\Psi$ on $U$ such that $\Psi|_{U \cap L} = \psi$.
\end{thm}

So to extend holomorphic functions from a hyperplane,
we need $H^{(0,1)}(U) = 0$.

\begin{proof}
After a translation and a linear map, assume
$L = \{ z \in \C^n : z_n = 0 \}$.  Write $z = (z',z_n)$ as usual.
Let $\chi \colon U \to \R$ be the function that is identically $1$ in some
neighborhood of $U \cap L$ and such that it is identically $0$ in some
neighborhood of $\{ z \in U : (z',0) \notin U \cap L \}$.
See \figureref{fig:extendingcutoff}.

\begin{myfig}
\subimport*{figures/}{extendingcutoff.pdf_t}
\caption{The cutoff function $\chi$.
Note that $\chi \equiv 0$ on a neighborhood of where it is not trivial
to extend $\psi$.\label{fig:extendingcutoff}}
\end{myfig}

We will define
\begin{equation*}
\Psi(z) = \chi(z) \psi(z') + z_n \eta(z) ,
\end{equation*}
where $\eta$ is chosen appropriately so that $\Psi$ is
$\bar{\partial}$-closed.  This form is defined in
$U$ as $\chi$ is identically zero where $\psi$ is undefined.
Clearly, if such an $\eta$ can be found,
then $\Psi|_{U \cap L} = \psi$.
Let us compute
$\bar{\partial} \Psi$
to see what is required of $\eta$.  Where $\psi$
is defined we get
\begin{equation*}
\bar{\partial} \Psi =
\bar{\partial}\chi \wedge \psi
+ \chi \cancelto{0}{\bar{\partial} \psi}
 + \cancelto{0}{\bar{\partial} z_n} \wedge \eta
 + z_n \bar{\partial} \eta
=
\bar{\partial}\chi \wedge \psi
 + z_n \bar{\partial} \eta .
\end{equation*}
As $\bar{\partial} \chi$ is $0$ in a neighborhood of $U \cap L$,
the form $\frac{- \bar{\partial}\chi \wedge \psi}{z_n}$ extends smoothly
through $U \cap L$.
Moreover, it is a
smooth $(p,q+1)$-form on $U$ as
$\bar{\partial}\chi$ is identically zero in a neighborhood of
where $\psi$ is undefined.
The form is
$\bar{\partial}$-closed as $\bar{\partial}^2 = 0$, $\bar{\partial}\psi = 0$,
and $\nicefrac{1}{z_n}$ is holomorphic.
By hypothesis, we find an $\eta$ such that
$\bar{\partial} \eta = \frac{- \bar{\partial}\chi \wedge \psi}{z_n}$,
and we are done.
\end{proof}

We remark that an extension works for zero sets of holomorphic functions, that
is, subvarieties
(see \chapterref{ch:analyticvarieties}), not just
hyperplanes, a result which is called the Cartan extension theorem, but we will
not prove this fact.  However, as an exercise prove the extension for
two hyperplanes.

\begin{exbox}
\begin{exercise}
Suppose that $U \subset \C^n$ is open with $H^{(p,q+1)}(U) = 0$ and
$L_1$ and $L_2$ are two affine hyperplanes such that $U \cap L_1 \cap L_2 =
\emptyset$.
Let $\psi_1$ be a smooth $\bar{\partial}$-closed $(p,q)$-form on
$U \cap L_1$
and $\psi_2$ be a smooth $\bar{\partial}$-closed $(p,q)$-form on
$U \cap L_2$.  Show that there is a smooth
$\bar{\partial}$-closed $(p,q)$-form
$\Psi$ on $U$ such that
$\Psi|_{U \cap L_1} = \psi_1$
and
$\Psi|_{U \cap L_2} = \psi_2$.
\end{exercise}
\end{exbox}

In what follows, when we talk about $U \cap L$ as an open set for a hyperplane
$L$, we think of it as an open set in $\C^{n-1}$.  More generally, if $L$
is a $k$-dimensional affine subspace, then we will treat $U \cap L$
as an open set in $\C^{n-k}$.

\begin{cor}
Suppose $U \subset \C^n$ is open and $L$ is an affine hyperplane.
If $H^{(p,q)}(U) = 0$ and
$H^{(p,q+1)}(U) = 0$ then $H^{(p,q)}(U \cap L) = 0$.
\end{cor}

\begin{proof}
Let $\psi$ be a $\bar{\partial}$-closed $(p,q)$-form on $U \cap L$.
As $H^{(p,q+1)}(U) = 0$, via the theorem,
there is a $\bar{\partial}$-closed $(p,q)$-form
$\Psi$ on $U$ such that
$\Psi|_{U \cap L} = \psi$.
As $H^{(p,q)}(U) = 0$, there is a $(p,q-1)$-form $\Phi$ on $U$
such that $\bar{\partial} \Phi = \Psi$.  It is not difficult to see that
$(\bar{\partial}\Phi)|_{U \cap L} = \bar{\partial}(\Phi|_{U \cap L})$ (true for any form),
so $\varphi = \Phi|_{U \cap L}$ is the solution to $\bar{\partial} \varphi = \psi$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove the claim that
for any $(p,q)$-form, $(\bar{\partial}\Phi)|_{U \cap L} =
\bar{\partial}(\Phi|_{U \cap L})$.
\end{exercise}
\end{exbox}

We have the following immediate corollary.

\begin{cor} \label{cor:dolgodown}
Suppose that $U \subset \C^n$ is open
such that $H^{(0,q)}(U) = 0$ whenever $1 \leq q \leq n-1$
and $L$ is an affine hyperplane.
Then $H^{(0,q)}(U \cap L) = 0$ whenever $1 \leq q \leq n-2$.
\end{cor}

\begin{exbox}
\begin{exercise}
Prove a more general corollary.
Suppose that $U \subset \C^n$ is a domain and suppose that
$L$ is a $k$-dimensional affine subspace $1 \leq k \leq n-1$.
Suppose that $H^{(0,q)}(U) = 0$ whenever $1 \leq q \leq n-1$.  Then
$H^{(0,q)}(U \cap L) = 0$ whenever $1 \leq q \leq k-1$,
and every $f \in \sO(U \cap L)$ has a holomorphic extension to $U$.
\end{exercise}
\end{exbox}

We now prove the general version of one direction of the theorem mentioned
in the previous section.  As an exercise, you proved by direct
construction that if $H^{(0,1)}(U) = 0$ in $\C^2$, then $U$ is a domain of
holomorphy.  We extend this result to $\C^n$.

\begin{thm} \label{thm:dolhomimpliesdoh}
Suppose $U \subset \C^n$ is domain such that $H^{(0,q)}(U) = 0$
whenever $1 \leq q \leq n-1$.  Then $U$ is a domain of holomorphy.
\end{thm}

\begin{proof}
We induct on dimension.  When $n=1$, the hypothesis just says that
$U$ is a domain, and any domain in $\C$ is a domain of holomorphy.
Assume the theorem holds for any domain in $\C^{n-1}$.

Let $U \subset \C^n$ be a domain and
suppose that $V$ and $W$ are open sets in $\C^n$
such that $\emptyset \not= V \subset U \cap W$, $W$ is connected, and $W$ contains points outside
of $U$.  That is, $V$ and $W$ are like in the definition of the domain of
holomorphy.  We want to show that there must exist at least one $F \in
\sO(U)$ for which there does not exist any $G \in \sO(W)$ such that $F=G$ on
$V$.
Consider a component of $W \cap U$ that contains some component of $V$.
Without loss of generality we could take $V$ to be any small ball in this
component of $W \cap U$.
It is not difficult to check (exercise) that there exists such a $V$
(move it around if you must)
so that there is a point $z_0 \in \partial U \cap \partial V \cap W$.

\begin{exbox}
\begin{exercise}
Prove that such a $V$ exists.
\end{exercise}
\end{exbox}

There is some affine hyperplane $L$ through $z_0$ such that
the boundary of $V \cap L$ (in the topology of $L$) includes $z_0$,
and hence the boundary of $U \cap L$ (in the topology of $L$)
includes $z_0$.
Here we use the fact that $V$ is a ball; pick an $L$ that includes the
normal direction to the boundary of $V$ at $z_0$.  See
\figureref{fig:extendingdohVW}.
By \corref{cor:dolgodown},
$H^{(0,q)}(U \cap L) = 0$ for $1 \leq q \leq n-2$.
By the induction hypothesis
(and \exerciseref{exercise:dobeaultdisconnected}),
every component of $U \cap L$ is a domain of holomorphy.
Since $z_0$ is in the boundary of $U \cap L$, there exists a holomorphic function
$f \in \sO(U \cap L)$ that does not extend (along $L$) through $z_0$.
As $H^{(0,1)}(U) = 0$, there exists an $F \in \sO(U)$
such that $F|_{U \cap L} = f$.  If there existed a $G \in \sO(W)$ that
agreed with $F$ on $V$, then $G|_{W \cap L}$ would be an extension of $f$
through $z_0$, which we know is impossible, so no such $G$ exists.
\begin{myfig}
\subimport*{figures/}{extendingdohVW.pdf_t}
\caption{Location of $z_0$ and the placement of $V$ and $L$ with
respect to $U$ and $W$.\label{fig:extendingdohVW}}
\end{myfig}
\end{proof}

If you think about what we really needed in the proof, it was not the cohomology
vanishing, we needed the extension.  It is sufficient to extend from
complex lines, that is, affine subspaces of dimension $1$, since
the intersection of $U \cap L$ in that case is always a domain of
holomorphy.  On the other hand, it is not sufficient to have extension
from hyperplanes, see \exerciseref{exercise:extensionfromhypnotenough}.

\begin{thm} \label{thm:extensionfromlinesmeansDOH}
Suppose $U \subset \C^n$ is domain such that for every
complex line $L$ and every $f \in \sO(U \cap L)$ there
exists an $F \in \sO(U)$ where $F|_{U \cap L} = f$.
Then $U$ is a domain of holomorphy.
\end{thm}

\begin{exbox}
\begin{exercise}
Prove \thmref{thm:extensionfromlinesmeansDOH}.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is domain of holomorphy
and $L$ is an affine subspace of dimension $k$.
Show that
every component of $U \cap L$ is a domain of holomorphy.
\end{exercise}

\begin{exercise}
Suppose $U \subset \C^n$ is domain and $k \in \N$
is such that
$H^{(0,q)}(U)=0$ for all $1 \leq q \leq n-k$,
and
every component of
$U \cap L$ is a domain of holomorphy for
every affine subspace $L$ of dimension $k$.
Prove that $U$ is a domain of holomorphy.
\end{exercise}

\begin{exercise} \label{exercise:extensionfromhypnotenough}
Extension from hyperplanes is not enough:
Find a domain $U \subset \C^3$ that is not a domain of holomorphy,
such that for every affine hyperplane $L$
and every $f \in \sO(U \cap L)$ there is an $F \in \sO(U)$
such that $F|_{U \cap L} = f$.
Hint: Modify \exampleref{example:noextend}.
\end{exercise}
\end{exbox}

\begin{remark}
We have by now stated several equivalent conditions for a domain to be a
domain of holomorphy, although we have not proved all the implications in
this book.  In particular, for a domain $U \subset \C^n$, the following are
equivalent:
\begin{enumerate}[(i)]
\item
$U$ is a domain of holomorphy.
\item
$U$ is Levi pseudoconvex (if $U$ has smooth boundary).
\item
$U$ is Hartogs pseudoconvex (continuous plurisubharmonic exhaustion function).
\item
$-\log \rho(z)$ is plurisubharmonic ($\rho$ is distance to $\partial U$).
\item
$U$ is convex with respect to plurisubharmonic functions.
\item
$U$ is holomorphically convex.
\item
$\dist(K,\partial U) = \dist(\hat{K}_U,\partial U)$ for every $K \subset
\subset U$.
\item
$H^{(0,q)}(U) = 0$ for all $1 \leq q \leq n-1$.
\item
Every $f \in \sO(U \cap L)$ extends holomorphically to $U$ for every
complex line $L$.
\end{enumerate}
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Cousin problems} \label{sec:cousin}

A chapter in many a book on one complex variable is devoted to the
Mittag-Leffler theorem on finding a meromorphic function with prescribed
poles, and another one on a theorem of Weierstrass for finding a holomorphic
function with a prescribed zero set.  The analogues of these results in
several complex variables are the so-called Cousin I and Cousin II
problems\footnote{Named for Pierre Cousin, the French mathematician, and not
for some family scuffle.}
respectively.  The Cousin I problem is an additive
version of the problem and obtains an analogue to Mittag-Leffler.
The Cousin II problem is the multiplicative version to obtain an
analogue of Weierstrass.

\begin{defn}[\myindex{Cousin I}]\index{First Cousin problem}
Suppose $U \subset \C^n$ is open.
Let $\{ U_{\iota} \}_{\iota \in I}$ be an open covering of $U$,
and
when $U_\iota \cap U_\kappa \not= \emptyset$,
let $h_{\iota\kappa} \in \sO(U_{\iota} \cap U_{\kappa})$ be such that
\begin{equation*}
\begin{aligned}
& h_{\iota\kappa}+h_{\kappa\iota} = 0 & &\text{in } U_\iota \cap U_\kappa ,
\\
& h_{\iota\kappa}+h_{\kappa\lambda}+h_{\lambda\iota} = 0 & & \text{in }
U_\iota \cap U_\kappa \cap U_{\lambda} .
\end{aligned}
\end{equation*}
The covering and the functions $h_{\iota\kappa}$ are called
\emph{\myindex{Cousin I data}}.
The \emph{\myindex{solution of the Cousin I problem}}
is a set of
holomorphic functions $f_\iota \in \sO(U_\iota)$ such that
\begin{equation*}
h_{\iota\kappa} = f_\iota-f_\kappa.
\end{equation*}
\end{defn}

Clearly if the functions $f_\iota$ exist, then their differences must
satisfy the two conditions.  The point is going the other way: finding
$f_\iota$ given $h_{\iota \kappa}$.
We remark that we could always take $\iota$, $\kappa$, and $\lambda$
to be distinct,
as the first condition means that $h_{\iota\iota} = 0$ for all $\iota$
otherwise, and the second condition then just reduces to the first if two
of the indices are equal.

\begin{exbox}
\begin{exercise}
The triple sum is enough to force the similar condition on $4$ or more
summands, but the double sum is not enough.  That is,
\begin{exparts}
\item
Find an example
where the first condition $h_{\iota\kappa}+h_{\kappa\iota} = 0$ is
satisfied but the second condition
$h_{\iota\kappa}+h_{\kappa\lambda}+h_{\lambda\iota} = 0$ is not.
\item
Show that if both conditions are satisfied (we have valid Cousin I data),
then
$h_{\iota\kappa}+h_{\kappa\lambda}+h_{\lambda\mu} + h_{\mu\iota} = 0$,
and similarly for any number of terms.
\end{exparts}
\end{exercise}
\end{exbox}

To see how Cousin I relates to Mittag-Leffler, note that Mittag-Leffler
could be stated by giving meromorphic functions locally and then trying
to piece them together.
Recall that meromorphic functions are locally
a ratio of holomorphic functions.
So suppose that we have a covering $\{ U_\iota \}$ and
in each $U_\iota$ we have a meromorphic function $g_\iota$ such that
$g_{\iota}-g_{\kappa}$
is holomorphic in $U_{\iota} \cap U_{\kappa}$ (or more precisely, extends to
be holomorphic on that set).  That is, if we were in one
dimension, the two functions would have the same principal part.
The solution is to find a global meromorphic function with the same
singular behavior (principal part).

On $U_\iota \cap U_\kappa$, let
\begin{equation*}
h_{\iota\kappa} = g_\iota-g_\kappa .
\end{equation*}
It is easy to see that we obtain Cousin I data.  Suppose
the Cousin I problem is solvable in $U$.  Then we would find
holomorphic $f_{\iota}$ as above.  We define a meromorphic function $f$
on $U$
by defining it in each $U_\iota$ as
\begin{equation*}
f = g_\iota - f_\iota .
\end{equation*}
The function is well-defined.  Indeed, on $U_\iota \cap U_\kappa$ the
possible definitions are
$g_\iota - f_\iota$
and
$g_\kappa - f_\kappa$, and their difference is zero:
\begin{equation*}
(g_\iota - f_\iota)-
(g_\kappa - f_\kappa)
=
(g_\iota - g_\kappa)
+
(f_\kappa - f_\iota)
=
h_{\iota\kappa}+h_{\kappa\iota} = 0.
\end{equation*}
The function $f$ has the same singularity as $g_\iota$ in $U_\iota$ since
$f-g_\iota$ is holomorphic.

So the data gives some local solution to some problem
and the solution of the Cousin problem gives
a way of gluing the local data together into a global solution.  We state
this as a proposition (the converse also holds but we do not prove it here):

\begin{prop}
Suppose $U \subset \C^n$ is open,
$\{ U_{\iota} \}_{\iota \in I}$ an open covering of $U$, and $g_\iota$
are meromorphic functions on each $U_\iota$ such that $g_\iota - g_\kappa$
is holomorphic on $U_\iota \cap U_\kappa$.  Suppose that the Cousin I
problem is solvable on $U$.  Then there exists a meromorphic function $f$ on
$U$ such that on each $U_\iota$, $f-g_\iota$ is holomorphic.
\end{prop}

In one dimension, Cousin I (that is, Mittag-Leffler) is solvable for
any domain in $\C$.
In several variables, Cousin I is solvable on domains of holomorphy in
$\C^n$, in particular, it is solvable when we can solve the
$\bar{\partial}$-problem for $(0,1)$-forms.

\begin{example}
Let us see an example where the Cousin I problem is not solvable.  Let $U =
\C^2 \setminus \{ 0 \}$, where we know that $H^{(0,1)}(U)$ is nontrivial.
Write $U = U_1 \cup U_2$ where
$U_1 = \{ (z,w) \in \C^2 : z \not= 0 \}$ and
$U_2 = \{ (z,w) \in \C^2 : w \not= 0 \}$.  On $U_1 \cap U_2$, define
\begin{equation*}
h_{12}(z,w) = \frac{1}{zw} , \quad
h_{21}(z,w) = \frac{-1}{zw} .
\end{equation*}
These are holomorphic functions giving Cousin I data.
Suppose the problem was solvable and we find
$f_1 \in \sO(U_1)$ and
$f_2 \in \sO(U_2)$ such that on $U_1 \cap U_2$, we have
\begin{equation*}
f_1 -f_2 = h_{12} = \frac{1}{zw} .
\end{equation*}
In other words, $f_1 = \frac{1}{zw}+f_2$ on $U_1 \cap U_2$.
But that means that $zf_1$ is holomorphic in $U_2$ and
therefore in $U$, and thus extends to $\C^2$.  Similarly, $wf_2$ extends to a
holomorphic function on $\C^2$.  Thus
$zwf_1 -zwf_2$ is a holomorphic function on $\C^2$ that vanishes at the
origin, but $zwf_1 -zwf_2 = zw(h_{12}) \equiv 1$ on $U_1 \cap U_2$,
which leads to a contradiction.
\end{example}

The Cousin I problem with smooth data is smoothly solvable in any domain.

\begin{lemma}
Suppose $U \subset \C^n$ is open,
$\{ U_{\iota} \}_{\iota \in I}$ be an open covering of $U$,
and let $h_{\iota\kappa}$ be smooth (not necessarily holomorphic) Cousin I
data.  Then there exist smooth (not necessarily holomorphic) solution
functions $f_\iota$.
\end{lemma}

\begin{proof}
Find a smooth partition of unity $\{ \varphi_\gamma \}_{\gamma \in \Gamma}$
subordinate to
the cover $\{ U_{\iota} \}_{\iota \in I}$.  That is, $\varphi_\gamma$ are
smooth functions of $U$ valued in $[0,1]$ that add up to $1$ at every point,
in a neighborhood of any point only finitely many $\varphi_\gamma$ are
nonzero, and each $\varphi_{\gamma}$ is supported in some $U_\kappa$,
so denote such $\kappa \in I$ as $\kappa_\gamma$.
For $z \in U_\iota$, let
\begin{equation*}
f_\iota(z) = \sum_{\gamma \in \Gamma} \varphi_\gamma(z) h_{\iota \kappa_\gamma}(z) .
\end{equation*}
Note why this is well-defined and smooth:
For $z \in U_\iota$, given a $\gamma$ and hence $\kappa_\gamma$,
either $\varphi_\gamma=0$ in some neighborhood of $z$,
or $z \in U_{\kappa_\gamma} \cap U_\iota$.
So each term can be interpreted as a smooth function on
$U_\iota$, and in a neighborhood of $z$, we are adding at most finitely
smooth functions, so $f_\iota$ is smooth in $U_\iota$.
For $z \in U_\iota \cap U_\lambda$,
\begin{equation*}
f_\iota(z)-f_\lambda(z) =
\sum_{\gamma \in \Gamma} \varphi_\gamma(z)
\bigl( h_{\iota \kappa_\gamma}(z) - h_{\lambda \kappa_\gamma}(z) \bigr)
=
\sum_{\gamma \in \Gamma} \varphi_\gamma(z)
h_{\iota\lambda}(z)
=
h_{\iota\lambda}(z) .
\qedhere
\end{equation*}
\end{proof}

The (holomorphic) Cousin I problem
is solvable on any domain of holomorphy and in general
on any domain with a trivial first Dolbeault cohomology group.

\begin{thm}
Suppose $U \subset \C^n$ is a domain with $H^{(0,1)}(U)=0$.
Then the Cousin I problem is solvable.
\end{thm}

\begin{proof}
Let
$\{ U_{\iota} \}_{\iota \in I}$ be an open covering of $U$,
and let $h_{\iota\kappa}$ be (holomorphic) Cousin I
data.
Using the lemma, find the smooth solutions $f_\iota$.
The functions $f_\iota$ need not be holomorphic, but
$f_\iota-f_\kappa = h_{\iota\kappa}$ are holomorphic, and so
$\bar{\partial}f_\iota-\bar{\partial}f_\kappa = \bar{\partial}(f_\iota-f_\kappa) = 0$.  The $(0,1)$-form $\eta$ given by
\begin{equation*}
\eta = \bar{\partial}f_\iota
\end{equation*}
is therefore well-defined on $U$.  Moreover, $\eta$ is
$\bar{\partial}$-closed, so by assumption on $U$,
there exists a smooth function $\psi$ such that $\bar{\partial}\psi = \eta$.
On each $U_\iota$,
\begin{equation*}
F_\iota = f_\iota - \psi .
\end{equation*}
On $U_\iota \cap U_\kappa$, we have
\begin{equation*}
F_\iota - F_\kappa = f_\iota-f_\kappa = h_{\iota \kappa} ,
\end{equation*}
and on $U_\iota$, we have
\begin{equation*}
\bar{\partial}F_\iota = \bar{\partial}f_\iota - \bar{\partial}\psi =
\eta-\eta  = 0.
\end{equation*}
Therefore, the $F_{\iota}$ give a solution to the Cousin I problem.
\end{proof}

\begin{cor}
Cousin I problem is solvable on any possibly unbounded polydisc in $\C^n$.
\end{cor}

\begin{exbox}
\begin{exercise}
Use the solution of the $\bar{\partial}$-problem in the disc to show that
if the Cousin I problem is solvable in a domain $U \subset \C^n$, then
$H^{(0,1)}(U) = 0$.  Hint: Solve locally and then follow the same idea
as trying to piece together the meromorphic function above.
\end{exercise}

\begin{exercise}
Formulate a version of Cousin I problem for integer-valued continuous
functions on domains in $\R^2$.  Prove that the problem is
not always solvable in $\R^2 \setminus \{ (0,0) \}$.
\end{exercise}
\end{exbox}

Let us briefly mention the second Cousin problem and its relation to the
Cousin I problem and to the theorem of Weierstrass.

\begin{defn}[\myindex{Cousin II}]\index{Second Cousin problem}
Suppose $U \subset \C^n$ is open.
Let $\{ U_{\iota} \}_{\iota \in I}$ be an open covering of $U$,
and
when $U_\iota \cap U_\kappa \not= \emptyset$,
let $h_{\iota\kappa} \in \sO(U_{\iota} \cap U_{\kappa})$ be nonvanishing
functions such that
\begin{equation*}
\begin{aligned}
& h_{\iota\kappa}h_{\kappa\iota} = 1 & &\text{in } U_\iota \cap U_\kappa ,
\\
& h_{\iota\kappa}h_{\kappa\lambda}h_{\lambda\iota} = 1 & & \text{in }
U_\iota \cap U_\kappa \cap U_{\lambda} .
\end{aligned}
\end{equation*}
The covering and the functions $h_{\iota\kappa}$ are called
\emph{\myindex{Cousin II data}}.
The \emph{\myindex{solution of the Cousin II problem}}
is a set of
nonvanishing holomorphic functions $f_\iota \in \sO(U_\iota)$ such that
\begin{equation*}
h_{\iota\kappa} = \frac{f_\iota}{f_\kappa}.
\end{equation*}
\end{defn}

Cousin II is the analogue of the Weierstrass product theorem, that is,
finding a function with a prescribed zero set.
Suppose that $M \subset U$ is locally given by the vanishing of
a single holomorphic function with a nonvanishing derivative
(a complex
submanifold of codimension $1$)\footnote{%
The nonvanishing condition on the derivative is not necessary, we use it for simplicity.},
that is, for every $p \in U$, there is a
neighborhood $U_\iota$ and a holomorphic $g_\iota$ with $dg_{\iota} \not= 0$
such that $g_\iota^{-1}(0) = M \cap U_\iota$.
On $U_\iota \cap U_\kappa$, let
\begin{equation*}
h_{\iota\kappa} = \frac{g_\iota}{g_\kappa}.
\end{equation*}

\begin{exbox}
\begin{exercise}
Prove that $h_{\iota\kappa}$ is holomorphic and nonvanishing.
\end{exercise}
\end{exbox}

Thus we have Cousin II data.  If the Cousin II problem is solvable,
we have $f_\iota$ as above.  Define a holomorphic function $f$ on $U$
by defining it on each $U_\iota$ via
\begin{equation*}
f = \frac{g_{\iota}}{f_\iota} .
\end{equation*}
Similarly as before, this gives a well-defined function, and clearly it
vanishes precisely on $M$.  Moreover, the derivative is nonzero on $M$.
We state this result as a proposition.

\begin{prop}
Suppose $U \subset \C^n$ is a domain on which the Cousin II problem is
solvable and $M \subset U$ is a complex hypersurface (locally the zero set
of a holomorphic function with nonvanishing derivative).
Then there exists an $f \in \sO(U)$ such that $f^{-1}(0) = M$ and
$df \not=0$ on~$M$.
\end{prop}

The second Cousin problem is not always solvable on every domain of
holomorphy like the Cousin I problem.  An extra condition on the topology of
the domain is necessary.  Interestingly, on a domain of holomorphy, if the
Cousin II problem is solvable just continuously, then it is solvable.
We will skip the proof of this fact, but let us describe how
the topological obstruction arises.
Suppose we have Cousin II data $h_{\iota\kappa}$.  We refine the
covering to make the sets $U_{\iota}$ and their intersections
$U_\iota\cap U_\kappa$ simply connected.  Then we take logarithms
$g_{\iota\kappa} = \log h_{\iota\kappa}$, where we pick the correct
branch to also get $g_{\kappa\iota} = -g_{\iota\kappa}$, so
$g_{\iota\kappa}+g_{\kappa\iota} = 0$.  For the triple sum
we get
\begin{equation*}
g_{\iota\kappa}+g_{\kappa\lambda}+g_{\lambda\iota} = 2\pi i
m_{\iota\kappa\lambda}
\end{equation*}
for some integer $m_{\iota\kappa\lambda}$.
It is not always possible to pick the branches in
such a way as to make $m_{\iota\kappa\lambda}=0$ for all indices.
Were it possible, we could try to solve
this corresponding Cousin I problem.
This is a question of
plain old singular cohomology, that is, topology.\footnote{%
For the interested reader, the needed extra cohomology condition is $H^2(U,\Z)=0$.}

\begin{exbox}
\begin{exercise}
\pagebreak[2]
Suppose $U \subset \C^n$ is a domain with $H^{(0,1)}(U)=0$
and $M \subset U$ is a complex hypersurface.
Suppose there is a continuous function $g$ on $U$
such that locally near every $p \in M$, if $r$ is a defining function for $M$
(holomorphic with nonvanishing derivative),
then $\nicefrac{r}{g}$ extends to be continuous and nonvanishing in a
neighborhood of $p$.  Prove that there exists an $f \in \sO(M)$
such that $f^{-1}(0) = M$ and $df \not= 0$ on $M$.
Hint: Cover with balls $U_{\iota}$ and in each ball define
$\log\bigl(\nicefrac{f_\iota}{g}\bigr)$, then obtain a Cousin I problem.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $U \subset \C$ be a domain and
assume the Cousin II problem is solvable (it always is in $\C$).
Prove the classical theorem of Weierstrass using Cousin II\@.  That is,
given a countable set of points in $U$ and multiplicities, and assuming the
set has no limit points in $U$, find a function $f \in \sO(U)$ that has
zeros precisely at the given points of precisely the given multiplicities.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Integral Kernels} \label{ch:integralkernels}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bochner--Martinelli kernel}

A generalization of
Cauchy's formula to several variables
is called the Bochner--Martinelli integral formula,
which reduces
to Cauchy's (Cauchy--Pompeiu) formula when $n=1$.
As for Cauchy's formula, we will prove the formula for all
smooth functions via Stokes' theorem.
First, let us define the \emph{\myindex{Bochner--Martinelli kernel}}:
\begin{equation*}
\omega(\zeta,z)
\overset{\text{def}}{=}
\frac{(n-1)!}{{(2\pi i)}^n}
\sum_{k=1}^n
\frac{\bar{\zeta}_k-\bar{z}_k}{\norm{\zeta-z}^{2n}}
\,
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_k } \wedge d\zeta_k \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n .
\end{equation*}
\glsadd{not:hatremove}%
The notation $\widehat{ d\bar{\zeta}_k }$ means that this term is
simply left out.

\begin{thm}[Bochner--Martinelli] \label{thm:bochnermartinelli}
\index{Bochner--Martinelli integral formula}
Let $U \subset \C^n$ be a bounded open set with smooth boundary and let
$f \colon \widebar{U} \to \C$ be a smooth function.
Then for $z \in U$,
\begin{equation*}
f(z) =
\int_{\partial U}
f(\zeta) \omega(\zeta,z)
-
\int_{U}
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z) .
\end{equation*}
In particular, if $f \in \sO(U)$, then
\begin{equation*}
f(z) =
\int_{\partial U}
f(\zeta) \omega(\zeta,z) .
\end{equation*}
\end{thm}

Recall that if $\zeta = x+iy$ are the coordinates in $\C^n$, the orientation that we assigned to $\C^n$ in
this book\footnote{Again, there is
no canonical orientation of $\C^n$, and
not all authors follow this (perhaps more prevalent) convention.}
is the one corresponding to the volume form
\glsadd{not:dV}%
\begin{equation*}
dV = dx_1 \wedge dy_1 \wedge dx_2 \wedge dy_2 \wedge \cdots \wedge dx_n \wedge dy_n .
\end{equation*}
With this orientation,
\begin{equation*}
d\zeta_1 \wedge d\bar{\zeta}_1 \wedge
d\zeta_2 \wedge d\bar{\zeta}_2 \wedge
\cdots \wedge
d\zeta_n \wedge d\bar{\zeta}_n = {(-2i)}^n dV ,
\end{equation*}
and hence
\begin{equation*}
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
d\bar{\zeta}_2 \wedge d\zeta_2 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n = {(2i)}^n dV .
\end{equation*}

\begin{exbox}
\begin{exercise}
Similarly to the Cauchy--Pompeiu formula,
note the singularity in the second term of the Bochner--Martinelli formula.
Prove that the integral still makes
sense (the function is integrable).
\end{exercise}

\begin{exercise}
Check that for $n=1$, the Bochner--Martinelli formula
reduces to the standard Cauchy--Pompeiu formula.
\end{exercise}
\end{exbox}

Recall the definition of $\partial$ and $\bar{\partial}$ from
\defnref{defn:danddbarform}, and recall that $d \eta = \partial \eta +
\bar{\partial} \eta$.

\begin{proof}[Proof of Bochner--Martinelli]
The structure of the proof is essentially the same as that of
the Cauchy--Pompeiu theorem for $n=1$, although some of the formulas are
more involved.

Let $z \in U$ be fixed.  Suppose $r > 0$ is small enough so that
$\overline{B_r(z)} \subset U$.  Orient both
$\partial U$ and $\partial B_r(z)$
positively.  As
$f(\zeta) \omega(\zeta,z)$ contains all the holomorphic $d\zeta_k$,
\begin{equation*}
\begin{split}
d \bigl( f(\zeta) \omega(\zeta,z) \bigr)
& =
\bar{\partial} \bigl( f(\zeta) \omega(\zeta,z) \bigr)
\\
& =
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z)
\\
& \phantom{=} +
f(\zeta)
\frac{(n-1)!}{{(2\pi i)}^n}
\sum_{k=1}^n
\frac{\partial}{\partial \bar{\zeta}_k} \left[
\frac{\bar{\zeta}_k-\bar{z}_k}{\norm{\zeta-z}^{2n}}
\right]
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n .
\end{split}
\end{equation*}
We compute
\begin{equation*}
\sum_{k=1}^n
\frac{\partial}{\partial \bar{\zeta}_k}
\left[
\frac{\bar{\zeta}_k-\bar{z}_k}{\norm{\zeta-z}^{2n}}
\right]
=
\sum_{k=1}^n
\left(
\frac{1}{\norm{\zeta-z}^{2n}}
-n
\frac{\abs{\zeta_k-z_k}^2}{\norm{\zeta-z}^{2n+2}}
\right)
= 0 .
\end{equation*}
Therefore,
$d \bigl( f(\zeta) \omega(\zeta,z) \bigr) = \bar{\partial} f(\zeta) \wedge
\omega(\zeta,z)$.
We apply Stokes:
\begin{equation*}
\begin{split}
\int_{\partial U}
f(\zeta) \omega(\zeta,z)
-
\int_{\partial B_r(z)}
f(\zeta) \omega(\zeta,z)
& =
\int_{U \setminus \overline{B_r(z)}}
d \bigl( f(\zeta) \omega(\zeta,z) \bigr)
\\
%& =
%\int_{U \setminus \overline{B_r(z)}}
%\bar{\partial} \bigl( f(\zeta) \omega(\zeta,z) \bigr)
%\\
& =
\int_{U \setminus \overline{B_r(z)}}
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z) .
\end{split}
\end{equation*}
Again, due to the integrability, which you showed in an exercise above,
the right-hand side converges to the integral over $U$ as $r \to 0$.
Just as for the Cauchy--Pompeiu formula, we now need to show that the integral
over $\partial B_r(z)$ goes to $f(z)$ as $r \to 0$.

So
\begin{equation*}
\int_{\partial B_r(z)}
f(\zeta) \omega(\zeta,z)
=
f(z)
\int_{\partial B_r(z)}
\omega(\zeta,z)
+
\int_{\partial B_r(z)}
\bigl(f(\zeta)-f(z)\bigr) \omega(\zeta,z) .
\end{equation*}
To finish the proof,
we will show that
$\int_{\partial B_r(z)}
\omega(\zeta,z) = 1$, and that the second term goes to zero.
We apply Stokes again
and note that the volume of $B_r(z)$ is
$\frac{\pi^n}{n!}r^{2n}$.
\begin{multline*}
\int_{\partial B_r(z)}
\omega(\zeta,z)
\\
\begin{aligned}
&=
\int_{\partial B_r(z)}
\frac{(n-1)!}{{(2\pi i)}^n}
\sum_{k=1}^n
\frac{\bar{\zeta}_k-\bar{z}_k}{\norm{\zeta-z}^{2n}} \,
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_k } \wedge d\zeta_k \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{\partial B_r(z)}
\sum_{k=1}^n(\bar{\zeta}_k-\bar{z}_k)
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_k } \wedge d\zeta_k \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
\!
d\left(
\sum_{k=1}^n(\bar{\zeta}_k-\bar{z}_k)
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
\widehat{ d\bar{\zeta}_k } \wedge d\zeta_k \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\right)
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
n~
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
&=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
n
{(2i)}^n dV
%\\
%&=
%\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
%n
%{(-2i)}^n
%\frac{\pi^n}{n!} r^{2n}
=
1 .
\end{aligned}
\end{multline*}

Next, we tackle the second term.
Via the same computation as above we find
\begin{multline*}
%\begin{split}
\int_{\partial B_r(z)}
\bigl(f(\zeta)-f(z)\bigr)
\omega(\zeta,z)
%&=
\\
=
\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\Biggl(
\int_{B_r(z)}
\bigl(f(\zeta)-f(z)\bigr)
n~
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n
\\
%&\phantom{=}\ +
%& +
+
%\frac{(n-1)!}{{(2\pi i)}^n}\frac{1}{r^{2n}}
\int_{B_r(z)}
\sum_{k=1}^n
\frac{\partial f}{\partial \bar{\zeta}_k}(\zeta)
(\bar{\zeta}_k-\bar{z}_k)
~
d\bar{\zeta}_1 \wedge d\zeta_1 \wedge
\cdots \wedge
d\bar{\zeta}_n \wedge d\zeta_n \Biggr).
%\end{split}
\end{multline*}
As $U$ is bounded, $\abs{f(\zeta)-f(z)} \leq M
\snorm{\zeta-z}$
and
$\abs{\frac{\partial f}{\partial \bar{\zeta}_k}(\zeta)
(\bar{\zeta}_k-\bar{z}_k)} \leq M \snorm{\zeta-z}$ for some $M$.
So for all $\zeta \in \partial B_r(z)$,
we have
$\abs{f(\zeta)-f(z)} \leq Mr$
and $\abs{\frac{\partial f}{\partial \bar{\zeta}_k}(\zeta)
(\bar{\zeta}_k-\bar{z}_k)} \leq Mr$.
Hence
\begin{multline*}
\abs{
\int_{\partial B_r(z)}
\bigl(f(\zeta)-f(z)\bigr)
\omega(\zeta,z)
}
\\
\leq
\frac{(n-1)!}{{(2\pi)}^n}\frac{1}{r^{2n}}
\left(
\int_{B_r(z)}
n 2^n Mr \, dV
+
\int_{B_r(z)}
n 2^n Mr \, dV
\right)
=
2 M r .
\end{multline*}
Therefore, this term goes to zero as $r \to 0$.
\end{proof}

One drawback of the Bochner--Martinelli formula $\int_{\partial U} f(\zeta)
\omega(\zeta,z)$ is that the kernel is not
holomorphic in $z$ unless $n=1$.  It does not simply produce
holomorphic functions.  If we differentiate in $\bar{z}$ underneath the
$\partial U$ integral, we do not necessarily obtain zero.
On the other hand, we have an explicit formula and this formula does not
depend on $U$.  This is not the case for the Bergman and Szeg\"o
kernels, which we will see next, although those are holomorphic in the
right way.

\begin{exbox}
\begin{exercise}
Prove that if $z \notin \widebar{U}$, then rather than $f(z)$ in the
formula you obtain
\begin{equation*}
\int_{\partial U}
f(\zeta) \omega(\zeta,z)
-
\int_{U}
\bar{\partial} f(\zeta) \wedge \omega(\zeta,z) = 0 .
\end{equation*}
\end{exercise}

\begin{exercise}
Suppose $f$ is holomorphic on a neighborhood of
$\overline{B_r(z)}$.
\begin{exparts}
\item
Using the Bochner--Martinelli formula, prove that
\begin{equation*}
f(z) = \frac{1}{V\bigl(B_r(z)\bigr)} \int_{B_r(z)} f(\zeta) \, dV(\zeta) ,
\end{equation*}
where $V\bigl(B_r(z)\bigr)$ is the volume of $B_r(z)$.
\item
Use part a) to prove the maximum principle for holomorphic functions.
\end{exparts}
\end{exercise}

\begin{exercise}
Use Bochner--Martinelli for the solution of $\bar{\partial}$ with compact
support.  That is, suppose $g = g_1 d\bar{z}_1 + \cdots + g_n d\bar{z}_n$
is a smooth compactly supported $(0,1)$-form
on $\C^n$, $n \geq 2$, and
$\frac{\partial g_k}{\partial \bar{z}_\ell} =
\frac{\partial g_\ell}{\partial \bar{z}_k}$ for all $k, \ell$.
Prove that
\begin{equation*}
\psi(z) = - \int_{\C^n} g(\zeta) \wedge \omega(\zeta,z)
\end{equation*}
is a compactly supported smooth solution to $\bar{\partial} \psi = g$.
Hint: Look at the previous proof.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Bergman kernel}

Let $U \subset \C^n$ be a domain.  Define
\emph{\myindex{Bergman space}} of $U$:
\glsadd{not:A2}%
\glsadd{not:L2}%
\begin{equation*}
A^2(U) \overset{\text{def}}{=} \sO(U) \cap L^2(U) .
\end{equation*}
That is, $A^2(U)$ denotes the space
of holomorphic functions $f \in \sO(U)$ such that
\glsadd{not:L2norm}%
\begin{equation*}
\snorm{f}_{A^2(U)}^2 \overset{\text{def}}{=} \snorm{f}_{L^2(U)}^2
= \int_U \sabs{f(z)}^2 dV < \infty .
\end{equation*}
$A^2(U)$ is
an inner product space with the
$L^2(U)$ inner product
\glsadd{not:L2innprod}%
\begin{equation*}
\linnprod{f}{g} \overset{\text{def}}{=} \int_U f(z) \overline{g(z)} \, dV .
\end{equation*}
We will prove that $A^2(U)$ is
complete, in other words, it is a Hilbert space.  We first
prove that
the $A^2(U)$ norm bounds the
uniform norm on compact sets.

\begin{lemma} \label{lemma:bergmanKbound}
Let $U \subset \C^n$ be a domain and $K \subset \subset U$ compact.  Then
there exists a constant $C_K$, such that
\begin{equation*}
\snorm{f}_K
=
\sup_{z \in K} \sabs{f(z)}
\leq C_K \snorm{f}_{A^2(U)}
\qquad \text{ for all } f\in A^2(U) .
\end{equation*}
Consequently, $A^2(U)$ is complete.
\end{lemma}

\begin{proof}
As $K$ is compact there exists an $r > 0$ such that
$\overline{\Delta_r(z)} \subset U$
for all $z \in K$.
Take any $z \in K$, and
apply \exerciseref{exercise:averageDelta} and Cauchy--Schwarz:
\begin{equation*}
\begin{split}
\abs{f(z)} &=
\abs{\frac{1}{V\bigl(\Delta_r(z)\bigr)} \int_{\Delta_r(z)} f(\xi) \,
dV(\xi)}
\\
& \leq
\frac{1}{\pi^n r^{2n}}
\sqrt{\int_{\Delta_r(z)} 1^2 \, dV(\xi)}
\sqrt{\int_{\Delta_r(z)} \abs{f(\xi)}^2 \, dV(\xi)}
\\
& =
\frac{1}{\pi^{n/2} r^n}
\snorm{f}_{A^2(\Delta_r(z))}
\leq
\frac{1}{\pi^{n/2} r^n}
\snorm{f}_{A^2(U)} .
\end{split}
\end{equation*}
Taking supremum over $z \in K$ proves the estimate.
Therefore, if $\{ f_\ell \}$ is a sequence of functions in $A^2(U)$
converging in $L^2(U)$ to some $f \in
L^2(U)$, then it converges uniformly on compact sets, and so $f \in \sO(U)$.
Consequently, $A^2(U)$ is a closed subspace of $L^2(U)$, and hence complete.
\end{proof}

For a bounded domain, $A^2(U)$ is always infinite-dimensional, see exercise
below.  There exist unbounded domains for which either
$A^2(U)$ is trivial (just the zero function, e.g., $U=\C^n$)
or even finite-dimensional.
When $n=1$, $A^2(U)$ is either trivial, or
infinite-dimensional.\footnote{For examples of nontrivial finite dimensional
$A^2(U)$ for $n \geq 2$
as well as the result in $n=1$, see:
Wiegerinck, J.J.O.O.\ \emph{Domains with
finite dimensional Bergman space.} Math.\ Z.\ \textbf{187} (1984),
559--562.}

\begin{exbox}
\begin{exercise}
Show that if a domain $U \subset \C^n$ is bounded, then $A^2(U)$ is
infinite-dimensional.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item
Show that $A^2(\C^n)$ is trivial (it is just the zero function).
\item
Show that $A^2(\D \times \C)$ is trivial.
\item
Find an example of an unbounded domain $U$ for which $A^2(U)$ is
infinite-dimensional.
Hint: Think in one dimension for simplicity.
\end{exparts}
\end{exercise}

\begin{exercise} \label{exercise:bergmanpunctureddisc}
\begin{exparts}
\item
Show that $A^2(\D)$ can be identified with $A^2(\D \setminus \{ 0 \})$,
that is, every function in the latter can be extended to a function in the
former.
\item
Let $U \subset \C^n$ be a domain, $f \in \sO(U)$, and $X = f^{-1}(0)$.
Show that every function in $A^2(U \setminus X)$ is a restriction of a
function in $A^2(U)$, that is, $A^2(U) \cong A^2(U \setminus X)$.
\end{exparts}
\end{exercise}
\end{exbox}

The lemma says that
point evaluation is a bounded linear
functional.
That is, fix $z \in U$ and take $K= \{ z \}$, then the linear operator
\begin{equation*}
f \mapsto f(z)
\end{equation*}
is a bounded linear functional.  By the Riesz--Fisher theorem, there exists
a $k_z \in A^2(U)$, such that
\begin{equation*}
f(z) = \linnprod{f}{k_z} .
\end{equation*}
Define the \emph{\myindex{Bergman kernel}} for $U$ as
\glsadd{not:Bergmanker}%
\begin{equation*}
K_U(z,\bar{\zeta}) \overset{\text{def}}{=} \overline{k_z(\zeta)} .
\end{equation*}
The function $K_U$ is defined as $(z,\bar{\zeta})$ vary over
\glsadd{not:Ustar}%
$U \times U^*$, where we write
\begin{equation*}
U^* = \{ \zeta \in \C^n : \bar{\zeta} \in U \}.
\end{equation*}
Then for all $f \in A^2(U)$, we have
\begin{equation} \label{eq:repropropBergman}
f(z)
=
\int_U f(\zeta) K_U(z,\bar{\zeta}) \, dV(\zeta) .
\end{equation}
This last equation is sometimes called
the \emph{\myindex{reproducing property}} of the kernel.

Note that the Bergman kernel depends on $U$, which is why we write it
as $K_U(z,\bar{\zeta})$.

\begin{prop}
The Bergman kernel $K_U(z,\bar{\zeta})$ is holomorphic in $z$,
antiholomorphic in $\zeta$, and
\begin{equation*}
\overline{K_U(z,\bar{\zeta})} = K_U(\zeta,\bar{z}) .
\end{equation*}
\end{prop}

\begin{proof}
As each $k_z$ is in $A^2(U)$, it is holomorphic in $\zeta$.  Hence, $K_U$ is
antiholomorphic in $\zeta$.  If we prove
$\overline{K_U(z,\bar{\zeta})} = K_U(\zeta,\bar{z})$, then we prove $K_U$
is holomorphic in $z$.

As $\overline{K_U(z,\bar{\zeta})} = k_z(\zeta)$ is in $A^2(U)$, then
\begin{equation*}
\begin{split}
\overline{K_U(z,\bar{\zeta})}
& =
\int_{U} \overline{K_U(z,\bar{w})} K_U(\zeta,\bar{w}) dV(w)
\\
& =
\overline{
\left(
\int_{U} \overline{K_U(\zeta,\bar{w})} K_U(z,\bar{w}) dV(w)
\right)
}
=
\overline{
\overline{
K_U(\zeta,\bar{z})
}}
=
K_U(\zeta,\bar{z}) . \qedhere
\end{split}
\end{equation*}
\end{proof}

Therefore, thinking of
$\bar{\zeta}$ as the variable, $K_U$ is a holomorphic function of
$2n$ variables.

\begin{example} \label{example:bergmankerneldisc}
\index{Bergman kernel!unit disc}%
\index{Szeg{\"o} kernel!unit disc}%
Let us compute the Bergman kernel (and
the Szeg{\"o} kernel of the next section while we're at it)
explicitly for the
unit disc $\D \subset \C$.  Let $f \in \sO(\D) \cap C(\widebar{\D})$, that
is, $f$ is holomorphic in $\D$ and continuous up to the boundary.
Let $z \in \D$.
Then
\begin{equation*}
f(z) = \frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{\zeta-z} \,
d\zeta .
\end{equation*}
On the unit circle, $\zeta \bar{\zeta} = 1$.  Let $ds$ be
the arc-length measure on the circle, parametrized as $\zeta =
e^{is}$.
Then $d\zeta = i e^{is} \, ds$ or $\bar{\zeta} d\zeta = i \,ds$, and
\begin{equation*}
f(z) = \frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{\zeta-z} \,
d\zeta
= \frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}}
\bar{\zeta} \, d\zeta
= \frac{1}{2\pi} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}} \, ds .
\end{equation*}
The integral is now a regular line integral of a function whose
singularity, which used to be inside the unit disc, disappeared
(we ``reflected it'' to the outside).
The kernel $\frac{1}{2\pi} \frac{1}{1-z\bar{\zeta}}$ is called the
\emph{Szeg{\"o} kernel}, which we will briefly mention next.
We apply Stokes to the second integral above:
\begin{equation*}
\begin{split}
\frac{1}{2\pi i} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}}
\bar{\zeta} \, d\zeta
&=
\frac{1}{2\pi i} \int_{\D} f(\zeta)
\frac{\partial}{\partial \bar{\zeta}} \left[
\frac{\bar{\zeta}}{1-z\bar{\zeta}} \right] \,
d\bar{\zeta} \wedge d\zeta
\\
&=
\frac{1}{\pi} \int_{\D}
\frac{f(\zeta)}{{(1-z\bar{\zeta})}^2} \, dA(\zeta) .
\end{split}
\end{equation*}
We therefore have a candidate for the 
Bergman kernel in the unit disc:
\begin{equation*}
K_{\D}(z,\bar{\zeta}) = \frac{1}{\pi} \frac{1}{{(1-z\bar{\zeta})}^2} .
\end{equation*}
That this function really is the Bergman kernel
follows from \exerciseref{exercise:Bergmanunique}.
That is, $K_{\D}$ is the unique conjugate symmetric
reproducing function that is in
$A^2({\D})$ for a fixed $\zeta$.
We have only shown the formula for functions continuous up to the boundary,
but those are dense in $A^2({\D})$.
\end{example}

\begin{example}
In an exercise you found that
$A^2(\C^n) = \{ 0 \}$.  Therefore,
$K_{\C^n}(z,\bar{\zeta}) \equiv 0$.
\end{example}

The Bergman kernel for a more general domain is diffcult (usually impossible)
to compute explicitly.  We do have the following formula however.

\begin{prop}
Suppose $U \subset \C^n$ is a domain, and
$\{ \varphi_\ell (z) \}_{\ell\in I}$ is a complete orthonormal system
for $A^2(U)$.  Then
\begin{equation*}
K_U(z,\bar{\zeta})
=
\sum_{\ell \in I} \varphi_\ell(z) \overline{\varphi_\ell(\zeta)} ,
\end{equation*}
with uniform convergence on compact subsets of $U \times U^*$.
\end{prop}

\begin{proof}
For a fixed $\zeta \in U$, the function $z \mapsto K_U(z,\bar{\zeta})$ is
in $A^2(U)$.
Expand this function
in terms of the basis and use the reproducing property of $K_U$:
\begin{equation*}
K_U(z,\bar{\zeta}) =
\sum_{\ell \in I}
\left(\int_U K_U(w,\bar{\zeta}) \overline{\varphi_\ell(w)} \, dV(w) \right)
\varphi_\ell(z)
=
\sum_{\ell \in I}
\overline{\varphi_\ell(\zeta)}
\varphi_\ell(z) .
\end{equation*}
The convergence is in $L^2$ as a function of $z$, for a fixed $\zeta$.
Let $K \subset \subset U$ be a compact set.
Via \lemmaref{lemma:bergmanKbound}, $L^2$ convergence in $A^2(U)$ is uniform convergence on
compact sets.  Therefore, for a fixed $\zeta$ the convergence is uniform in
$z \in K$.  In particular, we get pointwise convergence.  So,
\begin{equation*}
\sum_{\ell \in I}
\abs{
\varphi_\ell(z)
}^2
=
\sum_{\ell \in I}
\varphi_\ell(z)
\overline{\varphi_\ell(z)}
=
K_U(z,\bar{z})
\leq C_K < \infty ,
\end{equation*}
where $C_K$ is the supremum of $K_U(z,\bar{\zeta})$ on $K \times K^*$.
Hence for $(z,\bar{\zeta}) \in K \times K^*$,
\begin{equation*}
\sum_{\ell \in I}
\abs{
\varphi_\ell(z)
\overline{\varphi_\ell(\zeta)}
}
\leq
\sqrt{
\sum_{\ell \in I}
\abs{
\varphi_\ell(z)
}^2
}
\sqrt{
\sum_{\ell \in I}
\abs{
\varphi_\ell(\zeta)
}^2
}
\leq
C_K < \infty .
\end{equation*}
So the convergence is uniform on $K \times K^*$.
\end{proof}

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Show that if $U \subset \C^n$ is bounded, then
$K_U(z,\bar{z}) > 0$
for all $z \in U$.
\item
Why can this fail if $U$ is unbounded?
Find a (trivial) counterexample.
\end{exparts}
\end{exercise}

\begin{exercise} \label{exercise:Bergmanunique}
Show that given a domain $U \subset \C^n$, the Bergman kernel is the unique
function $K_U(z,\bar{\zeta})$ such that
\begin{exnumparts}
\item
for a fixed $\zeta$, $K_U(z,\bar{\zeta})$ is in $A^2(U)$,
\item
$\overline{K_U(z,\bar{\zeta})} =
K_U(\zeta,\bar{z})$,
\item
the reproducing property \eqref{eq:repropropBergman}
holds.
\end{exnumparts}
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be either the unit ball or the unit polydisc.
Show that
$A^2(U) \cap C(\widebar{U})$ is dense in $A^2(U)$.  In particular, this
exercise says we
only need to check the reproducing property on functions continuous up to
the boundary to show we have the Bergman kernel.
\end{exercise}

\begin{exercise}
Let $U, V \subset \C^n$ be two domains and $f \colon U \to V$ a
biholomorphism.  Prove
\begin{equation*}
K_U(z,\bar{\zeta}) = \det D f(z) \, \overline{\det D f (\zeta)} \,
K_V\bigl(f(z),\overline{f(\zeta)}\bigr) .
\end{equation*}
\end{exercise}

\begin{exercise}
Show that the Bergman kernel for the polydisc is
\begin{equation*}
K_{\D^n}(z,\bar{\zeta}) =
\frac{1}{\pi^n} \prod_{\ell=1}^n \frac{1}{{(1-z_\ell\bar{\zeta}_\ell)}^2}.
\end{equation*}
\end{exercise}

\begin{exercise}[Hard]
Show that for
some constants $c_\alpha$,
the set of all monomials $\frac{z^\alpha}{c_\alpha}$ gives a complete orthonormal
system of $A^2(\bB_n)$.  Hint: To show orthogonality
compute the integral using polar coordinates in each variable
separately, that is, let $z_\ell = r_\ell e^{i\theta_\ell}$ where $\theta \in
[0,2\pi]^n$ and $\sum_\ell r_\ell^2 < 1$.  Then show completeness by showing that
if $f \in A^2(\bB_n)$ is orthogonal to all $z^\alpha$, then $f = 0$.
Note that explicitly finding $c_\alpha = \sqrt{\frac{\pi^n \alpha!}{(n+\abs{\alpha})!}}$ requires
the classical $\beta$ function of special function theory.
\end{exercise}

\begin{exercise}
Using the previous exercise, show that the Bergman kernel for the unit ball
is
\begin{equation*}
K_{\bB_n}(z,\bar{\zeta}) =
\frac{n!}{\pi^n}\frac{1}{{(1-\linnprod{z}{\zeta})}^{n+1}},
\end{equation*}
where $\linnprod{z}{\zeta}$ is the standard inner product on $\C^n$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Szeg{\"o} kernel}

We use the same technique to create a reproducing kernel on the
boundary by starting with $L^2(\partial U, d\sigma)$ instead of $L^2(U)$.
We obtain a
kernel where we integrate over the boundary rather than the domain itself.
Let us give a quick overview, but let us not get into the details.

Let $U \subset \C^n$ be a bounded domain with smooth boundary.  Let
$C(\widebar{U}) \cap \sO(U)$ be the holomorphic functions in $U$
continuous up to the boundary.  The restriction of
$f \in C(\widebar{U}) \cap \sO(U)$ to $\partial U$ is a continuous
function,
and hence $f|_{\partial U}$ is in $L^2(\partial U,d\sigma)$, where $d\sigma$
is the surface measure on $\partial U$.
Taking a closure
of these restrictions in $L^2(\partial U)$ obtains the Hilbert space
$H^2(\partial U)$,
\glsadd{not:H2}%
which is called the \emph{\myindex{Hardy space}}.
The inner product on $H^2(\partial U)$ is the $L^2(\partial U,
d\sigma)$ inner product:
\begin{equation*}
\linnprod{f}{g} \overset{\text{def}}{=} \int_{\partial U} f(z)
\overline{g(z)} \, d\sigma(z) .
\end{equation*}

\begin{exbox}
\begin{exercise}
Show that monomials $z^\alpha$ are a complete orthonormal system in
$H^2(\partial \bB_n)$.
\end{exercise}

\begin{exercise}
Let $U \subset \C^n$ be a bounded domain with smooth boundary.
Prove that
$H^2(\partial U)$ is infinite-dimensional.
\end{exercise}
\end{exbox}

Given an $f \in H^2(\partial U)$, write the Poisson integral
\begin{equation*}
Pf(z) = \int_{\partial U} f(\zeta) \, P(z,\zeta) \, d \sigma(\zeta) ,
\end{equation*}
where $P(z,\zeta)$ is the Poisson kernel.  The Poisson integral
reproduces harmonic functions.  As holomorphic functions are harmonic, we
find that if $f \in C(\widebar{U}) \cap \sO(U)$, then $Pf = f$.

Although $f \in H^2(\partial U)$ is only defined on the boundary,
through the Poisson integral, we have the values
$Pf(z)$ for $z \in U$.
For each $z \in U$,
\begin{equation*}
f \mapsto Pf(z)
\end{equation*}
defines a continuous linear functional.  Again we find a $s_z \in
H^2(\partial U)$ such that
\begin{equation*}
Pf(z) = \linnprod{f}{s_z} .
\end{equation*}
For $z \in U$ and $\zeta \in \partial U$, define
\glsadd{not:Szegoker}%
\begin{equation*}
S_U(z,\bar{\zeta}) \overset{\text{def}}{=} \overline{s_z(\zeta)} ,
\end{equation*}
although for a fixed $z$ this is a function only defined almost everywhere
as it is an element of $L^2(\partial U,d\sigma)$.
The function $S_U$ is the \emph{\myindex{Szeg{\"o} kernel}}.
If $f \in H^2(\partial U)$, then
\begin{equation*}
Pf(z) = \int_{\partial U} f(\zeta) \, S_U(z,\bar{\zeta}) \, d\sigma(\zeta) .
\end{equation*}

As functions in $H^2(\partial U)$ extend to $\widebar{U}$, then
$f \in H^2(\partial U)$ may be considered a function on
$\widebar{U}$, where values in $U$ are given by $Pf$.  Similarly, we
extend $S(z,\bar{\zeta})$ to a function on $U \times \widebar{U}^*$ (where
the values on the boundary are defined only almost everywhere).
We state without proof that if $\{ \varphi_\ell \}_{\ell\in I}$ is a complete
orthonormal system for $H^2(\partial U)$, then
\begin{equation} \label{eq:formulaszego}
S_U(z,\bar{\zeta}) = \sum_{\ell \in I} \varphi_\ell(z)\overline{\varphi_\ell(\zeta)}
\end{equation}
for $(z,\bar{\zeta}) \in U \times U^*$, converging uniformly on compact subsets.
As before, this formula shows that $S$ is conjugate symmetric,
and so it extends to
$(U \times \widebar{U}^*) \cup (\widebar{U} \times U^*)$.

\begin{example}
In \exampleref{example:bergmankerneldisc}, we computed that
if $f \in C(\overline{\D}) \cap \sO(\D)$, then
\begin{equation*}
f(z) = \frac{1}{2\pi} \int_{\partial \D} \frac{f(\zeta)}{1-z\bar{\zeta}} \, ds .
\avoidbreak
\end{equation*}
In other words, $S_{\D}(z,\zeta) =
\frac{1}{\pi}
\frac{1}{1-z\bar{\zeta}}$ .
\end{example}

\begin{exbox}
\begin{exercise}
Using the formula \eqref{eq:formulaszego} compute $S_{\bB_n}$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Complex Analytic Varieties} \label{ch:analyticvarieties}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The ring of germs}

\begin{defn}
Let $p$ be a point in a topological space $X$.  Let $Y$ be a set and
$U, V \subset X$ be open neighborhoods of $p$.  Say that
two functions $f \colon U \to Y$ and
$g \colon V \to Y$ are equivalent if there exists a neighborhood
$W$ of $p$ such that $f|_W = g|_W$.
An equivalence class of functions under this relation defined in neighborhoods of $p$
is called a \emph{\myindex{germ of a function}}.
\glsadd{not:germf}%
The germ is denoted by $(f,p)$, but we may say $f$ when
the context is clear.
We usually restrict the functions to a certain category:
smooth, holomorphic, etc.
\end{defn}

The set of germs of complex-valued functions forms a
commutative ring, see exercise below to check the details.
For example, to multiply $(f,p)$ and $(g,p)$, take two representatives
$f$ and $g$ defined on a common neighborhood multiply them and
then consider the germ $(fg,p)$.  Similarly, $(f,p) + (g,p)$ is
defined as $(f+g,p)$.  It is easy to check that these operations are
well-defined.

\begin{exbox}
\begin{exercise}
Let $X$ be a topological space and $p \in X$.
Let $\sF$ be a class of complex-valued functions defined on open subsets
of such that whenever $f \colon U \to \C$ is in $\sF$ and $W \subset U$ is open,
then $f|_W \in \sF$, and such that whenever $f$ and $g$
are two functions in $\sF$, and $W$ is an open set where both are defined,
then $fg|_W$ and $(f+g)|_W$ are also in $\sF$.
Assume that all constant functions are in $\sF$.
Show that the ring operations defined above
on a set of germs at $p$ of functions from $\sF$ are well-defined,
and that the set of germs at $p$ of functions from $\sF$ is a commutative
ring.
\end{exercise}

\begin{exercise} \label{exercise:contgermsnothausdorf}
Let $X=Y=\R$ and $p=0$.  Consider the ring of germs of
continuous functions (or smooth functions).
Show that for every continuous $f \colon \R \to \R$ and every neighborhood
$W$ of $0$, there exists a $g \colon \R \to \R$
such that $(f,0) = (g,0)$, but $g|_W \not= f|_W$.
\end{exercise}
\end{exbox}

Germs are particularly useful for holomorphic functions because of the identity
theorem.  In particular, the behavior of
\exerciseref{exercise:contgermsnothausdorf} does not happen
for holomorphic functions.  Furthermore, for holomorphic functions, the
ring of germs is the same as the ring of convergent power series, see
exercise below.  No similar result is true for smooth functions.

\begin{defn}
Let $p \in \C^n$.
\glsadd{not:ringofgerms}%
Write
${}_n\sO_p = \sO_p$ as the ring of germs at $p$ of holomorphic functions.
\end{defn}

The ring of germs $\sO_p$ has many nice properties,
and it is generally a ``nicer'' ring than the ring $\sO(U)$ for
some open $U$, and so it is easier to work with if we are interested
in local properties and not the geometry of $U$.

\begin{exbox}
\begin{exercise}
\begin{exparts}
\item
Show that $\sO_p$ is an \myindex{integral domain} (has no zero divisors).
\item
Prove the ring of germs at $0 \in \R$ of smooth real-valued functions
is not an integral domain.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that the units (elements with multiplicative inverse)
of $\sO_p$ are the germs of functions which do not vanish at
$p$.
\end{exercise}

\begin{exercise}
\begin{exparts}
\item (easy)
Show that given a germ $(f,p) \in \sO_p$,
there exists a fixed open neighborhood $U$
of $p$ and a representative $f \colon U \to \C$ such that any other
representative $g$ can be analytically continued from $p$ to a holomorphic
function $U$.
\item
(easy) Given two representatives $f \colon U \to \C$
and $g \colon V \to \C$ of a germ $(f,p) \in \sO_p$,
let $W$ be the connected component of $U \cap V$
that contains $p$.  Prove that $f|_W = g|_W$.
\item
Find a germ $(f,p) \in \sO_p$, such that for
every representative $f \colon U \to \C$, we can
find another representative of $g \colon V \to \C$
of that same germ such that $g|_{U \cap V} \not= f|_{U \cap V}$.
Hint: $n=1$ is sufficient.
\end{exparts}
\end{exercise}

\begin{exercise}
Show that $\sO_p$ is isomorphic to the ring of convergent power series.
\end{exercise}
\end{exbox}


\begin{defn}
Let $p$ be a point in a topological space $X$.
Say that sets $A, B \subset X$ are equivalent
if there exists a neighborhood $W$ of $p$
such that $A \cap W = B \cap W$.
An equivalence class of sets under this relation
is called a \emph{\myindex{germ of a set}} at $p$.
\glsadd{not:germA}%
It is denoted by $(A,p)$, but we may write $A$ when
the context is clear.
\end{defn}

The concept of $(A,p) \subset (B,p)$ is defined in an obvious manner,
that is, there exist representatives $A$ and $B$, and a neighborhood $W$
of $p$ such that $A \cap W \subset B \cap W$.
Similarly,
if $(A,p)$, $(B,p)$ are germs and $A$, $B$
are some representatives of these germs, then
the intersection $(A,p) \cap (B,p)$
is the germ $(A \cap B,p)$, the union
$(A,p) \cup (B,p)$ is the germ $(A \cup B,p)$,
and the complement $(A,p)^c$ is the germ $(A^c,p)$.

\begin{exbox}
\begin{exercise}
Check that the definition of
subset, union, intersection, and complement of germs
of sets
is well-defined.
\end{exercise}
\end{exbox}


Let $R$ be some ring of germs of complex-valued
functions at $p \in X$ for some topological space $X$.
\glsadd{not:Zf}%
If $f$ is a complex-valued function,
let $Z_f$ be the zero set of $f$, that is $f^{-1}(0)$.
When $(f,p) \in R$ is a germ of a function
it makes sense to talk about the germ $(Z_f,p)$.  We take the zero
set of some representative and look at its germ at $p$.

\begin{exbox}
\begin{exercise}
Suppose $f$ and $g$ are two representatives of a germ $(f,p)$
show that the germs $(Z_f,p)$ and $(Z_g,p)$ are the same.
\end{exercise}

\begin{exercise}
Show that if $(f,p)$ and $(g,p)$ are in $R$
and $f$ and $g$ are some representatives, then
$(Z_f,p) \cup (Z_g,p) = (Z_{fg},p)$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Weierstrass preparation and division theorems} \label{sec:wpt}

Suppose
$f$ is (a germ of) a holomorphic function at a point $p \in \C^n$.
Write
\begin{equation*}
f(z) = \sum_{k=0}^\infty f_k(z-p),
\end{equation*}
where $f_k$ is a homogeneous polynomial of degree $k$,
that is, $f_k(tz) = t^k f_k(z)$.

\begin{defn}
Let $p \in \C^n$ and $f$ be a function holomorphic in a neighborhood of $p$.
If $f$ is not identically zero,
define
\glsadd{not:ord}%
\begin{equation*}
\ord_p f \overset{\text{def}}{=} \min \bigl\{ k \in \N_0 : f_k \not\equiv 0 \bigr\} .
\end{equation*}
If $f \equiv 0$, define $\ord_p f = \infty$.
We call the number $\ord_p f$ the \emph{\myindex{order of vanishing}} of $f$ at $p$.
\end{defn}

In other words, the order of vanishing of $f$ at $p$ is $k$ whenever all
partial derivatives of order less than $k$ vanish at $p$, and there exists
at least one derivative of order $k$ that does not vanish at $p$.

In one complex variable, a holomorphic function $f$ with $\ord_0 f = k$
can be written
(locally) as $f(z) = z^k u(z)$ for a nonvanishing holomorphic $u$.
Such a $u$ is a \emph{\myindex{unit}} in the ring $\sO_0$, that is,
an element with a multiplicative inverse.
In several variables, there is a similar theorem, or in fact a pair of theorems,
the so-called Weierstrass preparation and
division theorems.  We first need to replace $z^k$ with something.

\begin{defn}
Let $U \subset \C^{n-1}$ be open, $0 \in U$, and let $z' \in \C^{n-1}$ denote the
coordinates.
\glsadd{not:polyinOU}%
Suppose $P \in \sO(U)[z_n]$ is a monic polynomial of degree $k \geq 0$,
\begin{equation*}
P(z',z_n) = z_n^k + \sum_{\ell=0}^{k-1} c_\ell(z') \, z_n^\ell ,
\end{equation*}
where $c_\ell$ are holomorphic functions defined on $U$ such that
$c_\ell(0) = 0$ for all $\ell$.  Then $P$ is
called a \emph{\myindex{Weierstrass polynomial}} of degree $k$.
\glsadd{not:polyinO0}%
If the $c_\ell$ are germs in $\sO_0 = {}_{n-1}\sO_0$, then $P \in \sO_0[z_n]$ and
$P$ is called a \emph{\myindex{germ of a Weierstrass polynomial}}.
\end{defn}

The definition (and the theorem that follows) still holds for $n=1$.
If you read the
definition carefully, you will find that if $n=1$, then the only Weierstrass
polynomial of degree $k$ is $z^k$.
Note that for any $n$, if $k=0$, then $P = 1$.

The purpose of this section is to show that every holomorphic function
in $\sO_0$ is up to a unit
and a possible small rotation a Weierstrass
polynomial, which carries the zeros of $f$.
Consequently the
algebraic and geometric properties of
${}_n\sO_0$ can be understood via algebraic and geometric properties of
${}_{n-1}\sO_0[z_n]$.

\begin{thm}[\myindex{Weierstrass preparation theorem}]
Suppose $f \in \sO(U)$ for an open $U \subset \C^{n-1} \times \C$,
where $0 \in U$, and $f(0)=0$.  Suppose
$z_n \mapsto f(0,z_n)$ is not identically zero near the origin
and its order of vanishing at the origin is $k \geq 1$.

Then there exists an open polydisc $V = V' \times D \subset \C^{n-1} \times \C$
with $0 \in V \subset U$,
a unique $u \in \sO(V)$, $u(z) \not=0$ for all $z \in V$, and a unique
Weierstrass polynomial $P$ of degree $k$
with coefficients holomorphic in $V'$ such that
\begin{equation*}
f(z',z_n) = u(z',z_n) \, P(z',z_n) ,
\end{equation*}
and such that all $k$ zeros (counting multiplicity)
of $z_n \mapsto P(z',z_n)$ lie in $D$ for all $z' \in V'$
\end{thm}

\begin{proof}
There exists a small disc $D \subset \C$ centered at zero such that
$\{0\} \times \widebar{D} \subset U$ and such that
$f(0,z_n) \not= 0$ for $z_n \in \widebar{D} \setminus \{ 0 \}$.
By continuity
of $f$,
there is a small polydisc $V = V' \times D$ such that
$\widebar{V} \subset U$ and $f$ is not zero on
$V' \times \partial D$.
See \figureref{fig:wpreppoly} for the setup.
We will consider the zeros of
$z_n \mapsto f(z',z_n)$ for $z' \in V'$.  See \figureref{fig:wprepdiv}.
\begin{myfig}
\subimport*{figures/}{wpreppoly.pdf_t}
\caption{Setting up the neighborhood $V$, the two discs in
\figureref{fig:wprepdiv} are the vertical thick black line and the
thin dashed line.\label{fig:wpreppoly}}
\end{myfig}
\begin{myfig}
\subimport*{figures/}{wprepdiv.pdf_t}
\caption{The zeros of $z_n \mapsto f(z',z_n)$.\label{fig:wprepdiv}}
\end{myfig}

By the one-variable argument principle (\thmref{thm:onevarargprinc}) the number of zeros (with
multiplicity) of $z_n
\mapsto f(z',z_n)$ in $D$ is
\begin{equation*}
\frac{1}{2\pi i}
\int_{\partial D}
\frac{\frac{\partial f}{\partial z_n} (z',\zeta)}{f(z',\zeta)} ~d\zeta .
\end{equation*}
As $f(z',\zeta)$ does not vanish when $z' \in V'$ and $\zeta \in \partial
D$,
the expression above is a continuous integer-valued
function of $z' \in V'$.
The expression
is equal to $k$ when $z'=0$, and
so it is equal to $k$ for all $z' \in V'$.
Write
the zeros of $z_n \mapsto f(z',z_n)$ as $\alpha_1(z'),\ldots,\alpha_k(z')$, including
multiplicity.
The zeros are not ordered in any particular way---pick
\emph{some} ordering for every $z'$.
Write
\begin{equation*}
P(z',z_n)
=
\prod_{\ell=1}^k \bigl(z_n-\alpha_\ell(z')\bigr)
=
z_n^k + c_{k-1}(z') \, z_n^{k-1} + \cdots + c_0 (z') .
\end{equation*}
For a fixed $z'$, $P$ (and thus the coefficients $c_0,\ldots,c_{k-1}$)
is uniquely defined as its definition is independent of the ordering of the
zeros.
That $c_j(0) = 0$ for all $j$ follows as $\alpha_\ell(0)=0$ for all $\ell$.
As the above is the unique way to define a monic polynomial with these zeros
(\exerciseref{exercise:monicpolyunique}),
the uniqueness part of the theorem follows.
We need to show that the coefficients are holomorphic functions on $V'$,
and that $u$ is a holomorphic function on $V$.

No matter how you ordered the zeros for each $z'$,
the functions $\alpha_\ell$ may not be continuous in general (see
\exampleref{sqrt:example}).  However,
we will prove that the functions $c_\ell$ are holomorphic.  The functions
$c_\ell$ are
(up to sign)
the \emph{elementary symmetric functions}
of $\alpha_1,\ldots,\alpha_k$ (see below).  A standard
theorem in algebra
(Newton's identities, see \exerciseref{exercise:powersums})
says that the elementary symmetric functions are
polynomials in the so-called \emph{\myindex{power sum}} functions
in the $\alpha_\ell$s:
\begin{equation*}
s_m(z') = \sum_{\ell=1}^k \alpha_\ell{(z')}^m , \qquad m = 1,\ldots,k.
\end{equation*}
So if the power sums $s_m$ are holomorphic on $V'$, then
$c_\ell$ are holomorphic on $V'$.

A refinement of the argument principle (see \thmref{thm:onevarargprinc}) says:
If $h$ and $g$ are
holomorphic functions on a disc $D$, continuous on $\widebar{D}$,
such that $g$ has no zeros on $\partial D$, and $\alpha_1,\ldots,\alpha_k$
are the zeros of $g$ in $D$, then
\begin{equation*}
\frac{1}{2 \pi i}
\int_{\partial D} h(\zeta) \frac{g'(\zeta)}{g(\zeta)} ~d\zeta
= \sum_{\ell=1}^k h(\alpha_\ell) .
\end{equation*}
With $h(\zeta) = \zeta^m$ and $g(\zeta)=f(z',\zeta)$, the theorem says
\begin{equation*}
s_m(z') =
\sum_{\ell=1}^k \alpha_\ell{(z')}^m
=
\frac{1}{2\pi i}
\int_{\partial D}
\zeta^m
\frac{\frac{\partial f}{\partial \zeta} (z',\zeta)}{f(z',\zeta)} ~d\zeta .
\end{equation*}
The function $s_m$ is clearly continuous, and if we
differentiate under the integral
with $\frac{\partial}{\partial\bar{z}_1}, \ldots,
\frac{\partial}{\partial\bar{z}_{n-1}}$
we find that $s_m$
is holomorphic.  Thus $c_0,\ldots,c_{k-1}$ are holomorphic, and
so $P$ is a Weierstrass polynomial.

Finally, we wish to show that $P$ divides $f$ as claimed, that is,
that $u$ is holomorphic.
For each fixed $z'$, one variable theory says that
$z_n \mapsto \frac{f(z',z_n)}{P(z',z_n)}$ has only removable singularities,
and in fact, it has no zeros as we defined $P$
to exactly cancel them all out.
The Cauchy
formula on $\nicefrac{f}{P}$ then says that the function
\begin{equation*}
u(z',z_n) =
\frac{1}{2\pi i}
\int_{\partial D} \frac{f(z',\zeta)}{P(z',\zeta)(\zeta-z_n)} \,
d\zeta
\end{equation*}
is equal to $\frac{f(z',z_n)}{P(z',z_n)}$.
The function $u$ is clearly continuous and holomorphic in $z_n$
for each fixed $z'$.
Differentiating under the integral shows it is also holomorphic in $z'$.
\end{proof}

\begin{example} \label{sqrt:example}
Consider the zero set of
$f(z_1,z_2) = z_2^2 - z_1$,
a Weierstrass polynomial in $z_2$ of degree $k=2$.  So $z' = z_1$.
For all $z_1$ except the origin there are two zeros, $\pm \sqrt{z_1}$.
Call one of them $\alpha_1(z_1)$ and one of them $\alpha_2(z_1)$.  Recall
there is no continuous choice of a square root that works for all $z_1$,
so no matter how you choose, $\alpha_1$ and $\alpha_2$ will not be continuous.
At the origin there is one zero of multiplicity two,
so $\alpha_1(0) = \alpha_2(0) = 0$.
On the other hand, the
symmetric functions $c_1(z_1) = - \alpha_1(z_1) - \alpha_2(z_1) = 0$
and $c_0(z_1) = \alpha_1(z_1)\alpha_2(z_1) = -z_1$ are holomorphic.
See \figureref{fig:sqrt}.

\begin{myfig}
\includegraphics[align=c,width=3.0in]{figures/realsqrt.pdf}
\hspace{\fill}
\includegraphics[align=c,width=3.0in]{figures/imagsqrt.pdf}
\caption{Graphs of the real and imaginary parts of both branches
$\pm\sqrt{z_1}$.  A possible choice of branch $\alpha_1(z_1)$ is drawn darker; note the
discontinuity of its imaginary part.
We remark that the surface $z_2^2-z_1=0$ does not cross itself in $\C^2$.\label{fig:sqrt}}
\end{myfig}

The $k$ depends on the coordinates chosen.
Consider $g(z_1,z_2) = -f(z_2,z_1) = z_2-z_1^2$,
which is a Weierstrass polynomial in $z_2$ of degree $k=1$.
In these coordinates, there is only one zero for each $z'$,
$\alpha_1(z_1) = z_1^2$, and so $c_0(z_1) =
-z_1^2$.
\end{example}


A function $f(z_1,\ldots,z_n)$ is \emph{symmetric}\index{symmetric function}
if $f = f \circ p$ for all permutations of the variables $p$.
The \emph{\myindex{elementary symmetric functions}} of
$\alpha_1,\ldots,\alpha_k$ are the coefficients $\sigma_1,\ldots,\sigma_k$ of the polynomial
\begin{equation*}
\prod_{\ell=1}^k \bigl(t+\alpha_\ell\bigr)
=
t^k
+ \sigma_{1} \, t^{k-1} +
\cdots
+ \sigma_{k-2} \, t^2
+ \sigma_{k-1} \, t
+ \sigma_k .
\end{equation*}
So,
\begin{equation*}
\begin{aligned}
\sigma_{1} & = \alpha_1 + \alpha_2 + \cdots + \alpha_k, \\
\sigma_{2} & = \alpha_1 \alpha_2 \, + \, \alpha_1 \alpha_3 \, + \,
  \cdots \, + \, \alpha_{k-1} \alpha_k , \\
& \;\: \smash{\vdots} \\
\sigma_{k-1} & =
  \alpha_2 \alpha_3 \cdots \alpha_{k} \, + \,
  \alpha_1 \alpha_3 \alpha_4 \cdots \alpha_{k} \, + \, \cdots
  \,+\, \alpha_1 \alpha_2 \cdots \alpha_{k-1}, \\
\sigma_k & = \alpha_1 \alpha_2 \cdots \alpha_k.
\end{aligned}
\end{equation*}
For example, when $k=2$, then $\sigma_2 = \alpha_1\alpha_2$ and
$\sigma_1 = \alpha_1 + \alpha_2$.  The function $\sigma_1$ happens to
already be a power sum.  We can write $\sigma_2$
as a polynomial in the power sums:
\begin{equation*}
\sigma_2
=
\frac{1}{2}
\left(
{\bigl(\alpha_1 + \alpha_2\bigr)}^2
-
\bigl(\alpha_1^2 + \alpha_2^2\bigr)
\right) .
\end{equation*}
In general, as we said we can write any $\sigma_\ell$ in terms of the power
sums of the $\alpha_j$s.  The formulas for this are called the
\emph{\myindex{Newton's identities}} or \emph{\myindex{Girard--Newton formulas}},
although we will avoid writing these down explicitly, and we leave finding
them (or just proving that they exist) as an exercise.

\begin{exbox}
\begin{exercise} \label{exercise:powersums}
Show that elementary symmetric functions are polynomials in the power sums.
Equivalently, show that the elementary symmetric functions $\sigma_\ell$ can
be found in terms of the power sums of the $\alpha_j$s.
\end{exercise}

\begin{exercise} \label{exercise:symmetric}
Prove the \emph{\myindex{fundamental theorem of symmetric polynomials}}:
Every symmetric polynomial can be written as a polynomial in
the elementary symmetric functions.  Use the following procedure.
Using double induction, suppose the theorem is true if the number of
variables is less than $k$, and the theorem is true in $k$ variables
for degree less than $d$.
Consider a symmetric $P(z_1,\ldots,z_k)$ of degree $d$.
Write $P(z_1,\ldots,z_{k-1},0)$ by induction hypothesis as a polynomial
in the elementary symmetric functions of one less variable.  Use the
same coefficients, but plug in the elementary symmetric functions of $k$
variables except the symmetric polynomial in $k$ variables of degree $k$,
that is, except $z_1z_2\cdots z_k$.
You will obtain a symmetric function $L(z_1,\ldots,z_k)$ and you need to
show $L(z_1,\ldots,z_{k-1},0) = P(z_1,\ldots,z_{k-1},0)$.
Now use symmetry to prove that
\begin{equation*}
P(z_1,\ldots,z_k) =
L(z_1,\ldots,z_k) +
z_1z_2\cdots z_k Q(z_1,\ldots,z_k) .
\avoidbreak
\end{equation*}
Then note that $Q$ has lower degree and finish by induction.
\end{exercise}

\begin{exercise} \label{exercise:symmetricseries}
Extend the previous exercise to power series.  Suppose $f(z_1,\ldots,z_k)$
is a convergent symmetric power series at $0$, show that $f$ can be written
as a convergent power series in the elementary symmetric functions.
\end{exercise}

\begin{exercise} \label{exercise:symmetricseriesweier}
Suppose $P(z',z_n)$ is a Weierstrass polynomial of degree $k$,
and write the zeros as $\alpha_1(z'), \ldots, \alpha_k(z')$.
These are not holomorphic functions,
but suppose that $f$ is a symmetric convergent
power series at the origin in $k$ variables.  Show that
$f\bigl(\alpha_1(z'), \ldots, \alpha_k(z')\bigr)$ is a holomorphic function
of $z'$ near the origin.
\end{exercise}
\end{exbox}

The hypotheses of the preparation theorem are not an obstacle.  If a holomorphic
function $f$ is such that $z_n \mapsto f(0,z_n)$ vanishes identically,
then we can make a small linear change of
coordinates $L$ ($L$ can be a matrix arbitrarily close to the identity) such
that $f \circ L$ satisfies the hypotheses of the theorem.
For example, $f(z_1,z_2,z_3) = z_1z_3+z_2z_3$ does not satisfy the
hypotheses of the theorem as $f(0,0,z_3) \equiv 0$.  But for an arbitrarily
small $\epsilon \not= 0$, replacing
$z_2$ with $z_2 + \epsilon z_3$ leads to $\tilde{f}(z_1,z_2,z_3)
= f(z_1,z_2+\epsilon z_3,z_3) =
z_1z_3+z_2z_3 + \epsilon z_3^2$, and $\tilde{f}(0,0,z_3) = \epsilon z_3^2$.
Thence, $\tilde{f}$
satisfies the hypotheses of the theorem.

\begin{exbox}
\begin{exercise}
Prove the fact above about the existence of $L$ arbitrarily close to the
identity.
\end{exercise}

\begin{exercise} \label{exercise:monicpolyunique}
Prove that a monic polynomial $P(\zeta)$ of one variable is
uniquely determined by its zeros up to multiplicity:
If $P$ and $Q$ are two monic polynomials with the same zeros
up to multiplicity,
then $P=Q$.  That proves the uniqueness of the Weierstrass polynomial.
\end{exercise}

\begin{exercise}
Suppose $D \subset \C$ is a bounded domain, $0 \in D$,
$U' \subset \C^{n-1}$ is a domain, $0 \in U'$,
and $P \in \sO(U')[z_n]$ is a Weierstrass polynomial
such that $P(z',z_n)$ is not zero on $U' \times \partial D$.
Show that for every $z' \in U$, all zeros of $z_n \mapsto P(z',z_n)$ are in $D$.
\end{exercise}

\begin{exercise}
Let $D \subset \C$ be a bounded domain,
and
$U' \subset \C^{n-1}$ a domain.
Suppose
$f$ is a continuous function on
$U' \times \widebar{D}$ holomorphic on $U' \times D$,
where $f$ is zero on at least one point
of $U' \times D$, and $f$ is never zero on
$U' \times \partial D$.
Prove that
$z_n \mapsto f(z',z_n)$ has at least one zero in $D$ for every $z' \in U'$.
\end{exercise}
\end{exbox}

\pagebreak[2]
The order of vanishing of $f$ at the origin is a lower bound
on the number $k$ in the theorem.  The order of vanishing for a certain
variable may be larger than this lower bound.  If $f(z_1,z_2) =
z_1^2 + z_2^3$, then the $k$ we get is $3$, but $\ord_0 f = 2$.
We can make a small
linear change of coordinates to ensure $k = \ord_0 f$.
With the $f$ as above, $f(z_1 + \epsilon z_2,z_2)$ gives $k = 2$
as expected.

When $k=1$ in the Weierstrass preparation theorem, we obtain
the Weierstrass polynomial $z_n + c_0(z')$.  That is, the zero set of
$f$ is a graph of the holomorphic function $-c_0$.
Therefore, the Weierstrass theorem is a generalization of the
implicit function theorem to the case when $\frac{\partial f}{\partial z_n}$
is zero.  In such a case, we can still ``solve'' for $z_n$,
but we find a $k$-valued solution given by the zeros of the obtained Weierstrass polynomial.

There is an obvious statement of the preparation theorem for germs.

\begin{exbox}
\begin{exercise}
State and prove a germ version of the preparation theorem.
\end{exercise}
\end{exbox}

The next theorem is rather trivial in one variable.  Let $f$ be any
holomorphic function near the origin in $\C$ and take any $k \in \N$.
Let $r$ be the Taylor polynomial for $f$ at $0$ of degree $k-1$.  Then
$f-r$ is divisible by $z^k$, in other words, 
$f = q z^k + r$.
In several variables, we replace
$z^k$ with a Weierstrass polynomial and we still have this division
theorem.

\begin{thm}[\myindex{Weierstrass division theorem}]
Suppose $f$ is holomorphic near the origin, and suppose $P$
is a Weierstrass polynomial of degree $k \geq 1$ in $z_n$.  Then there exists
a neighborhood $V$ of the origin and unique $q,r \in \sO(V)$,
where $r$ is a polynomial in $z_n$ of degree less than $k$, and on $V$,
\begin{equation*}
f = qP + r .
\end{equation*}
\end{thm}

Note that $r$ need not be a Weierstrass polynomial; it need not be monic
nor do the coefficients need to vanish at the origin.  It is simply a
polynomial in $z_n$ with coefficients that are holomorphic functions
of the first $n-1$ variables.

\begin{proof}
Uniqueness is left as an exercise.  There exists
a connected neighborhood $V = V' \times D$ of the origin, where
$D$ is a disc,
$f$ and $P$ are continuous on $V' \times \widebar{D}$,
and $P$ is not zero on $V' \times \partial D$.
Let
\begin{equation*}
q(z',z_n) =
\frac{1}{2\pi i} \int_{\partial D} \frac{f(z',\zeta)}{P(z',\zeta)(\zeta-z_n)}
~d\zeta .
\end{equation*}
As $P$ is not zero on $V' \times \partial D$,
the function $q$
is holomorphic in $V$ (differentiate under the integral).
If $P$ did divide $f$, then $q$ would really be $\nicefrac{f}{P}$.
But if $P$ does not divide $f$, then
the Cauchy integral formula does not apply and $q$ is not equal to
$\nicefrac{f}{P}$.  Interestingly,
the expression does give the quotient in the division with remainder.

Write $f$ using the Cauchy integral formula in $z_n$ and
subtract $qP$ to obtain $r$:
\begin{multline*}
r(z',z_n) = f(z',z_n) - q(z',z_n)P(z',z_n)
\\
=
\frac{1}{2\pi i}
\int_{\partial D} \frac{f(z',\zeta)P(z',\zeta) - f(z',\zeta)P(z',z_n)}{P(z',\zeta)(\zeta-z_n)}
~d\zeta .
\end{multline*}
We need to show $r$ is a polynomial in $z_n$ of degree less than
$k$.  In the expression inside the integral, the numerator is
of the form $\sum_{\ell=1}^k h_\ell(z',\zeta)(\zeta^\ell-z_n^\ell)$ and is therefore
divisible by $(\zeta-z_n)$.
The numerator is a polynomial of degree $k$ in
$z_n$.  After dividing by $(\zeta-z_n)$,
the integrand becomes
a polynomial in $z_n$ of degree $k-1$.
Use linearity of the integral
to integrate the coefficients of the polynomial.  Each coefficient is a
holomorphic function in $V'$ and the proof is finished.  Some coefficients may have
integrated to zero, so we can only say that $r$ is a polynomial
of degree $k-1$ or less.
\end{proof}

For example, let $f(z,w) = e^z + z^4 e^w + z w^2 e^w + zw$ and $P(z,w)
= w^2 + z^3$.  Then $P$ is a Weierstrass polynomial in $w$ of degree $k=2$.  A bit
of computation shows
\begin{equation*}
\frac{1}{2\pi i}
\int_{\partial \D}
\frac{e^z + z^4 e^{\zeta} + z {\zeta}^2 e^{\zeta} + z \zeta}{(\zeta^2+z^3)(\zeta-w)}
d\zeta
=
z e^w
,
\quad \text{so} \quad
f(z,w) = \underbrace{\bigl( ze^w \bigr)}_{q} \underbrace{\bigl( w^2 + z^3 \bigr)}_{P} +
\underbrace{z w + e^z}_{r} .
\end{equation*}
Notice that $r$ is a polynomial of degree $1$ in $w$, but it is neither monic,
nor do the coefficients vanish at $0$.

\begin{exbox}
\begin{exercise}
Prove the uniqueness part of the theorem.  Hint: Don't forget that we
defined $V$ to be connected.
\end{exercise}

\begin{exercise}
State and prove a germ version of the division theorem.
\end{exercise}
\end{exbox}

The Weierstrass division theorem is a generalization of the division
algorithm for polynomials with coefficients in a field, such as the
complex numbers:  If $f(\zeta)$ is a
polynomial, and $P(\zeta)$ is a nonzero polynomial of degree $k$, then there exist
polynomials $q(\zeta)$ and $r(\zeta)$ with degree of $r$ less than $k$ such
that $f = qP + r$.  If the coefficients are in a commutative ring,
we can divide as long as $P$ is monic.
The Weierstrass division theorem says that
we can divide by a monic $P \in {}_{n-1}\sO_p[z_n]$,
even if $f$ is a holomorphic function (a ``polynomial of
infinite degree'').

\begin{remark}
Despite what it looks like given our proofs,
the preparation and division theorems are really theorems about power
series, and they also work with formal power series, that is, power
series which do not necessarily converge.  Another standard way to prove the
theorems is to prove the formal version and then to prove that in case we
stick in convergent power series, the series we obtain back are also
convergent.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The dependence of zeros on parameters} \label{sec:dependenceofzeros}

Let us prove that the zeros change holomorphically as long as they do not
come together.  We will prove shortly that the zeros come together
only on a small set---a zero set of a certain holomorphic function
called the discriminant.

A set of zeros of a function of one variable are said to be
\emph{\myindex{geometrically distinct}} if they are distinct
points of $\C$.  A zero is called
\emph{\myindex{geometrically unique}} if it is a unique complex number.
For example, ${(\zeta-1)}^2$ has a geometrically unique zero at $1$, and
${(\zeta-1)}^2(\zeta+1)$ has two geometrically distinct zeros, $1$ and $-1$.


\begin{prop} \label{prop:zerohol}
Let
$U' \subset \C^{n-1}$
and
$D \subset \C$
be domains,
and
$f \in \sO(U' \times D)$.
Suppose that for each fixed $z' \in U'$ the function
$z_n \mapsto f(z',z_n)$ has a geometrically unique zero $\alpha(z') \in D$.  Then $\alpha$ is
holomorphic in $U'$.
\end{prop}

The proposition shows that the regularity conclusion of the
holomorphic implicit function theorem holds under the hypothesis that there exists some locally
unique solution for $z_n$, regardless of the derivative vanishing or not.
Such a result holds only for holomorphic
functions and not for real-analytic or smooth functions.  For example, $x^2-y^3 = 0$ has a
unique real solution $y = x^{2/3}$, but $x^{2/3}$ is not even
differentiable.

\begin{proof}
We must show that $\alpha$
is holomorphic near any point, which, without loss
of generality, is the origin and $\alpha(0)=0$.
Apply the preparation
theorem to find $f = u P$,
where $P$ is a
Weierstrass polynomial in $\sO(V')[z_n]$ for some $V' \subset U'$
and all zeros of $z_n \mapsto P(z',z_n)$ are in $D$.
As $\alpha$ is a geometrically unique zero in $D$,
\begin{equation*}
P(z',z_n) = {\bigl(z_n-\alpha(z') \bigr)}^k = z_n^k - k \alpha(z') z_n^{k-1}
+ \cdots
\end{equation*}
The coefficients of $P$ are holomorphic, so $\alpha$ is holomorphic.
\end{proof}

\begin{prop} \label{prop:zeroshol}
Let
$U' \subset \C^{n-1}$
and
$D \subset \C$
be domains,
and
$f \in \sO(U' \times D)$.
Let $m \in \N$ be such that
for each $z' \in U'$, the function $z_n \mapsto f(z',z_n)$ has
precisely $m$ geometrically distinct zeros.
Then locally near each point in $U'$ there exist $m$ holomorphic functions
$\alpha_1(z'),\ldots,\alpha_m(z')$,
positive integers
$k_1,\ldots,k_m$,
and a nonvanishing holomorphic function $u$
such that
\begin{equation*}
f(z',z_n) = u(z',z_n) \prod_{\ell=1}^m {\bigl( z_n - \alpha_\ell(z')
\bigr)}^{k_\ell}
.
\end{equation*}
\end{prop}

Proof is left as an exercise.
We can only define $\alpha_1$ through $\alpha_m$ locally (on a
smaller domain) as we cannot consistently order
$\alpha_1$ through $\alpha_m$ as we move around
$U'$ if it is not simply connected.  If $U'$ is simply connected, then
the functions can be defined globally by analytic continuation.
For an example where $U'$ is not simply connected,
recall \exampleref{sqrt:example}. Consider $U' = \C \setminus \{ 0 \}$ and
think $D=\C$ rather than a disc for simplicity.  Then $U'$ is not simply
connected, and there do not exist continuous functions
$\alpha_1(z_1)$ and
$\alpha_2(z_1)$ defined in $U'$ that are zeros
of the Weierstrass polynomial, that is
$z_2^2 - z_1 =
\bigl(z_2-\alpha_1(z_1) \bigr)
\bigl(z_2-\alpha_2(z_1) \bigr)$.
These would be the two square roots of $z_1$, and there is no continuous
(let alone holomorphic) square root defined in $\C \setminus \{ 0 \}$.
Such roots can be chosen to be holomorphic on any smaller
simply connected open subset of $U'$, for
example, on any disc $\Delta \subset U'$.


\begin{exbox}
\begin{exercise}
Let $D \subset \C$ be a bounded domain, $U' \subset \C^{n-1}$
a domain,
$f$ a continuous function on $U' \times \widebar{D}$ holomorphic
on $U' \times D$,
where $f$ is zero on at least one point
of $U' \times D$, and $f$ is never zero on
$U' \times \partial D$.
Suppose that for each fixed $z' \in U'$, the function
$z_n \mapsto f(z',z_n)$
has at most one zero in $D$.  Prove that for each $z' \in U'$,
$z_n \mapsto f(z',z_n)$ has exactly one zero in $D$.
Note: By \propref{prop:zerohol}, that zero is
a holomorphic function.
\end{exercise}

\begin{exercise}
Prove \propref{prop:zeroshol}.
See the exercise above and \propref{prop:zerohol}.
\end{exercise}
\end{exbox}

\begin{thm} \label{thm:discrthm}
\pagebreak[2]
Let $D \subset \C$ be a bounded domain,
$U' \subset \C^{n-1}$ a domain,
and
$f \in \sO(U' \times D)$.
Suppose the zero set $f^{-1}(0)$ has no limit points
on $U' \times \partial D$.
Then there exists an $m \in \N$
and
a holomorphic function $\Delta \colon U' \to \C$, not identically zero, such
that for every $z' \in U' \setminus E$, where $E = \Delta^{-1}(0)$,
$z_n \mapsto f(z',z_n)$ has exactly $m$ geometrically distinct zeros in $D$,
and $z_n \mapsto f(z',z_n)$ has strictly less than $m$ geometrically distinct
zeros for $z' \in E$.
\end{thm}

The complement of a zero set of a holomorphic function is connected,
open, and dense.
We call $\Delta$ the \emph{\myindex{discriminant function}} and its zero set
$E$ the \emph{\myindex{discriminant set}}.
For the quadratic equation $a(z') z_n^2 + b(z') z_n + c(z') = 0$,
$\Delta$ is the discriminant we learned about in high school:
$\Delta = b^2-4ac$ (that is, this is equal to the $\Delta$
from the theorem assuming we get at least two zeros at some $z'$).

\begin{proof}
The zeros of $z_n \mapsto f(z',z_n)$ are isolated, and there are finitely
many for every $z'$ as $D$ is bounded and $f^{-1}(0)$ has no limit points on
$U' \times \partial D$.
For any $p' \in U'$, we define two useful paths.
Let $\gamma$ be the union of nonintersecting
small simple closed curves around small nonintersecting discs
in $D$, one around each geometrically distinct zero of $z_n \mapsto f(p',z_n)$.
Let $\lambda$ be a large closed path in $D$ going
exactly once around all the zeros and such that the interior of $\lambda$ is
in $D$.
Suppose $\gamma$ and $\lambda$ intersect no zeros.
See \figureref{fig:curve-around-each-zero}.
By continuity, the curves $\gamma$
and $\lambda$ do not intersect any zeros for $z'$ near $p'$.
Since the set $f^{-1}(0)$ is closed and the zeros
do not
accumulate on $U' \times \partial D$,
then
for $z'$ near $p'$ the zeros stay a positive distance away from
the boundary.  So $\lambda$ can be picked to go
around all the zeros of $z_n \mapsto f(z',z_n)$ exactly once for $z'$ near $p'$.

\begin{myfig}
\subimport*{figures/}{curve-around-each-zero.pdf_t}
\caption{Curve around each zero.\label{fig:curve-around-each-zero}}
\end{myfig}

Let $M(z')$ be the number of zeros (counting multiplicity) of 
$z_n \mapsto f(z',z_n)$.
Given any $p'$
pick a $\lambda$ as above that contains all zeros
of $z_n \mapsto f(z',z_n)$ for all $z'$ in some neighborhood of $p'$.
The argument principle show that
\begin{equation*}
M(z') = \frac{1}{2\pi i}
\int_{\lambda}
\frac{\frac{\partial f}{\partial z_n}(z',\zeta)}{f(z',\zeta)} d\zeta ,
\end{equation*}
and so $M$ is constant in this neighborhood (it is an integer-valued
continuous function).
So $M$ is a locally constant function on $U'$,
which is connected and so $M$ is constant.
The number of
geometrically distinct zeros at any $z'$ is bounded by $M$,
although the number of geometrically distinct zeros may not be constant.
Let $m$ be the maximal
number of geometrically distinct zeros and suppose that at some point
in $U'$, there are exactly $m$ geometrically distinct zeros.

Let $U_m' \subset U'$ be the set of $z' \in U'$ for which $z_n \mapsto f(z',z_n)$
has exactly $m$ geometrically distinct zeros.
Write $U'$ as a union of disjoint sets $U' = U_m' \cup E$, where $E = U'
\setminus U_m'$.
By definition of $m$, $U_m'$ is nonempty.  Suppose $p' \in U_m'$ and $\gamma$
goes around the zeros as above.  Let $\gamma_j$ be a single component curve
of the path $\gamma$ going around one of the zeros.
The argument principle with respect to $\gamma_j$ says that
$\gamma_j$ must contain at least one zero for all $z'$ near $p'$.
There are finitely many components of $\gamma$,
so for $z'$ in some neighborhood of $p'$,
$z_n \mapsto f(z',z_n)$ has at least $m$ zeros in $\gamma$
(at least one in each component),
and as $m$ is the maximum, it has exactly $m$ zeros.
In other words, $U_m'$ is open.

Locally on $U_m'$, there exist $m$ holomorphic functions
$\alpha_1, \ldots, \alpha_m$ giving the zeros by the previous proposition.
We cannot define these on all of $U_m'$ as we do not know how they are
ordered.
The function
\begin{equation*}
\Delta(z') = \prod_{j \not= k} \bigl( \alpha_j(z') - \alpha_k(z') \bigr)
\end{equation*}
defined for $z' \in U_m'$ does not depend on the order.
That means $\Delta$ is well-defined as a function on the open set $U_m'$,
and since $\alpha_k$ can locally be picked to be holomorphic,
$\Delta$ is holomorphic.

Let $p' \in E \cap \overline{U_m'}$,
so there are fewer than $m$ zeros at $p'$.
Suppose $\gamma$ and $\lambda$ are as above, so there are fewer than $m$
components of $\gamma$.
In each component $\gamma_j$ of $\gamma$, there is
at least one zero for all $z'$ near $p'$ by the same argument as above.
The path $\lambda$ goes around all the zeros of 
$z_n \mapsto f(z',z_n)$ for $z'$ near $p'$.
The number of between $\lambda$ and $\gamma$ at $z'$ is
\begin{equation*}
\frac{1}{2\pi i}
\int_{\lambda - \gamma}
\frac{\frac{\partial f}{\partial z_n}(z',\zeta)}{f(z',\zeta)} d\zeta ,
\end{equation*}
which is a continuous integer-valued function that is zero at $z'=p'$
and so it is zero in a neighborhood.  Thus there are no zeros between
$\gamma$ and $\lambda$.
As $\lambda$ goes around all the zeros for $z'$ near $p'$,
all zeros of $z_n \mapsto f(z',z_n)$
lie inside $\gamma$ for $z'$ near $p'$.
There exist such $z' \in U_m'$ arbitrarily near $p'$,
in which case, by pidgeonhole principle, some
component $\gamma_j$ contains at least two geometrically distinct zeros of
$z_n \mapsto f(z',z_n)$.
Let $\{ z'_\ell \}$
be an arbitrary sequence of points in $U_m'$ going to $p'$.
As the number of components of $\gamma$ is finite, we pass to a subsequence
so that there is some fixed component $\gamma_j$ of $\gamma$
where
$z_n \mapsto f(z'_\ell,z_n)$ has at least two distinct zeros in $\gamma_j$ for every $z'_\ell$.
Label the two distinct zeros
as $\alpha_1(z'_\ell)$ and $\alpha_2(z'_\ell)$.
At $p'$ there is only a single (geometrically) zero in $\gamma_j$, let
us name it $\alpha_1(p')$.
As $f^{-1}(0)$ is closed,
$\alpha_1(z'_\ell)$ and $\alpha_2(z'_\ell)$ both approach $\alpha_1(p')$ as
$\ell \to \infty$.
The zeros are bounded, so
$\lim_{\ell \to \infty} \Delta(z'_\ell) = 0$.  As the limit is zero for a
subsequence of an arbitrary sequence,
\begin{equation*}
\lim_{z' \in U_m' \to p'} \Delta(z') = 0 .
\end{equation*}

We have already defined $\Delta$ on $U_m'$, where it is nonzero,
so set $\Delta(z') = 0$ for $z' \in E$.
The function $\Delta$ is continuous on $U'$ and is zero precisely on
$E$ and holomorphic on $U_m'$.
Rad{\'o}'s theorem
(\thmref{thm:rado}) says that $\Delta$ is holomorphic in $U'$.
\end{proof}

The discriminant given above is really the discriminant of the set
$f^{-1}(0)$ rather than of the corresponding function, which we can,
via the preparation theorem, assume is a Weierstrass polynomial.
For Weierstrass polynomials, the discriminant is often defined as
$\prod_{j \not= k} \bigl( \alpha_j(z') - \alpha_k(z') \bigr)$ taking
multiple zeros into account, and therefore the ``discriminant'' could be
identically zero.  It will be clear from upcoming exercises that if
the Weierstrass polynomial is irreducible, then the two notions do in fact
coincide.

\begin{exbox}
\begin{exercise}
Prove that if $f \in \sO(U)$, then $U \setminus f^{-1}(0)$ is not simply
connected if $f^{-1}(0)$ is nonempty.  In particular, in the theorem, $U' \setminus E$ is not
simply connected if $E \not= \emptyset$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Let $D \subset \C$ be a bounded domain
and
$U' \subset \C^{n-1}$ a domain.
Suppose
$f$ is a continuous function on
$U' \times \widebar{D}$ holomorphic on $U' \times D$,
and $f$ is never zero on
$U' \times \partial D$.
Suppose $\gamma \colon [0,1] \to U'$ is continuous and
$f\bigl(\gamma(0),c\bigr) = 0$ for some $c \in D$.
Prove that there exists a continuous $\alpha \colon [0,1] \to \C$
such that $\alpha(0) = c$ and
$f\bigl(\gamma(t),\alpha(t)\bigr) = 0$ for all $t \in [0,1]$.
Hint: Start with a path arbitrarily close to $\gamma$
that misses the discriminant.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Properties of the ring of germs} \label{sec:propertiesofringofgerms}

Given a commutative ring $R$,
an \emph{\myindex{ideal}} $I \subset R$ is
a subset such that firstly, $f g \in I$ whenever $f \in R$ and $g \in I$,
and secondly, $g+h \in I$ whenever $g,h \in I$.
An intersection of ideals is again an ideal, and hence it makes sense to
talk about the smallest ideal containing a set of elements.
An ideal $I$ is generated by $f_1,\ldots,f_k$
if $I$ is the smallest ideal containing $\{ f_1,\ldots,f_k \}$.  We then
\glsadd{not:ideal}%
write $I = (f_1,\ldots,f_k)$.
Every element in $I$ can be written as $c_1 f_1 + \cdots + c_k f_k$
where $c_1,\ldots,c_k \in R$.
A \emph{\myindex{principal ideal}}
is an ideal generated by a single element, that is, $(f)$.

For convenience, when talking about germs of functions,
we often identify a representative
with the germ when the context is clear.
So by abuse of notation, we may write
$f \in \sO_p$ instead of $(f,p) \in \sO_p$ and
$(f_1,\ldots,f_k)$ instead of $\bigl((f_1,p),\ldots,(f_k,p)\bigr)$.
As in the following exercises.

\begin{exbox}
\begin{exercise}
\pagebreak[2]
\begin{exparts}
\item
Suppose $f \in \sO_p$, $f(p) \not= 0$, and
$(f)$ is the ideal generated by $f$.  Prove $(f) = \sO_p$.
\item
Let $\mathfrak{m}_p = (z_1-p_1,\ldots,z_n-p_n) \subset \sO_p$ be the ideal generated by the coordinate
functions.  Show that if $f(p) = 0$, then $f \in
\mathfrak{m}_p$.
\item
Show that if $I \subsetneq \sO_p$ is a \emph{\myindex{proper ideal}}
(ideal such that $I
\not= \sO_0$), then
$I \subset \mathfrak{m}_p$, that is, $\mathfrak{m}_p$ is a \emph{\myindex{maximal ideal}}.
\end{exparts}
\end{exercise}

\begin{exercise}\label{exercise:onedimOisPID}
\pagebreak[2]
Suppose $n=1$.  Show that ${}_1\sO_p$ is a
\emph{\myindex{principal ideal domain}}\index{PID} (PID)\@,
that is, every ideal is a principal ideal.
More precisely, show that given an ideal $I \subset {}_1 \sO_p$, then there
exists a $k=0,1,2,\ldots$, such that $I = \bigl( (z-p)^k \bigr)$.
\end{exercise}

\begin{exercise}\label{exercise:biholringiso}
If $U,V \subset \C^n$ are two neighborhoods of $p$
and $h \colon U \to V$ is a biholomorphism.  First
prove that it makes sense to talk about $f \circ h$
for any $(f,p) \in \sO_p$.  Then
prove that $f \mapsto f \circ h$ is a ring isomorphism.
\end{exercise}
\end{exbox}

A commutative ring $R$ is
\emph{\myindex{Noetherian}} if every ideal in $R$ is finitely generated.
That is, for every ideal $I \subset R$
there exist finitely many generators $f_1,\ldots,f_k \in I$:
Every $g \in I$ can be written as $g = c_1 f_1 + \cdots + c_k f_k$,
for some $c_1,\ldots,c_k \in R$.
In an exercise, you proved ${}_1\sO_p$ is a PID\@.
So ${}_1\sO_p$ is Noetherian.  In higher dimensions, the ring of germs may not
be a PID\@, but it is Noetherian.

\begin{thm}
$\sO_p$ is Noetherian.
\end{thm}

\begin{proof}
Without loss of generality, $p=0$.
The proof is by induction on dimension.
By \exerciseref{exercise:onedimOisPID}, ${}_1\sO_0$ is Noetherian.
By \exerciseref{exercise:biholringiso}, we are allowed a
biholomorphic change of coordinates near the origin.

For induction, suppose ${}_{n-1}\sO_0$ is Noetherian
and let $I \subset {}_n \sO_0$ be an ideal.
If $I = \{ 0 \}$
or $I = {}_n \sO_0$, then the assertion is obvious.  Therefore, assume
that all elements of $I$ vanish at the origin ($I \not= {}_n \sO_0$), and
that there exist elements that are not identically zero
($I \not= \{ 0 \}$).  Let $g$
be such an element.  After perhaps a linear change of coordinates,
assume $g$ is a Weierstrass polynomial in $z_n$
by the preparation theorem.

The ring ${}_{n-1}\sO_0[z_n]$ is a subring of ${}_n \sO_0$.
The set $J= {}_{n-1}\sO_0[z_n] \cap I$ is an ideal in the
ring ${}_{n-1}\sO_0[z_n]$.  By the Hilbert basis theorem (see
\thmref{thm:hilbertbasis} in the appendix for a proof), as
${}_{n-1}\sO_0$ is Noetherian, the ring
${}_{n-1}\sO_0[z_n]$ is also Noetherian.  Thus $J$ has finitely many
generators,
that is, $J = (h_1,\ldots,h_k)$ in the ring ${}_{n-1}\sO_0[z_n]$.

By the division theorem,
every $f \in I$ is of the form $f = qg+r$, where $r \in {}_{n-1}\sO_0[z_n]$
and $q \in {}_n\sO_0$.
As $f$ and $g$ are in $I$, so is $r$.
As $g$ and $r$ are in ${}_{n-1}\sO_0[z_n]$,
they are both in $J$.
Write
$g = c_1 h_1 + \cdots + c_k h_k$ and
$r = d_1 h_1 + \cdots + d_k h_k$.  Then
$f = (qc_1 + d_1) h_1 + \cdots + (qc_k + d_k) h_k$.
So
$h_1,\ldots,h_k$ also generate $I$ in ${}_n \sO_0$.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that every proper ideal $I \subset \sO_0$ where $I \not= \{ 0 \}$
is generated by Weierstrass polynomials.  As a technicality,
note that a
Weierstrass polynomial of degree $0$ is just $1$, so it works for $I = \sO_0$.
\end{exercise}

\begin{exercise}
We saw above that
${}_1\sO_p$ is a PID\@.  Prove that
if $n > 1$, then ${}_n\sO_p$ is not a PID\@.
\end{exercise}
\end{exbox}

In a commutative ring $R$, $f \in R$ is
\emph{\myindex{irreducible}} if $f$ is not a unit and whenever $f=gh$,
either $g$ or $h$ is a unit.

\begin{thm}
$\sO_p$ is a \emph{\myindex{unique factorization domain}}\index{UFD}
(UFD)\@.  That is, up to a
multiplication by a unit, every nonzero nonunit has a unique factorization into
irreducible elements of $\sO_p$.
\end{thm}

\begin{proof}
Again assume $p=0$ and induct on the dimension.
The one-dimensional statement is an exercise below.  If ${}_{n-1}\sO_0$ is a UFD\@, then
${}_{n-1}\sO_0[z_n]$ is a UFD by the Gauss lemma
(see \thmref{thm:gausslemma}).

Take $f \in {}_n\sO_0$ such that $f(0)=0$ and $f \not\equiv 0$.
After perhaps a linear change of coordinates
$f = qP$, for $q$ a unit in ${}_n\sO_0$,
and $P$ a Weierstrass polynomial in $z_n$.
As ${}_{n-1}\sO_0[z_n]$ is a UFD\@, $P$ has a unique
factorization in ${}_{n-1}\sO_0[z_n]$ into $P = P_1 P_2 \cdots P_k$.
So $f = q P_1 P_2 \cdots P_k$.  That $P_\ell$ are irreducible
in ${}_n\sO_0$ is left as an exercise.

Suppose $f = \tilde{q} g_1 g_2 \cdots g_m$ is another factorization.
The preparation theorem applies to each $g_\ell$.  Therefore, write
$g_\ell = u_\ell \widetilde{P}_\ell$ for a unit $u_\ell$ and a Weierstrass polynomial
$\widetilde{P}_\ell$.  We obtain
$f = u \widetilde{P}_1 \widetilde{P}_2 \cdots \widetilde{P}_m$ for a unit $u$.  By
uniqueness part of the preparation theorem we obtain
$P = \widetilde{P}_1 \widetilde{P}_2 \cdots \widetilde{P}_m$.
The conclusion is obtained by noting that
${}_{n-1}\sO_0[z_n]$ is a UFD\@.
\end{proof}

\begin{exbox}
\begin{exercise}
Prove that ${}_1\sO_p$ is a UFD\@.
\end{exercise}

\begin{exercise}
Show that an irreducible element of
${}_{n-1}\sO_0[z_n]$, is irreducible in
${}_{n}\sO_0$.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Varieties} \label{sec:varieties}

As before, if $f \colon U \to \C$
\glsadd{not:Zf}%
is a function, let $Z_f = f^{-1}(0) \subset U$ denote the zero set of $f$.

\begin{defn}
Let $U \subset \C^n$ be an open set.  Let $X \subset U$ be a set such that
near each point $p \in U$, there exists a neighborhood $W$ of $p$
and a family of holomorphic functions $\sF$ defined on $W$ such that
\begin{equation*}
W \cap X = \bigl\{ z \in W : f(z) = 0 \text{ for all } f \in \sF \bigr\}
= \bigcap_{f \in \sF} Z_f .
\end{equation*}
Then $X$ is called a
(\emph{complex} or \emph{complex-analytic})
\emph{\myindex{variety}}\index{complex variety}\index{complex-analytic variety}
or a
\emph{\myindex{subvariety}}\index{complex
subvariety}\index{complex-analytic subvariety} of $U$.
Sometimes $X$ is called an \emph{\myindex{analytic set}}.
We say $X \subset U$ is a proper subvariety if $\emptyset \not= X \subsetneq
U$.
\end{defn}

We generally leave out the ``complex'' from ``complex subvariety'' as it is
clear from context.  But you should know that there are other types of
subvarieties, namely real subvarieties given by real-analytic functions.  We
will not cover those in this book.

\begin{example}
The set $X = \{ 0 \} \subset \C^n$ is a subvariety as it is the only common vanishing
point of the functions $\sF = \{ z_1,\ldots,z_n \}$.  Similarly, $X= \C^n$
is a subvariety of $\C^n$, where we let $\sF = \emptyset$.
\end{example}

\begin{example}
The set defined by $z_2 = e^{1/z_1}$ is a subvariety of
$U = \bigl\{ z \in \C^2 : z_1 \not=0 \bigr\}$.  It is not a subvariety of
any open set larger than $U$.
\end{example}

It is useful to note what happens when
we replace ``near each point $p \in U$'' with ``near each point $p \in
X$.''  We get a slightly different concept, and $X$ is said to be a
\emph{\myindex{local variety}}.  A local variety $X$ is a subvariety of
some neighborhood of $X$, but it is not necessarily closed in $U$.  As a
simple example, the set
$X = \bigl\{ z \in \C^2 : z_1 = 0, \abs{z_2} < 1 \bigr\}$ is a
local variety, but not a subvariety of $\C^2$.  On the other hand, $X$
is a subvariety of the unit ball $\bigl\{ z \in \C^2 : \norm{z} < 1 \bigr\}$.

Note that $\sF$ depends on $p$ and near each point may have a different set of
functions.  Clearly the family $\sF$ is not unique.  A priori, we let $\sF$
be infinite, but let us note why it would be sufficient
to restrict to finite families $\sF$.

We work with germs of functions.  Recall, that when $(f,p)$ is a germ of a function
the germ $(Z_f,p)$ is the germ of the zero set of some representative.
Let
\glsadd{not:idealfromset}%
\begin{equation*}
I_p(X) \overset{\text{def}}{=}
\bigl\{ (f,p) \in \sO_p : (X,p) \subset (Z_f,p) \bigr\} .
\end{equation*}
That is, $I_p(X)$ is the set of germs of holomorphic functions vanishing on
$X$ near $p$. 
The sum of two functions that vanish on $X$ also vanishes on $X$,
and if a function vanishes on $X$, then any multiple of it also
vanishes on $X$.  So $I_p(X)$ is an ideal.  Really $I_p(X)$ depends only on
the germ of $X$ at $p$, so define $I_p\bigl((X,p)\bigr) = I_p(X)$.

As $\sO_p$ is Noetherian, every ideal in $\sO_p$ is finitely generated.
Let $I \subset \sO_p$ be an ideal generated by $f_1,f_2,\ldots,f_k$.
Write
\glsadd{not:vanishingset}%
\begin{equation*}
V_p(I) \overset{\text{def}}{=}
(Z_{f_1},p) \cap (Z_{f_2},p) \cap \cdots \cap (Z_{f_k},p) .
\end{equation*}
That is, $V_p(I)$ is the germ of the subvariety ``cut out'' by the elements of $I$,
since every
element of $I$ vanishes on the points where all the generators vanish.
Suppose representatives $f_1,\ldots,f_k$ of the generators are defined
in some neighborhood $W$ of $p$,
and a germ $(g,p) \in I$ has a representative $g$ defined in $W$
such that $g = c_1 f_1 + \cdots + c_k f_k$, where $c_k$ are also holomorphic
functions on $W$.  If $q \in Z_{f_1} \cap \cdots \cap Z_{f_k}$,
then $g(q) = 0$.  Thus,
$Z_{f_1} \cap \cdots \cap Z_{f_k} \subset Z_g$, or in terms of germs,
$V_p(I) \subset (Z_g,p)$.  The reason why we did not define $V_p(I)$ to be the
intersection of zero sets of all germs in $I$ is that this would be an
infinite intersection, and we did not define such an object for germs.

\begin{exbox}
\begin{exercise}
Show that $V_p(I)$ is independent of the choice of generators.
\end{exercise}

\begin{exercise}
Suppose $I_p(X)$ is generated by the functions $f_1, f_2, \ldots, f_k$.
Prove
\begin{equation*}
(X,p) = (Z_{f_1},p) \cap (Z_{f_2},p) \cap \cdots \cap (Z_{f_k},p) .
\end{equation*}
\end{exercise}

\begin{exercise}
Given a germ $(X,p)$ of a subvariety at $p$, show
$V\bigl(I_p(X)\bigr) = (X,p)$ (see above).
Then given an ideal $I \subset \sO_p$, show
$I_p\bigl(V_p(I)\bigr) \supset I$.
\end{exercise}

\begin{exercise}
Let $X \subset \C^n$ be a subvariety that is a complex cone,
in other words,
if $z \in X$, then $\lambda z \in X$ for all $\lambda \in \C$.
Prove that $I_0(X)$ is generated by finitely many
homogeneous polynomials.
Hint: Given any $f$ holomorphic near $0$ that vanishes on $X$,
write $f = \sum_k f_k(z)$ where $f_k$ are homogeneous polynomials.
Show that $f_k$ vanish on $X$.  Use that Hilbert basis theorem applies
and so the ring of polynomials is Noetherian.
\end{exercise}

\begin{exercise}
Suppose $\Omega \subset \C^n$ is a domain, $U \subset \C^k$ open,
$f \colon \Omega \to U$ holomorphic,
and $X \subset U$ is a subvariety.
Suppose that there exists a nonempty open subset $W \subset \Omega$ such
that $f(W) \subset X$.  Prove that $f(\Omega) \subset X$.
\end{exercise}
\end{exbox}

The ideal $I_p(X)$ is finitely generated.
Near each point $p$ only finitely many functions are
necessary to define a subvariety, that is, by an exercise above, those functions
``cut out'' the subvariety.  When one says
\emph{\myindex{defining functions}} for a germ of a subvariety, one generally
means that those functions generate the ideal, not just that their common
zero set happens to be the subvariety.  A theorem that we will not prove here
in full generality,
the \emph{\myindex{Nullstellensatz}}, says that if we take the germ of
a subvariety defined by functions
in an ideal $I \subset \sO_p$, and look at the ideal given by that subvariety,
we obtain the
radical of $I$.
The \emph{\myindex{radical}} of $I$ is defined as
$\sqrt{I} \overset{\text{def}}{=} \{ f : f^m \in I \text{ for some } m \}$.
In more concise language, the Nullstellensatz says
$I_p\bigl(V_p(I)\bigr) = \sqrt{I}$.
Germs of subvarieties are in
one-to-one correspondence with radical ideals of $\sO_p$.

\begin{example}
The subvariety $X = \{ 0 \} \subset \C^2$ can be given by
$\sF = \bigl\{ z_1^2, z_2^2 \bigr\}$.
If $I = \bigl(z_1^2,z_2^2\bigr) \subset \sO_0$ is the ideal of germs
generated by these two functions, then $I_0(X) \not= I$.  We have
seen that the ideal $I_0(X)$ is the maximal ideal $\mathfrak{m}_0 =
(z_1,z_2)$.
As $I \subset (z_1,z_2) = \mathfrak{m}_0$
and the square of $z_1$ and $z_2$ are both in $I$,
we find $\sqrt{I} = (z_1,z_2) = \mathfrak{m}_0$.
\end{example}

The local properties of a subvariety at $p$ are
encoded in the properties of
the ideal $I_p(X)$.  Therefore, the study of subvarieties often
involves the study of
the various algebraic properties of the ideals of $\sO_p$.
Let us also mention in passing that the other object
that is studied is the so-called
\emph{\myindex{coordinate ring}}
$\sO_p / I_p(X)$, which represents the functions on $(X,p)$.  That is, we
identify two functions if they differ by something in the ideal, since then
they are equal on $X$.

\medskip

At most points a subvariety is like a piece of $\C^k$, more
precisely like a graph over $\C^k$.
A graph of
$f \colon U' \subset \C^k \to \C^{n-k}$
is the set $\Gamma_f \subset U' \times \C^{n-k} \subset \C^k \times
\C^{n-k}$ defined by
\glsadd{not:graph}%
\begin{equation*}
\Gamma_f \overset{\text{def}}{=}
\bigl\{ (z,w) \in U' \times \C^{n-k} : w=f(z) \bigr\} .
\end{equation*}

\begin{defn}
\pagebreak[2]
Let $U \subset \C^n$ be open and
$X \subset U$ a subvariety.
Let $p \in X$ be a point where
after a permutation of coordinates,
the set $X$ is a graph of a holomorphic
mapping near $p$.
That is, after relabeling coordinates,
there is a neighborhood $U' \times U'' \subset \C^{k}
\times \C^{n-k}$ of $p$, for some $k =0,1,\ldots,n$,
and a holomorphic $f \colon U' \to \C^{n-k}$
such that
\begin{equation*}
X \cap (U' \times U'') = \Gamma_f .
\end{equation*}
Then $p$ is a \emph{\myindex{regular point}}
(or a \emph{\myindex{simple point}}) of $X$ and
the (complex) \emph{dimension}\index{dimension at a regular point}
of $X$ at $p$ is $k$.  We write $\dim_p X = k$.
As the ambient\footnote{The word \emph{\myindex{ambient}}
is used often to mean the set
that contains whatever object we are talking about.} dimension
is $n$ ($X$ is a subvariety of $U$), we say $X$ is of
\emph{\myindex{codimension}} $n-k$ at~$p$.
If all points of $X$ are regular points of dimension $k$, then $X$ is
a \emph{\myindex{complex manifold}}, or
a \emph{\myindex{complex submanifold}}\index{submanifold!complex}\index{manifold!complex},
of (complex) dimension~$k$.

\glsadd{not:Xreg}%
The set of regular points of $X$ is denoted by $X_{\mathit{reg}}$.  Any
point that is not regular is \emph{singular}\index{singular point}.
\glsadd{not:Xsing}%
The set of singular points of $X$ is denoted by $X_{\mathit{sing}}$.
\end{defn}

A couple of remarks are in order.
A subvariety $X$ can have
regular points of several different dimensions, although if a point
is a regular point of dimension $k$, then all nearby points are regular
points of dimension $k$ as the same $U'$ and $U''$ works.
In particular, $X_{\mathit{reg}}$ is an open subset of $X$.
An isolated point of $X$ is automatically a regular point of
dimension~$0$.
Sometimes the empty set is considered a complex manifold of dimension $-1$ (or
$-\infty$).
Although it may not perhaps be immediately clear, but it is not
difficult to show that the definition 
of a regular point is invariant under biholomorphic changes of coordinates
(\exerciseref{exercise:regularbiholomorphic}).
Finally, we remark is that dimension is well-defined
(\exerciseref{exercise:regdimwelldef}).

\begin{example}
The set $U = \C^n$ is a complex submanifold of dimension $n$
(codimension $0$).
In particular, $U_{\mathit{reg}} = U$ and $U_{\mathit{sing}} = \emptyset$.

The set $M = \bigl\{ z \in \C^3 : z_3 = z_1^2 - z_2^2 \bigr\}$ is a complex submanifold of
dimension $2$ (codimension $1$).  Again,
$M_{\mathit{reg}} = M$ and $M_{\mathit{sing}} = \emptyset$.

On the other hand, the so-called \emph{\myindex{cusp}},
$C = \bigl\{ z \in \C^2 : z_1^3-z_2^2 = 0 \bigr\}$ is not a complex
submanifold.  The origin is a singular point of $C$
(see exercise below).
At every other point, we can write $z_2 = \pm z_1^{3/2}$,
so $C_{\mathit{reg}} = C \setminus \{0\}$, and so $C_{\mathit{sing}} = \{ 0
\}$.
The dimension at every regular point is $1$.
See \figureref{fig:cuspintersect} for a
plot of $C$ in two real dimensions.

Another type of singularity could be where two complex manifolds
intersect.  For example, $X = \{ z \in \C^2 : z_1^2-z_2^2 = 0 \}$
is the union of the two complex manifolds of dimension 1 given by $z_1+z_2=0$ and
$z_1-z_2=0$.  In this case $X_{\mathit{sing}} = \{ 0 \}$ and
$X_{\mathit{reg}} = X \setminus \{ 0 \}$.
See \figureref{fig:cuspintersect} for a
plot of $X$ in two real dimensions.

\begin{myfig}
\medskip
\subimport*{figures/}{cusp.eepic}
\qquad
\qquad
\subimport*{figures/}{intersect.eepic}
\bigskip
\caption{The cusp $C$ (left), and the intersecting manifolds $X$
(right).\label{fig:cuspintersect}}
\end{myfig}
\end{example}


\begin{exbox}
\begin{exercise} \label{exercise:regdimwelldef}
Prove that if $p$ is a regular point of a subvariety $X \subset U \subset
\C^n$ of a domain $U$, then the dimension at $p$ is well-defined.  Hint: If there were two possible
$U'$ of different dimension (possibly different affine coordinates), construct a map
from one such $U'$ to another such $U'$ with nonvanishing derivative.
\end{exercise}

\begin{exercise}
Consider the cusp
$C = \bigl\{ z \in \C^2 : z_1^3-z_2^2 = 0 \bigr\}$.  Prove that
the origin is not a regular point of $C$.
\end{exercise}

\begin{exercise} \label{exercise:regularbiholomorphic}
Show that $p$ is a regular point of dimension $k$ of a subvariety $X$
if and only if there
exists a local biholomorphic change of coordinates that puts $p$ to the
origin and near $0$, $X$ is given by $w=0$, where $(z,w) \in \C^{k} \times
\C^{n-k}$.  In other words, if we allow a biholomorphic change of
coordinates instead of just reordering of coordinates,
we can let $f=0$ in the definition.
\end{exercise}
\end{exbox}

\pagebreak[2]
We also define dimension at a singular point.
Below, we will prove that the set of regular points is not only nonempty,
but also dense in any subvariety, and so arbitrarily near to every point,
there is a regular point.

\begin{defn}
Let $X \subset U \subset \C^n$ be a subvariety of $U$.  Let $p \in
X$ be a point.  We define the (complex)
\emph{\myindex{dimension}} of $X$ at $p$ to be
\glsadd{not:dimpX}%
\begin{equation*}
\dim_p X \overset{\text{def}}{=}
\max \bigl\{ k \in \N_0 : \text{ $\forall$ neighbhds.
$W$ of $p$, $\exists \, q \in W \cap X_{\mathit{reg}}$ with $\dim_q X = k$}
\bigr\} .
\end{equation*}
If $(X,p)$ is a germ and $X$ a representative,
the \emph{dimension} of $(X,p)$ is the dimension of
$X$ at $p$.
The dimension of the entire subvariety $X$ is defined to be
\glsadd{not:dimX}%
\begin{equation*}
\dim X \overset{\text{def}}{=}
\max_{p \in X_{\mathit{reg}}} \dim_p X .
\end{equation*}
We say that $X$ is of \emph{\myindex{pure dimension}} $k$ if at
all regular points $p$, dimension of $X$ at $p$ is $k$.
We say a germ $(X,p)$ is of pure dimension $k$ if there exists a representative
of $X$ that is of pure dimension $k$.
We define the word \emph{codimension} as before, that is, the
ambient dimension minus the dimension of $X$.
\end{defn}

\begin{example}
The cusp $C = \bigl\{ z \in \C^2 : z_1^3-z_2^2 = 0 \bigr\}$ is of
dimension $1$ at all the regular points, and the only singular point is the
origin.  Hence $\dim_0 C = 1$, and so $\dim C = 1$.  The subvariety $C$ is
of pure dimension $1$.
\end{example}

Let us restate
\thmref{thm:regptsdense} we proved in
\sectionref{sec:riemannextzerosetsinjmaps}
in the language of varieties.

\begin{thm} \label{thm:regptsdense2}
Let $U \subset \C^n$ be a domain and $f \in \sO(U)$.
Then $Z_f$ is empty,
$Z_f$ is a subvariety of pure codimension $1$,
or $Z_f = U$.
Furthermore,
$(Z_f)_{\mathit{reg}}$ is open and dense in $Z_f$.
\end{thm}

Let us improve on this for arbitrary varieties.

\begin{lemma} \label{lemma:regdense}
Let $U \subset \C^n$ be open and let $X \subset U$ be a subvariety,
then $X_{\mathit{reg}}$ is nonempty.  Consequently, $X_{\mathit{reg}}$ is
open and dense in $X$.
\end{lemma}

\begin{proof}
In the same way that we proved \thmref{thm:regptsdense},
by intersecting $X$ with arbitrary neighborhoods,
it is sufficient to show the existence of one regular point.

We will induct on dimension.  For $n=1$,
the only subvarieties are either isolated points or entire components of
$U$, so all points are regular.
Suppose the lemma is true in dimension $n-1$.
It is enough to find a regular point in some neighborhood of some point
$p \in X$.
Suppose $p = 0$ for simplicity.
Either $X$ contains a whole neighborhood of $0$, in which case
$0$ is a regular point, or there is some holomorphic function
$f$ near $0$ that vanishes on $X$.
After a small linear change of coordinates, the
Weierstrass preparation theorem applies and we can assume that $f$ is a
Weierstrass polynomial.  Write the variables as
$(z_1,\ldots,z_{n-1},z_n) = (z',z_n)$.
In some neighborhood $V$ of the origin (where $V \subset U$),
\thmref{thm:discrthm} applies to $f$, so
let $\Delta(z')$ denote the discriminant function.
Thinking of $\Delta$ as a function
on $V$, suppose that $\Delta(q) \not= 0$ for some $q \in X \cap V$.
Then there is some
neighborhood $W$ of $q$, such that $Z_f \cap W$ is a
graph of a holomorphic function and $X \cap W \subset Z_f \cap W$.
By a local biholomorphic change of variables, we make $W$ an open
subset of $\C^{n-1} \times \{ 0 \}$, so $X \cap W$ is really contained in
$\C^{n-1}$ and we apply induction.

So suppose $\Delta$ vanishes on $X$.  Note that
$\Delta$ is a function of $z'=(z_1,\ldots,z_{n-1})$.
After a linear change of variables in $z'$,
we apply the preparation theorem in $z'$ with respect to $z_{n-1}$
and get $\Delta = u P$ in some neighborhood of $0$,
where $u$ is a unit and $P$ is a Weierstrass polynomial
in $z_{n-1}$ with coefficients that only depend on
$z_1,\ldots,z_{n-2}$.
Let $\Delta'$ be the discriminant for $P$ and repeat the procedure
above.  Either $\Delta'$ is nonzero at some point of $X$,
in which case we apply the induction hypothesis as above,
or near the
origin, $X$ is contained in the zero set of $\Delta'$.  If $X$ is contained
in the zero set of $\Delta'$, we again apply the preparation theorem
and get a polynomial in $z_{n-2}$ with coefficients depending only on
$z_1,\ldots,z_{n-3}$.  Rinse and repeat.  Either at some point we could
apply the induction hypothesis, or we end after $n$ steps with $X$ being
in the zero set of the Weierstrass polynomial in one variable, that is,
$X$ is locally near the origin contained in the
set where $z_1 = 0$, and we can again apply the induction.
\end{proof}

We will state without proof that we actually have that the singular set is a
subvariety, that is, we have the following theorem.
We will prove it for varieties of pure codimension $1$
in the next section.

\begin{thm} \label{thm:varietysingularity}
Let $U \subset \C^n$ be open and let $X \subset U$
be a subvariety, then
$X_{\mathit{sing}} \subset X$ is a subvariety,
which is nowhere dense in $X$
and $\dim X_{\mathit{sing}} < \dim X$.
\end{thm}

\begin{exbox}
\begin{exercise}
Suppose that $X \subset U \subset \C^n$ is a subvariety
of a domain $U$, such that $X_{\mathit{reg}}$ is connected.  Show that $X$ is of
pure dimension.
\end{exercise}
\end{exbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypervarieties} \label{section:hypervarieties}

Pure codimension-$1$ subvarieties are particularly nice.
Sometimes pure codimension-$1$ subvarieties are called
\emph{hypervarieties}\index{hypervariety}.  We will prove two things for
hypervarieties.  First we will prove that locally, a hypervariety can be
defined via a single function,
and second, we will prove that the singular set of a hypervariety is a
subvariety.

\begin{thm} \label{thm:codim1var}
\pagebreak[0]
If $(X,p)$ is a germ of a pure codimension-$1$ subvariety, then
there is a germ of a holomorphic function $f$ at $p$
such that $(Z_f,p) = (X,p)$ and $I_p(X)$ is generated by $(f,p)$.
\end{thm}

\begin{proof}
We need to find a function that vanishes on $(X,p)$
and divides every other function that vanishes there.
There must exist at least one germ of a function that vanishes
on $X$ near $p$
(although it could vanish on a larger set).
Without loss of generality, assume $p=0$
and after a linear change
of coordinates the Weierstrass preparation theorem applies.
More precisely,
suppose $X$ is a pure codimension-$1$ subvariety
of a small enough
polydisc
$U' \times D \subset \C^{n-1} \times \C$ centered at the origin,
and the function that vanishes on $X$ is a
Weierstrass polynomial $P(z',z_n)$ defined for $z' \in U'$, and
all zeros of $z_n \mapsto P(z',z_n)$ are in $D$ for $z' \in U$.
\thmref{thm:discrthm} applies. Let
$E \subset U'$ be the discriminant set, a zero set of a
holomorphic function.
On $U' \setminus E$, there are a certain number of geometrically
distinct zeros of $z_n \mapsto P(z',z_n)$.

Let $X'$ be a topological component of
$X \setminus ( E \times D )$.
Above each $z' \in U' \setminus E$,
let $\alpha_1(z'),\ldots,\alpha_k(z')$
denote the distinct zeros that are in $X'$,
that is, $\bigl(z',\alpha_\ell(z')\bigr) \in X'$.
Near each point $X'$ is a graph of a holomorphic function over
$U' \setminus E$, and so we can locally choose 
$\alpha_1,\ldots,\alpha_k$ to be holomorphic.
Furthermore, this means that the set $X'$ contains only regular points of
$X$, which are of dimension $n-1$.
The number of
such geometrically distinct zeros in $X'$ above each point in
$U' \setminus E$ is locally constant.
As $U' \setminus E$ is connected
(\exerciseref{exercise:connectedcomplement}),
there exists a unique $k$.  Take
\begin{equation*}
F(z',z_n) = \prod_{\ell=1}^k \bigl( z_n-\alpha_\ell(z')\bigr)
=
z_n^k + \sum_{\ell=0}^{k-1} g_\ell(z') z_n^\ell .
\end{equation*}
The coefficients $g_\ell$ are well-defined for $z \in U' \setminus E$
as they are independent of how $\alpha_1,\ldots,\alpha_k$ are ordered.
The $g_\ell$ are holomorphic for $z \in U' \setminus E$
as locally we can ensure that each $\alpha_\ell$ is holomorphic.
The coefficients $g_\ell$ are bounded
on $U'$ and so extend to holomorphic functions of $U'$
via the Riemann extension theorem.
Hence, $F$ is a polynomial
in $\sO(U')[z_n]$.
The zeros of $F$
above $z' \in U' \setminus E$
are simple and give precisely $X'$.
The zeros of $F$ above $z' \in E$, must be
limits zeros above points of $U' \setminus E$
by the argument principle.
Consequently,
the zero set of $F$ is the closure of $X'$ in $U' \times D$.
It is left to the reader to check that
all the functions $g_\ell$ vanish at the origin and $F$ is a Weierstrass
polynomial, a fact that will be useful in the exercises below.

If the polynomial $P(z',z_n)$ is of degree $m$,
then $z' \mapsto P(z',z_n)$ has at most $m$ zeros.  Together with
the fact that $U' \setminus E$ is connected, this means that
$X \setminus (E \times D)$ has at most finitely many components (at
most $m$).
We find an $F$ for every topological component of $X \setminus ( E \times D )$
and we multiply those functions together to get $f$.
No open piece $M \subset X_{\mathit{reg}}$ can lie completely in $E \times D$,
as otherwise an open subset of $M$ would also be an open piece of $E \times D$,
see \exerciseref{exercise:hypersurfaceinhypervariety},
but we know that $P$ must vanish on $M$, which is impossible as
it only vanishes at finitely
many points for each fixed $z'$.
Therefore, as $X_{\mathit{reg}}$ is dense in $X$ (\lemmaref{lemma:regdense}),
the closure of $X \setminus (E \times D)$ contains $X$ and so $Z_f = X$.

The fact that this $f$ generates $I_p(X)$ is left as
\exerciseref{exercise:singlegenerator}.
\end{proof}

In other words, local properties of a codimension $1$ subvariety can be
studied by studying the zero set of a single Weierstrass polynomial.

\begin{example}
It is not true that
a subvariety in $\C^n$ of dimension $n-k$ (codimension $k$)
has $k$
holomorphic functions that ``cut it out.''  That only works for $k=1$.
The set defined by
\begin{equation*}
\rank
\begin{bmatrix}
z_1 & z_2 & z_3 \\
z_4 & z_5 & z_6
\end{bmatrix}
< 2
\end{equation*}
is a pure $4$-dimensional subvariety of $\C^6$, so of codimension $2$,
and the defining equations are
$z_1z_5-z_2z_4 = 0$,
$z_1z_6-z_3z_4 = 0$, and
$z_2z_6-z_3z_5 = 0$.  We state without proof that the unique singular point is the origin and there exist
no $2$ holomorphic functions near the origin
that define this subvariety.  In more technical
language, the subvariety is not a \emph{\myindex{complete intersection}}.
\end{example}

Interestingly, a small refinement of the proof of the theorem above gives
the following.
Same result holds
for higher codimension, but it is harder to prove.

\begin{cor}
Let $(X,p)$ is a germ of a subvariety of pure codimension $1$.  Then there exists
a neighborhood $U$ of $p$, a representative $X \subset U$ of $(X,p)$
and subvarieties $X_1,\ldots,X_k \subset U$
of pure codimension $1$ such that $(X_\ell)_{\mathit{reg}}$ is connected for
every $\ell$, and
$X = X_1 \cup \cdots \cup X_k$.
\end{cor}

\begin{proof}
A particular $X_\ell$ is defined by considering a topological component of $X \setminus
(E \times D)$ as in the proof of \thmref{thm:codim1var}, getting the $F$,
and setting $X_\ell = Z_F$.
The topological component is a connected set and it is dense in
$(X_\ell)_{\mathit{reg}}$, which proves the corollary.
\end{proof}

\begin{exbox}
\begin{exercise}
Suppose $p(z',z_n)$ is a Weierstrass polynomial of degree $k$ such that
for an open dense set of $z'$ near the origin
$z_n \mapsto p(z',z_n)$ has geometrically $k$ zeros, and such that the
regular points of $Z_p$ are connected.  Show that $p$ is
irreducible in the sense that if $p = rs$ for two Weierstrass polynomials
$r$ and $s$, then either $r=1$ or $s=1$.
\end{exercise}

\begin{exercise}
Suppose $f$ is a function holomorphic in a neighborhood of the origin with
$z_n \mapsto f(0,z_n)$ being of finite order.  Show that
\begin{equation*}
f = u p_1^{d_1} p_2^{d_2} \cdots p_\ell^{d_\ell} ,
\end{equation*}
where $p_k$ are Weierstrass polynomials of degree $\mu_k$ such that
for an open dense set of $z'$, $z_n \mapsto f(z',z_n)$
has $\mu_k$ geometrically distinct zeros (no multiple zeros),
the set of regular points of $Z_{p_k}$ are
connected, and $u$ is a nonzero holomorphic function near $0$.
Note: In the next section, these
polynomials will be the irreducible factors in the factorization of $f$.
\end{exercise}

\begin{exercise} \label{exercise:singlegenerator}
Prove the last part of \thmref{thm:codim1var}:
Show that if $(X,p)$ is a germ of a pure codimension-$1$ subvariety, then
the ideal $I_p(X)$ is a principal ideal (has a single generator).
\end{exercise}

\begin{exercise} \label{exercise:hypersurfaceinhypervariety}
Suppose $U \subset \C^n$ is open and $X \subset U$
is a subvariety of dimension $n-1$.  Suppose $M$
is a small piece of a complex submanifold of dimension $n-1$ such that
$M \subset X$.  Prove that $M$ agrees with $X_{\textit{reg}}$ on a dense
open set, that is,
for each $p$ a dense open subset of $M$,
there is a neighborhood $W$ of $p$ such that
$M \cap W = X_{\textit{reg}} \cap W$.
Hint: Consider coordinates where
$M$ is a graph and \thmref{thm:discrthm} applies to $X$.
\end{exercise}

\begin{exercise}
Suppose $I \subset \sO_p$ is a principal ideal.
Prove the
\emph{\myindex{Nullstellensatz}} for hypervarieties:
$I_p\bigl(V_p(I)\bigr) = \sqrt{I}$.  That is, show that if
$(f,p) \in I_p\bigl(V_p(I)\bigr)$, then $(f^k,p) \in I$ for some integer $k$.
\end{exercise}

\begin{exercise}
Suppose $X \subset U$ is a subvariety of pure codimension $1$ for an open set $U \subset \C^n$.
Let $X'$ be a topological component of $X_{\textit{reg}}$.  Prove that the
closure $\overline{X'}$ is a subvariety of $U$ of pure codimension $1$.
\end{exercise}
\end{exbox}

\begin{example} \label{example:discriminantnotsingset}
If $X$ is a hypervariety, 
the preparation theorem applies, and $E$ the corresponding discriminant set,
it is tempting to say that the singular set of $X$ is the
set $X \cap (E \times \C)$, which is a codimension-$2$ subvariety.  It is true that
$X \cap (E \times \C)$ will contain the singular set, but in general the
singular set is smaller.
A simple example of this behavior is the set defined by
$z_2^2 - z_1 = 0$.  The defining function is a Weierstrass
polynomial in $z_2$ and the discriminant set is given by $z_1 = 0$.
However, the subvariety has no singular points as it is the graph of $z_1$
over $z_2$.  See~\figureref{fig:sidewaysparabola}.

\begin{myfig}
\medskip
\subimport*{figures/}{sidewaysparabola.eepic}
\bigskip
\caption{The sideways parabola $z_2^2-z_1=0$ for real $z_1$ and $z_2$.
For each nonzero $z_1$, there are two solutions (for negative $z_1$ these
are obviously not real).  But for each fixed $z_2$, there is exactly
one solution.\label{fig:sidewaysparabola}}
\end{myfig}

A less trivial example $z_1^2+\cdots+z_k^2=0$,
where the singular set is of any given dimension $n-k$
is given in
\exerciseref{exercise:morsesingularity}.
\end{example}

Let us prove the version of \thmref{thm:varietysingularity} for
hypervarieties.
We now know that locally hypervarieties are defined by a single function,
so we can use the discriminant to locate singularities, but we must allow
infinitely many linear changes of coordinates.

\begin{thm} \label{thm:hypervarsingularity}
Let $U \subset \C^n$ be open and $X \subset U$
a subvariety of pure codimension $1$ (a hypervariety).
Then $X_{\mathit{sing}}$ is a subvariety of dimension less than or equal to
$n-2$.
\end{thm}

\begin{proof}
It is sufficient to consider a certain fixed point $p \in X$ and prove the
result locally near $p$.
At $p$, there is a single holomorphic function $f$ that defines the germ of
$(X,p)$ meaning that its zero set is equal to $X$ near $p$.
Without loss of generality, assume that $p=0$,
and after a linear change of coordinates, assume that we can apply
the preparation theorem and \thmref{thm:discrthm}
near the origin with
respect to each variable.
Then we can assume that $f$ is holomorphic in some neighborhood $W$,
$Z_f = X \cap W$, and
there exists an open neighborhood $V$ of the origin so that for every
variable $z_k$, $k=1,\ldots,n$, there is a polydisc $D=D_1 \times \cdots
\times D_n$ centered at the origin with
$\widebar{V} \subset D$ and $\widebar{D} \subset W$, where
\thmref{thm:discrthm} applies with respect to $z_k$, that is,
$Z_f \cap
(D_1 \times \cdots \times \partial D_k \times \cdots \times D_n)
= \emptyset$.

Consider a $q \in X_{\textit{reg}} \cap V$.  By definition, $X$ is a graph
near $q$, so after reordering variables,
we assume it is a graph of $z_n$
over $z'=(z_1,\ldots,z_{n-1})$.
Let $D$ be the corresponding polydisc and
write $D = D' \times D_n \subset \C^{n-1} \times \C$.  Let $E \subset D'$
be the discriminant set given by the function $\Delta \in \sO(D')$.
We think of $\Delta$ as a function in $\sO(D)$.
If $q \not\in E \times D_n$, then $\Delta(q) \not= 0$, so we have
found a function holomorphic in $V$ that is nonzero at $q$.
Let us start a collection $\sF$ of holomorphic functions on $V$,
one for each $q \in X_{\mathit{reg}} \cap V$,
and we put $\Delta$ in $\sF$.

Suppose that $\Delta(q) = 0$.
We may assume that the $f$ is the Weierstrass polynomial
in $z_n$ we found in the proof of \thmref{thm:codim1var}.  In particular, it is
a Weierstrass polynomial of degree $m$ and
for a generic $z'$ (outside of the discriminant set),
the function $z_n \mapsto f(z',z_n)$ has $m$ geometrically distinct roots
(so $m$ roots up to multiplicity as well).
Write $q = (q',q_n)$.
The zero of $z_n \mapsto f(q',z_n)$ at $z_n=q_n$ is simple, but the others
are not all simple as $q'$ is in the discriminant.
We will change variables so that the new vertical line through $q$
intersects $X$ only at simple zeros.  The root at $q$
is already simple and so we will rotate the line around $q$.

We will change variables to
$\tilde{z} = (\tilde{z}',z_n)$ where
$(z',z_n) = \bigl(\tilde{z}' + (q_n-z_n) \epsilon', z_n\bigr)$
and where $\epsilon' \in \C^{n-1}$ is small,
so that the chosen line becomes the vertical $\{ \tilde{z}' = q' \}$.
For small $\epsilon'$ and $\tilde{z}'$ near $q'$, the function
$z_n \mapsto f\bigl(\tilde{z}' + (q_n-z_n) \epsilon', z_n\bigr)$
still has exactly $m$ roots up to
multiplicity via the argument principle.
If $z_n \mapsto f\bigl(q' + (q_n-z_n) \epsilon', z_n\bigr)$ has $m$ geometrically
distinct roots, then so does 
$z_n \mapsto f\bigl(\tilde{z}' + (q_n-z_n) \epsilon', z_n\bigr)$
for $\tilde{z}'$ near $q'$
via the same argument with the $m$ small discs and
the argument principle as in the proof of \thmref{thm:discrthm}.
The problem of finding arbitrarily small $\epsilon'$ that do the trick is
left as an exercise.  It can be done one intersection of the line
$z_n \mapsto \bigl(q' + (q_n-z_n) \epsilon', z_n\bigr)$ with $X$ at a time,
that is, if we have an intersection of multiplicity $k$, a small generic change in
$\epsilon'$ will give us $k$ distinct intersections nearby.
See \exerciseref{exercise:nonverticalintersections}.

Take a slightly smaller polydisc
$\widetilde{D} = \widetilde{D}' \times D_n \subset D$
in the $\tilde{z}$ variables
such that
still $V \subset \widetilde{D}$ (we may need to pick $\epsilon'$ small
enough to arrange this)
and \thmref{thm:discrthm} applies in $\widetilde{D}$.
As the number
of distinct zeros of $f\bigl(\tilde{z}' + (q_n-z_n) \epsilon', z_n\bigr)$ is $m$
for all $\tilde{z}'$ near $q'$ including $\tilde{z}'=q'$,
the discriminant
$\widetilde{\Delta}$ in these variables does not vanish at $q'$.
We againt consider $\widetilde{\Delta}$ as a function
on $V$ and as it does not vanish at $q$, we add $\widetilde{\Delta}$ to $\sF$.
See \figureref{fig:Xsingvariety} for the setup.
We define $\sF$ by repeating for each
$q \in X_{\mathit{reg}} \cap V$.

\begin{myfig}
\medskip
\subimport*{figures/}{Xsingvariety.pdf_t}
\bigskip
\caption{Changing variables to make the discriminant not vanish at
$q$, where $L$ is the line $\{z' = q'\}$ while $\widetilde{L}$
is the tilted line $\{ \tilde{z}' = q' \}$.\label{fig:Xsingvariety}}
\end{myfig}

Next, if $q \in X \cap V$ is singular, then every discriminant function
used above must be zero at $q$;
outside of the discriminant set, all points of $X$ are graphs of the zeros and hence nonsingular.
That is, $\varphi(q)=0$ for every $\varphi \in \sF$.
Thus the common zero set of all the functions in $\sF$ intersected with
$X \cap V$ gives us precisely $X_{\textit{sing}} \cap V$, so
$X_{\textit{sing}}$ is a subvariety.  It cannot be of dimension $n-1$ as
if it were,
it would be a complex submanifold of dimension $n-1$ near some
point and then not all of those points would be singular for $X$,
see \exerciseref{exercise:hypersurfaceinhypervariety}.
\end{proof}

\pagebreak[2]
We used infinitely many functions in $\sF$ to define
$X_{\mathit{sing}}$.  It is possible to use finitely many near each point
as the ideals $I_p(X_{\mathit{sing}})$ are Noetherian,
but despite the temptation, it is not possible to do a single
generic linear change of variables and use just the
$n$ discriminant functions, one for each variable.
More than $n$ functions
may be necessary as situations like the one depicted in
\figureref{fig:Xsingvariety} may occur for some $q$,
no matter how we change
variables to start with.

\begin{exbox}
\begin{exercise} \label{exercise:nonverticalintersections}
\begin{exparts}
\item
Suppose $D \subset \C^n$ is a polydisc, $0 \in D$,
$q = (0,1) \in \C^{n-1} \times \C$,
and $P$ is a Weierstrass polynomial of degree $k$
such that for a generic $z'$ (not in the discriminant set),
$z_n \mapsto P(z',z_n)$ has $k$ simple zeros.
Prove that there exists a ball $B \subset \C^{n-1}$ centered at the origin
and a dense open set $W \subset B$
such that
for every $\epsilon' \in W$, the function
$z_n \mapsto \bigl( (1-z_n) \epsilon' , z_n \bigr)$
has exactly $k$ geometrically distinct zeros.
Hint: Change coordinates near the origin to make all these lines vertical.
\item
Show that part a) proves the claim in the proof of theorem.
\end{exparts}
\end{exercise}

\begin{exercise} \label{exercise:morsesingularity}
\pagebreak[2]
\begin{exparts}
\item
Prove that the hypervariety in $\C^n$, $n \geq 2$, given by $z_1^2 + z_2^2 + \cdots + z_n^2 = 0$
has an isolated singularity at the origin (that is, the origin is the only
singular point).
\item
For every $0 \leq k \leq n-2$, find a hypervariety $X$ of $\C^n$ whose set
of singular points is a subvariety of dimension $k$.
\end{exparts}
\end{exercise}
\end{exbox}

\section{Irreducibility, local parametrization, and Puiseux}

\begin{defn}
A germ of a subvariety $(X,p) \subset (\C^n,p)$ is
\emph{\myindex{reducible}} at $p$ if there exist
two germs $(X_1,p)$ and $(X_2,p)$ with
$(X_1,p) \not\subset (X_2,p)$ and
$(X_2,p) \not\subset (X_1,p)$ such that
$(X,p) = (X_1,p) \cup (X_2,p)$.
Otherwise, the germ $(X,p)$ is \emph{\myindex{irreducible}} at $p$.

Similarly globally, a subvariety $X \subset U$ is
\emph{reducible} in $U$ if there exist
two subvarieties
$X_1$ and $X_2$ of $U$ with
$X_1 \not\subset X_2$ and
$X_2 \not\subset X_1$ such that
$X = X_1 \cup X_2$.
Otherwise, the subvariety $X$ is \emph{irreducible} in $U$.
\end{defn}

\begin{example}
\pagebreak[2]
Local and global reducibility are different.
The subvariety given by
\begin{equation*}
z_2^2 = z_1{(z_1-1)}^2
\end{equation*}
is irreducible in $\C^2$ (the regular points are connected), but locally
at the point
$(1,0)$ it is reducible.  There, the subvariety is
a union of two graphs: $z_2 = \pm \sqrt{z_1}(z_1-1)$.
See \figureref{fig:locallyredcurve} for a plot in two real dimensions.

\begin{myfig}
\medskip
\subimport*{figures/}{locallyredcurve.eepic}
\bigskip
\caption{Locally reducible curve.\label{fig:locallyredcurve}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Prove a germ of a subvariety $(X,p)$ is irreducible
if and only if $I_p(X)$ is a prime ideal.
Recall an ideal $I$ is \emph{prime}\index{prime ideal}
if $ab \in I$ implies either $a \in I$ or $b
\in I$.
\end{exercise}

\begin{exercise}
Suppose a germ of a subvariety $(X,p)$ is of pure codimension $1$.
Prove $(X,p)$ is irreducible if and only if there
exists a representative of $X$ where $X_{\textit{reg}}$
is connected.  %Hint: See the proof of \thmref{thm:codim1var}.
\end{exercise}

\begin{exercise}
Let $X \subset U$ be a subvariety of pure codimension $1$ of a domain $U
\subset \C^n$.
Prove $X$ is irreducible if and only if $X_{\mathit{reg}}$
is connected.  Hint: See previous exercise.
\end{exercise}
\end{exbox}

For complex subvarieties, a subvariety is irreducible if
and only if the set of regular points is connected.  We omit the proof
in the general case, and for hypervarieties it is an exercise above.
It then makes sense that we can split a subvariety into its irreducible parts.

\begin{prop}
Let $(X,p) \subset (\C^n,p)$ be a germ of a subvariety.  Then there exist
finitely many irreducible subvarieties $(X_1,p),\ldots,(X_k,p)$ such that
$(X_1,p) \cup \ldots \cup (X_k,p) = (X,p)$ and such that $(X_m,p)
\not\subset (X_\ell,p)$ for all $m$ and $\ell$.
\end{prop}

\begin{proof}
Suppose $(X,p)$ is reducible:
Find $(Y_1,p)$ and $(Y_1,p)$ such that $(Y_1,p) \not\subset (Y_2,p)$,
$(Y_2,p) \not\subset (Y_1,p)$, and
$(Y_1,p) \cup (Y_2,p) = (X,p)$.
As $(Y_\ell,p) \subsetneq (X,p)$, then
$I_p(Y_\ell) \supsetneq I_p(X)$ for both $\ell$.  If both
$(Y_1,p)$ and $(Y_2,p)$ are irreducible, then stop, we are done.  Otherwise
apply the same reasoning to whichever (or both) $(Y_\ell,p)$ that was
reducible.  After finitely many steps you must come to a stop as you cannot
have an infinite ascending chain of ideals since $\sO_p$ is Noetherian.
\end{proof}

These $(X_1,p),\ldots,(X_k,p)$ are the
\emph{\myindex{irreducible components}}.
We omit the proof in general
that they are unique.
For a germ of a hypervariety,
the UFD property of ${}_{n}\sO_p$ gives the irreducible
components.  You found this factorization in an exercise above,
and so this factorization is unique.

Each irreducible component has the following structure.
We give the theorem without proof in the general case, although we have
essentially proved it already for pure codimension $1$ (to put it
together is left as an exercise).

\begin{thm}[\myindex{Local parametrization theorem}]
\pagebreak[2]
\label{localparthm}
Let $(X,0)$ be an irreducible germ of a subvariety of dimension $k$
in $\C^n$.  Let $X$ denote a representative of the germ.
Then after a possible linear change of coordinates, letting
$\pi \colon \C^n \to \C^k$ be the projection onto the first $k$
components, there exists a neighborhood $U \subset \C^n$
of the origin, and a proper subvariety $E \subset \pi(U)$
(the \emph{\myindex{discriminant set}}) such that
\begin{enumerate}[(i)]
\item $X' = X \cap U \setminus \pi^{-1}(E)$ is a connected
$k$-dimensional complex manifold that is dense in $X \cap U$.
\item $\pi \colon X' \to \pi(U) \setminus E$ is an $m$-sheeted covering map
for some integer $m$.
\item $\pi \colon X \cap U \to \pi(U)$ is a proper mapping.
\end{enumerate}
\end{thm}

\pagebreak[2]
The $m$-sheeted covering map in this case is a local biholomorphism
that is an $m$-to-1 map.

\begin{exbox}
\begin{exercise}
Use \thmref{thm:discrthm}
to prove the parametrization theorem if $(X,0)$ is
of pure codimension $1$.
\end{exercise}
\end{exbox}

Let $(z_1,\ldots,z_n)$ be the coordinates.
The linear change of coordinates needed in the theorem is
to ensure that the set defined by $z_1=z_2=\cdots=z_k = 0$ intersected
with $X$ is an isolated point at the origin.  This is precisely
the same condition needed to apply Weierstrass preparation theorem in the case
when $X$ is the zero set of a single function.

\medskip

We saw that hypersurfaces are the simpler cases of
subvarieties.  At the other end of the spectrum,
subvarieties of dimension $1$ are also reasonably simple for different reasons.
Locally, subvarieties of dimension $1$ are analytic discs.
Moreover, these disccs can be chosen to be one-to-one,
and so such subvarieties have a natural topological manifold structure
even at singular points.

\begin{example}
The image of the holomorphic map
$\xi \mapsto (\xi^2,\xi^3)$ is the cusp subvariety
defined by $z_1^3-z_2^2 = 0$ in $\C^2$.
\end{example}


The following theorem is often stated only in $\C^2$ for zero sets of
a single function although it follows in
the same way from the local parametrization theorem in higher-dimensional
spaces.  Of course, we only
proved that theorem (or in fact you the reader did so in an exercise), for
codimension-$1$ subvarieties, and therefore, we also only have a complete
proof of the following in $\C^2$.

\begin{thm}[\myindex{Puiseux}]
Let $(z,w) \in \C \times \C^{n-1}$ be coordinates.
Suppose $f \colon U \subset \C \times \C^{n-1} \to \C^\ell$
is a holomorphic map such that
$f(z,w) = 0$ defines a one-dimensional subvariety $X$ of $U$,
$0 \in X$,
and $w \mapsto f(0,w)$ has an isolated zero at the origin.
Then there exists an integer $k$ and a holomorphic function $g$ defined near
the origin in $\C$ such that
for all $\xi$ near the origin
\begin{equation*}
f\bigl(\xi^k,g(\xi)\bigr) = 0 .
\end{equation*}
\end{thm}

\begin{proof}
Without loss of generality
assume $(X,0)$
is irreducible, so that
the local parametrization theorem applies.
We work in a small disc $D \subset \C$ centered at the origin, so that the
origin is the unique point of the discriminant set (the subvariety
$E$).  Let $N = \{ z \in D : \Im z = 0 , \Re z \leq 0 \}$.
As $D \setminus N$ is simply connected, we have the well-defined functions
$\alpha_1(z),\ldots,\alpha_m(z)$ holomorphic on $D \setminus N$
that are solutions to $f\bigl(z,\alpha_j(z)\bigr) = 0$.
These functions continue analytically across $N$ away from the
origin.  The continuation equals one of the zeros, e.g.\ $\alpha_j(z)$
becomes $\alpha_\ell(z)$ (and by continuity it is the
same zero along the entire $N$).  So there is
a permutation $\sigma$ on $m$ elements such that as $z$ moves
counter-clockwise around the origin from the upper half-plane across $N$ to the
lower half-plane,
$\alpha_j(z)$ is continued as $\alpha_{\sigma(j)}(z)$.
There exists some number $k$ (e.g.\ $k=m!$) such that $\sigma^k$ is the identity.
As $\xi$ goes around
a circle around the origin, $\xi^k$ goes around the origin $k$ times.
Start at a positive real $\xi$ and start defining a
function $g(\xi)$ as
$\alpha_1(\xi^k)$.
Move $\xi$ around the origin counter-clockwise continuing $g$ analytically.
Divide the disc into sectors of angle $\nicefrac{2\pi}{k}$,
whose boundaries are where $\xi^k \in N$.
Transition to $\alpha_{\sigma(1)}(\xi^k)$ after we reach the boundary
of the first sector, then to
$\alpha_{\sigma(\sigma(1))}(\xi^k)$ after we reach the boundary of the next sector, and so on.
After $k$ steps, that is, as
$\xi$ moved all the way around the circle,
we are back at $\alpha_1(\xi^k)$,
because
$\sigma^k$ is the identity.
So $g(\xi)$ is a well-defined holomorphic function outside the origin.  Let
$g(0) = 0$, and $g$ is holomorphic at $0$ by the Riemann extension theorem.
See \figureref{fig:puiseux} for an example.
\begin{myfig}
\subimport*{figures/}{puiseux.pdf_t}
\caption{Proving Puiseux with $m = k = 4$.  The permutation $\sigma$ takes
$1$ to $2$, $2$ to $3$, $3$ to $4$,
and $4$ to $1$.  As $\xi$ moves along the short circular arrow on the right, $\xi^4$
moves along the long circular arrow on the left.  The definition of $g$ is
given in the right-hand diagram.\label{fig:puiseux}}
\end{myfig}
\end{proof}

\begin{exbox}
\begin{exercise}
\pagebreak[2]
Consider an irreducible germ
$(X,0) \subset (\C^2,0)$ defined
by an irreducible Weierstrass polynomial $f(z,w) = 0$ (polynomial in $w$)
of degree $k$.  Prove there exists a holomorphic $g$ such that
$f\bigl(z^k,g(z)\bigr) = 0$ and $z \mapsto \bigl(z^k,g(z)\bigr)$
is one-to-one and onto a neighborhood of $0$ in $X$.
\end{exercise}

\begin{exercise}
Suppose $(X,0) \subset (\C^2,0)$ is a germ of a one-dimensional subvariety.
Show that after a possible linear change of coordinates,
there are natural numbers
$d_1,\ldots,d_k$
and
holomorphic functions $c_1(z),\ldots,c_k(z)$ vanishing at $0$,
such that $X$ is given near $0$ by
\begin{equation*}
\prod_{\ell=1}^k {\bigl( w^{d_\ell} - c_\ell(z) \bigr)} = 0.
\end{equation*}
\end{exercise}

\begin{exercise}
\pagebreak[2]
Using the local parametrization theorem, prove that
if $(X,p)$ is an irreducible germ of a subvariety of dimension greater
than $1$, then there exists a neighborhood $U$ of $p$ and a closed subvariety
$X \subset U$ (whose germ at $p$ is $(X,p)$), such that for every
$q \in X$ there exists an irreducible subvariety $Y \subset X$
of dimension $1$ such that $p \in Y$ and $q \in Y$.
\end{exercise}

\begin{exercise}
\pagebreak[2]
Prove a stronger version of the exercise above.  Show that not only is there
a $Y$, but an analytic disc $\varphi \colon \D \to U$ such that
$\varphi(\D) \subset X$, $\varphi(0) = p$ and $\varphi(\nicefrac{1}{2}) =
q$.
\end{exercise}

\begin{exercise}
Suppose $X \subset U$ is a subvariety of a domain $U \subset \C^n$.
Show that $X$ is irreducble
if and only if
for every pair of points $p,q \in X$ there exists a finite sequence
of points $p_0 = p, p_1, \ldots, p_k = q$ in $X$, and a finite sequence of analytic discs
$\Delta_\ell \subset X$ such that $p_{\ell}$ and $p_{\ell-1}$ are in
$\Delta_\ell$.
\end{exercise}

\begin{exercise}\label{exercise:maxprincsubvar}
\index{maximum principle!subvarieties}
Prove a \emph{maximum principle for subvarieties} using the exercises above:
Suppose $X \subset U$ is an irreducible subvariety of an open set $U$,
and suppose $f \colon U \to \R \cup \{ - \infty \}$
is a plurisubharmonic function.  If the modulus of the restriction $f|_X$
achieves a maximum
at some point $p \in X$, then the restriction $f|_X$ is constant.
\end{exercise}

\begin{exercise}
Prove that an analytic disc (namely the image of the disc) in $\C^2$
is a one-dimensional local variety (that is, a subvariety of some
open subset of $\C^2$).
\end{exercise}
\end{exbox}

Using the Puiseux theorem, we often simply parametrize germs
of complex one-dimensional subvarieties.
And for larger-dimensional varieties, we can find
enough one-dimensional curves through any point and parametrize those.

It is not true that every irreducible subvariety is locally
an injective image of a piece of
$\C^k$ via a holomorphic map, but it is a very deep theorem,
the resolution of singularities, that says you can do so if you allow some points
where the function is not one-to-one.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Segre varieties and CR geometry} \label{sec:crgeomcr}

The existence of analytic discs (or subvarieties)
in boundaries of domains says a lot about the geometry of the boundary.

\begin{example}
Let $M \subset \C^n$ be a smooth real hypersurface containing
a complex hypersurface $X$ (zero set of a holomorphic function
with nonzero derivative), at $p \in X \subset M$.
Apply a local biholomorphic change of coordinates at $p$, so
that in the new coordinates
$(z,w) \in \C^{n-1} \times \C$,
$X$ is given by $w=0$, and $p$ is the origin.
The tangent hyperplane to $M$ at $0$ contains $\{ w=0 \}$.
By rotating the $w$ coordinate (multiplying it by $e^{i\theta}$),
we assume $M$ is tangent to the set $\bigl\{ (z,w) : \Im w = 0
\bigr\}$.
In other words,
$M$ is given by
\begin{equation*}
\Im w = \rho(z,\bar{z},\Re w) ,
\end{equation*}
where $d\rho = 0$.
As $w = 0$ on $M$, then $\rho = 0$
when $\Re w = 0$. That is, $\rho$
is divisible by $\Re w$.  So $M$ is defined by
\begin{equation*}
\Im w = (\Re w) \widetilde{\rho}(z,\bar{z},\Re w),
\end{equation*}
for a smooth function $\widetilde{\rho}$.
The Levi form at the origin vanishes.
As $p=0$ was an arbitrary point on $M \cap X$,
the Levi form of $M$ vanishes on $M \cap X$.
\end{example}

\begin{example}
The vanishing of the Levi form is not necessary if the complex varieties in
$M$ are smaller.  Consider $M\subset \C^3$ with a nondegenerate (but not definite)
Levi form:
\begin{equation*}
\Im w = \abs{z_1}^2-\abs{z_2}^2 .
\end{equation*}
For every $\theta \in \R$,
$M$ contains the complex line $L_\theta$,
given by $z_1 = e^{i\theta} z_2$ and
$w = 0$.  The union $\bigcup_\theta L_\theta$ of those
complex lines is not
contained in some single unique complex subvariety inside $M$.  Any complex
subvariety that contains all the lines $L_\theta$ must contain the entire complex hypersurface given
by $w = 0$, which is not contained in $M$.
See \figureref{fig:hyperquadgraph}.

\begin{myfig}
\includegraphics[width=0.4\textwidth]{figures/hyperquadgraph.pdf}
\caption{A trace of the hypersurface in the $(\Re z_1,\Re z_2,\Im w)$
space.  The traces of the two complex lines $L_0$ and $L_{\pi}$ in the plane
$w=0$ corresponding to 
$z_1=z_2$ and $z_1=-z_2$ are visible.\label{fig:hyperquadgraph}}
\end{myfig}
\end{example}

\begin{exbox}
\begin{exercise}
Let $M \subset \C^n$ be a smooth real hypersurface.
Show that if $M$ at $p$ contains a complex submanifold of (complex)
dimension more than
$\frac{n-1}{2}$, then the Levi form must be degenerate, that is, it must
have at least one zero eigenvalue.
\end{exercise}

\begin{exercise}
Let $M \subset \C^n$ be a smooth pseudoconvex real hypersurface
(one side of $M$ is pseudoconvex).
Suppose $M$ at $p$ contains a dimension $k$ complex submanifold $X$.
Show that the Levi form has at least $k$ zero eigenvalues.
\end{exercise}

\begin{exercise}
Find an example of a smooth real hypersurface $M \subset \C^n$ that contains a
germ of a singular complex-analytic subvariety $(X,p)$ through a point $p$,
which is unique in the sense that if $(Y,p)$ is another germ of a complex
analytic subvariety in $M$, then $(Y,p) \subset (X,p)$.
\end{exercise}
\end{exbox}

Let us discuss a tool, the \emph{\myindex{Segre variety}}, that allows us to
find such complex subvarieties inside $M$, and much more.  Segre varieties only
work in the real-analytic setting and rely on complexification.

Let $M \subset \C^n$ be a real-analytic hypersurface and $p \in M$.
Suppose $M \subset U$,
where $U \subset \C^n$ is a small domain with a defining function $r \colon
U  \to \R$ for $M$.  That is, $r$ is a real-analytic function in $U$ such that
$M = r^{-1}(0)$, but
$dr \not= 0$ on $M$.  Define
\glsadd{not:Ustar}%
\begin{equation*}
U^* = \bigl\{ z \in \C^n : \bar{z} \in U \bigr\} .
\end{equation*}
Suppose $U$ is small enough so that the Taylor series for $r$
converges in $U \times U^*$ when treating $z$ and $\bar{z}$ as separate
variables.  That is, $r(z,\zeta)$ is a well-defined function on
$U \times U^*$, and $r(z,\zeta) = 0$ defines a complexification $\sM$
in $U \times U^*$.  Assume also that $U$ is small enough that
the derivative $dr$ of the complexified $r$ does not vanish on
$\sM$ and that $\sM$ is connected.
See also \propref{prop:complexificationofrasurface}.

Given $q \in U$,
define the \emph{Segre variety}
\glsadd{not:Segrevar}%
\begin{equation*}
\Sigma_q(U,r) =
\bigl\{ z \in U : r(z,\bar{q}) = 0 \bigr\} =
\bigl\{ z \in U : (z,\bar{q}) \in \sM \bigr\} .
\end{equation*}
See a diagram in \figureref{fig:segrefig}.
A priory, the subvariety $\Sigma_p$ depends on $U$ and $r$.
However, if $\widetilde{r}$ is a
real-analytic function that complexifies to $U \times U^*$
and vanishes on $M$, it must also vanish on the complexification $\sM$.
If $\widetilde{r}$ is a defining function as above,
that is, $d\widetilde{r}$ does not vanish on its zero set
and the zero set of the complexified $\widetilde{r}$ is connected
in $U \times U^*$, then $\widetilde{r}(z,\zeta) = 0$ also
defines $\sM$.
Hence the actual $r$ does not matter.
As long as $q \in M$, then
$q \in \Sigma_q(U,r)$, and furthermore the
Segre variety is a complex hypersurface for every $q$.
It is not hard to see that
if $\widetilde{U}$ is a small neighborhood of $q$, the same $r$ is
a defining function in
$\widetilde{U}$, and we
get the same complexification in $\widetilde{U} \times \widetilde{U}^*$.
So the germ at $q \in U$ is well-defined, and we write
\begin{equation*}
\Sigma_q = \bigl( \Sigma_q(U,r) , q \bigr) .
\end{equation*}
The Segre variety is
well-defined as a germ, and so often when one talks about $\Sigma_q$
without mentioning the $U$ or $r$, then one means some small enough
representative of a Segre variety or the germ itself.

\begin{myfig}
\medskip
\subimport*{figures/}{segrefig.pdf_t}
\caption{Diagram of $\Sigma_q(U,r)$.
The ``$M$'' is in quotation marks as it is really in the $z$ space not in
the diagonal, but we identify it with a subset of the diagonal in this
picture.\label{fig:segrefig}}
\end{myfig}

\begin{exbox}
\begin{exercise}
Let $r \colon U \to \R$ be a real-valued
real-analytic function that complexifies to
$U \times U^*$.  Show that
$r(z,\bar{\zeta}) = 0$
if and only if
$r(w,\bar{\zeta}) = 0$.  In other words,
$z \in \Sigma_{\zeta}(U,r)$ if and only if
$\zeta \in \Sigma_z(U,r)$.
\end{exercise}
\end{exbox}

\begin{example}
Consider a real-analytic hypersurface $M$ given by
\begin{equation*}
\Im w = (\Re w) \rho(z,\bar{z},\Re w) ,
\end{equation*}
with $\rho$ vanishing at the origin.
The hypersurface given by $w=0$ lies in $M$.
Rewrite in terms of $w$ and $\bar{w}$,
\begin{equation*}
\frac{w-\bar{w}}{2i} = \left(\frac{w+\bar{w}}{2}\right)
\rho\left(z,\bar{z},\frac{w+\bar{w}}{2}\right) .
\end{equation*}
Set $\bar{z} = \bar{w} = 0$ to find
\begin{equation*}
\frac{w}{2i} = \left(\frac{w}{2}\right)
\rho\left(z,0,\frac{w}{2}\right) .
\end{equation*}
As $\rho$ vanishes at the origin, then near the origin the equation
defines the complex hypersurface given by $w=0$,
that is, $\Sigma_0$ is defined by $w = 0$.
So $\Sigma_0$ is precisely the complex hypersurface that lies inside $M$.
\end{example}

The last example is not a fluke.
The most important property of Segre varieties is that it locates complex
subvarieties in a real-analytic submanifold.
We will phrase it in terms of analytic discs, which is
enough as complex subvarieties can be filled with analytic discs,
as we have seen.

\begin{prop}
Let $M \subset \C^n$ be a real-analytic hypersurface and $p \in M$.
Suppose $\Delta \subset M$ is an analytic disc
through $p$.  Then as germs $(\Delta,p) \subset \Sigma_p$.
\end{prop}

\begin{proof}
Let $U$ be a neighborhood of $p$ where a representative
of $\Sigma_p$ is defined, that is, assume that $\Sigma_p$ is
a closed subset of $U$, and suppose $r(z,\bar{z})$ is the corresponding
defining function.
Let $\varphi \colon \D \to \C^n$ be the parametrization of $\Delta$
with $\varphi(0) = p$.  We can restrict $\varphi$ to a smaller disc around the
origin, and since we are only interested in the germ of $\Delta$ at $p$ this
is sufficient (if there are multiple points of $\D$
that go to $p$, we repeat the argument for each one).
So let us assume without loss of generality that $\varphi(\D) = \Delta \subset U$.
Since $\Delta \subset M$, we have
\begin{equation*}
r\bigl(\varphi(\xi),\overline{\varphi(\xi)}\bigr) =
r\bigl(\varphi(\xi),\bar{\varphi}(\bar{\xi})\bigr) = 0 .
\end{equation*}
The function $\xi \mapsto
r\bigl(\varphi(\xi),\bar{\varphi}(\bar{\xi})\bigr)$ is a real-analytic
function of $\xi$, and therefore for some
small neighborhood of the origin, it complexifies.  In fact, it complexifies
to $\D \times \D$ as $\varphi(\xi) \in U$ for all $\xi \in \D$.
So we can treat $\xi$ and $\bar{\xi}$ as separate variables.  By
complexification, the equation holds for all such independent
$\xi$ and $\bar{\xi}$.  Set $\bar{\xi} = 0$ to obtain
\begin{equation*}
0 =
r\bigl(\varphi(\xi),\bar{\varphi}(0)\bigr) =
r\bigl(\varphi(\xi),\bar{p}\bigr)
\qquad \text{for all $\xi \in \D$}.
\avoidbreak
\end{equation*}
In particular, $\varphi(\D) \subset \Sigma_p$ and the result follows.
\end{proof}

\begin{exbox}
\begin{exercise}
Show that if a real-analytic real hypersurface $M \subset \C^n$ is strongly
pseudoconvex at $p \in M$ (one side of $M$ is strongly pseudoconvex at $p$),
then $\Sigma_p \cap
(M,p) = \{p\}$ (as germs).
\end{exercise}

\begin{exercise}
Use the proposition and the exercise above to show that if a real-analytic
real hypersurface $M$ is strongly
pseudoconvex, then $M$ contains no analytic discs.
\end{exercise}
\end{exbox}

We end our discussion of Segre varieties by its perhaps most well-known
application, the so-called Diederich--Forn\ae ss lemma.  Although
we state and prove it only for real-analytic hypersurfaces it works in greater generality.
There are two parts to it, although it is generally the corollary
that is called the \emph{\myindex{Diederich--Forn\ae ss lemma}}.

First, for real-analytic hypersurfaces each point has a fixed neighborhood
such that germs of complex subvarieties contained in the hypersurface extend
to said fixed neighborhood.

\begin{thm}[Diederich--Forn\ae ss]
\pagebreak[2]
Suppose $M \subset \C^n$ is a real-analytic hypersurface.  For every
$p \in M$ there exists a neighborhood $U$ of $p$ with the following
property:
If $q \in M \cap U$ and
$(X,q)$ is a germ of a complex subvariety
such that $(X,q) \subset (M,q)$,
then there exists a complex subvariety $Y \subset U$ (in
particular a closed subset of $U$) such that $Y \subset M$ and $(X,q)
\subset (Y,q)$.
\end{thm}

\begin{proof}
Suppose $U$ is a polydisc centered at $p$,
small enough so that the defining function
$r$ of $M$ complexifies to $U \times U^*$ as above.
Suppose $q \in M \cap U$ is a point such that $(X,q)$ is a germ of a
positive-dimensional complex subvariety with $(X,q) \subset (M,q)$.
Most points of a subvariety are regular, so
without loss of generality assume $q$ is a regular point, that is,
$(X,q)$ is a germ of a complex submanifold.
Let $X$ be a representative of the germ $(X,q)$ such that $X \subset M$,
and $X \subset U$, although we do not assume it is closed.

Assume $X$ is an
image of an open subset $V \subset \C^k$ via a holomorphic surjective mapping $\varphi \colon V \to
X$.  Since $r\bigl(\varphi(\xi),\overline{\varphi(\xi)}\bigr) = 0$
for all $\xi \in V$, then we may treat $\xi$ and $\bar{\xi}$ separately.
In particular,
$r(z,\bar{\zeta}) = 0$ for all $z,\zeta \in X$.

Define complex subvarieties $Y', Y \subset U$ (closed in $U$) by
\begin{equation*}
Y' = \bigcap_{a \in X} \Sigma_a(U,r)
\qquad \text{and} \qquad
Y = \bigcap_{a \in Y'} \Sigma_a(U,r) .
\end{equation*}
If $a \in Y'$ and $b \in X$, then $r(a,\bar{b}) = 0$.
Because $r$ is real-valued,
$r(b,\bar{a}) = 0$.  Therefore,
$X \subset Y \subset Y'$.  Furthermore, $r(z,\bar{z}) = 0$
for all $z \in Y$, and so $Y \subset M$.
\end{proof}

\begin{cor}[Diederich--Forn\ae ss]
Suppose $M \subset \C^n$ is a compact real-analytic hypersurface.
Then there does not exist any point $q \in M$ such that
there exists a germ of a positive-dimensional complex subvariety
$(X,q)$ such that $(X,q) \subset (M,q)$.
\end{cor}

\begin{proof}
Let $S \subset M$ be the set of points through which there exists
a germ of a positive-dimensional complex subvariety contained in $M$.
As $M$, and hence $\widebar{S}$, is compact,
there must exist a point $p \in \widebar{S}$
that is furthest from
the origin.  After a rotation by a unitary and rescaling assume
$p=(1,0,\ldots,0)$.  Let $U$ be the neighborhood from the previous
theorem around $p$.  There exist germs of varieties in $M$ through points
arbitrarily close to $p$.  So for any distance $\epsilon > 0$,
there exists a subvariety $Y \subset U$ (in particular, $Y$ closed in $U$)
of positive dimension with $Y \subset M$ that contains points
$\epsilon$ close to $p$.  Consider the function $\Re z_1$, whose modulus attains a
strict maximum on $\widebar{S}$ at $p$.  Because $\Re z_1$ achieves a maximum
strictly smaller than $1$ on $\partial U \cap \widebar{S}$, for a small enough $\epsilon$,
we would obtain a pluriharmonic function with a strict
maximum on $Y$, which is impossible by the maximum principle for
varieties that you proved in \exerciseref{exercise:maxprincsubvar}.
The picture would look as in \figureref{fig:forndied}.
\begin{myfig}
\medskip
\subimport*{figures/}{forndied.pdf_t}
\caption{Contradicting the maximum principle at $p$.\label{fig:forndied}}
\end{myfig}
\end{proof}

\begin{example}
The results above do not work
in the smooth setting.  Let us disprove the theorem in the smooth
setting.  Disproving the corollary is an exercise.
Let $g \colon \R \to \R$ be a smooth function that is
strictly positive for $\sabs{t} > 1$, and $g(t) = 0$ for all $\sabs{t} \leq 1$.
Define $M$ in $(z,w) \in \C^{n-1} \times \C$ by
\begin{equation*}
\Im w = g\bigl(\snorm{z}^2 + (\Re w)^2\bigr) .
\end{equation*}
The $M$ is a smooth real hypersurface.
Consider $p = (0,\ldots,0,1) \in M$.  For every $0 < s < 1$, let
$q_s = (0,\ldots,0,s) \in M$ and $X_s = \bigl\{ (z,w) \in M :
w = s \bigr\}$.  Each $X_s$ is the closure of a local complex subvariety of dimension $n-1$
and $(X_s,q_s) \subset (M,q_s)$.  The size (diameter) of $X_s$ goes to
zero as $s \to 1$ and $X_s$ cannot extend to a
larger complex subvariety inside $M$.  So, no neighborhood $U$ at
$p$ (as in the theorem) exists.
\end{example}

\begin{exbox}
\begin{exercise}
Find a compact smooth real hypersurface $M \subset \C^n$ that contains a germ
of a positive dimensional complex subvariety.
\end{exercise}
\end{exbox}

\vspace{1in}

\ldots and that is how using sheep's bladders can prevent
earthquakes!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% No sections in this appendix
\counterwithin{thm}{chapter}
\chapter{Basic Notation and Terminology} \label{ap:basicnotation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Is this overly too nitpicky to include?

We quickly review some basic notation used in this
book that is perhaps not described elsewhere.
We use $\C$, $\R$ for complex and real numbers, and $i$ for imaginary unit
(a square root of $-1$).  We use
$\N = \{ 1,2,3, \ldots \}$ for
the natural numbers,
$\N_0 = \{ 0,1,2,3, \ldots \}$ for the zero-based natural numbers,
and $\Z$ for all integers.
When we write $\C^n$ or $\R^n$ we implicitly mean that $n \geq 1$, unless
otherwise stated.

\glsadd{not:setminus}%
We denote set subtraction by $A \setminus B$, meaning all elements of
$A$ that are not in $B$.
We denote complement of a set by $X^c$.  The ambient set
should be clear.  So, for example, if $X \subset \C$ naturally,
then $X^c = \C \setminus X$.
\glsadd{not:closure}%
Topological closure of a set $S$ is denoted by $\widebar{S}$, its
boundary is denoted by
\glsadd{not:boundary}%
$\partial S$.  If $S$ is a relatively compact subset of $X$
(its closure in $X$ is compact) or compact, we write $S \subset \subset X$.

\glsadd{not:function}%
A function with domain $X$ and codomain $Y$ we denote by
$f \colon X \to Y$.  The direct image of $S$ by if is $f(S)$.
The notation $f^{-1}$ for the inverse image of sets and
single points.  When $f$ is bijective (one-to-one and onto),
we use $f^{-1}$ for the inverse mapping.  So $f^{-1}(T)$ for
a set $T \subset Y$ denotes the points of $X$ that $f$ maps to $T$.
For a point $q$, $f^{-1}(q)$ denotes the points that map to $q$,
but if the mapping is bijective, then it means the unique point
mapping to $q$.
To define a function without giving it a name, we use
\glsadd{not:mapsto}%
\begin{equation*}
x \mapsto F(x),
\end{equation*}
where $F(x)$ would generally be some formula giving the output.
The notation
\glsadd{not:restriction}%
$f|_S$
is the restriction of $f$ to $S$:
a function
$f|_S \colon S \to Y$ such that $f|_S(x) = f(x)$ for all $x \in S$.
A function $f \colon U \to \C$ is
\emph{\myindex{compactly supported}} if
the \emph{\myindex{support}}, that is the set
$\overline{\{ p \in U : f(p) \not= 0 \}}$, is compact.
If $f(x) = g(x)$ for all $x$ in the domain,
we write
\glsadd{not:identeq}%
\begin{equation*}
f \equiv g ,
\end{equation*}
and we say that $f$ and $g$ are identically equal.
The notation
\glsadd{not:composition}%
\begin{equation*}
f \circ g
\end{equation*}
denotes the composition defined by $x \mapsto
f\bigl(g(x)\bigr)$.

To define $X$ to be $Y$ rather than just show equality, we write
\glsadd{not:definition}%
\begin{equation*}
X
\overset{\text{def}}{=}
Y .
\end{equation*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% No sections in this appendix
\counterwithin{thm}{chapter}
\chapter{Results from One Complex Variable} \label{ap:onevarresults}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We review some results from one complex variable useful for
reading this book.
The reader should first look through \sectionref{sec:motivation}
for basic notation and motivation, although we review some of the
results again here.
Let $U \subset \C$ be open.
A function
$f \colon U \to \C$ is \emph{holomorphic} if it is complex
differentiable at every point, that is,
\begin{equation*}
f'(z) = \lim_{h \in \C \to 0} \frac{f(z+h) - f(z)}{h}
\end{equation*}
exists for all $z \in U$.  For example, polynomials and rational
functions in $z$ are holomorphic.  Perhaps the most important holomorphic
function is the solution to the differential equation $f'(z) = f(z)$, $f(0) = 1$,
the complex exponential,
\begin{equation*}
f(z) = e^z = e^{x+iy} = e^x \bigl( \cos y + i \sin(y) \bigr) .
\end{equation*}


A piecewise-$C^1$ \emph{path} (or curve)\index{path}\index{curve} in $\C$
is a continuous $\gamma \colon [a,b] \to \C$,
continuously differentiable except at finitely many points,
such that one-sided limits of $\gamma'(t)$ exist at all $t \in [a,b]$
and such that $\gamma'$ (or its one-sided limits) is never zero.
By abuse of notation, when $\gamma$ is used as a set, we mean
the image
$\gamma\bigl([a,b]\bigr)$.
For a continuous $f \colon \gamma \to \C$, define
\glsadd{not:pathint}%
\begin{equation*}
\int_\gamma f(z) \, dz
\overset{\text{def}}{=}
\int_a^b f\bigl(\gamma(t)\bigr) \gamma'(t) \, dt .
\end{equation*}
As $\gamma'$ is continuous at all but finitely many points,
the integral is well-defined.  Similarly, one defines the more general
path integral in $dz = dx + i\,dy$ and $d\bar{z} = dx - i\, dy$.
Let
$z = \gamma(t) = \gamma_1(t) + i \, \gamma_2(t) = x + i\, y$
parametrize the path.  Then
\begin{equation*}
\begin{split}
\int_\gamma f(z) \, dz + g(z) \, d\bar{z}
& =
\int_\gamma
\Bigl(f\bigl(x+i\, y\bigr) + g\bigl(x+i\, y\bigr) \Bigr) \, dx
+
i \, \Bigl( f\bigl(x+i\, y\bigr) - g\bigl(x+i\, y\bigr) \Bigr) \, dy
\\
& =
\int_a^b
\Bigl(
f\bigl(\gamma(t)\bigr) \gamma'(t)
+
f\bigl(\gamma(t)\bigr) \overline{\gamma'(t)}
\Bigr) \, dt
\\
& =
\int_a^b
\biggl(
\Bigl(f\bigl(\gamma(t)\bigr) + g\bigl(\gamma(t)\bigr) \Bigr) \,
\gamma_1'(t)
+
i \, \Bigl( f\bigl(\gamma(t)\bigr) - g\bigl(\gamma(t)\bigr) \Bigr)
\gamma_2'(t)
\biggr)
\, dt .
\end{split}
\end{equation*}

A path is \emph{closed}\index{closed path}
if $\gamma(a) = \gamma(b)$,
and a path is \emph{simple}\index{simple path}
if $\gamma|_{(a,b]}$ is one-to-one
with the possible exception of $\gamma(a) = \gamma(b)$.

An open $U \subset \C$ has \emph{\myindex{piecewise-$C^1$ boundary}} if
for each $p \in \partial U$ there is an open neighborhood $W$ of $p$
such that
$\partial U \cap W = \gamma\bigl((a,b)\bigr)$
where $\gamma \colon [a,b] \to \C$ is an injective piecewise-$C^1$ path,
and such that each $p \in \partial U$ is in the closure of $\C \setminus
\widebar{U}$.  Intuitively, the boundary is locally a piecewise-$C^1$ curve that
locally cuts the plane into two open pieces.
If at each point where the parametrization of $\partial U$ is differentiable
the domain is on the left ($\gamma'(t)$ rotated by $\frac{\pi}{2}$ points into the
domain), then the boundary is \emph{\myindex{positively oriented}}.
As in the introduction, we have the following version of Cauchy integral formula.

\begin{thm}[Cauchy integral formula]
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, and let
$f \colon \widebar{U} \to \C$ be a continuous function
holomorphic in $U$.
Then for $z \in U$,
\begin{equation*}
f(z) =
\frac{1}{2\pi i}
\int_{\partial U}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta .
\end{equation*}
\end{thm}

Usually the theorem is stated with
\emph{winding numbers}\index{winding number}.
The winding number is the number of times a closed path $\gamma$ ``goes
around'' a point $p \not\in \gamma$.
More precisely it is defined by
\begin{equation*}
n(\gamma;p)
\overset{\text{def}}{=}
\frac{1}{2\pi i} \int_{\gamma} \frac{1}{z-p} \, dz .
\end{equation*}
It is easy to show that $n(\gamma;p)$ is always an integer and
it is constant on the components of $\C \setminus \gamma$.
It is also defined on \emph{cycles}\index{cycle}, which are just
formal sums of closed paths
$\Gamma = \gamma_1 + \gamma_2 + \cdots + \gamma_n$,
by simply summing the corresponding integrals.

A common statement of the Cauchy integral formula with
winding numbers is that if
$U$ is open, $f \colon U \to \C$ holomorphic, and $\gamma$ is
a closed piecewise $C^1$ path (or cycle) in $U$, such that
$n(\gamma;p) = 0$ for all $p \not\in U$, then
\begin{equation*}
n(\gamma;z) f(z) =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{\zeta-z}
\,
d \zeta .
\end{equation*}

By the \emph{\myindex{Jordan curve theorem}}, a simple closed path
divides the plane into two components, one bounded and one unbounded.
The bounded component is called the \emph{\myindex{interior}} of $\gamma$,
and the unbounded component, called the \emph{\myindex{exterior}}.
It can be shown that for a piecewise-$C^1$ path $\gamma$,
$n(\gamma;p) = \pm 1$ for $p$ in the interior of
$\gamma$ and $n(\gamma;p) = 0$ for $p$ in the exterior of $\gamma$.
We say $\gamma$ is oriented positively if $n(\gamma;p) = 1$ on the interior.

More generally, if $U$ is a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, then one can show that $\partial U$
is composed of finitely
many simple closed paths oriented in such a way that $n(\partial U;p) = 1$
for $p \in U$ and $n(\partial U;p) = 0$ for $p \in \C \setminus \widebar{U}$.


One way to get at the Cauchy integral formula is via Green's theorem,
which is the Stokes' theorem
in two dimensions.  In the versions we state, one needs to
approximate the open set by smaller open sets from the inside to insure
the partial derivatives are bounded.  See
\thmref{thm:generalizedcauchy}.  Let us state Green's theorem using
the $dz$ and $d\bar{z}$ for completeness.  See \appendixref{ap:diffforms}
for an overview of differential forms.

\begin{thm}[Green's theorem]\index{Green's theorem} \label{thm:greens}
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, and let
$f \colon \widebar{U} \to \C$ and 
$g \colon \widebar{U} \to \C$
be continuous
with bounded continuous partial derivatives in $U$.
Then
\begin{multline*}
\int_{\partial U} f(z) \, dz + g(z) \, d\bar{z}
=
\int_{U} d \Bigl( f(z) \, dz + g(z) \, d\bar{z} \Bigr)
=
\int_{U}
\left(
\frac{\partial g}{\partial z}
-
\frac{\partial f}{\partial \bar{z}}
\right)
\, dz \wedge d\bar{z}
\\
=
(-2i)
\int_{U}
\left(
\frac{\partial g}{\partial z}
-
\frac{\partial f}{\partial \bar{z}}
\right)
\, dx \wedge dy
=
(-2i)
\int_{U}
\left(
\frac{\partial g}{\partial z}
-
\frac{\partial f}{\partial \bar{z}}
\right)
\, dA.
\end{multline*}
\end{thm}

It is sufficient to prove the conclusion of Green's
with $g\equiv 0$, and it is sometimes stated that way.
The Cauchy integral formula is equivalent to
Cauchy's theorem:

\begin{thm}[Cauchy]
Let $U \subset \C$ be a bounded open set with piecewise-$C^1$ boundary
$\partial U$ oriented positively, and let
$f \colon \widebar{U} \to \C$ be a continuous function
holomorphic in $U$.  Then
\begin{equation*}
\int_{\partial U}
f(z) \, dz = 0 .
\end{equation*}
\end{thm}

Again, the alternative statement with winding numbers
 is that if $U$ is open, $f \colon U \to \C$ holomorphic, and $\gamma$ is
a closed piecewise $C^1$ path (or cycle) in $U$, such that
$n(\gamma;p) = 0$ for all $p \not\in U$, then
the integral of $f$ over $\gamma$ vanishes.

There is a converse to Cauchy.  A triangle $T \subset \C$ is
the convex hull of the three vertices (we include the inside of the
triangle), and $\partial T$ is the boundary of the triangle oriented
counter-clockwise.  We state the following theorem as an
``if and only if,'' even though, usually it is only the reverse direction that
is called Morera's theorem.

\begin{thm}[Morera] \label{thm:onevarmorera}
Suppose $U \subset \C$ is an open set, and $f \colon U \to \C$
is continuous.  Then $f$ is holomorphic
if and only if
\begin{equation*}
\int_{\partial T} f(z) \, dz = 0
\qquad
\text{for all triangles } T \subset U.
\end{equation*}
\end{thm}

As we saw in the introduction, a holomorphic function has a power series.

\begin{prop}
If $U \subset \C$ is open and $f \colon U \to \C$ is holomorphic,
then $f$ is infinitely differentiable, and if $\Delta_\rho(p) \subset \C$
is a disc, then $f$ has a power series that
converges absolutely uniformly on compact subsets of $\Delta_\rho(p)$:
\begin{equation*}
f(z) = \sum_{k=0}^\infty c_k {(z-p)}^k ,
\end{equation*}
where given a simple closed (piecewise-$C^1$) path $\gamma$
going once counter-clockwise
around $p$ inside $\Delta_\rho(p)$,
\begin{equation*}
c_k = \frac{f^{(k)}(p)}{k!} =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-z)}^{k+1}}
\,
d \zeta  .
\end{equation*}
\end{prop}

\emph{\myindex{Cauchy estimates}} follow:  If $M$
is the maximum of $\sabs{f}$ on the circle $\partial \Delta_r(p)$, then
\begin{equation*}
\sabs{c_k} \leq \frac{M}{r^k} .
\end{equation*}
Conversely, if a power series satisfies such estimates,
it converges on $\Delta_r(p)$.

A holomorphic $f \colon \C \to \C$ that is
\emph{\myindex{entire}}.  An immediate application of Cauchy estimates
is Liouville's theorem:

\begin{thm}[Liouville]\index{Liouville's theorem}
If $f$ is entire and bounded, then $f$ is constant.
\end{thm}

And as a holomorphic function has a power series it satisfies the
identity theorem:

\begin{thm}[Identity]\index{identity theorem} \label{thm:onevaridentity}
Suppose $U \subset \C$ is a domain and $f \colon U \to \C$ is holomorphic.
If the zero set $f^{-1}(0)$ has a limit point in $U$, then
$f \equiv 0$.
\end{thm}

Another consequence  of the Cauchy integral formula is that there is
a differential equation characterizing holomorphic functions.

\begin{prop}[Cauchy--Riemann equations]
Let $U \subset \C$ be open.
A function $f \colon U \to \C$ is holomorphic if and only if
$f$ is continuously differentiable and
\begin{equation*}
\frac{\partial f}{\partial \bar{z}}
=
\frac{1}{2}
\left(
\frac{\partial f}{\partial x} + i
\frac{\partial f}{\partial y}
\right)
 = 0 \qquad \text{on $U$.}
\end{equation*}
\end{prop}

Yet another consequence of the Cauchy formula (and one can make an argument
that everything in this appendix is a consequence of the Cauchy formula)
is the open mapping theorem.

\begin{thm}[Open mapping theorem]\index{open mapping theorem}
Suppose $U \subset \C$ is a domain and
$f \colon U \to \C$ is holomorphic and not constant.
Then $f$ is an open mapping, that is,
$f(V)$ is open whenever $V$ is open.
\end{thm}

The real and imaginary parts $u$ and $v$ of a holomorphic function $f =
u+iv$ are harmonic, that is
$\nabla^2 u = \nabla^2 v = 0$, where $\nabla^2$ is the
Laplacian.
A domain $U$ is \emph{\myindex{simply connected}} if every simple closed
path
is homotopic in $U$ to a constant, in other words, if the domain has no
holes.  For example a disc is simply connected.

\begin{prop}
If $U \subset \C$ is a simply connected domain and $u \colon U \to \R$
is harmonic, then there exists a harmonic function $v \colon U \to \R$
such that $f = u+iv$ is holomorphic.
\end{prop}

The function $v$ is called the \emph{\myindex{harmonic conjugate}} of
$u$.  For further review of harmonic functions see
\sectionref{sec:harmonic} on harmonic functions.
We have the following versions of the maximum principle.

\begin{thm}[Maximum principles]\index{maximum principle}
Suppose $U \subset \C$ is a domain.
\begin{enumerate}[(i)]
\item
If $f \colon U \to \C$ is holomorphic and $\sabs{f}$
achieves a local maximum in $U$, then $f$ is constant.
\item
If $U$ is bounded and $f \colon \widebar{U} \to \C$ is holomorphic in $U$
and continuous, then $\sabs{f}$ achieves its maximum on $\partial U$.
\item
If $f \colon U \to \R$ is harmonic
achieves a local maximum or a minimum in $U$, then $f$ is constant.
\item
If $U$ is bounded and $f \colon \widebar{U} \to \R$ is harmonic in $U$
and continuous, then $f$ achieves its maximum and minimum on $\partial U$.
\end{enumerate}
\end{thm}

The first two items are sometimes called the \emph{\myindex{maximum modulus principle}}.
The maximum principle immediately implies the following lemma.

\begin{lemma}[Schwarz's lemma]\index{Schwarz's lemma}
\pagebreak[2]
Suppose $f \colon \D \to \D$ is holomorphic and $f(0) = 0$, then
\begin{enumerate}[(i)]
\item $\sabs{f(z)} \leq \sabs{z}$, and
\item $\sabs{f'(0)} \leq 1$.
\end{enumerate}
Furthermore, if $\sabs{f(z_0)} = \sabs{z_0}$ for some $z_0 \in \D \setminus
\{ 0 \}$
or $\sabs{f'(0)} = 1$, then
for some $\theta \in \R$ we have $f(z) =
e^{i\theta} z$ for all $z \in \D$.
\end{lemma}

The theorem above is actually quite general.

\begin{thm}[Riemann mapping theorem]\index{Riemann mapping theorem}
If $U \subset \C$ is a nonempty simply connected domain such that $U \neq \C$,
then $U$ is biholomorphic to $\D$.  Given $z_0 \in U$
there exists a unique biholomorphic $f \colon U \to \D$
such that $f(z_0) = 0$, $f'(z_0) > 0$, and $f$
maximizes $\sabs{f'(z_0)}$ among all injective holomorphic maps to $\D$ such
that $f(z_0) = 0$.
\end{thm}

Schwarz's lemma can also be used to classify the automorphisms of the disc
(and hence any simply connected domain).  Let
$\Aut(\D)$ denote the group of biholomorphic (both $f$ and
$f^{-1}$ are holomorphic) self maps of the
disc to itself.

\begin{prop}
If $f \in \Aut(\D)$, then there exists an $a \in \D$
and $\theta \in \R$ such that
\begin{equation*}
f(z) = e^{i\theta} \frac{z-a}{1-\bar{a}z} .
\end{equation*}
\end{prop}

Speaking of automorphisms.  We have the following
version of inverse function theorem.

\begin{thm}
Suppose $U$ and $V$ are open subsets of $\C$.
\begin{enumerate}[(i)]
\item
If $f \colon U \to V$ is holomorphic and bijective (one-to-one and onto),
then $f'(z) \not= 0$ for all $z \in V$, and $f^{-1} \colon V \to U$
is holomorphic.  If $f(p) = q$, then
\begin{equation*}
\left(f^{-1}\right)(q) = \frac{1}{f'(p)} .
\end{equation*}
\item
If $f \colon U \to V$ is holomorphic, $f(p) = q$,
and $f'(p) \not= 0$, then there exists a neighborhood $W$ of $q$
and a holomorphic function $g \colon W \to U$ that is
one-to-one and $f\bigl(g(z)\bigr) = z$ for all $z \in W$.
\end{enumerate}
\end{thm}

\pagebreak[2]
The Riemann mapping theorem actually follows from the following
theorem about existence of branches of the logarithm.

\begin{thm}
Suppose $U \subset \C$ is a simply connected domain, and $f \colon U \to \C$
is a holomorphic function without zeros in $U$.  Then there exists a
holomorphic function $L \colon U \to \C$ such that
\begin{equation*}
e^L = f .
\end{equation*}
In particular, we can take roots:
For every $k \in \N$, there exists a holomorphic function
$g \colon U \to \C$ such that
\begin{equation*}
g^k = f .
\end{equation*}
\end{thm}

In one complex variable, zeros of holomorphic functions
can be divided out.  Moreover, zeros
of holomorphic functions are of finite order unless
the function is identically zero.

\begin{prop}
Suppose $U \subset \C$ is a domain and
$f \colon U \to \C$ is holomorphic, not identically zero, and $f(p) = 0$
for some $p \in U$.  There exists a $k \in \N$ and
a holomorphic function $g \colon U \to \C$,
such that $g(p) \not= 0$ and
\begin{equation*}
f(z) = {(z-p)}^k g(z) \qquad \text{for all $z \in U$.}
\end{equation*}
\end{prop}

The number $k$ above is called the \emph{order}\index{order of a zero}
or \emph{multiplicity}\index{multiplicity of a zero}
of the zero at $p$.
We can use this fact and the existence of roots to show that every
holomorphic function is locally like $z^k$.  The function $\varphi$
below can be thought of as a local change of coordinates.

\begin{prop}
Suppose $U \subset \C$ is a domain and
$f \colon U \to \C$ is holomorphic, not identically zero, and $p \in U$.
Then there exists a $k \in \N$,
a neighborhood $V \subset U$ of $p$,  and
a holomorphic function $\varphi \colon V \to \C$ with
$\varphi'(p) \not= 0$, such that
\begin{equation*}
{\bigl(\varphi(z)\bigr)}^k = f(z) - f(p)
\qquad \text{for all $z \in V$.}
\end{equation*}
\end{prop}

Convergence of holomorphic functions is the same as for continuous
functions: uniform convergence on compact subsets.
Sometimes this is called \emph{\myindex{normal convergence}}.

\begin{prop}
Suppose $U \subset \C$ is open and $f_k \colon U \to \C$ is
a sequence of holomorphic functions which converge uniformly
on compact subsets of $U$ to $f \colon U \to \C$.  Then $f$ is holomorphic,
and every derivative $f_k^{(\ell)}$ converges uniformly on compact subsets
to the derivative $f^{(\ell)}$.
\end{prop}

Holomorphic functions satisfy a Heine--Borel-like property:

\begin{thm}[Montel]\index{Montel's theorem}\label{thm:onevarmontel}
Suppose
$U \subset \C$ is open and
$f_n \subset U \to \C$ is a sequence of holomorphic functions.
If $\{ f_n \}$ is uniformly bounded on compact subsets of $U$,
then there exists a subsequence converging uniformly on compact subsets
of $U$.
\end{thm}

A sequence of holomorphic functions cannot create or delete zeros out of thin air:

\begin{thm}[Hurwitz]\index{Hurwitz's theorem}\label{thm:onevarhurwitz}
Suppose $U \subset \C$ is a domain and
$f_n \subset U \to \C$ is a sequence of holomorphic functions
converging uniformly on compact subsets of $U$ to $f \colon U \to \C$.
If $f$ is not identically zero and $z_0$ is a zero of $f$,
then there exists a disc $\Delta_r(z_0)$ and an $N$, such that
for all $n \geq N$, $f_n$ has the same number of zeros (counting
multiplicity) in $\Delta_r(z_0)$ as $f$ (counting multiplicity).
\end{thm}

A common application, and sometimes the way
the theorem is stated, is that if $f_n$ have no zeros in $U$, then
either the limit $f$ is identically zero, or it also has no zeros.


\pagebreak[2]
If $U \subset \C$ is open, $p \in U$, and
$f \colon U \setminus \{ p \} \to \C$ is holomorphic, we say that
$f$ has an \emph{\myindex{isolated singularity}} at $p$.
An isolated singularity is \emph{removable}\index{removable singularity}
if there exists a holomorphic function $F \colon U \to \C$
such that $f(z) = F(z)$ for all $z \in U \setminus \{ p \}$.
An isolated singularity is a \emph{\myindex{pole}} if
\begin{equation*}
\lim_{z \to p} f(z) = \infty \qquad \text{(that is,
$\sabs{f(z)} \to \infty$ as $\sabs{z-p} \to 0$)}.
\end{equation*}
An isolated singularity that is neither removable nor a pole is said to be
\emph{essential}\index{essential singularity}.

At nonessential isolated singularities
the function blows up to a finite integral order.
The first part of the following proposition is
usually called the \emph{\myindex{Riemann extension theorem}}.

\begin{prop} \label{prop:onevarclassifysing}
Suppose $U \subset \C$ is an open set, $p \in U$,
and $f \colon U \setminus \{p\} \to \C$ holomorphic.
\begin{enumerate}[(i)]
\item \label{prop:onevarclassifysing:i}
If $f$ is bounded (near $p$ is enough), then $p$ is a removable singularity.
\item \label{prop:onevarclassifysing:ii}
If $p$ is a pole, there exists a $k \in \N$ such that
\begin{equation*}
g(z) = {(z-p)}^k f(z)
\end{equation*}
is bounded near $p$ and hence $g$ has a removable singularity at $p$.
\end{enumerate}
\end{prop}

The number $k$ above is called the \emph{order}\index{order of a pole}
of the pole.  There is a symmetry between zeros and poles:
If $f$ has a zero of order $k$, then $\frac{1}{f}$ has a pole of order $k$.
If $f$ has a pole of order $k$, then $\frac{1}{f}$
has a removable singularity, and the extended function has a zero of order
$k$.

Let $\bP^1 = \C \cup \{\infty\}$ be the \emph{\myindex{Riemann sphere}}.
The topology on $\bP^1$ is given by insisting that
the function $\frac{1}{z}$ is a homeomorphism
of $\bP^1$ to itself,
where $\frac{1}{\infty} = 0$ and $\frac{1}{0} = \infty$.
A function $f \colon U \to \bP^1$ is called \emph{\myindex{meromorphic}},
if it is not
identically $\infty$, is holomorphic on $U \setminus f^{-1}(\infty)$,
and has poles at $f^{-1}(\infty)$.
A holomorphic function with poles is meromorphic
by setting the value to be $\infty$ at the poles.
A meromorphic function is one that can locally be written as a quotient of
holomorphic functions.


At an isolated singularity we can expand a holomorphic function
via the so-called
\emph{\myindex{Laurent series}} by adding all negative powers.
The Laurent series also characterizes the type of the singularity.

\begin{prop}
If $\Delta \subset \C$ is a disc centered at $p \in \C$,
and $f \colon \Delta \setminus \{p\} \to \C$ holomorphic,
then there exists a double sequence $\{ c_{k} \}_{k = -\infty}^\infty$
such that
\begin{equation*}
f(z) = \sum_{k=-\infty}^\infty c_k {(z-p)}^k ,
\end{equation*}
converges absolutely uniformly on compact subsets of $\Delta$.  If $\gamma$
is a simple closed piecewise-$C^1$ path going once counter-clockwise around
$p$ in $\Delta$, then
\begin{equation*}
c_k =
\frac{1}{2\pi i}
\int_{\gamma}
\frac{f(\zeta)}{{(\zeta-z)}^{k+1}}
\,
d \zeta  .
\end{equation*}
The singularity at $p$ is
\begin{enumerate}[(i)]
\item \emph{removable} if $c_k = 0$ for all $k < 0$.
\item \emph{pole} of order $\ell \in \N$ if $c_k = 0$ for all $k < -\ell$ and $c_{-\ell}
\not= 0$.
\item \emph{essential} if for every exist infinitely negative $k$
such that $c_k \not= 0$.
\end{enumerate}
\end{prop}

\pagebreak[3]
Suppose that $f$ has an isolated singularity at $p$.
The part of the series for negative $k$,
that is, 
$\sum_{k=-\infty}^{-1} c_k {(z-p)}^k$,
is called the
the \emph{\myindex{principal part}}.
The singularity is removable if the principal part is zero, it is a pole if
the principal part is a finite sum, and essential if the 
principal part is an infinite series.

We call the $c_{-1}$ corresponding to $p$
the \emph{\myindex{residue}} of $f$ at $p$, and write it
as $\operatorname{Res}(f,p)$.
The proposition says that for a small $\gamma$ around $p$ in the positive direction,
\begin{equation*}
\operatorname{Res}(f,p) = c_{-1} = \frac{1}{2\pi i} \int_\gamma f(z) \, dz
.
\end{equation*}
Combining this equation with Cauchy's theorem tells us that to compute
integrals of functions with isolated singularities we simply need to find
the residues,
which tend to be simpler to
compute.  For example, if $p$ is a simple pole (of order $1$), then
\begin{equation*}
\operatorname{Res}(f,p) = \lim_{z \to p} (z-p)f(z) .
\end{equation*}

\begin{thm}[Residue theorem]\index{residue theorem}\label{thm:residue}
Suppose $U \subset \C$ is an open set, and $\gamma$ is a piecewise-$C^1$
closed path in $U$
such that $n(\gamma;p)=0$ for all $p \not\in U$.
Suppose that $f \colon U \setminus S \to \C$ is a holomorphic function with isolated
singularities in a finite set $S$, and suppose $S$ lies in the interior of $\gamma$.
Then
\begin{equation*}
\int_{\gamma} f(z) \, dz = 2\pi i \sum_{p \in S} n(\gamma;p) \operatorname{Res}(f,p) .
\end{equation*}
\end{thm}

If $\gamma$ is a simple closed curve positively oriented,
then $n(\gamma;p)=1$ for all $p$ in
its interior, and we can replace the hypothesis on $\gamma$
by requiring that the interior of $\gamma$ lies in $U$ and hence avoid
mentioning winding numbers.

The identity theorem says that zeros of a nonconstant holomorphic $f$
have no limit points; they are isolated points.
Since $\frac{1}{f}$ is a meromorphic
function with zeros at the poles of $f$, poles are also
isolated.  Zeros and poles of  can be counted fairly easily.

\begin{thm}[Argument principle]\index{argument principle}\label{thm:onevarargprinc}
Suppose $U \subset \C$ is an open set, and $\gamma$ is a piecewise-$C^1$
closed path in $U$ such that
$n(\gamma;p)=0$ for all $p \not\in U$ and $n(\gamma;p) \in \{ 0,1 \}$ for
all $p \not\in \gamma$.
Suppose that $f \colon U \to \bP^1$ is a meromorphic function with no zeros
or poles on $\gamma$.
Then
\begin{equation*}
\frac{1}{2\pi i}
\int_\gamma \frac{f'(z)}{f(z)} \, dz
= N - P ,
\end{equation*}
where $N$ is the number of zeros of $f$ inside $\gamma$ and $P$ is the
number of poles inside $\gamma$, both counted with multiplicity.

Furthermore, suppose $h \colon U \to \C$ is holomorphic.
Let $z_1,\ldots,z_N$ be the zeros of $f$ inside $\gamma$ and
$w_1,\ldots,w_P$ be the poles of $f$ inside $\gamma$.
Then
\begin{equation*}
\frac{1}{2\pi i}
\int_\gamma h(z) \frac{f'(z)}{f(z)} \, dz
=
\sum_{k=1}^N h(z_k)
\quad
-
\quad
\sum_{k=1}^P h(w_k) .
\end{equation*}
\end{thm}

Again, if $\gamma$ is simple closed and positively oriented,
the hypothesis on $\gamma$ could be replaced with the requirement that
the interior of $\gamma$ lies in $U$.
The proof is an immediate application of the residue theorem.  Simply
compute the residues at the zeros and poles of $f$.  In particular,
if $f$ has a zero at $p$ or multiplicity $m$, then $h(z) \frac{f'(z)}{f(z)}$
has a simple pole
at $p$ with residue $m\, h(p)$.  Similarly, if $f$ has a pole at $p$ of
order $m$,
then $h(z) \frac{f'(z)}{f(z)}$ has a simple pole with residue $-m\, h(p)$ at $p$.

Another useful way to count zeros is Rouch\'e's theorem.

\begin{thm}[Rouch\'e]\index{Rouch\'e's theorem}\label{thm:onevarrouche}
Suppose $U \subset \C$ is an open set, and $\gamma$ is a piecewise-$C^1$
closed path in $U$ such that
$n(\gamma;p)=0$ for all $p \not\in U$ and $n(\gamma;p) \in \{ 0,1 \}$ for
all $p \not\in \gamma$.
Suppose that $f \colon U \to \C$ and $g \colon U \to \C$
are holomorphic functions such that
\begin{equation*}
\sabs{f(z)-g(z)} < \sabs{f(z)}+\sabs{g(z)}
\end{equation*}
for all $z \in \gamma$.  Then $f$ and $g$
have the same number of zeros
inside $\gamma$ (up to multiplicity).
\end{thm}

In the classical statement of the theorem the weaker
inequality $\sabs{f(z)-g(z)} < \sabs{f(z)}$ is used.
Notice that either inequality precludes any zeros on $\gamma$ itself.

A holomorphic function with an essential singularity achieves essentially
every value.  A weak version of this result (and an easy to prove one)
is the \emph{\myindex{Casorati--Weierstrass theorem}}:
If a holomorphic $f$ has an essential singularity at $p$,
then for every neighborhood $W$ of $p$, $f\bigl(W \setminus \{p\}\bigr)$ is dense in
$\C$.  Let us state the much stronger theorem of Picard:
A function with an essential singularity is very wild.
It achieves every value (except possibly one) infinitely often.

\begin{thm}[Picard's big theorem]\index{Picard's big theorem}
Suppose $U \subset \C$ is open, $f \colon U \setminus \{ p \} \to \C$
is holomorphic, and $f$ has an essential singularity at $p$.  Then for every
neighborhood $W$ of $p$, $f\bigl(W \setminus \{ p \}\bigr)$ is either $\C$
or $\C$ minus a point.
\end{thm}

For example, $e^{1/z}$ has an essential singularity at the origin
and the function is never $0$.
Since we stated the big theorem, let us also state the little theorem.

\begin{thm}[Picard's little theorem]\index{Picard's little theorem}
If $f \colon \C \to \C$ is holomorphic,
then
$f(\C)$ is either $\C$ or $\C$ minus a point.
\end{thm}


One theorem from algebra that is important in complex analysis, and becomes
perhaps even more important in several variables is the fundamental theorem
of algebra.  It really is a theorem of complex analysis and its standard
proof is via the maximum principle.

\begin{thm}[Fundamental theorem of algebra]\index{fundamental theorem of algebra}
\label{thm:fundamentalthmalg}%
If $P \colon \C \to \C$ is a nonzero polynomial of degree $k$,\linebreak[2]
then $P$
has exactly $k$ zeros (roots) in $\C$ counted with multiplicity.
\end{thm}

The set of rational functions is dense in the space of holomorphic
functions, and we even have control over where the poles need to be.
Note that a nonconstant polynomial has a ``pole at infinity'' meaning
$P(z) \to \infty$ as $z \to \infty$.  Letting $\bP^1$ again be the Riemann
sphere, we have Runge's approximation theorem.

\begin{thm}[Runge]\index{Runge's approximation theorem}
Suppose $U \subset \C$ is an open set and $A \subset \bP^1 \setminus U$
is a set containing at least one point from each component of
$\bP^1 \setminus U$.  Suppose $f \colon U \to \C$ is holomorphic.
Then for every $\epsilon > 0$ and every compact
$K \subset \subset U$, there exists a rational function $R$ with poles in $A$
such that
\begin{equation*}
\sabs{R(z) - f(z)} < \epsilon \qquad \text{for all $z \in K$}.
\end{equation*}
\end{thm}

Perhaps a surprising generalization of the
classical Weierstrass approximation theorem,
and one of my favorite one-variable theorems,
is Mergelyan's theorem.
It may be good to note that Mergelyan does not follow from Runge.

\begin{thm}[Mergelyan]\index{Mergelyan's theorem} \label{thm:mergelyan}
Suppose $K \subset \subset \C$ is a compact set such that $\C \setminus K$
is connected and
$f \colon K \to \C$ is a continuous function that is
holomorphic in the interior $K^\circ$.
Then for every $\epsilon > 0$ and every compact
$K \subset \subset U$, there exists a polynomial $P$
such that
\begin{equation*}
\sabs{P(z) - f(z)} < \epsilon \qquad \text{for all $z \in K$}.
\end{equation*}
\end{thm}

The reason why the theorem is perhaps
surprising is that $K$ may have only a
small or no interior.  Using a closed interval $K=[a,b]$ of the real line we
recover the Weierstrass approximation theorem.

\medskip

Given an open set $U \subset \C$, we say $U$ is
\emph{\myindex{symmetric with respect to the real axis}} if
$z \in U$ implies $\bar{z} \in U$.  We divide $U$ into
three parts
\begin{equation*}
U_+ = \{ z \in U : \Im z > 0 \}, \qquad
U_0 = \{ z \in U : \Im z = 0 \}, \qquad
U_- = \{ z \in U : \Im z < 0 \}.
\end{equation*}
We have the following theorem for extending (reflecting) holomorphic functions past
boundaries.

\begin{thm}[Schwarz reflection principle]
\index{Schwarz reflection principle}%
Suppose $U \subset \C$ is a domain symmetric with respect to the real axis,
$f \colon U_+ \cup U_0 \to \C$ a continuous function holomorphic on $U_+$
and real-valued on $U_0$.  Then the function $g \colon U \to \C$
defined by
\begin{equation*}
g(z) = f(z) \quad \text{if $z \in U_+ \cup U_0$},
\qquad
g(z) =
\overline{f(\bar{z})} \quad \text{if $z \in U_-$},
\avoidbreak
\end{equation*}
is holomorphic on $U$.
\end{thm}

In fact, the reflection is really about harmonic functions.

\begin{thm}[Schwarz reflection principle for harmonic functions]
% this is probably not needed: \index{Schwarz reflection principle for harmonic functions}%
Suppose $U \subset \C$ is a domain symmetric with respect to the real axis,
$f \colon U_+ \cup U_0 \to \R$ a continuous function harmonic on $U_+$
and zero on $U_0$.  Then the function $g \colon U \to \R$
defined by
\begin{equation*}
g(z) = f(z) \quad \text{if $z \in U_+ \cup U_0$},
\qquad
g(z) =
-f(\bar{z}) \quad \text{if $z \in U_-$},
\end{equation*}
\avoidbreak
is harmonic on $U$.
\end{thm}

Functions may be defined locally, and continued along paths.
Suppose $p$ is a point and $D$ is a disc centered at $p \in D$.
A holomorphic function $f \colon D \to \C$ can be
\emph{analytically continued}\index{analytic continuation}
along a path
$\gamma \colon [0,1] \to \C$, $\gamma(0) = p$,
if for every $t \in [0,1]$ there exists
a disc $D_t$ centered at $\gamma(t)$, where $D_0=D$, and a holomorphic function
$f_t \colon D_t \to \C$, where $f_0 = f$, and for each $t_0 \in [0,1]$ there is an
$\epsilon > 0$ such that if $\sabs{t-t_0} < \epsilon$, then
$f_t = f_{t_0}$
in $D_t \cap D_{t_0}$.  The monodromy theorem says that as long as there are
no holes, analytic continuation defines a function uniquely.

\begin{thm}[Monodromy theorem]\index{Monodromy theorem}
If $U \subset \C$ is a simply connected domain, $D \subset U$ a disc and
$f \colon D \to \C$ a holomorphic function that can be analytically
continued from $p \in D$ to every $q \in U$ along every path from $p$ to $q$, then there exists
a unique holomorphic function $F \colon U \to \C$ such that $F|_D = f$.
\end{thm}

\medskip

An interesting and useful theorem getting an inequality in the opposite
direction from
Schwarz's lemma, and one which is often not covered in a one-variable
course is the Koebe $\frac{1}{4}$-theorem.
Think of why no such theorem could possibly hold for just smooth
functions.  At first glance the theorem should seem quite counterintuitive,
and at second glance, it should seem outright outrageous.

\begin{thm}[Koebe quarter theorem]\index{Koebe quarter theorem}\index{Koebe
$\frac{1}{4}$-theorem}
Suppose $f \colon \D \to \C$ is holomorphic and injective.  Then
$f(\D)$ contains a disc of radius $\frac{\sabs{f'(0)}}{4}$ centered at $f(0)$.
\end{thm}

The $\frac{1}{4}$ is sharp, that is, it is the best it can be.

\medskip

Finally, it is useful to factor out all the zeros of a holomorphic function,
not just finitely many.  Similarly, we can work with poles.

\begin{thm}[Weierstrass product theorem]\index{Weierstrass product theorem}
Suppose $U \subset \C$ is a domain, $\{ a_k \}$, $\{ b_k \}$ are
countable sets in $U$
with no limit points in $U$, and $\{ n_k \}$, $\{ m_k \}$ countable sets of
natural numbers.
Then there exists a meromorphic function $f$ of $U$ whose
zeros are exactly at $a_k$, with orders given by $n_k$, and
poles are exactly at $b_k$, with orders given by $m_k$.
\end{thm}

For a more explicit statement, we need infinite products.  The product
$\prod_{k=1}^\infty (1+a_k)$
\emph{converges} if the sequence of partial products
$\prod_{k=1}^n (1+a_k)$ converges.  We say that the product
\emph{converges absolutely} if
$\prod_{k=1}^\infty (1+\sabs{a_k})$
converges, which is equivalent to $\sum_{k=1}^\infty \sabs{a_k}$ converging.

Define
\begin{equation*}
E_0(z) = (1-z), \qquad
E_m(z) = (1-z) \exp\left( z +\frac{z^2}{2} + \cdots + \frac{z^m}{m} \right)
.
\avoidbreak
\end{equation*}
The function $E_m\bigl(\nicefrac{z}{a}\bigr)$ has a zero of order $1$ at $a$.

\begin{thm}[Weierstrass factorization theorem]\index{Weierstrass factorization theorem}
Let $f$ be an entire function with zeros (repeated according to multiplicity) at points of
the sequence $\{ a_k \}$ except the zero at
the origin, whose order is $m$ (possibly $m=0$).  Then there exists an
entire function $g$ and a sequence $\{ p_k \}$ such that
\begin{equation*}
f(z) = z^m e^{g(z)} \prod_{k=1}^\infty E_{p_k}\left(\frac{z}{a_k}\right) ,
\end{equation*}
converges uniformly absolutely on compact subsets.
\end{thm}

The $p_k$ are chosen such that
\begin{equation*}
\sum_{k=1}^\infty {\abs{\frac{r}{a_k}}}^{1+p_k}
\end{equation*}
converges for all $r > 0$.

A companion to the Weierstrass product theorem,
which says that you can prescribe zeros,
is the Mittag-Leffler theorem, which says that you
can prescribe principal parts of poles.

\begin{thm}[Mittag-Leffler]\index{Mittag-Leffler theorem}
Suppose $U \subset \C$ is open, $S \subset U$ is a countable set with no
limit point in $U$, and for every $p \in S$ there is a principal part
\begin{equation*}
P_p(z) = \sum_{n=1}^{k_p} \frac{c_{p,n}}{{(z-p)}^n}
\end{equation*}
of a pole of order $k_p$.  Then there exists a meromorphic function $f$ in $U$
with poles precisely at points of $S$, and for each $p \in S$,
the principal part of $f$ at $p$ is $P_p$.
\end{thm}


\begin{center}
* * *
\end{center}

There are many other useful theorems in one complex variable, and we could
spend a lot of time listing them all.
However, hopefully the listing above is useful
as a refresher for the reader of the most common results, some of which are
used in this book, some of which are useful in the exercises, and some of
which are just too interesting not to mention.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% No sections in this appendix
\counterwithin{thm}{chapter}
\chapter{Differential Forms and Stokes' Theorem} \label{ap:diffforms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Differential forms come up quite a bit in this book, especially
in \chapterref{ch:dbar} and \chapterref{ch:integralkernels}.
Let us overview their definition
and state the general Stokes' theorem.  No proofs are given, this appendix
is just a bare bones guide.
For a more complete introduction to differential forms,
see Rudin~\cite{Rudin:principles}.

The short story about differential forms is that a $k$-form is an object
that can be integrated (summed) over a $k$-dimensional object,
taking orientation into account.
For simplicity, as in most of this book,
everything in this appendix is stated for smooth
($C^\infty$) objects to avoid worrying about how much regularity is needed.

The main point of differential forms is to find the proper context for the
Fundamental theorem of calculus,
\begin{equation*}
\int_a^b f'(x) \, dx = f(b)-f(a) .
\end{equation*}
We interpret both sides as integration.  The left-hand side is an integral
of the $1$-form $f'\, dx$ over the $1$-dimensional interval $[a,b]$
and the right-hand side is an integral of the $0$-form (a function)
$f$ over the $0$-dimensional (two-point) set $\{ a, b \}$.  Both sides
consider orientation, $[a,b]$ is integrated from $a$ to $b$,
$\{a\}$ is oriented negatively and
$\{b\}$ is oriented positively.  The two-point set
$\{a,b\}$ is the boundary of $[a,b]$, and the orientation of $\{ a,b \}$
is induced by the orientation of $[a,b]$.

Let us define the objects over which we integrate, that is,
smooth submanifolds of $\R^n$.
Our model for a $k$-dimensional submanifold-with-boundary
is the upper-half-space and its boundary:
\begin{equation*}
\bH^k
\overset{\text{def}}{=}
\{ x \in \R^k : x_k \geq 0 \} ,
\qquad
\partial \bH^k
\overset{\text{def}}{=}
\{ x \in \R^k : x_k = 0 \} ,
\end{equation*}

\begin{defn}
Let $M \subset \R^n$ have the induced subspace topology.
Let $k \in \N_0$.
Let $M$ have the property that
for each $p \in M$, there exists a neighborhood $W \subset \R^n$ of $p$,
a point $q \in \bH^k$, a neighborhood $U \subset \bH^k$ of $q$,
and a smooth one-to-one open\footnote{By open, we mean that $\varphi(V)$ is
a relatively open set of $M$ for every open set $V \subset U$.}
mapping $\varphi \colon U \to M$ such that
$\varphi(q) = p$,
the derivative $D\varphi$ has rank $k$ at all points, and $\varphi(U) = M \cap W$.
Then $M$ is an
\emph{\myindex{embedded submanifold-with-boundary}}\index{submanifold-with-boundary}
of dimension $k$.
The map $\varphi$ is called a
\emph{\myindex{local parametrization}}\index{parametrization}.
If $q$ is such that $q_k = 0$ (the last component is zero), then
$p = \varphi(q)$ is a \emph{boundary point}\index{boundary of a submanifold}.
\glsadd{not:boundary}%
Let $\partial M$ denote the set
of boundary points.  If $\partial M = \emptyset$, then we say
$M$ is simply an
\emph{\myindex{embedded submanifold}}\index{submanifold}\index{submanifold!embedded}.
\end{defn}

The situation for a boundary point and an interior point is depicted in
\figureref{fig:maif-bound}.
Note that $W$ is a bigger neighborhood in $\R^n$ than
the image $\varphi(U)$.

\begin{myfig}
\subimport*{figures/}{manif-bound.pdf_t}
\caption{Parametrization at
an interior and a boundary point of a submanifold.\label{fig:maif-bound}}
\end{myfig}

Completely correctly, we should say \emph{submanifold of $\R^k$}.
Sometimes people (including me)
say \emph{\myindex{manifold}} when they mean \emph{submanifold}.
A manifold is a more abstract concept, but
all submanifolds are manifolds.
The word \emph{embedded} has to do with the topology on $M$, and this
has to do with the condition $\varphi(U) = M \cap W$ and $\varphi$ being open.
The condition means that $\varphi$ is a homeomorphism onto $M \cap W$.
It is important that $W$ is an open set in $\R^n$.  For our
purposes here, all submanifolds will be embedded.
We have also made some economy in the definition.  If $q$ is not on the
boundary of $\bH^k$, then we might as well have used $\R^k$ instead of
$\bH^k$.
A submanifold is something that is locally like $\R^k$, and if it has a
boundary, then near the boundary it is locally like $\bH^k$ near a point
of $\partial \bH^k$.

We also remark that submanifolds are often defined in reverse rather than by
parametrizations, that is, by starting with the (relatively) open sets $M \cap W$,
and the maps $\varphi^{-1}$, calling those \emph{\myindex{charts}},
and calling the entire set of charts an \emph{\myindex{atlas}}.
The particular version of the definition we have chosen makes
it easy to evaluate integrals in the same way that parametrizing curves
makes it easy to evaluate integrals.

Examples of such submanifolds are domains with smooth boundaries
as in \defnref{def:hypersurface}, we can take the inclusion map $x \mapsto
x$ as our parametrization.  The domain is then the submanifold $M$
and $\partial M$ is the boundary of the domain.
Domains are the key application for our purposes.
Another example are smooth curves.

If $M$ is an embedded submanifold-with-boundary
of dimension $k$, then $\partial M$ is also an embedded submanifold of dimension
$k-1$.  Simply restrict the parametrizations to the boundary of $\bH^k$.

We also need to define an orientation.

\begin{defn}
Let $M \subset \R^n$ be an embedded submanifold-with-boundary of dimension $k
\geq 2$.  Suppose
a set of parametrizations can be chosen such that
each point of $M$ is in the image of one of the parametrizations,
and
if
$\varphi \colon U \to M$ and $\widetilde{\varphi} \colon \widetilde{U} \to
M$ are two parametrizations such
that $\varphi(U) \cap \widetilde{\varphi}(\widetilde{U}) \not= \emptyset$, then the
\emph{\myindex{transition map}} (automatically smooth) defined by
\begin{equation*}
{\widetilde{\varphi}}^{~{-1}} \circ \varphi
\end{equation*}
on $\varphi^{-1}\bigl(\varphi(U) \cap \widetilde{\varphi}(\widetilde{U})\bigr)$ (in other
words, wherever it makes sense) is orientation preserving, that is,
\begin{equation*}
\det D\bigl({\widetilde{\varphi}}^{~{-1}} \circ \varphi \bigr) > 0
\end{equation*}
at all points.
The set of such parametrizations is the \emph{\myindex{orientation}} on $M$,
and we usually take the maximal set of such parametrizations.

If $M$ is oriented, then the restrictions of the parametrizations
to $\partial \bH^k$ give an orientation on $\partial M$.  We say
this is the \emph{\myindex{induced orientation}} on $\partial M$.
\end{defn}

For dimensions $k=0$ (isolated points) and $k=1$ (curves) we must define
orientation differently.  For $k=0$, we simply give each point an
orientation of $+$ or $-$.  For $k=1$, we need to allow parametrization by
open subsets not only of $\bH^1 = [0,\infty)$, but also $-\bH^1 = (-\infty,0]$.
The definition is the same otherwise.  To define the orientation
of the boundary, if the boundary point corresponds to the $0$ in
$[0,\infty)$ we give this boundary point the orientation
$-$, and if it corresponds to the $0$ in $(-\infty,0]$, then
we give this point the orientation $+$.  The reason for this complication is
that unlike in $\R^k$ for $k \geq 2$, the set $\bH^1 = [0,\infty)$ cannot be
``rotated'' (in $\R^1$) or mapped via an orientation preserving map onto
$-\bH^1 = (-\infty,0]$, but in $\R^2$ the upper-half-plane $\bH^2$ can
be rotated to the lower-half-plane $-\bH^2 = \{ x \in \R^2 : x_2 \leq 0 \}$.
For computations, it is often useful for compact curves
with endpoints (boundary) to just give one
parametrization from $[0,1]$ or perhaps $[a,b]$, then $a$ corresponds to the
$-$ and $b$ corresponds to the $+$.

The fact that the transition map is smooth does require a proof, which is a
good exercise in basic analysis.  It requires a bit of care at boundary
points.

An orientation allows us to have a well-defined integral on $M$, just
like a curve needs to be oriented in order to define a line integral.
However, unlike for curves, not every submanifold of dimension
more than one is \emph{orientable}, that is, admits an orientation.
A classical nonorientable example is the M\"obius strip.

\medskip

Now that we know what we integrate ``on'', let us figure out what ``it'' is
that we integrate.  Let us start with $0$-forms.
We define $0$-forms as smooth functions (possibly complex-valued).
Sometimes we need a function defined just on a submanifold.  A function $f$ defined
on a submanifold $M$ is smooth when $f \circ \varphi$ is smooth on $U$
for every parametrization $\varphi \colon U \to M$.  Equivalently,
one can prove that $f$ is the restriction of some smooth function
defined on some neighborhood of $M$ in $\R^n$.

A $0$-form $\omega$ defined on a $0$-dimensional oriented submanifold $M$
is integrated as
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{p \in M} \epsilon_p \omega(p) ,
\end{equation*}
where $\epsilon_p$ is the orientation of $p$ given as $+1$ or $-1$.
To avoid problems of integrability,
one can assume that $\omega$ is compactly supported (it is
nonzero on at most finitely many points of $M$) or that $M$ is compact (it
is a finite set).

The correct definition of a $1$-form is that it is a ``smooth section'' of
the dual of the vector bundle $T \R^n$.
It is something that eats
a vector field, and spits out a function.
We use the pairing notation $\langle \omega , v \rangle$
\glsadd{not:pairing}%
instead of the functional
notation $\omega(v)$ to indicate linearity in $v$ (and in $\omega$).
The $1$-form
$dx_k$ is supposed to be the object that does
\begin{equation*}
\left\langle
dx_k,
\frac{\partial}{\partial x_k}
\right\rangle
=
1,
\qquad
\left\langle
dx_k ,
\frac{\partial}{\partial x_\ell}
\right\rangle
=
0 \quad \text{if $\ell\not=k$}.
\end{equation*}
For our purposes here, just suppose that a $1$-form
in $\R^n$ is an object of the form
\begin{equation*}
\omega = g_1 dx_1 + g_2 dx_2 + \cdots + g_n dx_n ,
\end{equation*}
where $g_1, g_2, \ldots, g_n$ are smooth functions.  That is, a $1$-form is
at each point a linear combination of $dx_1, dx_2, \ldots, dx_n$ that
varies smoothly from point to point.
Suppose $M$ is a one-dimensional submanifold
(possibly with boundary), $\varphi \colon U \to M$
is a parametrization compatible with the orientation of $M$,
and $g_k$ is supported in $\varphi(U)$.
Define
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{k=1}^n
\int_U g_k\bigl( \varphi(t) \bigr) \varphi_k'(t) \, dt ,
\end{equation*}
where the integral $\int_U \cdots \, dt$
is evaluated with the usual positive orientation
(left to right) as $U \subset \R$, and $\varphi_k$ is the $k$th component
of $\varphi$.

Generally, a $1$-form has support bigger than just $\varphi(U)$.  In this
case, one needs to use a so-called partition of unity to write $\omega$
as a locally finite sum
\begin{equation*}
\omega = \sum_{\iota} \omega_\iota ,
\end{equation*}
where each $\omega_\iota$ has support in the image of a single
parametrization.  By locally finite, we mean that on each compact
neighborhood only finitely many $\omega_\iota$ are nonzero.
Define
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{\iota}
\int_M \omega_\iota .
\end{equation*}
The definition makes sense only
if this sum actually exists.  For example, if $\omega$ is
compactly supported, then this sum is only finite, and so it exists.

Higher degree forms are constructed out of $1$-forms and $0$-forms
by the so-called wedge product.  Given a $k$-form $\omega$
and an $\ell$-form $\eta$,
\glsadd{not:wedge}%
\begin{equation*}
\omega \wedge \eta
\end{equation*}
is a $(k+\ell)$-form.  We require the wedge product to be bilinear at each point:
If $f$ and $g$ are smooth functions, then
\begin{equation*}
(f \omega + g \eta) \wedge \xi =
f (\omega \wedge \xi) + g (\eta \wedge \xi)
, \qquad
\text{and}
\qquad
\omega \wedge (f \eta + g \xi) =
f (\omega \wedge \eta ) +
g ( \omega \wedge \xi) .
\end{equation*}
The wedge product is not commutative; we require it to be
anticommutative on $1$-forms. If $\omega$ and $\eta$ are $1$-forms,
then
\begin{equation*}
\omega \wedge \eta = - \eta \wedge \omega .
\end{equation*}
The negative keeps track of orientation.
When $\omega$ is a $k$-form and $\eta$ is an $m$-form,
\begin{equation*}
\omega \wedge \eta = {(-1)}^{k m} \eta \wedge \omega .
\end{equation*}

We wedge together the basis $1$-forms to
get all $k$-forms.
A $k$-form is then an expression
\begin{equation*}
\omega =
\sum_{\ell_1=1}^n
\sum_{\ell_2=1}^n
\cdots
\sum_{\ell_k=1}^n
g_{\ell_1,\ldots,\ell_k}
\,
dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}  ,
\end{equation*}
where $g_{\ell_1,\ldots,\ell_k}$ are smooth functions.
We can simplify even more.  Since the wedge
is anticommutative on $1$-forms,
\begin{equation*}
dx_\ell \wedge dx_m =
-dx_m \wedge dx_\ell
,
\qquad
\text{and}
\qquad
dx_\ell \wedge dx_\ell = 0 .
\end{equation*}
In other words, every form
$dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}$
is either zero, if any two indices from $\ell_1,\ldots,\ell_k$ are equal, or
can be put into the form
$\pm dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}$, where $\ell_1 < \ell_2 < \cdots < \ell_k$.
Thus, a $k$-form can always be written as
\begin{equation*}
\omega =
\sum_{1 \leq \ell_1 < \ell_2 < \cdots < \ell_k \leq n}
g_{\ell_1,\ldots,\ell_k}
\,
dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}  .
\end{equation*}

In general, just like $1$-forms are linear functionals of vector fields,
$k$-forms are alternating multilinear functions of $k$ vector fields.
To simplify matters, let us note how
$k$ vectors are plugged into
$dx_{\ell_1} \wedge dx_{\ell_2} \wedge \cdots \wedge dx_{\ell_k}$.
Consider vector fields $X_1,\ldots,X_k$ given by
$X_j = \sum_{m=1}^n c_{m j} \frac{\partial}{\partial x_m}$.
As $k$-forms are alternating and multilinear, instead of plugging in
the $k$-tuple $(X_1,\ldots,X_k)$, we write it as taking the
wedge product $X_1 \wedge \cdots \wedge X_k$, where the wedge has the
same properties as for forms.
Then
\begin{equation*}
\bigl\langle
dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}
,
X_1 \wedge \cdots \wedge X_k
\rangle
=
\det
\left(
\begin{bmatrix}
c_{\ell_1 1} & c_{\ell_1 2} & \cdots & c_{\ell_1 k} \\
c_{\ell_2 1} & c_{\ell_2 2} & \cdots & c_{\ell_2 k} \\
\vdots & \vdots & \ddots & \vdots \\
c_{\ell_k 1} & c_{\ell_k 2} & \cdots & c_{\ell_k k}
\end{bmatrix}
\right) .
\end{equation*}
That is, each $dx_{\ell_j}$ picks out the $\ell_j$th row out of the matrix
of all coefficients and we take the determinant.  Here is an explicit
example for $k=2$:
\begin{equation*}
\left\langle
dx_1 \wedge dx_2
,
\left(
a \frac{\partial}{\partial x_1} +
b \frac{\partial}{\partial x_2}
\right)
\wedge
\left(
c \frac{\partial}{\partial x_1} +
d \frac{\partial}{\partial x_2}
\right)
\right\rangle
=
ad-bc .
\end{equation*}

Consider
an oriented $k$-dimensional submanifold $M$
(possibly with boundary), a parametrization $\varphi \colon U \to M$
from the orientation,
and a $k$-form $\omega$
supported in $\varphi(U)$ (that is each $g_{\ell_1,\ldots,\ell_k}$ is supported in
$\varphi(U)$).
Denote by $t \in U \subset \R^k$
the coordinates on $U$.  Define
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{1 \leq \ell_1 < \ell_2 < \cdots < \ell_k \leq n}
\int_U
g_{\ell_1,\ldots,\ell_k}\bigl(\varphi(t)\bigr)
\det D (\varphi_{\ell_1},\varphi_{\ell_2},\ldots,\varphi_{\ell_k})
\,
dt
\end{equation*}
where the integral $\int_U \cdots\, dt$ is evaluated in the
usual orientation on $\R^k$ with $dt$ the standard
measure on $\R^k$ (think $dt = dt_1 dt_2 \cdots dt_n$), and
$D (\varphi_{\ell_1},\varphi_{\ell_2},\ldots,\varphi_{\ell_k})$
denotes the derivative of the mapping whose $m$th component
is $\varphi_{\ell_m}$.

Similarly as before, if $\omega$ is not supported in the
image of a single parametrization, then using a partition of unity,
write
\begin{equation*}
\omega = \sum_{\ell} \omega_\ell
\end{equation*}
as a locally finite sum,
where each $\omega_\ell$ has support in the image of a single
parametrization of the orientation.
Then
\begin{equation*}
\int_M \omega
\overset{\text{def}}{=}
\sum_{\ell}
\int_M \omega_\ell .
\end{equation*}
Again, the sum has to exist, such as when $\omega$ is compactly supported
and the sum is finite.

The only nontrivial differential forms on $\R^n$
are $0,1,2,\ldots,n$ forms.  The only $n$-forms are
object of the form
\begin{equation*}
f(x) \,
dx_1 \wedge dx_2 \wedge \cdots \wedge dx_n .
\end{equation*}
The form $dx_1 \wedge dx_2 \wedge \cdots \wedge dx_n$ is called the
volume form.  Integrating it over a domain (an $n$-dimensional submanifold)
gives the standard volume integral.

More generally, one defines integration of $k$-forms over $k$-chains,
which are just linear combinations of smooth submanifolds, but we do not
need that level of generality.

\medskip

In computations, we can avoid sets of zero measure ($k$-dimensional),
so we can ignore the boundary of the submanifold.  Similarly, if we parametrize
several subsets we can leave out a measure zero subset.
Let us give a few examples of computations.

\begin{example} \label{example:diffformscircleint}
Consider the circle $S^1 \subset
\R^2$.  We use a parametrization $\varphi \colon (-\pi,\pi) \to S^1$
where $\varphi(t) = \bigl(\cos(t),\sin(t)\bigr)$, so the circle is oriented
counter-clockwise.
Let $\omega(x_1,x_2) = P(x_1,x_2) \, dx_1 + Q(x_1,x_2) \, dx_2$, then
\begin{equation*}
\int_{S^1} \omega =
\int_{-\pi}^{\pi}
\Bigl(
P\bigl(\cos(t),\sin(t)\bigr) \bigl(-\sin(t)\bigr)  +
Q\bigl(\cos(t),\sin(t)\bigr) \cos(t) \Bigr) \, dt .
\end{equation*}
We can ignore the point $(-1,0)$ as a single point is of $1$-dimensional
measure zero.
\end{example}

\begin{example}
Consider a domain $U \subset \R^n$, then $U$ is an oriented submanifold.
We use the parametrization $\varphi \colon U \to U$, where $\varphi(x) =
x$.  Then
\begin{equation*}
\int_U f(x) \, dx_1 \wedge dx_2 \wedge \cdots \wedge dx_n
=
\int_U f(x) \, dx_1 \, dx_2 \,  \cdots \, dx_n
=
\int_U f(x) \, dV ,
\end{equation*}
where $dV$ is the standard volume measure.
\end{example}

\begin{example}
Finally, consider $M$
the upper hemisphere of the unit sphere $S^2 \subset \R^3$ as a submanifold
with boundary.  That is consider
\begin{equation*}
M = \bigl\{ x \in \R^3 : x_1^2+x_2^2+x_3^2=1, x_3 \geq 0 \bigr\} .
\end{equation*}
The boundary is the circle in the $(x_1,x_2)$-plane:
\begin{equation*}
\partial M = \bigl\{ x \in \R^3 : x_1^2+x_2^2=1, x_3 = 0 \bigr\} .
\end{equation*}
Consider the parametrization of $M$ using the spherical coordinates
\begin{equation*}
\varphi(\theta,\psi) =
\bigl(\cos(\theta) \sin(\psi),
 \sin(\theta) \sin(\psi),
 \cos(\psi)\bigr)
\end{equation*}
for $U$ given by $-\pi < \theta < \pi$, $0 < \psi \leq \nicefrac{\pi}{2}$.  After
a rotation this is a subset of a half-plane with the points corresponding to
$\psi = \nicefrac{\pi}{2}$ corresponding to boundary points.  We miss the
points where $\theta = \pi$, including the point $(0,0,1)$, but the set of
those points is a $1$-dimensional curve, and so a set of $2$-dimensional
measure zero. For the purposes of integration we can ignore it.
Let
\begin{equation*}
\omega(x_1,x_2,x_3) =
P(x_1,x_2,x_3) \, dx_1 \wedge dx_2
+ Q(x_1,x_2,x_3) \, dx_1 \wedge dx_3
+ R(x_1,x_2,x_3) \, dx_2 \wedge dx_3 .
\end{equation*}
Then
\begin{equation*}
\begin{split}
\int_M \omega & =
\int_{-\pi}^\pi
\int_{0}^{\pi/2}
\biggl[
P\bigl(\varphi(\theta,\psi)\bigr) \biggl(
\frac{\partial \varphi_1}{\partial \theta}
\frac{\partial \varphi_2}{\partial \psi}
-
\frac{\partial \varphi_2}{\partial \theta}
\frac{\partial \varphi_1}{\partial \psi}
\biggr)
\\
& \hspace{1.8cm}
+
Q\bigl(\varphi(\theta,\psi)\bigr) \biggl(
\frac{\partial \varphi_1}{\partial \theta}
\frac{\partial \varphi_3}{\partial \psi}
-
\frac{\partial \varphi_3}{\partial \theta}
\frac{\partial \varphi_1}{\partial \psi}
\biggr)
\\
& \hspace{1.8cm}
+
R\bigl(\varphi(\theta,\psi)\bigr) \biggl(
\frac{\partial \varphi_2}{\partial \theta}
\frac{\partial \varphi_3}{\partial \psi}
-
\frac{\partial \varphi_3}{\partial \theta}
\frac{\partial \varphi_2}{\partial \psi}
\biggr)
\biggr]\, d\theta \, d\psi
\\
& =
\int_{-\pi}^\pi
\int_{0}^{\pi/2}
\Bigl[
P\bigl(\cos(\theta)\sin(\psi),\sin(\theta)\sin(\psi),\cos(\psi)\bigr)
\bigl(-\cos(\psi)\sin(\psi) \bigr)
\\
& \hspace{1.8cm}
+
Q\bigl(\cos(\theta)\sin(\psi),\sin(\theta)\sin(\psi),\cos(\psi)\bigr)
\sin(\theta)\sin^2(\psi)
\\
& \hspace{1.8cm}
+
R\bigl(\cos(\theta)\sin(\psi),\sin(\theta)\sin(\psi),\cos(\psi)\bigr)
\bigl(-\cos(\theta)\sin^2(\psi)\bigr)
\Bigr]
\,
d\theta
\,
d\psi .
\end{split}
\end{equation*}

The induced orientation on the boundary $\partial M$
is the counter-clockwise orientation used in
\exampleref{example:diffformscircleint},
because that is the parametrization we get when we restrict
to the boundary, $\varphi(\theta,\nicefrac{\pi}{2}) =
\bigl(\cos(\theta),\sin(\theta),0\bigr)$.
\end{example}

\medskip

The derivative on $k$-forms is the
\emph{\myindex{exterior derivative}},
which is a linear operator that eats $k$-forms and spits out
$(k+1)$-forms.  For a $k$-form
\begin{equation*}
\omega =
g_{\ell_1,\ldots,\ell_k}
\,
dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}  ,
\end{equation*}
define the exterior derivative $d\omega$ as
\glsadd{not:dpsi}%
\begin{equation*}
d\omega
\overset{\text{def}}{=}
dg_{\ell_1,\ldots,\ell_k}
\wedge
dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k}  =
\sum_{m=1}^n
\frac{\partial g_{\ell_1,\ldots,\ell_k}}{\partial x_m} \,
dx_m \wedge
dx_{\ell_1} \wedge
dx_{\ell_2} \wedge
\cdots \wedge
dx_{\ell_k} .
\end{equation*}
Then define $d$ on every $k$-form by extending it linearly.

For example,
\begin{multline*}
d \left(
P \, dx_2 \wedge dx_3
+
Q \, dx_3 \wedge dx_1
+
R \, dx_1 \wedge dx_2
\right)
\\
=
\frac{\partial P}{\partial x_1} \, dx_1 \wedge dx_2 \wedge dx_3
+
\frac{\partial Q}{\partial x_2} \, dx_2 \wedge dx_3 \wedge dx_1
+
\frac{\partial R}{\partial x_3} \, dx_3 \wedge dx_1 \wedge dx_2
\\
=
\left(
\frac{\partial P}{\partial x_1}
+
\frac{\partial Q}{\partial x_2}
+
\frac{\partial R}{\partial x_3}
\right) \, dx_1 \wedge dx_2 \wedge dx_3 .
\end{multline*}
You should recognize the divergence of the vector field $(P,Q,R)$ from vector
calculus.
All the various derivative operations in $\R^3$ from vector calculus make an
appearance.
If $\omega$ is a $0$-form in $\R^3$, then $d\omega$ is like the gradient.
If $\omega$ is a $1$-form in $\R^3$, then $d\omega$ is like the curl.
If $\omega$ is a $2$-form in $\R^3$, then $d\omega$ is like the divergence.

A quick computation gives the \emph{\myindex{Leibniz rule}}:
If $\omega$ is a $p$-form and $\eta$ is a $q$-form, then
\begin{equation*}
d(\omega \wedge \eta) = 
d\omega \wedge \eta + {(-1)}^p \omega \wedge d\eta .
\end{equation*}


\medskip

Because partial derivatives commute, we find that for every $\omega$,
\begin{equation*}
d(d\omega) = 0 .
\end{equation*}
The equation is sometimes written as $d^2=0$.
If $\Lambda^k(M)$ denotes the $k$-forms on an $n$-dimensional submanifold $M$,
then we get a so-called complex
\begin{equation*}
0
\overset{d}{\to}
\Lambda^0(M)
\overset{d}{\to}
\Lambda^1(M)
\overset{d}{\to}
\Lambda^2(M)
\overset{d}{\to}
\cdots
\overset{d}{\to}
\Lambda^n(M)
\overset{d}{\to}
0 .
\end{equation*}
The initial $d$ from the trivial space to $\Lambda^0(M)$ is just the zero
map.
One can study the topology of $M$
by computing from this complex the so-called
\emph{\myindex{de Rham cohomology}} groups
$H_{dR}^0(M),\ldots,H_{dR}^n(M)$, that is,
$\frac{\operatorname{ker}(d \colon \Lambda^k
\to \Lambda^{k+1})}{\operatorname{im}(d \colon \Lambda^{k-1} \to \Lambda^k)}$,
which measure global solvability of the differential
equation $d\omega = \eta$ for an unknown $\omega$.  These groups
are isomorphic (the de Rham theorem) to the singular cohomology groups 
$H^0(M,\R),\ldots,H^n(M,\R)$.  Variations
on this idea abound and one appears in \chapterref{ch:dbar}, but we digress.

\medskip

We now state \emph{\myindex{Stokes' theorem}}, sometimes
called the \emph{\myindex{generalized Stokes' theorem}} to distinguish it
from the classical Stokes' theorem you know from vector calculus, which is a special
case.\footnote{Interestingly, Stokes had nothing to do with proving either version
aside from liking the theorem.}

\begin{thm}[Stokes]
Suppose $M \subset \R^n$ is an embedded
compact smooth oriented $(k+1)$-dimensional submanifold-with-boundary,
$\partial M$ has the induced orientation,
and $\omega$ is a smooth $k$-form defined on
$M$.  Then
\begin{equation*}
\int_{\partial M} \omega = \int_{M} d\omega .
\end{equation*}
\end{thm}

One can get away with less regularity, both on $\omega$ and $M$ (and
$\partial M$) including ``corners.''
In $\R^2$, it is easy to state
in more generality, see Green's theorem (\thmref{thm:greens}).
The classical Stokes' theorem is just the generalized
Stokes' theorem with $n=3$, $k=2$.  Classically instead of
using differential forms, the line integral is an integral of a vector field
instead of a $1$-form
$\omega$, and its derivative $d\omega$ is the curl operator.

To at least get a flavor of the theorem, we prove it in a simpler
setting, which however is often almost good enough, and it is the key idea
in the proof.
Suppose $U \subset \R^n$ is a domain such that for each $k=1,\ldots,n$,
there exist two smooth functions $\alpha_k$ and $\beta_k$ and $U$
as a set is given by
\begin{equation*}
\begin{aligned}
& (x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n) \in \pi_k(U) ,
\\
& \alpha_k(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)
\leq x_k \leq
\beta_k(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n) ,
\end{aligned}
\end{equation*}
where $\pi_k(U)$ is the projection of $U$ onto the
$(x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)$ components.
Orient $\partial U$ as usual.
Write $x' = (x_1,\ldots,x_{k-1},x_{k+1},\ldots,x_n)$, and let $dV_{n-1}$ be the
volume form for $\R^{n-1}$.
Consider the $(n-1)$-form
\begin{equation*}
\omega = f
dx_1 \wedge \cdots \wedge dx_{k-1} \wedge dx_{k+1} \wedge \cdots \wedge dx_n
.
\end{equation*}
Then $d\omega = \frac{\partial f}{\partial x_k} dx_1 \wedge \cdots \wedge
dx_n$.
By the fundamental theorem of
calculus,
\begin{equation*}
\begin{split}
\int_U d\omega &=
\int_U \frac{\partial f}{\partial x_k} \, dV_n
\\
& =
\int_{\pi_k(U)}
\int_{\alpha_k(x')}^{\beta_k(x')}
\frac{\partial f}{\partial x_k} \, dx_k \, dV_{n-1}
\\
& =
\int_{\pi_k(U)}
f(x_1,\ldots,x_{k-1}, \beta_k(x'), x_{k+1}, \ldots, x_n)
\,dV_{n-1}
\\
& \phantom{=xxx}
-
\int_{\pi_k(U)}
f(x_1,\ldots,x_{k-1}, \alpha_k(x'), x_{k+1}, \ldots, x_n)
\,dV_{n-1}
=
\int_{\partial U} \omega .
\end{split}
\end{equation*}
Any $(n-1)$-form can be written as a sum of forms like $\omega$ for various
$k$.  Integrating each one of them in the correct direction provides the
result.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% No sections in this appendix
\counterwithin{thm}{chapter}
\chapter{Basic Terminology and Results from Algebra} \label{ap:algebra}

We quickly review some basic definitions
and a result or two
from commutative
that we need in
\chapterref{ch:analyticvarieties}.  See a book such as
Zariski--Samuel~\cite{ZariskiSamuel} for a full reference.

\begin{defn}
A set $G$ is called a \emph{\myindex{group}} if it has an operation
$x * y$ defined on it and it satisfies the following axioms:
\begin{enumerate}[({G}1)]
\item If $x \in G$ and $y \in G$, then $x * y \in G$.
\item \emph{(associativity)}
$(x*y)*z = x*(y*z)$ for all $x,y,z \in G$.
\item \emph{(identity)}
There exists an element $1 \in G$ such that
$1*x = x$ for all $x \in G$.
\item \emph{(inverse)}
For every element $x\in G$ there exists an element $x^{-1} \in G$
such that $x * x^{-1} = 0$.
\end{enumerate}
A group $G$ is called \emph{\myindex{abelian}} if it also satisfies:
\begin{enumerate}[resume*]
\item \emph{(commutativity)}
$x*y = y*x$ for all $x,y \in G$.
\end{enumerate}

A subset $K \subset G$ is called a \emph{\myindex{subgroup}}
if $K$ is a group with the same operation as the group $G$.
If $G$ and $H$ are groups, a function $f \colon G \to H$ is a
\emph{\myindex{group homomorphism}}\index{homomorphism}
if it respects the group law, that is, $f(a * b) = f(a) * f(b)$.  If $f$ is
bijective, then it is a
\emph{\myindex{group isomorphism}}\index{isomorphism}.
\end{defn}

An example of a group is a group of automorphisms.  For example, let
$U \subset \C$ be open and suppose $G$ is the set of
biholomorphisms $f \colon U \to U$.  Then $G$ is a group
under composition, but $G$ is not necessarily abelian:  If
$U=\C$, then $f(z)= z+1$ and $g(z)=-z$ are members of $G$, but
$f\circ g (z) = -z+1$ and $g \circ f(z) = -z-1$.



\begin{defn}
A set $R$ is called a \emph{\myindex{commutative ring}}\index{ring} if it has two operations
defined on it, addition $x+y$ and multiplication $xy$, and if it satisfies
the following axioms:
\begin{enumerate}[({A}1)]
\item If $x \in R$ and $y \in R$, then $x+y \in R$.
\item \emph{(commutativity of addition)}
$x+y = y+x$ for all $x,y \in R$.
\item \emph{(associativity of addition)}
$(x+y)+z = x+(y+z)$ for all $x,y,z \in R$.
\item There exists an element $0 \in R$ such that
$0+x = x$ for all $x \in R$.
\item For every element $x\in R$ there exists an element $-x \in R$
such that $x + (-x) = 0$.
\end{enumerate}
\begin{enumerate}[({M}1)]
\item If $x \in R$ and $y \in R$, then $xy \in R$.
\item \emph{(commutativity of multiplication)}
$xy = yx$ for all $x,y \in R$.
\item \emph{(associativity of multiplication)}
$(xy)z = x(yz)$ for all $x,y,z \in R$.
\item There exists an element $1 \in R$ (and $1 \not= 0$) such that
$1x = x$ for all $x \in R$.
\item[(D)] \emph{(distributive law)} $x(y+z) = xy+xz$
for all $x,y,z \in R$.
\end{enumerate}

The ring $R$ is called an \emph{\myindex{integral domain}}
if in addition to being a commutative ring:
\begin{enumerate}%[({ID}1)]
\item[(ID)] 
$xy=0$ implies that $x=0$ or $y=0$.
\end{enumerate}

The ring $R$ is called a \emph{\myindex{field}}
if in addition to being a commutative ring:
\begin{enumerate}%[({F}1)]
\item[(F)] For every $x\in R$ such that $x \not= 0$ there exists an element
$\nicefrac{1}{x} \in R$
such that $x(\nicefrac{1}{x}) = 1$.
\end{enumerate}

In a commutative ring $R$, the elements $u \in R$ for which
there exists an inverse $\nicefrac{1}{u}$ as above are called
\emph{units}\index{unit}.

If $R$ and $S$ are rings, a function $f \colon R \to S$ is a
\emph{\myindex{ring homomorphism}}\index{homomorphism}
if it respects the ring operations, that is,
$f(a + b) = f(a) + f(b)$ and
$f(ab) = f(a)f(b)$, and such that $f(1) = 1$.
If $f$ is
bijective, then it is called a
\emph{\myindex{ring isomorphism}}\index{isomorphism}.
\end{defn}

Namely, a commutative ring is an abelian additive group (by
additive group we just mean we use $+$ for the operation and
$0$ for the respective identity), with multiplication thrown in.
If the multiplication
also defines a group on the set of nonzero elements, then the ring is a
field.  A ring that is not commutative is one that does not satisfy
commutativity of multiplication.  Some authors define ring
without asking for the existence of $1$.

A ring that often comes up in this book is the ring of holomorphic
functions.  Let $\sO(U)$ be the set of holomorphic functions defined
on an open set $U$.  Pointwise addition and multiplication give
a ring structure on $\sO(U)$.  The set of units is the set of
functions that never vanish in $U$.  The set of units
is a multiplicative group.

\medskip

Given a commutative ring $R$, let $R[x]$ be the set of polynomials
\begin{equation*}
P(x) = c_k x^k + c_{k-1} x^{k-1} + \cdots + c_1 x + c_0 ,
\end{equation*}
where $c_0,\ldots,c_k \in R$.  The integer $k$ is the
\emph{\myindex{degree}} of the polynomial and $c_k$ is the
\emph{\myindex{leading coefficient}} of $P(x)$.  If the
leading coefficient is $1$, then $P$ is \emph{\myindex{monic}}.
If $R$ is a commutative ring, then so is $R[x]$.
Similarly, we define the commutative ring $R[x_1,\ldots,x_n]$ of
polynomials in $n$ indeterminates.

The most basic result about polynomials,
\thmref{thm:fundamentalthmalg}
the fundamental theorem of algebra, which states that every
nonconstant polynomial over $R=\C$ has a root, is really a theorem in
one complex variable.

\begin{defn}
Let $R$ be a commutative ring.
A subset $I \subset R$ is
an \emph{\myindex{ideal}}
if $f \in R$ and $g,h \in I$ implies
that $fg \in I$ and $g+h \in I$.
In short, $I \subset R$ is an additive subgroup such that $RI = I$.
Given a set of elements
$S \subset R$,
the \emph{\myindex{ideal generated by $S$}}
is the intersection $I$ of all ideals containing $S$.
If $S = \{ f_1,\ldots,f_k \}$ is a finite set, we say $I$ is
\emph{finitely generated}\index{finitely generated ideal},
and we write
\glsadd{not:ideal}%
$I = (f_1,\ldots,f_k)$.
A \emph{\myindex{principal ideal}} is an ideal generated by a single
element.
An integral domain where every ideal is a principal ideal is called
a \emph{\myindex{principal ideal domain}}\index{PID} or a PID\@.
A commutative ring $R$ is \emph{\myindex{Noetherian}}
if every ideal in $R$ is finitely generated.
\end{defn}

It is not difficult to prove that ``an ideal generated by $S$'' really is
an ideal, that is, the intersection of ideals is an ideal.
If an ideal $I$ is generated by $f_1,\ldots,f_k$, then every
$g \in I$ can be written as
\begin{equation*}
g = c_1 f_1 + \cdots + c_k f_k,
\end{equation*}
for some $c_1,\ldots,c_k \in R$.  Clearly the set of such elements is
the smallest ideal containing $f_1,\ldots,f_k$.

\begin{thm}[Hilbert basis theorem] \label{thm:hilbertbasis}
\index{Hilbert basis theorem}%
If $R$ is a Noetherian commutative ring, then $R[x]$ is Noetherian.
\end{thm}

As the proof is rather short, we include it here.

\begin{proof}
Suppose $R$ is Noetherian, and
$I \subset R[x]$ is an ideal.
Starting with the polynomial $f_1$ of minimal degree in $I$,
construct a (possibly finite) sequence of polynomials $f_1,f_2,\ldots$
such that $f_k$ is the polynomial of minimal degree
from the set $I \setminus (f_1,\ldots,f_{k-1})$.
The sequence of degrees $\deg(f_1),\deg(f_2),\ldots$ is by construction
nondecreasing.
Let $c_k$ be the leading coefficient of $f_k$.

As $R$ is Noetherian, there exists a finite $k$ such
that $(c_1,c_2,\ldots,c_m) \subset (c_1,c_2,\ldots,c_k)$ for all $m$.
Suppose for contradiction there exists a $f_{k+1}$, that is, the sequence
of polynomials
did not end at $k$.  In particular,
$(c_1,\ldots,c_{k+1}) \subset (c_1,\ldots,c_k)$ or
\begin{equation*}
c_{k+1} = a_1 c_1 + \cdots a_k c_k .
\end{equation*}
As degree of $f_{k+1}$ is at least the degree of $f_1$ through $f_k$,
we can define the polynomial
\begin{equation*}
g =
a_1 x^{\deg(f_{k+1})-\deg(f_1)} f_1
+
a_2 x^{\deg(f_{k+1})-\deg(f_2)} f_2
+
\cdots
+
a_k x^{\deg(f_{k+1})-\deg(f_k)} f_k .
\end{equation*}
The polynomial $g$ has the same degree as $f_{k+1}$,
and in fact it also has the same leading term,
$c_{k+1}$.  On the other hand, $g \in (f_1,\ldots,f_{k})$ while
$f_{k+1} \notin (f_1,\ldots,f_k)$ by construction.  The polynomial
$g-f_{k+1}$ is also not in
$(f_1,\ldots,f_k)$, but as the leading terms canceled,
$\deg(g-f_{k+1}) < \deg(f_{k+1})$, but that is a contradiction, so $f_{k+1}$
does not exist and
$I = (f_1,\ldots,f_k)$.
\end{proof}

\begin{defn}
An element $f \in R$ is \emph{\myindex{irreducible}}
if $f$ is not a unit and
whenever $f = gh$ for two elements $g,h \in R$, then either $g$ or
$h$ is a unit.
An integral domain $R$ is a
\emph{\myindex{unique factorization domain}}\index{UFD}
(UFD) if up to
multiplication by a unit, every nonzero nonunit has a unique factorization into
irreducible elements of $R$.
\end{defn}

One version of a result called the Gauss lemma says that just like
the property of being Noetherian, the property of being a UFD is retained
when we take polynomials.

\begin{thm}[Gauss lemma]\label{thm:gausslemma}
\index{Gauss lemma}%
If $R$ is a commutative ring that is a UFD\@, then $R[x]$ is a UFD\@.
\end{thm}

The proof is not difficult, but it is perhaps beyond the scope of this book.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% some sections in this appendix, so go back to section counting
\counterwithin{thm}{section}
\chapter{Results From Real Analysis} \label{ap:analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Measure theory review} \label{apsec:measure}

The beginning of this course does not require the Lebesgue integral,
however,
knowing it may make some of the earlier results easier to understand and the
exercises easier to work.  In some of the later chapters, Lebesgue integral
does become necessary in several places.  To make the first reading of the
entire book easier for a student who has not had a course on measure theory
yet, we present the basic ideas of the Lebesgue integral and list the
results that make it so useful.
We avoid getting into the details of the definition and simply state the
useful results without proof.  A reader who is interested can consult,
for example, \cite{Rudin:principles} or \cite{Rudin:analysis}.

Given a set $X$,
we designate a collection $\sM$ of
subsets of $X$, called the \emph{\myindex{measurable sets}}.
The collection $\sM$ should be a
\emph{$\sigma$-algebra}\index{sigma-algebra@$\sigma$-algebra}, meaning that
it is closed under taking complements, countable unions, and
countable intersections.
On these measurable sets
we define a measure, that is, a function $\mu \colon \sM \to \R$,
such that $\mu \geq 0$,
$\mu(\emptyset) = 0$, and $\mu$ is
\emph{$\sigma$-additive}\index{sigma-additive@$\sigma$-additive},
that is, the measure of a union of countably many disjoint sets is the
sum of the measures.  If $X$ is the euclidean space $\R^n$, there always
exists a measure called the \emph{\myindex{Lebesgue measure}} that will agree with the
standard $n$-dimensional volume on simple sets such as rectangles.
A complication is that not all subsets of $\R^n$ can then be measurable.
We say that $(X,\sM,\mu)$ is a \emph{\myindex{measure space}}.

A function $f \colon X \to \R$ is
\emph{measurable}\index{measurable function} if its sublevel
sets are measurable.
Since one generally has to work hard to produce a
\myindex{nonmeasurable function}
in the measure spaces we consider,
the reader may be forgiven for assuming every function in
this book is measurable.
A function is
\emph{simple}\index{simple function}
if its support is of finite
measure and it only has finitely many values, in which case the integral is
defined as
\begin{equation*}
\int_X f\, d \mu
\overset{\text{def}}{=}
\sum_{y \in f(X)}
y \, \mu\bigl(f^{-1}(y)\bigr) .
\end{equation*}
That is, on the set where $f(x)=y$ we define the integral as the value of
the function times the measure of the set and then we add these up.
If the function is actually a step function and the measure was the Lebesgue
measure, this is the same as would be done for the Riemann integral.
The integral of a nonnegative measurable $f$ is defined by
\begin{equation*}
\int_X f\, d \mu
\overset{\text{def}}{=}
\sup_{\substack{\varphi \leq f \\ \varphi \text{ is simple}}}
\int_X \varphi \, d\mu .
\end{equation*}
The integral of any real-valued measurable function is
then defined by writing $f = f_+ - f_-$ for
nonnegative functions $f_+$ and $f_-$, as long as
the integrals of at least one of these is not infinite, and writing
\begin{equation*}
\int_X f\, d \mu
\overset{\text{def}}{=}
\int_X f_+\, d \mu
-
\int_X f_-\, d \mu .
\end{equation*}
Similarly the integral of
complex-valued measurable functions is defined by writing $f = u + i \, v$,
that is,
\begin{equation*}
\int_X f\, d \mu
\overset{\text{def}}{=}
\int_X u\, d \mu
-
i
\int_X v\, d \mu .
\end{equation*}
The most common class of functions we deal with is the class of
\emph{$L^1$-integrable}\index{L1-integrable@$L^1$-integrable}
(or simply~$L^1$)
functions, which are the functions such that
\begin{equation*}
\int_X \sabs{f} \, d\mu < \infty .
\end{equation*}
For any function where the integral is defined we obtain
the most basic estimate
\begin{equation*}
\abs{\int_X f \, d\mu} \leq
\int_X \sabs{f} \, d\mu .
\end{equation*}
For the purposes of integration, we often allow nonnegative
functions to take on the value $\infty$ at some points.
In general, we allow our functions to
be undefined on a set of measure zero if we are integrating them since
changing a function on a measure zero set does not change the integral.

There are a couple of things to notice about the definition.  First,
because step functions are simple functions with respect to the Lebesgue measure,
the integration is a generalization of the Riemann integral on the real line
and on $\R^n$ in general in the sense that the two integrals agree when they
are both defined.

Second, many more functions (all measurable functions in fact)
can be limits of simple functions, and the integral is defined as a limit of
such integrals, one would, rightly, expect that limits can easily pass under
the integral and we no longer need to worry about integrability of the
limit.

Besides integration, one often forgotten feature of this setup is that it
applies to series.  For a countable set $X$ such as $\N$ we can define
the so-called \emph{\myindex{counting measure}}, where every set $S \subset
X$ is measurable and $\mu(S)$ is simply the number of elements in~$S$.
If $z_n = f(n)$ is a function defined on $X$, then we write
\begin{equation*}
\sum_{n \in X} z_n = \int_X f \, d\mu .
\end{equation*}
So the following theorems also apply to series, where being $L^1$ simply
means that the series is absolutely summable.

We say that something happens \emph{\myindex{almost everywhere}} if the set
where it does not happen is of measure zero.  Similarly we may say that this
something happens for \emph{almost every} $x \in X$.  Note that if a
sequence of measurable functions converges to a function $f$ almost
everywhere, then this function can be assumed to be measurable (it is equal
almost everywhere to a measurable function).  We have the following three
theorems, which despite appearances are actually just equivalent to each
other, but each statement is useful in different situations.

\begin{thm}[Fatou's lemma]\index{Fatou's lemma}
\pagebreak[2]
Let $(X,\sM,\mu)$ be a measure space and
$\{ f_n \}$ a sequence of nonnegative measurable functions that converges
almost everywhere to a function~$f$.  Then
\begin{equation*}
\int_X f \, d\mu \leq \liminf_{n\to \infty} \int_X f_n \, d\mu .
\end{equation*}
\end{thm}

\begin{thm}[Monotone convergence theorem]\index{Monotone convergence theorem}
\pagebreak[2]
Let $(X,\sM,\mu)$ be a measure space,
$\{ f_n \}$ a sequence of nonnegative measurable functions
where $f_n \leq f_{n+1}$ almost everywhere for all $n$, and suppose $\{f_n
\}$ converges to $f$ almost everywhere.  Then
\begin{equation*}
\int_X f \, d\mu = \lim_{n\to \infty} \int_X f_n \, d\mu .
\end{equation*}
\end{thm}

\begin{thm}[Dominated convergence theorem]\index{Dominated convergence theorem}
\pagebreak[2]
Let $(X,\sM,\mu)$ be a measure space,
$\{ f_n \}$ a sequence of measurable complex-valued functions converging
almost everywhere to $f$, and $g \colon X \to [0,\infty]$ an
$L^1$ function such that $\sabs{f(x)} \leq g(x)$ for almost every
$x$.  Then
\begin{equation*}
\int_X f \, d\mu = \lim_{n\to \infty} \int_X f_n \, d\mu .
\end{equation*}
\end{thm}

A common application of this theorem is differentiating
under the integral.

\begin{thm}[Differentiation under the integral]\index{Differentiation under the integral}
Let $U \subset \R$ be open and $(X,\sM,\mu)$ be a measure space.
Suppose $f \colon U \times X \to \C$ is such that
for each $t \in U$, $x \mapsto f(t,x)$ is~$L^1$,
for almost every $x$, $\frac{\partial f}{\partial t}$ exists on all of $U$,
and $g \colon X \to [0,\infty]$ is an $L^1$ function such that
$\babs{\frac{\partial f}{\partial t}(t,x)} \leq g(x)$ for all $t \in U$
and almost every $x \in X$.  Then for all $t \in U$,
\begin{equation*}
\frac{d}{dt}
\int_X f(t,x) \, d\mu(x)
=
\int_X
\frac{\partial f}{\partial t}(t,x) \, d\mu(x) .
\end{equation*}
\end{thm}

A measure space is \emph{$\sigma$-finite}\index{sigma-finite@$\sigma$-finite}
if it is a countable union of sets of
finite measure.  For example, the euclidean space with Lebesgue measure
is $\sigma$-finite because it is a union of balls, which are of finite
measure.  We often want to write an integral over a product space as an
iterated integral, such as writing an integral over a subset of $\R^n$ using
$n$ one-dimensional integrals.  If
$(X,\sM,\mu)$ and $(Y,\sN,\nu)$ are product spaces, we can define a product
measure space by requiring that $\mu \times \nu (A \times B) = \mu(A)\nu(B)$
(we again skip details).  First, for nonnegative functions we obtain the
following simple theorem where no integrability needs to be checked, and we
are allowing things to be infinite if needed.

\begin{thm}[Tonelli]\index{Tonelli theorem}
Suppose $(X,\sM,\mu)$ and $(Y,\sN,\nu)$ are $\sigma$-finite measure spaces
and $f \colon X \times Y \to \R$ is a nonnegative measurable function.
Then:
\begin{enumerate}[(i)]
\item For almost every $x \in X$, $y \mapsto f(x,y)$ is measurable, and
for almost every $y \in Y$, $x \mapsto f(x,y)$ is measurable.
\item The functions $y \mapsto \int_X f(x,y) \, d\mu(x)$ and $x \mapsto
\int_Y f(x,y) \, d\nu(y)$ are measurable.
\item
\begin{equation*}
\begin{split}
\int_{X \times Y} f(x,y) d(\mu\times \nu)
& =
\int_Y \left( \int_X f(x,y) \, d\mu(x) \right) d\nu(y)
\\
& =
\int_X \left( \int_Y f(x,y) \, d\nu(y) \right) d\mu(x) .
\end{split}
\end{equation*}
\end{enumerate}
\end{thm}

In general there is the Fubini theorem.  A measure is
\emph{complete}\index{complete measure}, if every subset of a measure zero
set is also measurable.  A measure can be completed by simply throwing
those sets in, but it is a minor technicality that the product of two
measure spaces is not in general complete and must be completed.  This is an
issue for measurability of the functions involved, but the functions that
one usually considers in applications are easily shown
measurable in all of these measure spaces and their completions.

\begin{thm}[Fubini]\index{Fubini theorem}
Suppose $(X,\sM,\mu)$ and $(Y,\sN,\nu)$ are complete measure spaces
and $f \colon X \times Y \to \R$ is $L^1$-integrable.
Then:
\begin{enumerate}[(i)]
\item For almost every $x \in X$, $y \mapsto f(x,y)$ is $L^1$-integrable,
and for almost every $y \in Y$, $x \mapsto f(x,y)$ is $L^1$-integrable.
\item The functions $y \mapsto \int_X f(x,y) \, d\mu(x)$ and
$x \mapsto \int_Y f(x,y) \, d\nu(y)$ is $L^1$-integrable.
\item
\begin{equation*}
\begin{split}
\int_{X \times Y} f(x,y) d(\mu\times \nu)
& =
\int_Y \left( \int_X f(x,y) \, d\mu(x) \right) d\nu(y)
\\
& =
\int_X \left( \int_Y f(x,y) \, d\nu(y) \right) d\mu(x) .
\end{split}
\end{equation*}
\end{enumerate}
\end{thm}

Tonelli theorem is often applied in tandem with the Fubini theorem.  Tonelli
establishes integrability and Fubini is used to write the integral we need
as iterated integral, or swap the order of integrations.

The Tonelli and Fubini theorems are useful in simplifying the
development of the power series by using the counting measure as we
mentioned above.  They are also useful for swapping a series summation and
integration such as
\begin{equation*}
\int_0^1 \sum_{n=1}^\infty a_n(x) \, dx
=
\sum_{n=1}^\infty \int_0^1 a_n(x) \, dx .
\end{equation*}

\pagebreak[2]
Here are also a couple of useful estimates for the Lebesgue integral.
First, we have an infinite-dimensional version of Cauchy--Schwarz.

\begin{thm}[Cauchy--Schwarz]
Suppose $(X,\sM,\mu)$ is a measure space and $f \colon X \to \C$ and
$g \colon X \to \C$
are so-called $L^2$ functions on $X$, that is,
they are measurable and
$\int_X \sabs{f}^2 d\mu < \infty$ and
$\int_X \sabs{g}^2 d\mu < \infty$.  Then
\begin{equation*}
\abs{
\int_X f \bar{g} d\mu
}^2
\leq
\int_X \sabs{f}^2 d\mu
\int_X \sabs{g}^2 d\mu .
\end{equation*}
\end{thm}

Next, we have the integral version of the inequality
resulting from a convex combination of the values of a convex function.

\begin{thm}[Jensen's inequality]
Suppose $(X,\sM,\mu)$ is a probability measure space, that is, $\mu(X)=1$,
$\varphi \colon \R \to \R$ is convex, and $f$ is measurable.  Then
\begin{equation*}
\varphi \left(
\int_X f d\mu
\right)
\leq
\int_X \varphi \circ f d\mu
\end{equation*}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Classical convexity} \label{apsec:convexity}

A set $S \subset \R^n$ is \emph{convex}\index{convex}
(or as we will say in the main text
\emph{\myindex{geometrically convex}}\index{convex!geometrically})
if for every $x,y \in S$ and every $\lambda \in [0,1]$ the
point $(1-\lambda) x + \lambda y$ is in $S$.
Interior and closure of convex sets is convex.
Arbitrary intersection of convex sets is convex, and increasing unions
are convex.  Given a set $S$ the \emph{\myindex{convex hull}} of $S$ is
the intersection of all convex sets containing $S$.  The closed convex hull
is the closure of that.

A \emph{\myindex{hyperplane}} $H \subset \R^n$ is the set of solutions $x$ of the equation
$x \cdot a = b$ for some $a \in \R^n$ and $b \in \R$.  A
\emph{\myindex{closed half-space}} is the set of points defined by
$x \cdot a \geq b$.  A function of the form $x \mapsto x \cdot a + b$
is called a \emph{\myindex{real affine function}}.

\begin{thm}[Supporting hyperplane theorem]
If $S \subset \R^n$ is convex and $x_0 \in \partial S$, then there exists a
supporting hyperplane through $x_0$.  That is, there exists an $a \in \R^n$
and $b \in \R$ such that $x_0 \cdot a = b$ ($x_0$ is on the hyperplane),
and $x \cdot a \geq b$ for all $x \in S$ ($S$ is in the closed half-space
defined by that hyperplane).
\end{thm}

Note that the supporting hyperplanes need not be unique.

\begin{thm}[Minkowski]
If $S,T \subset \R^n$ are two nonempty disjoint convex sets.  Then there is
a separating hyperplane, that is, there exists an $a \in \R^n$ and a $b \in
\R$ such that $x \cdot a \geq b$ for all $x \in S$ and
$x \cdot a \leq b$ for all $x \in T$.
\end{thm}

We can also put these together:

\begin{cor}
A closed convex set $S \subset \R^n$ is the union of all closed half-spaces
containing~$S$.  More generally, for any set $S$, the closed convex hull of
$S$ is the intersection of all closed half-spaces that contain $S$.
\end{cor}

A point $x_0 \in S$ is called an \emph{\myindex{extreme point}}
if for every $x,y \in S$ and $\lambda \in [0,1]$ such that
$(1-\lambda) x + \lambda y = x_0$ we have $x_0 = x$ or $x_0 = y$.
A point $x_0 \in S$ is called an \emph{\myindex{exposed point}}
if there is an affine linear function whose restriction to
$S$ achieves a strict maximum at $x_0$, in other words if there is a
supporting hyperplane which intersects $S$ at exactly one point.

\begin{thm}[Straszewicz]
Let $S \subset \R^n$ be closed and convex.  Then the set of extreme points
of $S$ is the closure of the set of exposed points of $S$.
\end{thm}

\begin{thm}[Krein--Milman]
Let $K \subset \R^n$ be compact and convex, then it is the convex hull of
its extreme points.
\end{thm}

What is useful a couple of times for us in this book is that a compact
convex set has exposed points.  Or more generally, the convex hull of a
compact set has exposed points.

Given a convex set $S$, a function $f \colon S \to \R$ is
\emph{convex}\index{convex function} if
for every $x,y \in S$ and $\lambda \in [0,1]$, we have
\begin{equation*}
f\bigl((1-\lambda) x + \lambda y\bigr) \leq
(1-\lambda) f(x) + \lambda f(y) .
\end{equation*}
Alternatively, $f$ is convex if its \emph{\myindex{epigraph}}
is a convex set, where
\begin{equation*}
\operatorname{epigraph} f
\overset{\text{def}}{=}
\bigl\{ (x,y) \in S \times \R : f(x) \leq y \bigr\} .
\end{equation*}
So arguments about convex sets translate to convex functions.
For example,
the supporting hyperplane theorem shows a couple of rather interesting
facts.  First, convex functions have ``tangent'' hyperplanes although not
unique by considering a supporting hyperplane of the epigraph:

\begin{prop}
Suppose $S \subset \R^n$ is a convex set, $f \colon S \to \R$ a
convex function, and $x_0 \in S$.  Then there exists an affine function
$g$ such that $g(x) \leq f(x)$ for all $x \in S$ and $g(x_0) = f(x_0)$.
\end{prop}

This proposition has the following consequence:
\begin{equation*}
f(x_0) = \sup \bigl\{ g(x_0) : g \text{ is an affine function such that } g(x) \leq
f(x) \text{ for all } x \in S \bigr\} .
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Smooth bump functions and partitions of unity} \label{apsec:partofunity}

The function $f(x) = 0$ for $x \leq 0$ and $f(x) = e^{-1/x}$ for $x > 0$ is
a smooth ($C^\infty$) function that is zero for all $x \leq 0$ and positive
(and less than $1$)
for $x > 0$.  The function $g(x) = \frac{f(x)}{f(x)+f(1-x)}$ is a smooth
function that is zero for all $x \leq 0$ and $1$ for all $x \geq 1$.
By modifying such examples we obtain the following
\emph{\myindex{bump function}}:

\begin{thm}[Bump function]
Suppose $U \subset \R^n$ is open and $K \subset U$ is compact.  Then there
exists a smooth function $\varphi \colon \R^n \to [0,1]$ such that
$\varphi$ is compactly supported in $U$ and $\varphi \equiv 1$ on a
neighborhood of $K$.
\end{thm}

By support, $\supp \varphi$, we mean the closure of the set
$\{ x : \varphi(x) \not= 0 \}$, and by compactly supported in $U$
we mean that $\supp \varphi$ is a compact subset of $U$.
Another variant of a bump function is the Urysohn lemma:

\begin{thm}[Smooth Urysohn lemma]
Suppose $U \subset \R^n$ is open and $A,B \subset U$ are disjoint closed (in
subspace topology) subsets. Then there
exists a smooth function $\varphi \colon U \to [0,1]$ such that
$\varphi=0$ on $A$ and $\varphi=1$ on $B$.
\end{thm}

These functions are used usually for localizing some problem, or extending
a smooth function to all of $U$ or all of $\R^n$.  One can also ask for
such bump functions to glue together nicely.
Suppose $U \subset \R^n$ is open and $\{ U_\iota \}_{\iota \in I}$
is an \emph{\myindex{open cover}} of $U$, that is, $U = \bigcup_\iota U_\iota$.

\begin{thm}[Smooth partition of unity]\index{smooth partition of unity}
\pagebreak[2]
Suppose $U \subset \R^n$ is open and $\{ U_\iota \}_{\iota \in I}$
is an open cover of $U$, then there exists a
\emph{\myindex{partition of unity subordinate to the cover}}.
That is, there exist a family
$\{ \varphi_{\kappa} \}_{\kappa \in K}$ of
smooth compactly supported functions $\varphi_{\kappa} \colon \R^n \to
[0,1]$ such that:
\begin{enumerate}[(i)]
\item
For each $\kappa \in K$, there is some $\iota \in I$
such that $\supp \varphi_\kappa \subset U_{\iota}$.
\item
For each point $x \in U$, there is a neighborhood on which
all but finitely many $\varphi_\kappa$ vanish.
\item
For every $x \in U$, we have $\sum_{\kappa \in K} \varphi(x) = 1$.
\end{enumerate}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\myDOI#1{\href{http://dx.doi.org/#1}{#1}}



%FIXME
%\cleardoublepage
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Further Reading}
\markboth{FURTHER READING}{FURTHER READING}
\begin{bibchapter}[Further Reading] \label{ch:furtherreading}

%Here we list useful books for extra reading.

\begin{biblist}[\normalsize]

\bib{BER:book}{book}{
   author={Baouendi, M. Salah},
   author={Ebenfelt, Peter},
   author={Rothschild, Linda Preiss},
   title={Real submanifolds in complex space and their mappings},
   series={Princeton Mathematical Series},
   volume={47},
   publisher={Princeton University Press},
   place={Princeton, NJ},
   date={1999},
   pages={xii+404},
   isbn={0-691-00498-6},
   review={\MR{1668103}},
   %review={\MR{1668103 (2000b:32066)}},
}


\bib{Boggess}{book}{
   author={Boggess, Albert},
   title={CR manifolds and the tangential Cauchy-Riemann complex},
   series={Studies in Advanced Mathematics},
   publisher={CRC Press},
   place={Boca Raton, FL},
   date={1991},
   pages={xviii+364},
   isbn={0-8493-7152-X},
   review={\MR{1211412}},
   %review={\MR{1211412 (94e:32035)}},
}

\bib{Chirka}{book}{
   author={Chirka, E. M.},
   title={Complex analytic sets},
   series={Mathematics and its Applications (Soviet Series)},
   volume={46},
   %note={Translated from the Russian by R. A. M. Hoksbergen},
   publisher={Kluwer Academic Publishers Group},
   place={Dordrecht},
   date={1989},
   pages={xx+372},
   isbn={0-7923-0234-6},
   review={\MR{1111477}},
   %review={\MR{1111477 (92b:32016)}},
}

\bib{DAngelo}{book}{
   author={D'Angelo, John P.},
   title={Several complex variables and the geometry of real hypersurfaces},
   series={Studies in Advanced Mathematics},
   publisher={CRC Press},
   place={Boca Raton, FL},
   date={1993},
   pages={xiv+272},
   isbn={0-8493-8272-6},
   review={\MR{1224231}},
   %review={\MR{1224231 (94i:32022)}},
}

\bib{GunningRossi}{book}{
   author={Gunning, Robert C.},
   author={Rossi, Hugo},
   title={Analytic functions of several complex variables},
   publisher={Prentice-Hall Inc.},
   place={Englewood Cliffs, N.J.},
   date={1965},
   pages={xiv+317},
   %review={\MR{0180696 (31 \#4927)}},
   review={\MR{0180696}},
}

\bib{Hormander}{book}{
   author={H{\"o}rmander, Lars},
   title={An introduction to complex analysis in several variables},
   series={North-Holland Mathematical Library},
   volume={7},
   edition={3},
   publisher={North-Holland Publishing Co.},
   place={Amsterdam},
   date={1990},
   pages={xii+254},
   isbn={0-444-88446-7},
   review={\MR{1045639}},
   %review={\MR{1045639 (91a:32001)}},
}

\bib{Krantz}{book}{
   author={Krantz, Steven G.},
   title={Function theory of several complex variables},
   series={The Wadsworth \& Brooks/Cole Mathematics Series},
   edition={2},
   publisher={Wadsworth \& Brooks/Cole Advanced Books \& Software},
   place={Pacific Grove, CA},
   date={1992},
   pages={xvi+557},
   isbn={0-534-17088-9},
   review={\MR{1162310}},
   %review={\MR{1162310 (93c:32001)}},
}

\bib{Lebl:ca}{book}{
   author={Lebl, Ji\v{r}\'i},
   title={Guide to Cultivating Complex Analysis, Working the Complex Field},
   note={\url{https://www.jirka.org/ca/}}
}

\bib{Rudin:principles}{book}{
   author={Rudin, Walter},
   title={Principles of mathematical analysis},
   edition={3},
   note={International Series in Pure and Applied Mathematics},
   publisher={McGraw-Hill Book Co., New York-Auckland-D\"usseldorf},
   date={1976},
   pages={x+342},
   review={\MR{0385023}},
}

\bib{Rudin:ball}{book}{
   author={Rudin, Walter},
   title={Function theory in the unit ball of ${\bf C}^{n}$},
   series={Grundlehren der Mathematischen Wissenschaften [Fundamental
   Principles of Mathematical Science]},
   volume={241},
   publisher={Springer-Verlag},
   place={New York},
   date={1980},
   pages={xiii+436},
   isbn={0-387-90514-6},
   review={\MR{601594}},
   %review={\MR{601594 (82i:32002)}},
}

\bib{Rudin:analysis}{book}{
   author={Rudin, Walter},
   title={Real and complex analysis},
   edition={3},
   publisher={McGraw-Hill Book Co., New York},
   date={1987},
   pages={xiv+416},
   isbn={0-07-054234-1},
   review={\MR{924157}},
}

\bib{Whitney}{book}{
   author={Whitney, Hassler},
   title={Complex analytic varieties},
   publisher={Addison-Wesley Publishing Co., Reading, Mass.-London-Don
   Mills, Ont.},
   date={1972},
   pages={xii+399},
   review={\MR{0387634}},
   %review={\MR{0387634 (52 \#8473)}},
}

\bib{ZariskiSamuel}{book}{
   author={Zariski, Oscar},
   author={Samuel, Pierre},
   title={Commutative algebra, Volume I},
   series={The University Series in Higher Mathematics},
   note={With the cooperation of I.\ S.\ Cohen},
   publisher={D.\ Van Nostrand Company, Inc., Princeton, New Jersey},
   date={1958},
   pages={xi+329},
   review={\MR{0090581}},
}

\end{biblist}
\end{bibchapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\cleardoublepage
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{\indexname}
\microtypesetup{protrusion=false}
\printindex
\microtypesetup{protrusion=true}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% automake on glossaries doesn't work if the index is before the glossary.
% That's why the List of Notation is last, no other reason.  Problem is
% that printindex does a clearpage which screws up the delayed write18
% that glossaries sets up
%

\begingroup
\renewcommand{\pagelistname}{Page}
\setglossarystyle{long3colheader}
% correctly set up with cellspace
\renewenvironment{theglossary}%
  {\setlength\cellspacetoplimit{4pt}
   \setlength\cellspacebottomlimit{4pt}
   \setlength\LTleft{0pt}
   \setlength\LTright{0pt}
   \markboth{LIST OF NOTATION}{LIST OF NOTATION}
   \begin{longtable}{Sl @{\extracolsep{\fill}} Sl @{\extracolsep{\fill}} Sl}}%
  {\end{longtable}}%
\cleardoublepage
\microtypesetup{protrusion=false}
\printglossary[title=List of Notation]
\microtypesetup{protrusion=true}
\endgroup

\end{document}
